{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba40b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0667bac8",
   "metadata": {},
   "source": [
    "## Supervised Learning Regression Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31acd3f",
   "metadata": {},
   "source": [
    "### R-squared: This measures the variation of a regression model. R-squared either increases or remains the same when new predictors are added to the model. ( R-kvadrat: Bu, reqressiya modelinin dəyişməsini ölçür. Modelə yeni proqnozlaşdırıcılar əlavə edildikdə R-kvadrat ya artır, ya da eyni qalır.)\n",
    "### Adjusted R-squared: This measures the variation for a multiple regression model, and helps you determine goodness of fit. Unlike R-squared, adjusted R-squared only adds new predictors to its model if it improves the model’s predicting power. (Düzəliş edilmiş R-kvadrat: Bu, çoxlu reqressiya modeli üçün variasiyanı ölçür və uyğunluğun yaxşılığını müəyyən etməyə kömək edir. R-kvadratdan fərqli olaraq, düzəliş edilmiş R-kvadrat modeli yalnız modelin proqnozlaşdırma gücünü yaxşılaşdırdığı halda modelinə yeni proqnozlaşdırıcılar əlavə edir.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76550179",
   "metadata": {},
   "source": [
    "#### https://images.app.goo.gl/Qgeqz1LPsGkEAfoD9\n",
    "#### https://images.app.goo.gl/xRogYdJjCpqKPc937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0b212c",
   "metadata": {},
   "source": [
    "## SST, SSR, SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d2871",
   "metadata": {},
   "source": [
    "**SST, SSR, və SSE** statistika və regresiya analizində istifadə olunan əsas ölçülərdir. Bu ölçülər modelin nə dərəcədə yaxşı uyğunlaşdığını və modeldəki variasiyaların nə qədərinin izah edildiyini qiymətləndirmək üçün istifadə olunur.\n",
    "\n",
    "**1. SST (Total Sum of Squares):** Ümumi kvadratlar cəmi (Total Sum of Squares) \n",
    "- Bu, `y` dəyişəninin ümumi dəyişkənliyini ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\bar{y})$ isə müşahidə olunan dəyərlərin orta qiymətidir.\n",
    "\n",
    "**2. SSR (Regression Sum of Squares):** Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- Bu, modelin nə dərəcədə uyğun gəldiyini ölçür, yəni modelin izah etdiyi variasiyadır.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $( \\hat{y}_i )$ model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**3. SSE (Error Sum of Squares):** Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- Bu, modelin izah edə bilmədiyi, qalıq variasiyanı ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\hat{y}_i )$ isə model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**Əlaqə**:\n",
    "Bu üç ölçü arasında belə bir əlaqə var:\n",
    "$$\n",
    "\\text{SST} = \\text{SSR} + \\text{SSE}\n",
    "$$\n",
    "Bu formula, ümumi variasiyanın izah edilən variasiyaya (SSR) və izah edilməmiş qalıq variasiyaya (SSE) bölünməsini göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345e34bc",
   "metadata": {},
   "source": [
    "## R2 and Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619e63b",
   "metadata": {},
   "source": [
    "**R-squared (R²)** və **Adjusted R-squared** regresiya modelinin uyğunluğunu qiymətləndirmək üçün istifadə olunan iki əsas ölçüdür. \n",
    "\n",
    "### 1. **R-squared (R²)**\n",
    "R², modelin nə qədər yaxşı izah etdiyini, yəni müstəqil dəyişənlərin asılı dəyişən üzərindəki variasiyanı nə qədər izah etdiyini göstərir. \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **SSR** - Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- **SSE** - Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- **SST** - Ümumi kvadratlar cəmi (Total Sum of Squares)\n",
    "\n",
    "R² dəyəri həmişə 0 ilə 1 arasında olur. 1-ə yaxın olduqda modelin yüksək uyğunluğunu, 0-a yaxın olduqda isə zəif uyğunluğu göstərir.\n",
    "\n",
    "### 2. **Adjusted R-squared**\n",
    "**Adjusted R²** (Düzəldilmiş R²) modelə əlavə edilən dəyişənlərin sayını nəzərə alır və modelin sadəcə dəyişənlərin sayına görə şişirdilməməsini təmin edir. Əgər modelə faydalı olmayan bir dəyişən əlavə edilərsə, Adjusted R² dəyəri azalacaq. Bu xüsusiyyət Adjusted R²-i daha etibarlı edir.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **R²** - R-squared dəyəri\n",
    "- **n** - Nümunə ölçüsü (müşahidələrin sayı)\n",
    "- **p** - Müstəqil dəyişənlərin sayı\n",
    "\n",
    "**Adjusted R²** dəyəri hər zaman **R²**-dən kiçik və ya ona bərabərdir. Bu, modelin sadəcə əlavə dəyişənlərin sayı ilə deyil, onların əhəmiyyəti ilə də əlaqəli olduğunu göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21ed24b",
   "metadata": {},
   "source": [
    "### R2 = SSregression / SST = 1 - SSresiduals / SST\n",
    "### R2 adjusted = 1 - (1 - R2) * (n - 1) / ( n - p - 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5052549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7111220768354529\n",
      "Adjusted R^2: 0.7110611578301462\n",
      "MAE: 2.15\n",
      "MSE: 7.17\n",
      "RMSE: 2.68\n",
      "Pearson Korelyasiya Əmsalı: 0.8434059453729525\n",
      "Pearson Korelyasiya Əmsalı (scipy ilə): 0.8434059453729523\n",
      "Intercept (kəsik): 7.603902211896262\n",
      "Slope (meyl əmsalı): 0.9982101225691038\n",
      "Xətti tənlik: y = 0.9982101225691038 * x + 7.603902211896262\n",
      "Yeni feature x_1 = 23\n",
      "Predict: y = 30.56273503098565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week4\\day1\\Weather.csv\")\n",
    "dataset = data.copy()\n",
    "\n",
    "# Data\n",
    "x = dataset['MinTemp'].values.reshape(-1,1)\n",
    "y = dataset['MaxTemp'].values.reshape(-1,1)\n",
    "\n",
    "# Train-test bölünməsi\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "# Model təlimi\n",
    "regressor = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Metriklərin hesablamaq\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "print('R^2:', R2)\n",
    "\n",
    "n = x_test.shape[0]\n",
    "p = x_test.shape[1]\n",
    "adjusted_R2 = 1 - (1-R2)*(n-1)/(n-p-1)\n",
    "print('Adjusted R^2:', adjusted_R2)\n",
    "\n",
    "MAE = round(mean_absolute_error(y_test, y_pred), 2)\n",
    "print(\"MAE:\", MAE)\n",
    "\n",
    "MSE = round(mean_squared_error(y_test, y_pred), 2)\n",
    "print(\"MSE:\", MSE)\n",
    "\n",
    "RMSE = round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Pearson korelyasiya əmsalını hesablamaq\n",
    "correlation_matrix = np.corrcoef(y_test.flatten(), y_pred.flatten())\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print('Pearson Korelyasiya Əmsalı:', pearson_corr)\n",
    "\n",
    "# Alternativ olaraq, scipy istifadə etməklə\n",
    "pearson_corr, _ = pearsonr(y_test.flatten(), y_pred.flatten())\n",
    "print('Pearson Korelyasiya Əmsalı (scipy ilə):', pearson_corr)\n",
    "\n",
    "# Intercept və Slope tapmaq\n",
    "intercept = regressor.intercept_[0]\n",
    "slope = regressor.coef_[0][0]\n",
    "print(\"Intercept (kəsik):\", intercept)\n",
    "print(\"Slope (meyl əmsalı):\", slope)\n",
    "\n",
    "# Xətti tənliyi qurmaq\n",
    "print(f\"Xətti tənlik: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Yeni feature\n",
    "x_1 = 23\n",
    "print(f'Yeni feature x_1 = {x_1}')\n",
    "\n",
    "# Predict\n",
    "print(f\"Predict: y = {slope * x_1 + intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b065a",
   "metadata": {},
   "source": [
    "## Supervised Learning Classification Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2b083",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "#### Accuracy, modelin düzgün təsnif etdiyi nümunələrin ümumi nümunələrə olan nisbətini göstərir.\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394276f",
   "metadata": {},
   "source": [
    "#### AUC (Area Under the Curve):\n",
    "#### AUC, ROC (Receiver Operating Characteristic) əyrisinin altında qalan sahəni ölçür. ROC əyrisi modelin True Positive Rate (TPR) və False Positive Rate (FPR) göstəricilərini müxtəlif qərar hədlərinə (threshold) görə təyin edir. AUC dəyəri 0 ilə 1 arasında dəyişir və modelin təsnifat qabiliyyətini qiymətləndirir. Yüksək AUC dəyəri modelin müsbət və mənfi sinifləri yaxşı ayırdığını göstərir.\n",
    "#### TPR = TP / (TP+FN)\n",
    "#### FPR = FP / (FP+TN)\n",
    "\n",
    "#### AUC dəyərinin formulu birbaşa verilmir, amma AUC əsasən ROC əyrisinin altında qalan sahədir və inteqrasiya vasitəsilə hesablanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1310546",
   "metadata": {},
   "source": [
    "#### https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)\n",
    "#### PRECISON = TP / TP + FP\n",
    "#### RECALL = TP / TP + FN\n",
    "### F1 = 2 * ( (precision *  recall) / (precision+  recall) )\n",
    "### F0.5 = 1.25 * ( (precision * recall) / (0.25 * precision + recall) )\n",
    "### F2 = 5 * ( (precision * recall) / (4 * precision + recall) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e35012",
   "metadata": {},
   "source": [
    "### F1 score, modelin həm dəqiqlik, həm də recall göstəricilərində yaxşı performans göstərib-göstərmədiyini dəyərləndirir və balanslı bir metrikdir. Bir modelin F1 score-u yüksəkdirsə, demək olar ki, model həm yanlış müsbət, həm də yanlış mənfi nəticələri minimallaşdırmağa nail olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf5b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                     Predicted\n",
    "#                     *******************************************     \n",
    "#                     *  Positive        *     Negative         *\n",
    "#         ***************************************************************************************\n",
    "#         * Positive  *   True Positive  *    False Negative    *    Sensitivity                *\n",
    "#         *           *     (TP)         *  (FN)Type II error   *  TP / (TP + FN)               *  \n",
    "# Actual  ***************************************************************************************   \n",
    "#         * Negative  *  False Positive  *   True Negative      *    Specificity                *  \n",
    "#         *           * (FP)Type I error *      (TN)            *  TN / (TN + FP)               * \n",
    "#         *************************************************************************************** \n",
    "#         *           *  Precision       *  Negative Predictive *    Accuracy                   *   \n",
    "#         *           * TP / (TP + FP)   *    TN / (TN + FN)    * TP + TN / (TP + TN + FP + FN) *\n",
    "#         *************************************************************************************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c98215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487ad4d7",
   "metadata": {},
   "source": [
    "## True Positive Rate and  False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518be03",
   "metadata": {},
   "source": [
    "#### True Positive Rate (TPR) və False Positive Rate (FPR) təsnifat modellərinin performansını qiymətləndirmək üçün istifadə edilən iki mühüm göstəricidir. Onlar modelin müsbət və mənfi təsnifatlarla necə başa çıxdığını izah edirlər.\n",
    "\n",
    "#### 1. True Positive Rate (TPR)\n",
    "#### True Positive Rate (TPR), digər adı ilə Recall və ya Sensitivity (Sensitivlik), həqiqətən müsbət olan nümunələrin neçə faizinin model tərəfindən düzgün olaraq müsbət kimi təsnif edildiyini göstərir.\n",
    "\n",
    "#### TPR formulu:\n",
    "\n",
    "#### TPR = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n",
    "\n",
    " \n",
    "#### True Positives (TP): Modelin müsbət olaraq təsnif etdiyi və həqiqətən müsbət olan nümunələr.\n",
    "#### False Negatives (FN): Modelin mənfi olaraq təsnif etdiyi, lakin əslində müsbət olan nümunələr.\n",
    "#### TPR yüksəkdirsə, bu o deməkdir ki, model həqiqətən müsbət olanları yaxşı tanıyır və az sayda yanlış mənfi nəticə verir.\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "#### False Positive Rate (FPR), həqiqətən mənfi olan nümunələrin neçə faizinin model tərəfindən səhvən müsbət kimi təsnif edildiyini göstərir. FPR, modelin yanlış müsbət nəticələr vermə tendensiyasını göstərir.\n",
    "\n",
    "#### FPR formulu:\n",
    "#### FPR = 1 - Specificity = 1 - TN / (TN + FP)\n",
    "#### FPR = False Positives (FP) /  (False Positives (FP) + True Negatives (TN))\n",
    " \n",
    "#### False Positives (FP): Modelin müsbət olaraq təsnif etdiyi, lakin əslində mənfi olan nümunələr.\n",
    "#### True Negatives (TN): Modelin mənfi olaraq təsnif etdiyi və həqiqətən mənfi olan nümunələr.\n",
    "#### FPR yüksəkdirsə, bu, modelin çox sayda yanlış müsbət nəticə verdiyini göstərir.\n",
    "\n",
    "#### Nəticə:\n",
    "#### TPR yüksəkdirsə, bu, modelin həqiqətən müsbət olanları yaxşı tanıdığını göstərir.\n",
    "#### FPR aşağıdırsa, bu, modelin yanlış müsbət nəticələri azaldığını göstərir.\n",
    "#### Bu iki göstərici adətən ROC (Receiver Operating Characteristic) əyrisi ilə birlikdə istifadə edilir və AUC (Area Under the Curve) dəyəri ilə modelin performansı qiymətləndirilir. TPR və FPR dəyərlərinin tarazlaşdırılması modelin ümumi effektivliyini təmin edir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abfc11",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f40ab1",
   "metadata": {},
   "source": [
    "#### https://www.saedsayad.com/images/MLR_r2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7a43a",
   "metadata": {},
   "source": [
    " Ridge :  (yi − ŷi)^2 + λ ∗ Slope^2  \n",
    " Lasso: (yi − ŷi)^2 + λ ∗ |Slope|\n",
    "\n",
    "Bu ifadə, ümumiyyətlə, cərimə terminləri (penalization terms) ilə birlikdə istifadə olunan optimallaşdırma məqsəd funksiyalarından biridir. Buradakı simvollar və ifadələrin mənası belədir:\n",
    "\n",
    "- `(yi − ŷi)^2`: Bu, müşahidə olunan dəyərlər (yi) ilə proqnozlaşdırılan dəyərlər (ŷi) arasındakı fərqlərin kvadratıdır. Bu fərq, adətən xəta və ya qalıq (residual) adlanır. Kvadrat halında ifadə edilməsinin səbəbi fərqlərin müsbət və mənfi olmasının fərqini aradan qaldırmaq və daha böyük fərqləri cəza olaraq daha çox nəzərə almaqdır.\n",
    "  \n",
    "- `λ ∗ Slope`: Bu, cərimə terminidir. Burada `λ` (lambda) cərimə əmsalıdır. Bu, modelin sadəliyinə nə qədər əhəmiyyət verildiyini göstərir. `Slope` isə modelin meyli və ya meyl əmsalının (məsələn, xətti reqressiyada meyl əmsalı beta) qiymətidir. Cərimə termini modelin çox mürəkkəb olmasının qarşısını almaq üçün əlavə olunur.\n",
    "\n",
    "Bu cür cərimə terminləri Ridge və ya Lasso kimi reqressiya modellərində istifadə olunur. Məqsəd, sadəcə modelin dəqiqliyini artırmaq deyil, həm də modelin sadəliyini və ümumiləşdirmə qabiliyyətini (overfitting-i aradan qaldırmaq) təmin etməkdir. Lambda (`λ`) çox böyük olarsa, model həddindən artıq sadə ola bilər; əksinə, çox kiçik olarsa, model mürəkkəb və ya overfit ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d74db131",
   "metadata": {},
   "source": [
    "## Ridge an Lasso haqqinda bilmek isteyirem. ne is gorurler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16efde",
   "metadata": {},
   "source": [
    "Ridge və Lasso reqressiyası, xətti reqressiya modellərində dəyişənlərin seçilməsi və modelin sadələşdirilməsi məqsədilə istifadə olunan iki məşhur cərimələndirmə (penalization) metodudur. Hər ikisi overfitting-in qarşısını almaq üçün reqressiya əmsallarına cərimə tətbiq edir, lakin bunu fərqli yollarla edirlər.\n",
    "\n",
    "### 1. **Ridge Regression**\n",
    "   - **Nədir?**\n",
    "     Ridge reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların kvadratlarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ (βj^2)` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Ridge reqressiyası, əmsalların böyük olmasının qarşısını almaq üçün onları kiçikləşdirir. Bu metod xüsusilə yüksək kollinear (bir-birinə bağlı) dəyişənlər olduğu hallarda effektivdir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L2 norması (kvadrat cərimə).\n",
    "     - **Əsas üstünlüyü:** Overfitting-in qarşısını alır və modelin stabilliyini artırır.\n",
    "     - **Dəyişənlər:** Bütün dəyişənlər saxlanılır, lakin əmsalları kiçikləşdirilir.\n",
    "\n",
    "### 2. **Lasso Regression**\n",
    "   - **Nədir?**\n",
    "     Lasso reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların modullarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ |βj|` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Lasso reqressiyası, bəzi əmsalların tamamilə sıfıra bərabər olmasına səbəb ola bilər. Bu, modeldə əhəmiyyətsiz olan dəyişənlərin tamamilə çıxarılmasına imkan verir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L1 norması (modul cərimə).\n",
    "     - **Əsas üstünlüyü:** Dəyişənlərin seçilməsi və modelin sadələşdirilməsi.\n",
    "     - **Dəyişənlər:** Bəzi dəyişənlər sıfırlanır və modeldən çıxarılır.\n",
    "\n",
    "### **Ridge və Lasso Arasındakı Fərqlər:**\n",
    "   - **Cərimə növü:** Ridge L2 cəriməsi istifadə edərək əmsalları kiçikləşdirir, Lasso isə L1 cəriməsi ilə bəzi əmsalları sıfıra çevirir.\n",
    "   - **Dəyişənlərin seçilməsi:** Ridge bütün dəyişənləri saxlayır, lakin Lasso bəzi dəyişənləri tamamilə çıxarır.\n",
    "   - **Yüksək korrelyasiya:** Ridge yüksək korrelyasiyalı dəyişənlər üçün daha uyğundur, çünki o, bu dəyişənləri bir arada saxlayır, Lasso isə yüksək korrelyasiyalı dəyişənlərin bir qismini sıfırlaya bilər.\n",
    "\n",
    "### **Ridge və Lasso'nun İstifadəsi:**\n",
    "   - Ridge reqressiyası daha çox dəyişən olduğu və onların əhəmiyyətinin hamısının qorunması lazım olduğu hallarda istifadə olunur.\n",
    "   - Lasso reqressiyası isə dəyişənlərin seçilməsi və daha sadə modellər qurmaq lazım olduğu hallarda istifadə olunur.\n",
    "\n",
    "Bu metodlar, xüsusilə böyük verilənlər dəstləri və yüksək miqdarda dəyişən olan problemlər üçün faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7e9fc0b",
   "metadata": {},
   "source": [
    "## Cross_Validation haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312bdd9",
   "metadata": {},
   "source": [
    "Cross-validation, maşın öyrənməsində modelin performansını qiymətləndirmək üçün istifadə olunan bir texnikadır. Bu üsul, modelin overfitting (modelin təlim məlumatlarına həddən artıq uyğunlaşması) və underfitting (modelin həm təlim, həm də test məlumatlarını yaxşı öyrənməməsi) probleminin qarşısını almağa kömək edir. Cross-validation, modeli müxtəlif alt qruplar üzərində təlim edərək və test edərək modelin ümumiləşdirmə qabiliyyətini qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Nədir?\n",
    "Cross-validation, ümumiyyətlə, məlumat dəstəsini bir neçə hissəyə (fold) bölmək və bu hissələrdən birini test dəsti kimi, qalanlarını isə təlim dəsti kimi istifadə etmək üsuludur. Bu proses bir neçə dəfə təkrarlanır və nəticədə hər bir hissə bir dəfə test dəsti kimi istifadə olunur. Bu təkrarlamalar nəticəsində əldə edilən nəticələr ortalanır və modelin performansı bu ortalama üzərindən qiymətləndirilir.\n",
    "\n",
    "### Növləri:\n",
    "1. **K-Fold Cross-Validation:**\n",
    "   - **Ən çox istifadə olunan üsuldur.** Məlumat dəstəsi K sayda hissəyə bölünür (məsələn, K=5 və ya K=10).\n",
    "   - **Proses:** Məlumat dəstəsi K hissəyə bölünür və model K dəfə təlim edilir. Hər dəfə bir hissə test dəsti kimi istifadə olunur, qalan K-1 hissə isə təlim üçün istifadə edilir. Nəticədə K dəfə modelin performansı ölçülür və bu nəticələr ortalanır.\n",
    "   - **Üstünlükləri:** Bütün məlumatlar həm təlim, həm də test üçün istifadə olunduğundan, bu üsul daha dəqiq və ümumi nəticələr verir.\n",
    "\n",
    "2. **Stratified K-Fold Cross-Validation:**\n",
    "   - K-Fold Cross-Validation-a bənzərdir, amma burada məlumat dəstəsi siniflərin paylanmasına uyğun olaraq bölünür. Bu, xüsusilə balanssız siniflər olan məlumat dəstələrində faydalıdır.\n",
    "   - Siniflərin hər fold içində təmsil olunmasını təmin edir.\n",
    "\n",
    "3. **Leave-One-Out Cross-Validation (LOOCV):**\n",
    "   - **Hər dəfə bir nümunə çıxarılaraq test üçün istifadə edilir.** Qalan bütün nümunələr təlim üçün istifadə olunur.\n",
    "   - **Proses:** Məlumat dəstəsinin hər bir nümunəsi bir dəfə test dəsti kimi istifadə olunur, qalan nümunələr isə təlim üçün istifadə edilir. Bu üsul çox vaxt aparsa da, daha dəqiq nəticələr verir.\n",
    "   - **Üstünlükləri:** Xüsusilə kiçik məlumat dəstələrində modelin performansını dəqiq ölçmək üçün yaxşıdır.\n",
    "\n",
    "4. **Time Series Cross-Validation:**\n",
    "   - Zaman sırasına görə məlumatları böldüyü üçün, zaman sıralı məlumatlarla işləyən modellər üçün istifadə olunur.\n",
    "   - Məlumatlar zamanla əlaqəli olduqda, gələcək məlumatların proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "5. **Nested Cross-Validation:**\n",
    "   - Bu üsul hiperparametr tənzimləməsi ilə model performansını qiymətləndirmək üçün istifadə olunur.\n",
    "   - Birinci dərəcəli cross-validation modeli tənzimləmək üçün, ikinci dərəcəli cross-validation isə performansı qiymətləndirmək üçün istifadə edilir.\n",
    "\n",
    "### Cross-Validation Üstünlükləri:\n",
    "- **Ümumiləşdirmə:** Modelin məlumatlara həddən artıq uyğunlaşmasını (overfitting) qarşısını almağa kömək edir.\n",
    "- **Dəqiqlik:** Məlumat dəstəsinin bütün hissələri həm təlim, həm də test üçün istifadə olunduğundan, modelin performansını daha dəqiq ölçür.\n",
    "- **Stabil qiymətləndirmə:** Bir çox təkrarlama ilə ortalama nəticələr əldə olunur, bu da modelin performansını daha stabil şəkildə qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Zamanı Dikkat Edilmesi Gerekenler:\n",
    "- **Məlumatın Bərabər Bölünməsi:** Məlumat dəstəsinin fold-lara bərabər şəkildə bölündüyündən əmin olunmalıdır.\n",
    "- **Zaman Əlaqəsi:** Zamanla əlaqəli məlumatlar varsa, zaman sırası gözlənilməlidir.\n",
    "- **Hesablama Məsrəfləri:** Daha çox fold daha dəqiq nəticələr verir, lakin hesablama vaxtını artırır.\n",
    "\n",
    "Cross-validation, maşın öyrənməsində model performansını qiymətləndirmək və tənzimləmək üçün əsas vasitələrdən biridir. Bu texnika, modelin ümumiləşdirmə qabiliyyətini artırır və həm akademik, həm də sənaye tətbiqlərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655fb9a",
   "metadata": {},
   "source": [
    "Cross-validation prosesini cədvəl şəklində göstərmək üçün hər bir fold-un təlim və test dəstlərinə necə bölündüyünü və nəticələrin ortalanmasını cədvəl formasında təsvir edə bilərik.\n",
    "\n",
    "### 5-Fold Cross-Validation Cədvəli\n",
    "\n",
    "| Fold  | Təlim Dəsti                   | Test Dəsti      | F1-score |\n",
    "|-------|--------------------------------|-----------------|----------|\n",
    "| 1     | Fold 2, 3, 4, 5                | Fold 1          | F1-score_1|\n",
    "| 2     | Fold 1, 3, 4, 5                | Fold 2          | F1-score_2|\n",
    "| 3     | Fold 1, 2, 4, 5                | Fold 3          | F1-score_3|\n",
    "| 4     | Fold 1, 2, 3, 5                | Fold 4          | F1-score_4|\n",
    "| 5     | Fold 1, 2, 3, 4                | Fold 5          | F1-score_5|\n",
    "\n",
    "### Ümumi F1-score:\n",
    "$$ \n",
    "\\text{Ortalama F1-score} = \\frac{\\text{F1-score}_1 + \\text{F1-score}_2 + \\text{F1-score}_3 + \\text{F1-score}_4 + \\text{F1-score}_5}{5}\n",
    " $$\n",
    "\n",
    "Bu ifadə hər bir fold üçün hesablanmış F1-score dəyərlərinin ortalamasını verir.\n",
    "\n",
    "Bu cədvəl hər fold üçün təlim və test dəstlərinin necə bölündüyünü və nəticələrin (məsələn, F1-score) necə toplandığını göstərir. Nəticədə, bütün fold-ların performans göstəriciləri ortalanaraq modelin ümumi performansı müəyyən edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218735b9",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "accuracies = cross_validate(estimator=NB_model, X=X_train_scaled, \n",
    "                            y=y_train, cv=4, scoring = ['f1', 'roc_auc', 'recall', 'precision', 'accuracy'])\n",
    "accuracies\n",
    "\n",
    "output\n",
    "{'fit_time': array([0.004, 0.004, 0.004, 0.004]),\n",
    " 'score_time': array([0.033, 0.031, 0.03 , 0.048]),\n",
    " 'test_f1': array([0.8136, 0.8333, 0.8308, 0.8276]),\n",
    " 'test_roc_auc': array([0.866 , 0.8983, 0.8308, 0.9161]),\n",
    " 'test_recall': array([0.7742, 0.8065, 0.9   , 0.7742]),\n",
    " 'test_precision': array([0.8571, 0.8621, 0.7714, 0.8889]),\n",
    " 'test_accuracy': array([0.807 , 0.8246, 0.8036, 0.8214])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982d185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db5fba0",
   "metadata": {},
   "source": [
    "## bes λ nece tapilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c67a8",
   "metadata": {},
   "source": [
    "Lambda (`λ`) dəyəri, modelin cərimə əmsalıdır və onun optimal dəyəri modelin performansını yaxşılaşdırmaq üçün seçilir. Lambda-nı təyin etmək üçün bir neçə üsul mövcuddur:\n",
    "\n",
    "### 1. **Cross-Validation (Çapraz Doğrulama)**\n",
    "   - **Addım 1:** Verilənlər toplusunu bir neçə hissəyə bölmək (məsələn, 5 və ya 10 fold cross-validation).\n",
    "   - **Addım 2:** Müxtəlif lambda dəyərləri üçün modelin performansını hər bir hissədə qiymətləndirmək.\n",
    "   - **Addım 3:** Hər bir fold üçün modelin performansını ölçmək və ortalama performansı hesablayaraq ən yaxşı nəticə verən lambda dəyərini seçmək.\n",
    "\n",
    "### 2. **Grid Search**\n",
    "   - Müəyyən bir aralıqda lambda dəyərləri təyin olunur (məsələn, 0.001-dən 10-a qədər).\n",
    "   - Hər bir lambda dəyəri üçün model qurulur və performans ölçülür.\n",
    "   - Ən yaxşı performansı verən lambda seçilir.\n",
    "\n",
    "### 3. **Regularization Paths**\n",
    "   - Bu üsulda, müxtəlif lambda dəyərləri üçün reqressiya əmsallarının dəyişməsi analiz edilir.\n",
    "   - Bu qrafik üzərində, reqressiya əmsallarının stabilləşdiyi nöqtədəki lambda dəyəri seçilir.\n",
    "\n",
    "### 4. **Information Criteria (AIC, BIC)**\n",
    "   - Akaike Information Criterion (AIC) və ya Bayesian Information Criterion (BIC) kimi informasiya meyarları ilə lambda seçilə bilər. Bu meyarlar modeli uyğunlaşdırma keyfiyyətini və onun mürəkkəbliyini qiymətləndirir.\n",
    "\n",
    "Bu üsulların hər biri fərqli vəziyyətlər üçün uyğun ola bilər. Ən çox istifadə olunan metod isə Cross-Validation üsuludur, çünki bu metod həm modelin performansını yaxşı ölçməyə, həm də overfitting-in qarşısını almağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8eba0b",
   "metadata": {},
   "source": [
    "## penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0 bu paramertler haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4199",
   "metadata": {},
   "source": [
    "Bu parametrlər, `sklearn.linear_model.LogisticRegression` sinfində ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə olunur. Aşağıda hər bir parametrin detallı izahı verilmişdir:\n",
    "\n",
    "### 1. **`penalty='elasticnet'`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsini tətbiq edir. ElasticNet cərimələndirməsi həm L1 (Lasso) həm də L2 (Ridge) normlarını birləşdirir. Yəni, ElasticNet modeli həm bəzi əmsalları sıfıra yaxınlaşdırır (Lasso kimi), həm də digər əmsalları kiçikləşdirir (Ridge kimi).\n",
    "   - **Necə işləyir?**: Bu parametr təkbaşına işləməz, onu aktivləşdirmək üçün `l1_ratio` parametri və uyğun `solver` lazımdır.\n",
    "\n",
    "### 2. **`solver='saga'`**\n",
    "   - **İşlevi:** `solver` parametri Logistic Regression modelinin təlimi üçün istifadə edilən optimizasiya alqoritmini təyin edir. `saga`, həm L1, həm də ElasticNet cərimələndirmələrini dəstəkləyən bir solverdir. Bu alqoritm böyük verilənlər dəstlərində və sparse (sıx olmayan) xüsusiyyətlərə malik olan modellərdə çox effektivdir.\n",
    "   - **Nə üçün vacibdir?**: ElasticNet cərimələndirməsi üçün `saga` solverini seçmək məcburidir, çünki yalnız `saga` (və `liblinear`) L1 normunu dəstəkləyir, ElasticNet isə həm L1, həm də L2 normlarını birləşdirdiyindən, onun üçün də uyğundur.\n",
    "\n",
    "### 3. **`l1_ratio=0.5`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsi daxilində L1 və L2 cərimələri arasındakı balansı təyin edir. \n",
    "     - **`l1_ratio=0`** olduqda, ElasticNet yalnız L2 (yəni, Ridge) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=1`** olduqda, ElasticNet yalnız L1 (yəni, Lasso) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=0.5`** dəyəri həm L1, həm də L2 cərimələndirmələrinin qarışığını tətbiq edir, yəni hər ikisini bərabər şəkildə qarışdırır.\n",
    "   - **İstifadə məqsədi:** Bu parametr L1 və L2 normları arasında uyğun bir balans yaratmaq üçündür. Verilən problemə uyğun optimal balansı tapmaq üçün `l1_ratio` dəyərini dəyişdirərək təcrübə etmək mümkündür.\n",
    "\n",
    "### 4. **`C=1.0`**\n",
    "   - **İşlevi:** `C` parametri modelin cərimələndirmə əmsalını təyin edir. `C` dəyəri cərimələndirmənin tərs əmsalıdır (yəni, `C` nə qədər böyükdürsə, cərimə bir o qədər kiçikdir və əksinə).\n",
    "   - **Nə üçün vacibdir?**: \n",
    "     - **Kiçik `C` dəyəri:** Daha böyük cərimələndirməyə səbəb olur, yəni modelin sadələşdirilməsinə və əmsalların kiçikləşdirilməsinə (və ya sıfıra yaxınlaşmasına) gətirib çıxarır.\n",
    "     - **Böyük `C` dəyəri:** Daha az cərimələndirməyə səbəb olur və model daha çox uyğunlaşır (overfitting riskini artırır).\n",
    "   - **Standart dəyər:** `C=1.0` balanslı bir dəyərdir, lakin onu artırıb azaltmaqla modelin performansını dəyişdirə bilərsiniz.\n",
    "\n",
    "### **Yekun:**\n",
    "Bu parametrlər birlikdə ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə edilir. `penalty='elasticnet'` ElasticNet cərimələndirməsini tətbiq edir, `solver='saga'` bu cərimələndirməni dəstəkləyən optimizasiya alqoritmidir, `l1_ratio=0.5` L1 və L2 normları arasında balansı təyin edir, və `C=1.0` isə cərimələndirmənin dərəcəsini müəyyən edir. Bu parametrlər sizin modelinizin kompleksliyini idarə etmək və overfitting riskini azaldaraq daha dəqiq proqnozlar vermək üçün istifadə edilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab96bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe20b15",
   "metadata": {},
   "source": [
    "## solver nedir umumiyyetle ve imkan varsa formulasini yaz ve ya adi dilde izah ele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd31be",
   "metadata": {},
   "source": [
    "`Solver`, statistik modellərin və optimizasiya problemlərinin həlli üçün istifadə olunan bir alqoritmdir. Xüsusən, `sklearn.linear_model.LogisticRegression` sinifində olduğu kimi, xətti modellərdə (xətti reqressiya, logistika reqressiyası və s.) əmsalları tapmaq üçün bu alqoritmlərdən istifadə olunur. \n",
    "\n",
    "### **Solver Nədir?**\n",
    "\n",
    "- **Nə iş görür?**: Solver, optimizasiya problemini həll edərək modelin əmsallarını tapır. Bu prosesdə məqsəd, itki funksiyasını (loss function) minimallaşdırmaq və ən uyğun model parametrlərini tapmaqdır.\n",
    "- **İş prinsipi:** Solver modelin parametrlərini iterativ şəkildə dəyişdirir və itki funksiyasını tənzimləyərək ən yaxşı nəticəyə çatmağa çalışır. Hər bir iterasiyada modelin parametrləri yenilənir və daha az itki ilə nəticə əldə etməyə çalışılır.\n",
    "\n",
    "### **Populyar Solver-lər və İş Prinsipləri**\n",
    "\n",
    "1. **`liblinear`**\n",
    "   - **Xüsusiyyətləri:** Çoxlu miqdarda xüsusiyyətə malik olan (feature) kiçik və orta miqdarda verilənlər üçün yaxşıdır. L1 və L2 cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Koordinat Descent alqoritmindən istifadə edir. L1 və L2 cərimələndirməsi ilə səmərəlidir.\n",
    "\n",
    "2. **`newton-cg`**\n",
    "   - **Xüsusiyyətləri:** Daha böyük verilənlər dəstləri üçün uyğundur. L2 cərimələndirməsini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Newton-Raphson metodundan istifadə edir. Hər iterasiyada ikinci dərəcəli diferensial məlumatlardan istifadə edərək əmsalları yeniləyir.\n",
    "\n",
    "3. **`saga`**\n",
    "   - **Xüsusiyyətləri:** Böyük verilənlər dəstləri və sıx olmayan (sparse) verilənlər üçün yaxşıdır. L1, L2 və ElasticNet cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Stochastic Average Gradient Descent (SAG) metodunun inkişaf etdirilmiş formasıdır. Bu, gradient descend metodu ilə əmsalları iterativ şəkildə yeniləyir.\n",
    "\n",
    "4. **`lbfgs`**\n",
    "   - **Xüsusiyyətləri:** L2 cərimələndirməsini dəstəkləyir və böyük verilənlər üçün yaxşı nəticələr verir.\n",
    "   - **İş Prinsipi:** Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) alqoritmindən istifadə edir. İkinci dərəcəli optimizasiya üsulu olaraq parametrlərin optimallaşdırılmasına kömək edir.\n",
    "\n",
    "### **Solver-lərin İstifadəsi**\n",
    "\n",
    "Solver-lər müxtəlif optimizasiya alqoritmləri ilə modelin parametrlərini optimallaşdırmağa kömək edir. Onların əsas məqsədi itki funksiyasını minimallaşdırmaqdır.\n",
    "\n",
    "### **Formulalar**\n",
    "\n",
    "Solver-lərin istifadə etdiyi əsas formulalar ümumiyyətlə optimizasiya və gradient descend metodlarına əsaslanır. Burada ən çox istifadə olunan metodlardan biri olan gradient descent formulunu təqdim edirəm:\n",
    "\n",
    "**Gradient Descent Formula:**\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} $$\n",
    "\n",
    "- **$\\theta_{j}$**: Modelin parametridir.\n",
    "- **$\\alpha$**: Öğrenmə sürətidir (learning rate).\n",
    "- **$J(\\theta)$**: İtki funksiyasıdır (loss function).\n",
    "- **$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$**: İtki funksiyasının $\\theta_{j}$ parametrinə görə gradientidir.\n",
    "\n",
    "### **Adi Dildə İzahetmə**\n",
    "\n",
    "Solver-lər modelin \"öyrənmə\" mərhələsində əmsalları tapmağa kömək edir. Məsələn, Logistic Regression modelini qurduğunuz zaman, solver modelin düzgün parametrlərini tapmaq üçün müxtəlif üsullardan istifadə edir. Hər iterasiyada solver əmsalları dəyişdirərək itki funksiyasını (modelin səhvini) azaldır və nəticədə ən yaxşı parametrləri tapmağa çalışır. \n",
    "\n",
    "Solver-lərin seçimləri, verilənlərin ölçüsünə, cərimələndirmə növünə və modelin tələblərinə görə fərqlənə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82674380",
   "metadata": {},
   "source": [
    "## itki funksiayasi nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d078b70",
   "metadata": {},
   "source": [
    "İtki funksiyası (loss function), statistik modelləşdirmədə modelin nə qədər düzgün proqnoz verdiyini ölçmək üçün istifadə olunan bir funksiyadır. Bu funksiya modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqi kvantlaşdırır. İtki funksiyasının məqsədi modelin performansını qiymətləndirmək və modelin parametrlərini optimallaşdırmaq üçün istifadə olunur.\n",
    "\n",
    "İtki funksiyası müxtəlif modellərdə müxtəlif formullarla təyin olunur. Burada iki əsas növ itki funksiyasını təqdim edirəm: **Xətti Reqressiya** və **Logistik Reqressiya** üçün.\n",
    "\n",
    "### **1. Xətti Reqressiya üçün İtki Funksiyası (Mean Squared Error - MSE)**\n",
    "\n",
    "Xətti reqressiya modelində itki funksiyası adətən **Mean Squared Error (MSE)** ilə təyin olunur. MSE, modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqin kvadratlarının ortalamasını ölçür. \n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı (datanın ölçüsü).\n",
    "- **$y_i$**: Həqiqi dəyər (ground truth).\n",
    "- **$\\hat{y_i}$**: Modelin proqnozu (predicted value).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- MSE, modelin nə qədər yanıldığını ölçür. Kiçik MSE dəyərləri modelin daha yaxşı nəticə verdiyini göstərir.\n",
    "- Kvadratlı fərqlərin ortalaması olduğu üçün, böyük səhvləri daha çox vurğulayır.\n",
    "\n",
    "### **2. Logistik Reqressiya üçün İtki Funksiyası (Log Loss / Binary Cross-Entropy Loss)**\n",
    "\n",
    "Logistik reqressiya modelində itki funksiyası adətən **Log Loss** (və ya Binary Cross-Entropy Loss) ilə təyin olunur. Bu itki funksiyası modelin proqnozlarının doğru etiketlərlə nə qədər uyğun olduğunu ölçür.\n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı.\n",
    "- **$y_i$**: Həqiqi etiket (1 və ya 0).\n",
    "- **$\\hat{y_i}$**: Modelin ehtimal proqnozu (0 ilə 1 arasında).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- Log Loss, proqnozların həqiqi etiketlərlə nə qədər uyumlu olduğunu qiymətləndirir.\n",
    "- Əgər modelin proqnozu həqiqi etiketə yaxınsa, Log Loss dəyəri kiçik olacaqdır.\n",
    "- Log Loss, ehtimal dəyərləri ilə işləyir və aşağı ehtimallar üçün yüksək cəzalar verir.\n",
    "\n",
    "### **Əlavə Nümunələr:**\n",
    "\n",
    "- **Hinge Loss**: SVM (Support Vector Machine) modellərində istifadə olunur. Formulu belədir: \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y_i})\n",
    "  $$\n",
    "  burada **$y_i$** doğru etiketlərdir (1 və ya -1), **$\\hat{y_i}$** isə proqnoz edilən etiketlərdir.\n",
    "\n",
    "İtki funksiyası modelin öyrənmə prosesində əsas rol oynayır, çünki modelin parametrləri itki funksiyasını minimuma endirmək üçün optimallaşdırılır. Hər bir model və problem üçün itki funksiyası fərqli ola bilər və bu, modelin məqsədinə və xüsusiyyətlərinə əsaslanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcb53f6",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LogisticRegression daxilinde ridge lasso elasticnet tetbiqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987371e",
   "metadata": {},
   "source": [
    "`sklearn.linear_model.LogisticRegression` sinfi daxilində Ridge, Lasso və ElasticNet cərimələndirmələrini tətbiq etmək mümkündür. Bu cərimələndirmələr `penalty` parametri vasitəsilə seçilir. Aşağıda hər bir cərimələndirmə üsulunun necə tətbiq edildiyini göstərən nümunələr təqdim edirəm.\n",
    "\n",
    "### 1. **Ridge Regression (L2 Penalty)**\n",
    "Ridge cərimələndirməsi `penalty='l2'` parametri ilə həyata keçirilir. Bu, `LogisticRegression` sinfinin standart cərimələndirmə üsuludur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Məlumat dəstini bölmək\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli L2 cəriməsi ilə\n",
    "ridge_model = LogisticRegression(penalty='l2', C=1.0)  # C parametri lambda-nın tərsidir\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ridge Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed563de",
   "metadata": {},
   "source": [
    "### 2. **Lasso Regression (L1 Penalty)**\n",
    "Lasso cərimələndirməsi `penalty='l1'` parametri ilə həyata keçirilir. Bunun üçün `solver` parametrini də uyğun olaraq `liblinear` və ya `saga` seçmək lazımdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)  # SAGA və ya liblinear solver\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Lasso Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4951",
   "metadata": {},
   "source": [
    "### 3. **ElasticNet Regression**\n",
    "ElasticNet cərimələndirməsi üçün həm `l1_ratio`, həm də `penalty='elasticnet'` parametrləri istifadə olunur. Burada da `solver` olaraq `saga` istifadə edilməlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d461ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)\n",
    "elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = elasticnet_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ElasticNet Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb3bb",
   "metadata": {},
   "source": [
    "### Parametrlərin İzahı:\n",
    "- **`penalty`**: Cərimələndirmə növünü təyin edir (`l1`, `l2`, `elasticnet`).\n",
    "- **`C`**: Cərimə əmsalını təyin edir. Daha böyük `C` daha az cərimələndirmə deməkdir (`C` dəyəri lambda-nın tərsidir).\n",
    "- **`l1_ratio`**: ElasticNet üçün L1 və L2 arasında balans yaradır. `l1_ratio=0` yalnız Ridge, `l1_ratio=1` isə yalnız Lasso cəriməsi deməkdir.\n",
    "- **`solver`**: Problemi həll etmək üçün istifadə edilən alqoritmdir. `liblinear` və `saga` solverləri L1 və ElasticNet cərimələndirməsini dəstəkləyir.\n",
    "\n",
    "Bu nümunələrdə Logistic Regression daxilində Ridge, Lasso və ElasticNet cərimələndirmə üsullarını tətbiq edə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555e2d88",
   "metadata": {},
   "source": [
    "## Ridge, Lasso ve Elasticnet  svm, KNN, Decision tree-de de isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa227",
   "metadata": {},
   "source": [
    "Ridge, Lasso və ElasticNet kimi cərimələndirmə üsulları xətti modellərdə, xüsusilə xətti reqressiya və logistika reqressiya modellərində tətbiq olunan texnikalardır. Bu cərimələndirmə metodları, xətti modellərin daha yaxşı ümumiləşdirilməsini təmin etmək üçün nəzərdə tutulmuşdur. Ancaq digər maşın öyrənməsi alqoritmlərində (məsələn, SVM, KNN, Decision Tree) bu cərimələndirmə üsullarının eyni formada işlədilməsi mümkün deyil, çünki bu alqoritmlər fərqli optimizasiya və iş prinsiplərinə malikdir.\n",
    "\n",
    "### **Ridge, Lasso və ElasticNet Cərimələndirmələri**\n",
    "- **Ridge (L2 Penalty):** Əmsalların böyüklüyünü cərimələyir və onların kiçik olmasına səbəb olur. Overfitting riskini azaldır.\n",
    "- **Lasso (L1 Penalty):** Bəzi əmsalları tam sıfıra bərabər edir, yəni xüsusiyyət seçimi (feature selection) rolunu oynayır.\n",
    "- **ElasticNet:** Həm L1, həm də L2 cərimələndirməsini birləşdirir və xüsusiyyətlər arasındakı əlaqələri idarə edir.\n",
    "\n",
    "### **Bu Texnikalar Aşağıdakı Alqoritmlərdə Tətbiq Edilə Bilərmi?**\n",
    "\n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - **SVM Cərimələndirməsi:** SVM-də **cərimə parametri (C)** Ridge Regression-a bənzər şəkildə işləyir. Bu parametr səhv təsnifatları cərimələndirərək marjanı böyütməyə çalışır. Yəni, SVM-də L1 və L2 cəriməsi istifadə edilməsə də, itki funksiyası və cərimələndirmə prinsipləri cərimə parametri `C` ilə idarə olunur.\n",
    "   - **L1 və L2 Cərimələndirməsi:** SVM-də **LinearSVC** sinfi daxilində `penalty='l1'` və `penalty='l2'` parametrləri ilə L1 və L2 cərimələri tətbiq oluna bilər.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "   - **KNN-də Cərimələndirmə:** KNN alqoritmində cərimələndirmə tətbiq olunmur, çünki bu alqoritm xüsusiyyət əmsalları ilə deyil, məsafə metrikləri ilə işləyir. KNN sadəcə verilənlərdəki ən yaxın nöqtələrə əsaslanaraq qərar verir və əmsalların böyüklüyü kimi anlayışlar burada mövcud deyil.\n",
    "   \n",
    "3. **Decision Tree**\n",
    "   - **Decision Tree Cərimələndirməsi:** Decision Tree alqoritmlərində də Ridge və Lasso kimi cərimələndirmələr tətbiq edilmir, çünki bu alqoritm əmsallarla işləmək əvəzinə, verilənləri ağac strukturu şəklində bölür.\n",
    "   - **Overfitting-i İdarə Etmək:** Overfitting riskini azaltmaq üçün Decision Tree alqoritmində **ağacın dərinliyini məhdudlaşdırmaq** və ya **minimum nümunə sayını** təyin etmək kimi texnikalardan istifadə olunur.\n",
    "\n",
    "### **Nəticə**\n",
    "- **Ridge, Lasso və ElasticNet** xətti modellərə aid texnikalardır və onlar SVM-də müəyyən formada tətbiq oluna bilər.\n",
    "- **KNN** və **Decision Tree** kimi qeyri-xətti modellərdə isə bu texnikalar tətbiq edilmir, çünki bu alqoritmlər əmsallarla deyil, fərqli prinsiplərlə işləyirlər.\n",
    "\n",
    "Bu cərimələndirmə metodlarının işlədiyi modelləri və digər modellərdə alternativ üsulları başa düşmək maşın öyrənməsində doğru texnikaları seçmək üçün vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3321d294",
   "metadata": {},
   "source": [
    "## Bes Random Forest-de Ridge, Lasso ve Elasticnet isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e03f2f",
   "metadata": {},
   "source": [
    "**Random Forest** alqoritmində **Ridge**, **Lasso** və **ElasticNet** kimi cərimələndirmə üsulları birbaşa tətbiq olunmur. Bunun səbəbi Random Forest-in təməlində **Decision Tree** alqoritmi dayanmasıdır və Decision Tree əmsallarla işləmək əvəzinə, verilənləri qaydalara görə bölən bir alqoritmdir.\n",
    "\n",
    "### **Niyə Random Forest-də Ridge, Lasso və ElasticNet İşləmir?**\n",
    "\n",
    "1. **Decision Tree ilə İşləmə Prinsipi:**\n",
    "   - Random Forest bir çox decision tree-dən ibarət olan ansambl metodudur. Hər bir decision tree verilənlər dəstini qaydalara görə bölərək təsnifat və ya reqressiya işləri görür.\n",
    "   - Decision Tree-lər verilənləri əmsallarla təhlil etmədiyi üçün, Ridge, Lasso və ElasticNet cərimələri ilə işləmirlər. Bu cərimələr xətti modellərdə əmsalları tənzimləmək üçün istifadə olunur, lakin Decision Tree-lərdə belə bir əmsal yoxdur.\n",
    "\n",
    "2. **Overfitting-lə Mübarizə:**\n",
    "   - Random Forest alqoritmi decision tree-lərin təbii overfitting meylini azaldır. Bir çox ağacın nəticələrini birləşdirərək (və fərqli verilənlər dəstləri və xüsusiyyətlər üzərində hər bir ağacı təlim etdirərək), Random Forest daha sabit və ümumiləşdirilmiş nəticələr verir.\n",
    "   - Random Forest modelində overfitting-in qarşısını almaq üçün ağacın dərinliyini məhdudlaşdırmaq, minimum nümunə sayını təyin etmək və ağaclar arasında müxtəliflik yaratmaq kimi metodlardan istifadə olunur. Bu üsullar Ridge, Lasso və ElasticNet cərimələndirmələrinin funksiyasını əvəz edir.\n",
    "\n",
    "### **Random Forest üçün Alternativlər:**\n",
    "Ridge, Lasso və ElasticNet kimi cərimələndirmələr xətti modellərdə xüsusiyyətlərin seçimi və modelin sadələşdirilməsi üçün istifadə olunur. Random Forest-də isə xüsusiyyətlərin seçimi avtomatik şəkildə edilir, çünki hər ağac yalnız bir hissə xüsusiyyəti istifadə edir. \n",
    "\n",
    "Əgər xüsusiyyət seçimi və ya əmsallar üzərində müəyyən bir cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərdən istifadə etməlisiniz, məsələn:\n",
    "\n",
    "- **Linear Regression** üçün Ridge, Lasso və ElasticNet.\n",
    "- **Logistic Regression** üçün L1, L2 və ElasticNet.\n",
    "\n",
    "Random Forest-də belə bir ehtiyac yoxdur, çünki o, ansambl metodlarının təbiəti ilə artıq overfitting-ə qarşı dayanır və xüsusiyyət seçimi prosesini təbii olaraq həyata keçirir.\n",
    "\n",
    "### **Nəticə:**\n",
    "Random Forest-də Ridge, Lasso və ElasticNet kimi cərimələndirmələr işləməz, çünki bu cərimələndirmələr xətti modellər üçün nəzərdə tutulmuşdur və Random Forest-in iş prinsipinə uyğun gəlmir. Əgər modeldə xüsusiyyət seçimi və cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərə baxmaq daha uyğun olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a911b4c3",
   "metadata": {},
   "source": [
    "## Types of Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3c35f",
   "metadata": {},
   "source": [
    "Ensemble learning metodları, bir neçə zəif modelin birləşdirilərək daha güclü və sabit nəticələr əldə edilməsinə imkan verir. Bu metodlar fərqli modelləri və ya eyni modelin fərqli versiyalarını istifadə edərək nəticələrin ortalamasını alır və ya səs çoxluğunu təyin edir. Aşağıda Ensemble learning-in əsas növləri haqqında geniş məlumat verilir:\n",
    "\n",
    "### 1. **Voting**\n",
    "Voting, müxtəlif modellərin proqnozlarının birləşdirilməsi üsuludur. Burada əsas məqsəd bir neçə fərqli modelin proqnozlarını birləşdirərək daha dəqiq nəticə əldə etməkdir.\n",
    "\n",
    "- **Hard Voting**: Hər modelin proqnozları nəzərə alınır və ən çox səs verilən sinif seçilir. Yəni, hansı sinif daha çox model tərəfindən seçilirsə, o sinif yekun proqnoz kimi təyin edilir.\n",
    "- **Soft Voting**: Hər modelin ehtimal proqnozları nəzərə alınır və ortalama ehtimallar əsasında yekun proqnoz verilir.\n",
    "\n",
    "### 2. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging, Bootstrap metodunu istifadə edərək təkrarlı olaraq verilənlərdən müxtəlif alt dəstələr seçir və hər bir alt dəstədə fərqli modellər qurur. Bu metod əsasən yüksək dəyişkənliyə malik modellərin stabilləşdirilməsi üçün istifadə edilir.\n",
    "\n",
    "- **Random Forest**: Bagging-in ən məşhur tətbiqi olan Random Forest, müxtəlif decision tree-lərdən ibarət bir ansambl yaradır. Hər bir decision tree fərqli verilənlər dəstəsində qurulur və nəticələr səs çoxluğuna əsasən təyin edilir.\n",
    "\n",
    "### 3. **Boosting**\n",
    "Boosting, ardıcıl olaraq zəif modellər qurur və hər yeni model əvvəlki modelin düzgün proqnoz verə bilmədiyi nümunələri düzəltməyə çalışır. Bu metod zəif modelləri güclü modelə çevirmək üçün nəzərdə tutulub.\n",
    "\n",
    "- **AdaBoost**: AdaBoost, hər modelin düzgün və ya səhv proqnozlarına görə ağırlıqlarını dəyişdirərək zəif modelləri ardıcıl qurur. Səhv proqnozlar daha çox ağırlıq alır, doğru proqnozlar isə daha az.\n",
    "- **Gradient Boosting**: Gradient Boosting, qalıqların üzərində işləyən modellər yaradır. Hər yeni model əvvəlki modelin səhvlərini düzəltməyə çalışır.\n",
    "- **XGBoost**: Gradient Boosting-in optimallaşdırılmış versiyasıdır. Hesablama performansını artırmaq və overfitting-i azaltmaq üçün müxtəlif optimallaşdırmalar təqdim edir.\n",
    "- **CatBoost**: KatBoost, əsasən kateqorik verilənlərlə işləmək üçün optimallaşdırılmış bir Boosting metodudur. Bu metod xüsusilə kateqorik dəyişənlərin avtomatik olaraq kodlaşdırılmasını və optimal performans təmin etməsini asanlaşdırır.\n",
    "- **LightGBM**: LightGBM, Gradient Boosting-in daha yüngül və sürətli bir variantıdır. Bu metod böyük verilənlər dəstələri üzərində effektiv şəkildə işləyir və hesablama müddətini əhəmiyyətli dərəcədə azaldır.\n",
    "\n",
    "### 4. **Model Stacking**\n",
    "Model Stacking, fərqli modellərin proqnozlarını birləşdirərək ikinci səviyyəli bir model qurur. Bu ikinci səviyyəli model ilkin modellərin proqnozlarına əsaslanaraq yekun proqnoz verir.\n",
    "\n",
    "- **Base Models**: İlk səviyyədə bir neçə müxtəlif model istifadə olunur və hər biri verilənlər dəstəsində təlim olunur.\n",
    "- **Meta-Model**: İkinci səviyyədə, ilkin modellərin proqnozlarını input olaraq istifadə edən bir meta-model qurulur. Bu meta-model yekun proqnozu təyin edir.\n",
    "\n",
    "### Nəticə\n",
    "Bu metodlar Ensemble learning-in müxtəlif tətbiq üsullarıdır və hər biri fərqli üstünlüklərə malikdir. Bagging yüksək dəyişkənlikli modelləri stabilləşdirmək üçün istifadə edilir, Boosting isə zəif modelləri güclü modellərə çevirmək üçün. Voting sadə birləşdirmə üsuludur, Stacking isə daha kompleks və güclü ansambl modellər yaratmağa imkan verir.\n",
    "\n",
    "CatBoost və LightGBM kimi spesifik alqoritmlər isə Boosting texnikalarının optimallaşdırılmış formalarını təklif edirlər və müxtəlif problem sahələrində yüksək performans təmin edə bilirlər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ab1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e37dec7",
   "metadata": {},
   "source": [
    "## 3. Boosting modellerin daha etrafli izahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869bd18",
   "metadata": {},
   "source": [
    "Boosting metodları maşın öyrənməsində güclü və effektiv modellər qurmaq üçün geniş istifadə olunur. AdaBoost, Gradient Boosting, XGBoost, CatBoost və LightGBM kimi müxtəlif Boosting alqoritmləri mövcuddur. Hər birinin özünəməxsus xüsusiyyətləri, üstünlükləri və məhdudiyyətləri vardır. Aşağıda bu modellərin detallı fərqlərini və hansı vəziyyətdə hansı modelin daha uyğun olacağını izah edirəm.\n",
    "\n",
    "## 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni zəif modelin əvvəlki modellərin səhvlərinə fokuslanaraq qurulmasını təmin edir. Yəni, səhv proqnoz verilən nümunələrə daha çox ağırlıq verilir.\n",
    "- **Ağırlıq Yeniləmə:** Hər iterasiyada nümunələrin ağırlıqları yenilənir. Səhv proqnoz verilən nümunələrin ağırlığı artırılır, doğru proqnoz verilənlərin isə azaldılır.\n",
    "- **Zəif Modellər:** Adətən, sadə qərar ağacları (decision stumps) kimi zəif modellər istifadə olunur.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sadə və asan tətbiq edilə biləndir.\n",
    "- Overfitting riskini azaldır.\n",
    "- Sürətli təlim prosesinə malikdir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Məsələn, çoxlu sayda outliers olan verilənlərdə performansı zəif ola bilər.\n",
    "- Kompleks verilənlər üçün optimal olmaya bilər.\n",
    "\n",
    "## 2. Gradient Boosting\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** Gradient Boosting, hər yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışdığı iterativ bir prosesdir.\n",
    "- **Qalıq Əsasında Təkmilləşdirmə:** Hər iterasiyada əvvəlki modellərin səhvləri üzərində işləyərək ümumi dəqiqliyi artırır.\n",
    "- **Loss Funksiyaları:** Müxtəlif itki funksiyalarını dəstəkləyir, bu da onu müxtəlif problemlərə uyğunlaşdırır.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Yüksək dəqiqlik və performans.\n",
    "- Çeşidli problemlərə tətbiq oluna bilir (regressiya, təsnifat və s.).\n",
    "- Parametrlərin geniş konfiqurasiyası ilə optimallaşdırma imkanı.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Hesablama baxımından bahalı ola bilər.\n",
    "- Overfitting riski yüksəkdir, xüsusilə çox sayda iterasiya ilə.\n",
    "\n",
    "## 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Optimallaşdırılmış Gradient Boosting:** Hesablama sürətini və yaddaş istifadəsini optimallaşdırır.\n",
    "- **Regularizasiya:** L1 və L2 regularizasiyası vasitəsilə overfitting-i azaldır.\n",
    "- **Paralel Təlim:** Paralel hesablama imkanları ilə təlim prosesini sürətləndirir.\n",
    "- **Tree Pruning:** Daha effektiv ağac kəsimi alqoritmləri istifadə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sürətli və effektiv, böyük verilənlər üzərində yaxşı işləyir.\n",
    "- Yüksək performans və dəqiqlik.\n",
    "- Çoxlu sayda parametr tuning imkanları.\n",
    "- Kaggle kimi yarışmalarda geniş istifadə olunur.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Parametr tuning mürəkkəb ola bilər.\n",
    "- Daha çox resurs tələb edir.\n",
    "\n",
    "## 4. CatBoost\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Kateqorik Dəyişənlərin Avtomatik Emalı:** Kateqorik verilənləri avtomatik olaraq kodlaşdırır, əlavə ön emal tələb etmir.\n",
    "- **Order Boosting:** Overfitting-i azaltmaq üçün xüsusi təlim strategiyası istifadə edir.\n",
    "- **Default Parametrlər:** Yaxşı performans üçün az parametr tuning tələb edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Kateqorik verilənlərlə asan işləyir.\n",
    "- Overfitting riski aşağıdır.\n",
    "- Performansı yüksək, xüsusən kateqorik xüsusiyyətlər çox olan verilənlər üçün.\n",
    "- Minimal ön emal tələb edir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Bəzi hallarda digər modellərə nisbətən daha yavaş ola bilər.\n",
    "- Bəzi platformalarda performans fərqləri ola bilər.\n",
    "\n",
    "## 5. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Sürətli Təlim:** Gradient Boosting metodunu daha sürətli və yüngül etmək üçün optimallaşdırılmışdır.\n",
    "- **Histogram-based:** Histogram əsasında təlim alqoritmi daha az yaddaş istifadə edir və sürətli təlim təmin edir.\n",
    "- **Leaf-wise Tree Growth:** Dərin ağaclar quraraq daha yaxşı dəqiqlik əldə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Böyük verilənlər dəstləri üçün idealdır.\n",
    "- Yüksək sürət və effektivlik.\n",
    "- Daha az yaddaş tələb edir.\n",
    "- Yaxşı performans və dəqiqlik.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Overfitting riski, xüsusilə dərin ağaclar istifadə edildikdə.\n",
    "- Kateqorik xüsusiyyətlər üçün əlavə ön emal tələb edə bilər (CatBoost-dan fərqli olaraq).\n",
    "\n",
    "## Hansını Necə Tətbiq Etmək?\n",
    "\n",
    "### **AdaBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Sadə problemlər və kiçik verilənlər üçün idealdır.\n",
    "- **Misal:** Klassik təsnifat problemləri, məsələn, spam filtrləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riski azaldılmışdır, lakin çoxlu outliers olan verilənlər üçün uyğun deyil.\n",
    "\n",
    "### **Gradient Boosting:**\n",
    "- **Uyğun Vəziyyətlər:** Yüksək dəqiqlik tələb edən müxtəlif problemlər üçün geniş istifadə olunur.\n",
    "- **Misal:** Proqnozlaşdırma modelləri, satış təhlili.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Optimal parametr tuning ilə yüksək performans əldə etmək mümkündür, amma hesablama baxımından bahalı ola bilər.\n",
    "\n",
    "### **XGBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Böyük verilənlər və yüksək performans tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Kaggle yarışmaları, böyük biznes məlumat analizi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Parametr tuning mürəkkəb ola bilər, amma yüksək performans təmin edir.\n",
    "\n",
    "### **CatBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Kateqorik xüsusiyyətlər çox olan verilənlər üçün ən yaxşı seçimdir.\n",
    "- **Misal:** Marketinq analizi, müştəri seqmentləşdirməsi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Minimal ön emal tələb edir və kateqorik verilənlərlə asan işləyir.\n",
    "\n",
    "### **LightGBM:**\n",
    "- **Uyğun Vəziyyətlər:** Çox böyük verilənlər və yüksək sürət tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Real-time təhlil sistemləri, böyük verilənlər dəstləri ilə işləyən biznes tətbiqləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riskinə diqqət edilməli, xüsusən dərin ağaclar istifadə edildikdə.\n",
    "\n",
    "## Nəticə\n",
    "\n",
    "Hansı Boosting metodunu seçməyiniz layihənizin tələblərinə və verilənlərinizin xüsusiyyətlərinə bağlıdır. Aşağıdakı tövsiyələrdən istifadə edə bilərsiniz:\n",
    "\n",
    "- **Kateqorik verilənlər çoxdursa:** **CatBoost** ən yaxşı seçimdir.\n",
    "- **Böyük verilənlər üzərində sürətli və effektiv performans istəyirsinizsə:** **LightGBM** uyğun olar.\n",
    "- **Optimal performans və geniş optimallaşdırma imkanları üçün:** **XGBoost** istifadə edilə bilər.\n",
    "- **Sadə və kiçik verilənlər üçün:** **AdaBoost** və ya **Gradient Boosting** istifadə edilə bilər.\n",
    "\n",
    "Əlavə olaraq, müxtəlif modelləri sınayıb onların performansını müqayisə etmək, layihəniz üçün ən uyğun olanını müəyyənləşdirməyə kömək edəcəkdir. Modellərin hər biri müxtəlif vəziyyətlərdə üstünlük göstərə bilər, buna görə də verilənlərinizin xüsusiyyətlərini və layihənizin tələblərini diqqətlə qiymətləndirmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b062ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd571f2",
   "metadata": {},
   "source": [
    "## Voting de regressor tetbiqi haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6587a",
   "metadata": {},
   "source": [
    "**Voting Regressor** də mövcuddur və bu metod, müxtəlif regresiya modellərinin nəticələrini birləşdirərək yekun proqnoz çıxarmaq üçün istifadə olunur. Voting Regressor, modellərin birləşdirilməsində fərqli strategiyalar istifadə edərək, bir neçə fərqli regressoru birləşdirir. \n",
    "\n",
    "### **Voting Regressor**\n",
    "Voting Regressor, bir neçə fərqli regressoru birləşdirərək yekun proqnoz əldə edir. Burada əsas ideya, müxtəlif modellərin proqnozlarını birləşdirərək ümumi nəticəni yaxşılaşdırmaqdır. Voting Regressor iki növ ola bilər:\n",
    "\n",
    "1. **Averaging (Orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır və onların ortalaması çıxarılır.\n",
    "   - Məsələn, üç fərqli model (Model 1, Model 2, Model 3) bir verilənlər dəstəsi üzərində tətbiq edildikdə, hər bir modelin proqnozu alınır və bu proqnozların arifmetik ortalaması nəticə kimi qəbul edilir.\n",
    "\n",
    "2. **Weighted Averaging (Çəkili orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır, lakin hər bir modelin proqnozuna müəyyən bir çəki verilir. Daha güclü performansa sahib modellərə daha çox çəki verilir.\n",
    "   - Bu metod, daha yaxşı performans göstərən modellərin nəticələrə daha çox təsir etməsini təmin edir.\n",
    "\n",
    "### **Voting Regressor-un üstünlükləri**\n",
    "- **Model müxtəlifliyi**: Fərqli modellərin birləşdirilməsi ilə, hər bir modelin zəif cəhətləri digər modellər tərəfindən kompensasiya edilə bilər.\n",
    "- **Təkmilləşdirilmiş performans**: Model müxtəlifliyi nəticəsində Voting Regressor daha sabit və dəqiq nəticələr təmin edə bilər.\n",
    "\n",
    "### **Nümunə: Voting Regressor**\n",
    "Tutaq ki, siz Linear Regression, Decision Tree Regressor və Random Forest Regressor modellərindən istifadə edirsiniz. Bu modellərin proqnozlarını Voting Regressor ilə birləşdirə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Ayrı-ayrı modelləri təyin edək\n",
    "model1 = LinearRegression()\n",
    "model2 = DecisionTreeRegressor()\n",
    "model3 = RandomForestRegressor()\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('lr', model1), \n",
    "    ('dt', model2), \n",
    "    ('rf', model3)\n",
    "])\n",
    "\n",
    "# Modeli təlim etdirin və proqnoz verin\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "```\n",
    "\n",
    "Bu yanaşma, modellərin hər birinin güclü tərəflərindən faydalanmağa imkan verir və beləliklə, yekun proqnoz daha dəqiq ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec88396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e038e07b",
   "metadata": {},
   "source": [
    "## bagging de regressor modelleri tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129672",
   "metadata": {},
   "source": [
    "Bəli, Bagging yalnız klassifikatorlar deyil, həm də regressor modelləri üçün tətbiq edilə bilər. Bagging metodunun əsas məqsədi, modelin təlim edildiyi verilənlər dəstələrini müxtəlif alt dəstələrə bölərək hər bir alt dəstədə fərqli modellər qurmaqdır. Bu metod, həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Bagging Regressor**\n",
    "- **Bagging Regressor**: Əsas model olaraq hər hansı bir regresor seçilə bilər, məsələn, Decision Tree Regressor, Linear Regression, və s. Bagging Regressor, müxtəlif verilənlər alt dəstələri üzərində bu əsas modelləri qurur və nəticələrin ortalamasını alaraq yekun proqnoz verir.\n",
    "  \n",
    "- **Random Forest Regressor**: Bagging-in spesifik bir tətbiqi olan Random Forest, Decision Tree Regressor-larının bir ansamblından ibarət olan bir regresor modeldir. Random Forest Regressor, verilənlər dəstəsindən təkrarən seçilmiş alt dəstələr üzərində bir neçə decision tree qurur və nəticədə bütün ağacların proqnozlarının ortalamasını alaraq yekun proqnoz verir.\n",
    "\n",
    "Bagging metodunun regressorlar üzərində istifadəsi, modelin variansını azaldır və daha stabil proqnozlar əldə etməyə kömək edir. Bu, xüsusilə decision tree kimi yüksək dəyişkənlikli modellərdə faydalıdır, çünki Bagging bu dəyişkənliyi stabilləşdirir və overfitting-in qarşısını alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe3b13a",
   "metadata": {},
   "source": [
    "## bes boosting de regressor tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845b0ce",
   "metadata": {},
   "source": [
    "Bəli, Boosting metodları da regressor modelləri üçün tətbiq oluna bilər. Boosting, əsas modelin ardıcıl olaraq qurulması ilə işləyir, burada hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu metod da həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Boosting Regressor**\n",
    "- **AdaBoost Regressor**: AdaBoost metodunun regresiya üçün tətbiqi. Burada hər bir ardıcıl model əvvəlki modelin proqnoz səhvlərinə daha çox diqqət yetirir. Nəticədə, modelin performansı ardıcıl olaraq yaxşılaşır.\n",
    "\n",
    "- **Gradient Boosting Regressor**: Gradient Boosting, hər yeni modelin qalıqları (yəni əvvəlki modellərin proqnoz səhvləri) üzərində qurulması ilə işləyir. Bu metod çox populyardır və yüksək dəqiqlik təmin edir. Scikit-learn kitabxanasında Gradient Boosting Regressor olaraq tətbiq edilir.\n",
    "\n",
    "- **XGBoost, LightGBM, və CatBoost**: Bunlar Boosting-in daha inkişaf etmiş versiyalarıdır və böyük verilənlər üzərində sürətli və effektiv təlim etmə qabiliyyəti ilə tanınır. Hər üç metod da regresiya problemləri üçün tətbiq oluna bilər və xüsusilə böyük ölçülü və mürəkkəb verilənlər dəstələri üzərində çox yaxşı nəticələr verir.\n",
    "\n",
    "Boosting metodları regressor modellərində də çox uğurlu ola bilər, xüsusən də verilənlər dəstəsində qeyri-xətti əlaqələr və mürəkkəb strukturlar olduqda. Bu metodlar modelin performansını artırmaq üçün ardıcıl olaraq səhvləri azaldır və nəticədə daha yaxşı proqnozlar təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8580f2f1",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96744d1",
   "metadata": {},
   "source": [
    "Aşağıdakı cədvəldə Voting, Bagging və Boosting metodlarının əsas fərqləri göstərilib:\n",
    "\n",
    "| **Xüsusiyyət**                | **Voting**                                      | **Bagging**                                   | **Boosting**                                      |\n",
    "|-------------------------------|-------------------------------------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **İş prinsipi**               | Fərqli modellərin nəticələrini birləşdirir      | Eyni modelin bir neçə varyantını təlim edir   | Modellər ardıcıl olaraq təlim edilir, səhvləri düzəldir |\n",
    "| **Modellərin birləşdirilməsi**| Bir neçə fərqli model (ən çox istifadə edilənlər) | Eyni modelin fərqli alt nümunələr üzərində təlimi | Hər bir növbəti model əvvəlkilərin səhvlərini düzəldir |\n",
    "| **İşləmə tərzi**              | Paralel                                         | Paralel                                        | Sequential (ardıcıl)                               |\n",
    "| **Overfitting riski**         | Aşağı                                           | Aşağı                                          | Daha yüksək, amma düzgün parametrlərlə idarə edilə bilər |\n",
    "| **Çeviklik**                  | Çox müxtəlif modelləri birləşdirə bilir         | Əsasən eyni modelin təkrarlanması               | Modellər daha çox özünü tənzimləyir, çətin nümunələrə fokuslanır |\n",
    "| **Hesablama mürəkkəbliyi**    | Orta                                            | Aşağı                                          | Yüksək, çünki ardıcıl təlim tələb olunur          |\n",
    "| **Məşhur alqoritmlər**        | VotingClassifier, VotingRegressor               | Random Forest, BaggingClassifier               | AdaBoost, XGBoost, Gradient Boosting, LightGBM, CatBoost |\n",
    "| **Paralel İşləmə**            | Bəli                                            | Bəli                                           | Xeyr                                              |\n",
    "| **Stabillik**                 | Artırır, modelləri birləşdirir                  | Artırır, varyansı azaldır                      | Riskli, ancaq yaxşı idarə olunarsa dəqiqlik artır  |\n",
    "| **Tətbiq sahələri**           | Hər iki klassifikasiya və reqressiya problemləri | Hər iki klassifikasiya və reqressiya problemləri | Əsasən klassifikasiya, bəzən reqressiya            |\n",
    "\n",
    "Bu cədvəl Voting, Bagging və Boosting metodlarının əsas xüsusiyyətlərini və fərqlərini ümumi şəkildə göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167de2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60b3719",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1071",
   "metadata": {},
   "source": [
    "| **Xüsusiyyət**                      | **Voting**                                      | **Bagging**                                      | **Boosting**                                  |\n",
    "|-------------------------------------|-------------------------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **Əsas İdeya**                      | Müxtəlif modellərin nəticələrinin birləşdirilməsi | Eyni modelin müxtəlif nümunələr və xüsusiyyətlər üzərində təlim edilməsi | Modellərin ardıcıl şəkildə təlim edilməsi və səhvlərin düzəldilməsi |\n",
    "| **Model Birləşdirmə**               | Fərqli modellər (məs. SVM, DT, LR)               | Eyni modelin fərqli versiyaları (məs. bir neçə decision tree) | Hər bir model əvvəlki modelin səhvlərini düzəldir |\n",
    "| **Nəticə Birləşdirmə**              | Səs çoxluğu və ya ortalama ilə                   | Orta/median (regressiya) və ya səs çoxluğu (klassifikasiya) | Ardıcıl modellərin nəticələri birləşdirilir |\n",
    "| **Paralel/Sequential İşləmə**       | Paralel                                         | Paralel                                          | Sequential, yəni ardıcıl |\n",
    "| **Overfitting Risk**                | Orta səviyyədə                                  | Aşağı                                            | Yüksək (əgər düzgün təlim edilməzsə)        |\n",
    "| **Əsas Məqsəd**                     | Modellərin ümumi dəqiqliyini artırmaq            | Varyansı azaltmaq və stabil nəticələr əldə etmək | Səhvləri azaltmaq və dəqiqliyi artırmaq     |\n",
    "| **İstifadə Edilən Məşhur Algoritmlər** | VotingClassifier                                | Random Forest, BaggingClassifier                 | AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost |\n",
    "| **Parametr Sayı**                   | Əsasən modellərin sayına bağlıdır               | Estimator, n_estimators, max_samples, max_features, bootstrap | Learning rate, n_estimators, base_estimator |\n",
    "| **Hesablama Çətinliyi**             | Aşağı və ya orta                                 | Orta                                             | Yüksək                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5443ad3",
   "metadata": {},
   "source": [
    "### X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size= 0.3, stratify = y, random_state=42) \n",
    "* stratify nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d473ff",
   "metadata": {},
   "source": [
    "`stratify` parametri, `train_test_split` funksiyasında verilən dataset-i təlim və test dəstlərinə bölərkən, target dəyişəninin (`y`) siniflərinin nisbətini qorumaq üçün istifadə edilir. Bu, dataset-dəki siniflərin balanslı qalmasını təmin edir.\n",
    "\n",
    "### Nümunə:\n",
    "Tutaq ki, dataset-də iki sinif var: `0` və `1`. Əgər dataset-inizdə `0` sinfi 70% və `1` sinfi 30% təşkil edirsə, `stratify=y` olaraq `train_test_split` funksiyasına verildikdə, həm təlim, həm də test dəstlərində bu nisbətlər təxminən qorunacaq. Bu, modelin hər iki sinifi daha yaxşı öyrənməsinə kömək edir, xüsusilə də sinif balansı qeyri-bərabər olduqda.\n",
    "\n",
    "Əgər `stratify` parametrini verməsəniz və dataset sinifləri qeyri-bərabərdirsə, təlim və test dəstlərində siniflərin nisbəti fərqli ola bilər, bu da modelin performansına mənfi təsir göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bb581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7202b8ed",
   "metadata": {},
   "source": [
    "## confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad768cf",
   "metadata": {},
   "source": [
    "`confusion_matrix` funksiyasının Sklearn-də qaytardığı qarışıqlıq matrisi aşağıdakı formada olur:\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | True Negative (TN)         | False Positive (FP)        |\n",
    "| **Əsl Sinif 1** | False Negative (FN)        | True Positive (TP)         |\n",
    "\n",
    "### Açıklama:\n",
    "- **True Negative (TN):** Əslində 0 olan nümunələr və düzgün 0 olaraq proqnozlaşdırılanlar.\n",
    "- **False Positive (FP):** Əslində 0 olan nümunələr, amma səhvən 1 olaraq proqnozlaşdırılanlar.\n",
    "- **False Negative (FN):** Əslində 1 olan nümunələr, amma səhvən 0 olaraq proqnozlaşdırılanlar.\n",
    "- **True Positive (TP):** Əslində 1 olan nümunələr və düzgün 1 olaraq proqnozlaşdırılanlar.\n",
    "\n",
    "### Confusion Matrix-dəki Hüceyrələr\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | TN (True Negative)         | FP (False Positive)        |\n",
    "| **Əsl Sinif 1** | FN (False Negative)        | TP (True Positive)         |\n",
    "\n",
    "Bu cədvəl confusion matrix-dəki hüceyrələrin yerləşməsini göstərir. Sklearn-də `confusion_matrix` funksiyasını istifadə edərək əldə etdiyiniz matrisi bu qaydada oxuya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b4016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1d0a",
   "metadata": {},
   "source": [
    "## xgbt.score(X_train, y_train)\n",
    "* bu neyin score dur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52720c",
   "metadata": {},
   "source": [
    "`xgbt.score(X_train, y_train)` metodu, `xgbt` modelinin (`XGBClassifier` obyektinin) təlim məlumat dəstində (`X_train`, `y_train`) əldə etdiyi dəqiqliyi (`accuracy`) qaytarır.\n",
    "\n",
    "### Ətraflı:\n",
    "- **`X_train`:** Təlim üçün istifadə olunan xüsusiyyətlər (features).\n",
    "- **`y_train`:** Təlim üçün istifadə olunan hədəf dəyişəni (labels).\n",
    "- **`xgbt.score`:** Dəqiqlik hesabını qaytarır, yəni təlim dəstində proqnozların neçə faizinin düzgün olduğunu göstərir.\n",
    "\n",
    "Əgər modelin proqnozları `y_train` ilə uyğun gəlirsə, `score` metodu təlim dəstindəki ümumi düzgün proqnozların nisbətini verir. Dəqiqlik hesabı 0 ilə 1 arasında dəyişir, burada 1 tam dəqiqliyi (bütün proqnozlar doğrudur) göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce0fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7da861e",
   "metadata": {},
   "source": [
    " xgbt = XGBClassifier( max_depth = 2,\n",
    "*             learning_rate = 0.2,\n",
    "*             objective  = \"multi:softmax\",\n",
    "*             num_class = 2,\n",
    "*             booster = \"gbtree\",\n",
    "*             n_estimators = 10,\n",
    "*             random_state = 123)\n",
    "\n",
    "### Parametrler haqqinda melumet ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fef5f",
   "metadata": {},
   "source": [
    "`XGBClassifier` üçün istifadə olunan parametrler aşağıdakılardır:\n",
    "\n",
    "### **Parametrlər və Təsvirləri:**\n",
    "\n",
    "1. **`max_depth`**\n",
    "   - **Təsvir:** Hər bir decision tree-nin maksimum dərinliyi.\n",
    "   - **Qeyd:** Dərinlik artdıqca model daha kompleks olur, amma overfitting riski də artır.\n",
    "\n",
    "2. **`learning_rate` (və ya `eta`)**\n",
    "   - **Təsvir:** Hər bir tree-nin təsirini azaldır. Kiçik dəyərlər daha yaxşı nəticələr verə bilər, amma daha çox tree tələb edə bilər.\n",
    "   - **Qeyd:** Düşük learning rate modelin öyrənməsini yavaşlatır və bu, daha çox `n_estimators` tələb edir.\n",
    "\n",
    "3. **`objective`**\n",
    "   - **Təsvir:** Məqsəd funksiyasını təyin edir. `multi:softmax` çox sinifli təsnifat üçün istifadə olunur.\n",
    "   - **Qeyd:** Məqsəd funksiyası, modelin öyrənmə məqsədini müəyyən edir. `multi:softmax` birbaşa siniflərin proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "4. **`num_class`**\n",
    "   - **Təsvir:** Siniflərin sayı. `objective=\"multi:softmax\"` istifadə edilərkən təyin edilir.\n",
    "   - **Qeyd:** Bu parametr çox sinifli təsnifat problemini həll edərkən mütləq olaraq təyin edilməlidir.\n",
    "\n",
    "5. **`booster`**\n",
    "   - **Təsvir:** Hansı növ boosterlə (tree-based, linear) modelləşdiriləcəyini müəyyən edir. `gbtree`, `gblinear` və `dart` seçimləri var.\n",
    "   - **Qeyd:** `gbtree` ağac əsaslı boosterlə daha kompleks problemləri həll edə bilər, `gblinear` isə xətləri modelləşdirmək üçün istifadə olunur.\n",
    "\n",
    "6. **`n_estimators`**\n",
    "   - **Təsvir:** Modeldə istifadə olunan tree-lərin sayı.\n",
    "   - **Qeyd:** Daha çox tree əlavə edildikdə model daha yaxşı öyrənə bilər, amma daha uzun təlim vaxtı və overfitting riski ilə birlikdə gəlir.\n",
    "\n",
    "7. **`random_state`**\n",
    "   - **Təsvir:** Modelin təsadüfi işlərini təyin edən ədədi dəyər.\n",
    "   - **Qeyd:** Təyin edilmiş dəyər modelin hər dəfə eyni nəticələr verməsini təmin edir, bu da nəticələrin təkrarolunmasını asanlaşdırır.\n",
    "\n",
    "### Parametrlərin Tətbiqi\n",
    "\n",
    "Yuxarıdakı parametr dəyərləri ilə XGBoost modeli qurarkən:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbt = XGBClassifier(\n",
    "    max_depth=2,                 # Tree dərinliyi\n",
    "    learning_rate=0.2,           # Təsir dərəcəsi\n",
    "    objective=\"multi:softmax\",   # Çox sinifli təsnifat\n",
    "    num_class=2,                 # İki sinif\n",
    "    booster=\"gbtree\",            # Tree-based boosting\n",
    "    n_estimators=10,             # 10 tree\n",
    "    random_state=123             # Təkrar nəticələr üçün təsadüfi dəyər\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri öz modelinizdə tənzimləməklə, performansını artırmaq və modelin tələblərinə uyğunlaşdırmaq mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e05a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173a3fd2",
   "metadata": {},
   "source": [
    "## Regression ucun en yaxsi model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff926d",
   "metadata": {},
   "source": [
    "Regressiya problemləri üçün ən yaxşı model müxtəlif amillərə, o cümlədən verilənlərin təbiətinə, modelin interpretasiya qabiliyyətinə və performansına bağlı olaraq dəyişir. Aşağıda ən çox istifadə olunan və yüksək performans verən regressiya modellərinin qısa izahı verilmişdir:\n",
    "\n",
    "### 1. **Linear Regression**\n",
    "   - **İstifadə Sahəsi:** Sadə, xətti əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Asan tətbiq edilir və interpretasiya olunur.\n",
    "   - **Məhdudiyyətləri:** Non-linear (qeyri-xətti) əlaqələri yaxşı tuta bilmir; outliers-ə həssasdır.\n",
    "\n",
    "### 2. **Ridge və Lasso Regression**\n",
    "   - **İstifadə Sahəsi:** Yüksək ölçülü verilənlər və ya multicollinearity olan problemlər.\n",
    "   - **Üstünlükləri:** Regularization (cərimələndirmə) istifadə edərək modelin sadələşdirilməsi və overfitting-in azaldılması.\n",
    "   - **Məhdudiyyətləri:** Non-linear əlaqələr üçün optimal deyil.\n",
    "\n",
    "### 3. **Polynomial Regression**\n",
    "   - **İstifadə Sahəsi:** Non-linear əlaqələr.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Model çox kompleks ola bilər və overfitting riski yüksəkdir.\n",
    "\n",
    "### 4. **Decision Tree Regressor**\n",
    "   - **İstifadə Sahəsi:** Həm xətti, həm də xətti olmayan əlaqələr üçün istifadə olunur.\n",
    "   - **Üstünlükləri:** Asan interpretasiya olunur və outliers-lərə daha az həssasdır.\n",
    "   - **Məhdudiyyətləri:** Overfitting riski yüksəkdir, xüsusilə ağac çox dərin olduqda.\n",
    "\n",
    "### 5. **Random Forest Regressor**\n",
    "   - **İstifadə Sahəsi:** Decision Tree-dən daha stabil və dəqiq proqnozlar tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Overfitting riski azdır; yaxşı ümumi performans verir.\n",
    "   - **Məhdudiyyətləri:** Ağacların sayı çox olduqda daha çox hesablama resursu tələb edir.\n",
    "\n",
    "### 6. **Gradient Boosting Regressor**\n",
    "   - **İstifadə Sahəsi:** Yüksək dəqiqlik tələb edən non-linear əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Yüksək performans; yaxşı ümumi dəqiqlik.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və parametr tuning mürəkkəbdir.\n",
    "\n",
    "### 7. **XGBoost, LightGBM, və CatBoost Regressor**\n",
    "   - **İstifadə Sahəsi:** Böyük verilənlər, kompleks əlaqələr və yüksək dəqiqlik tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Yüksək sürət və performans; müxtəlif optimallaşdırmalar və avtomatik kateqorik xüsusiyyətlərin emalı.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning mürəkkəb ola bilər.\n",
    "\n",
    "### 8. **Support Vector Regression (SVR)**\n",
    "   - **İstifadə Sahəsi:** Xətti olmayan və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri yaxşı tuta bilir və margin-based regressiya təmin edir.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning çətin ola bilər və böyük verilənlər dəstləri üçün yavaş ola bilər.\n",
    "\n",
    "### 9. **Neural Networks (MLP Regressor)**\n",
    "   - **İstifadə Sahəsi:** Çox böyük və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan kompleks əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və interpretasiya etmək çətindir.\n",
    "\n",
    "### Hansı Modeli Seçmək?\n",
    "- **Sadə, xətti əlaqələr:** Linear Regression, Ridge və ya Lasso Regression.\n",
    "- **Xətti olmayan əlaqələr:** Polynomial Regression, Decision Tree Regressor, SVR.\n",
    "- **Yüksək performans və kompleks modellər:** Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "- **Böyük və çox ölçülü verilənlər:** LightGBM, CatBoost, Neural Networks.\n",
    "\n",
    "Verilənlərinizin xüsusiyyətlərinə uyğun olaraq modelləri sınamaq və onların performansını müqayisə etmək daha yaxşı nəticələr əldə etməyə kömək edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e6dea1",
   "metadata": {},
   "source": [
    "## Polynomial Regression model qur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26e2bc",
   "metadata": {},
   "source": [
    "Aşağıda Polynomial Regression modelini qurmaq üçün tam kod nümunəsi verilmişdir:\n",
    "\n",
    "### Adım 1: Lazımi kitabxanaların yüklənməsi\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "\n",
    "### Adım 2: Verilənlərin yüklənməsi və hazırlanması\n",
    "```python\n",
    "# Verilənləri yükləyin\n",
    "credit = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week6\\day1\\CreditCardDefault.csv\")\n",
    "credit.drop([\"ID\"], axis=1, inplace=True)\n",
    "\n",
    "# X və y dəyişənlərini təyin edin\n",
    "X = credit.iloc[:, 0:23]\n",
    "y = credit.iloc[:, -1]\n",
    "\n",
    "# Verilənləri təlim və test dəstlərinə bölün\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### Adım 3: Polynomial Features yaratmaq\n",
    "```python\n",
    "# Polynomial features yaratmaq üçün PolynomialFeatures sinifini çağırın\n",
    "degree = 2  # Polynomial dərəcəsi\n",
    "poly = PolynomialFeatures(degree)\n",
    "\n",
    "# Təlim və test dəstləri üçün polynomial xüsusiyyətlər yaradın\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "```\n",
    "\n",
    "### Adım 4: Linear Regression modeli ilə polynomial regression tətbiqi\n",
    "```python\n",
    "# Linear Regression modelini qurun\n",
    "model = LinearRegression()\n",
    "\n",
    "# Polynomial verilənlər üzərində modeli təlim edin\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Test dəsti üzərində proqnozlar edin\n",
    "y_pred = model.predict(X_test_poly)\n",
    "```\n",
    "\n",
    "### Adım 5: Model performansının qiymətləndirilməsi\n",
    "```python\n",
    "# MSE və R2 dəyərlərini hesablayın\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "```\n",
    "\n",
    "### İcra\n",
    "Bu kod parçalarını addım-addım icra etdikdən sonra Polynomial Regression modelini qurmuş olacaqsınız. Kodun sonunda modelin performansını `Mean Squared Error (MSE)` və `R-squared (R2)` dəyərləri ilə ölçə biləcəksiniz.\n",
    "\n",
    "**Qeyd:** `degree` parametrini dəyişdirərək polynomial modelin dərəcəsini tənzimləyə bilərsiniz. Çox yüksək dərəcələr overfitting-ə səbəb ola bilər, buna görə dərəcəni diqqətlə seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a4509e",
   "metadata": {},
   "source": [
    "## Modellerde threshold-u nece deyisim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2571ed",
   "metadata": {},
   "source": [
    "Threshold (məhdud) dəyərini dəyişdirmək, xüsusilə təsnifat modelləri üçün vacibdir, çünki default olaraq modellər çox vaxt 0.5 threshold ilə işləyir. Threshold dəyərini dəyişdirməklə, modelin proqnoz nəticələrinin \"positive\" və \"negative\" olaraq təsnif edilmə meyarını dəyişdirə bilərsiniz.\n",
    "\n",
    "### Adımlar\n",
    "\n",
    "1. **Model proqnozlarını ehtimallar kimi əldə edin.**\n",
    "   - Təsnifat modellərində `.predict_proba()` metodu istifadə edərək ehtimal dəyərlərini ala bilərsiniz.\n",
    "\n",
    "2. **Threshold-a əsasən proqnozları təsnif edin.**\n",
    "   - İstədiyiniz threshold dəyərinə əsasən ehtimalları `positive` və ya `negative` olaraq təsnif edəcəksiniz.\n",
    "\n",
    "### Kod nümunəsi\n",
    "\n",
    "Məsələn, bir təsnifat modeli üçün threshold-u necə dəyişdirəcəyinizi göstərən bir nümunə:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Model proqnozları ehtimal kimi alındı\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Positive class üçün ehtimal\n",
    "\n",
    "# Threshold dəyərini təyin edin\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "# Threshold-a əsasən proqnozları təsnif edin\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### İzahat:\n",
    "- **`predict_proba`:** Bu metod modelin hər bir sinif üçün ehtimallarını qaytarır. `[:, 1]` pozitiv sinifin ehtimallarını seçir.\n",
    "- **`threshold = 0.3`:** İstədiyiniz threshold dəyərini təyin edirsiniz. Burada 0.3 olaraq seçilib.\n",
    "- **`np.where(y_probs > threshold, 1, 0)`:** Ehtimalları seçilən threshold dəyərinə əsasən 1 (positive) və ya 0 (negative) olaraq təsnif edir.\n",
    "\n",
    "Bu yanaşma, threshold dəyərini dəyişdirərək modelin performansını müxtəlif threshold dəyərləri üçün optimallaşdırmağa və balanslaşdırmağa imkan verir. Bu, xüsusilə sinif disbalansı olan verilənlər dəstlərində faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e4b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb3e2f3",
   "metadata": {},
   "source": [
    "## Butun modellerde threshold-u eyni cur deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6350af",
   "metadata": {},
   "source": [
    "Bəli, threshold dəyərini dəyişdirmə üsulu demək olar ki, bütün təsnifat modellərində tətbiq edilə bilər. Lakin bu proses yalnız ehtimal əsaslı təsnifat modelləri üçün keçərlidir. Aşağıda bəzi ümumi modellər üçün bu yanaşmanı necə tətbiq etmək barədə izahatlar verilmişdir:\n",
    "\n",
    "### 1. **Logistic Regression, Random Forest, Gradient Boosting və s.**  \n",
    "Bu modellər ehtimal proqnozları qaytarır və threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 2. **Support Vector Machine (SVM)**\n",
    "SVM modelləri default olaraq ehtimal vermir, lakin `probability=True` olaraq modeli qurmaqla ehtimal əsaslı proqnozlar əldə edə bilərsiniz. Sonra threshold-u dəyişdirmək mümkündür.\n",
    "\n",
    "### 3. **Neural Networks (MLPClassifier)**\n",
    "Sinif ehtimalları `.predict_proba()` metodu ilə əldə edilir və threshold-a əsasən dəyişdirilə bilər.\n",
    "\n",
    "### 4. **Naive Bayes**\n",
    "Naive Bayes modelləri də ehtimallar qaytarır, buna görə threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 5. **K-Nearest Neighbors (KNN)**\n",
    "KNN də `.predict_proba()` metodu ilə ehtimallar qaytarır və bu ehtimallara əsasən threshold dəyişdirilə bilər.\n",
    "\n",
    "### Tətbiq etmək üçün Ümumi Yanaşma:\n",
    "```python\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Pozitiv sinif ehtimalları\n",
    "\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### Qeyd:\n",
    "- **Decision Tree** kimi bəzi modellərdə ehtimal dəyərləri, leaf düyünlərinin təxminləri ilə hesablanır və `.predict_proba()` metodu vasitəsilə əldə edilə bilər.\n",
    "- **Threshold dəyərini dəyişdirmək**, modelin təsnifat balansını dəyişdirməyə imkan verir, bu da sinif disbalansı olan verilənlərdə xüsusilə faydalıdır.\n",
    "\n",
    "Beləliklə, ehtimal əsaslı təsnifat modellərində threshold dəyərini dəyişdirmək mümkün və çox vaxt faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a600c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6ead7e",
   "metadata": {},
   "source": [
    "## Boosting modellerinde, meselen XGBoost goturek. Bu modeli tetbiq ederken ozu arxa plnada hansi modeller qurur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab38e",
   "metadata": {},
   "source": [
    "Boosting modellərində, məsələn, XGBoost istifadə edərkən, modelin arxa planda qurduğu zəif modelləri (base learners) başa düşmək vacibdir. Boosting metodları ardıcıl olaraq zəif modelləri birləşdirərək daha güclü bir model qurur. XGBoost, Gradient Boosting metodunun optimallaşdırılmış bir versiyasıdır və bu arxa planda aşağıdakı addımları həyata keçirir:\n",
    "\n",
    "### XGBoost-un Arxa Planında Qurulan Modellər\n",
    "\n",
    "1. **Zəif Modellərin (Base Learners) Qurulması:**\n",
    "   - **Decision Trees:** XGBoost, ən çox Decision Tree-lərdən istifadə edir. Bu ağaclar, çox vaxt \"stumps\" olaraq da adlandırılan çox sadə qərar ağaclarıdır. XGBoost-da bu ağaclar çox vaxt qısa və çox dərin olmur; bu, hər bir ağacın çox sadə olduğunu bildirir.\n",
    "\n",
    "2. **Ardıcıl Tətbiq:**\n",
    "   - **Gradient Descent:** XGBoost, hər yeni ağacı əvvəlki modellərin səhvlərini düzəltmək üçün qurur. Bu, gradient descent optimallaşdırma metodu vasitəsilə həyata keçirilir. Hər bir ağacın yaradılması, əvvəlki ağacların səhvlərinə (residuals) əsaslanır.\n",
    "\n",
    "3. **Öyrənmə Sürəti (Learning Rate):**\n",
    "   - **Learning Rate:** Hər yeni ağacın təsirini tənzimləyən bir öyrənmə sürəti (learning rate) istifadə edilir. Bu, hər bir ağacın proqnozlarına əlavə ediləcək ağırlığı müəyyən edir. Daha kiçik öyrənmə sürəti ilə daha çox ağac tələb olunur.\n",
    "\n",
    "4. **Regularization:**\n",
    "   - **Regularization:** XGBoost-da modelin overfitting riskini azaltmaq üçün cərimələndirmə (regularization) tətbiq edilir. Bu, ağacların böyüməsini tənzimləmək və modelin kompleksliyini azaltmaq məqsədini güdür. XGBoost L1 (Lasso) və L2 (Ridge) cərimələndirmə üsullarını dəstəkləyir.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **Early Stopping:** XGBoost, modelin performansını qiymətləndirmək üçün validasiya dəstini istifadə edir və performansın yaxşılaşmadığı yerə qədər ağacları qurmağa davam edir. Bu, overfitting-i azaltmağa kömək edir.\n",
    "\n",
    "### XGBoost-un İş Prinsipi:\n",
    "\n",
    "1. **Başlanğıc Model:** Əvvəlcə bütün verilənlər üçün proqnozları təsadüfi bir başlanğıc dəyəri ilə başlatır.\n",
    "2. **Residual Hesablanması:** Təxmin edilmiş dəyərlərlə real dəyərlər arasındakı fərqi hesablamaqla qalıqları (residuals) tapır.\n",
    "3. **Yeni Ağacın Qurulması:** Qalıqları (residuals) düzəltmək üçün yeni bir Decision Tree qurur. Bu ağac qalıqların düzəldilməsinə kömək edir.\n",
    "4. **Proqnozlara Əlavə:** Yeni ağacın təxminləri əvvəlki proqnozlara əlavə edilir.\n",
    "5. **Yenidən Hesablamalar:** Bu proses təkrar olunur, hər dəfə yeni ağac əvvəlki ağacların səhvlərini düzəldir.\n",
    "\n",
    "Bu üsulla, XGBoost bir neçə zəif modelin (Decision Trees) ardıcıl olaraq birləşdirilməsi ilə güclü və dəqiq bir model qurur. Bu yanaşma, modelin öyrənmə qabiliyyətini artırır və daha yaxşı nəticələr əldə etməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdcfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a3e09c",
   "metadata": {},
   "source": [
    "## Boosting modelleri arxa planda hansi modelleri qurur. Bir bir hamisi haqqinda yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c83a6",
   "metadata": {},
   "source": [
    "Boosting modelləri arxa planda zəif modellər (base learners) qurur. Ən çox istifadə olunan boosting modelləri bunlardır: AdaBoost, Gradient Boosting, XGBoost, LightGBM, və CatBoost. Aşağıda hər birinin arxa planda qurduğu modellərin xüsusiyyətlərini izah edirəm:\n",
    "\n",
    "### 1. **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** Ən çox **Decision Trees** (adətən \"stumps\" adlanan çox sadə ağaclar) istifadə edilir. Hər bir ağac, əvvəlki ağacların səhvlərinə daha çox diqqət yetirir.\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni ağacın əvvəlki ağacların səhvlərini düzəltməyə çalışır. Səhv təsnif edilən nümunələrə daha çox ağırlıq verilir, beləliklə, yeni ağaclar bu nümunələrə daha çox diqqət yetirir.\n",
    "\n",
    "### 2. **Gradient Boosting**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** istifadə edilir. Bu ağaclar adətən daha dərin və daha kompleks ola bilər, lakin hələ də çox sadə modellərdir.\n",
    "- **İş Prinsipi:** Hər yeni ağac, əvvəlki ağacların qalıqları üzərində işləyərək səhvləri düzəltməyə çalışır. Hər ağac gradient descent metodunu istifadə edərək əvvəlki ağacların səhvlərini azaldır.\n",
    "\n",
    "### 3. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha kiçik və daha sadə).\n",
    "- **İş Prinsipi:** XGBoost, Gradient Boosting metodunun optimallaşdırılmış versiyasıdır. Bu metod eyni zamanda L1 (Lasso) və L2 (Ridge) cərimələndirmə tətbiq edir, və ağacların böyüməsini tənzimləyərək overfitting-i azaldır. Hər ağac əvvəlki ağacların səhvlərinə əsaslanaraq yaradılır və öyrənmə sürəti (learning rate) ilə tənzimlənir.\n",
    "\n",
    "### 4. **LightGBM (Light Gradient Boosting Machine)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha yüngül və daha sürətli öyrənən ağaclar).\n",
    "- **İş Prinsipi:** LightGBM, Gradient Boosting-in daha sürətli və daha az resurs tələb edən variantıdır. Ağacları \"leaf-wise\" bölmələrdən istifadə edərək qurur, bu da daha dərin və daha təkmil ağaclara gətirib çıxarır. Bu yanaşma, daha sürətli və daha yaxşı performans verir, xüsusilə böyük verilənlər dəstələrində.\n",
    "\n",
    "### 5. **CatBoost (Categorical Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (daha optimallaşdırılmış və kateqorik dəyişənləri avtomatik emal edən ağaclar).\n",
    "- **İş Prinsipi:** CatBoost, Gradient Boosting-in xüsusilə kateqorik verilənlərlə işləməyə optimallaşdırılmış versiyasıdır. Bu model kateqorik dəyişənlərin avtomatik kodlaşdırılmasını təmin edir və şumun qarşısını almaq üçün müxtəlif texnikalardan istifadə edir. Ağaclar adətən daha qısa və effektiv olur, modelin öyrənmə sürəti yüksəkdir.\n",
    "\n",
    "### Ümumi Yanaşma:\n",
    "Bütün bu boosting modelləri, zəif modelləri (adətən Decision Trees) bir-birinin ardınca quraraq, əvvəlki modellərin səhvlərini düzəltməyə çalışır. Hər yeni model əvvəlki modellərin qalıqları üzərində işləyir və modelin ümumi performansını artırır.\n",
    "\n",
    "Bu yanaşmaların hər biri, müxtəlif optimallaşdırma və cərimələndirmə üsulları ilə modelin performansını artırmağa və overfitting-i azaltmağa çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5b5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24406bd5",
   "metadata": {},
   "source": [
    "## Bagging-de eyni model qurulur? Boosting-de eyni model qurulur yoxsa bir nece model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a180d",
   "metadata": {},
   "source": [
    "Bagging və Boosting metodları arasında əsas fərq ondan ibarətdir ki, hər biri modelləri necə qurur və birləşdirir.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** metodunda bir neçə eyni model (həmin növ model) qurulur, lakin bu modellər fərqli verilənlər dəstləri üzərində öyrədilir. \n",
    "\n",
    "- **Eyni Model:** Bəli, **bagging** metodunda eyni növ modeldən bir neçə dəfə istifadə edilir. Məsələn, əgər bagging metodunda Decision Tree istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Verilənlər Dəstləri:** Bagging, orijinal verilənlər dəstinin təkrar nümunələri ilə yaradılan fərqli verilənlər dəstlərini (bootstrap samples) istifadə edərək hər bir modeli öyrədir.\n",
    "- **Final Nəticə:** Hər bir modelin proqnozları birləşdirilir. Təsnifat üçün bu, adətən səsvermə (majority voting) üsulu ilə, regresiyada isə ortalama (averaging) ilə həyata keçirilir.\n",
    "\n",
    "**Nəticə:** Bagging metodunda bir neçə eyni model yaradılır, lakin bu modellər fərqli verilənlər üzərində öyrədilir və final nəticə bütün modellərin nəticələrinin birləşdirilməsi ilə alınır.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Boosting** metodunda isə modellər ardıcıl olaraq qurulur, və hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu prosesdə modellərin hər biri eyni növ olsa da, onların hər biri müxtəlif verilənlər üzərində və fərqli səhvlər üzərində öyrədilir.\n",
    "\n",
    "- **Eyni Model:** Bəli, **boosting** metodunda da eyni növ model istifadə edilir. Məsələn, Gradient Boosting-də Decision Tree-lər istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Modellərin Qurulması:** Lakin burada fərq ondan ibarətdir ki, hər bir yeni model əvvəlki modellərin səhvlərini düzəltmək üçün qurulur, yəni hər model fərqli şəkildə optimallaşdırılmış bir variant kimi fəaliyyət göstərir.\n",
    "- **Final Nəticə:** Hər bir modelin nəticələri birləşdirilir, lakin bu birləşmə hər bir modelin səhvlərini düzəltmək üçün fərqli çəkilərdən istifadə edir.\n",
    "\n",
    "**Nəticə:** Boosting metodunda eyni növ model bir neçə dəfə qurulur, lakin bu modellər bir-birindən fərqlidir, çünki hər biri əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu modellər ardıcıl olaraq qurulur və nəticələr birləşdirilərək daha güclü bir model yaradılır.\n",
    "\n",
    "### Ümumi Qısa Xülasə:\n",
    "\n",
    "- **Bagging:** Eyni növ model, fərqli verilənlər dəstləri üzərində paralel olaraq öyrədilir və nəticələr birləşdirilir.\n",
    "- **Boosting:** Eyni növ model, ardıcıl olaraq qurulur və hər bir yeni model əvvəlki modellərin səhvlərini düzəldir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365bb8e2",
   "metadata": {},
   "source": [
    "## Modelin overfitting edib etmediyini nece bilim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0322b2",
   "metadata": {},
   "source": [
    "Modelin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə üsul və göstəricidən istifadə etmək mümkündür. Overfitting, modelin təlim məlumatlarına həddindən artıq uyğunlaşdığı, lakin test və ya yeni məlumatlarla yaxşı performans göstərmədiyi bir vəziyyətdir. Overfitting olub-olmadığını müəyyən etmək üçün aşağıdakı yanaşmaları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Train-Test Split və Performans Müqayisəsi**\n",
    "   - **Train Score:** Modelin təlim (train) məlumatlarındakı performansını ölçün. Yüksək dəqiqlik (accuracy) və ya digər performans metrikləri (məsələn, R² score) modelin təlim məlumatlarına çox yaxşı uyğunlaşdığını göstərə bilər.\n",
    "   - **Test Score:** Modelin test (validation) məlumatlarındakı performansını ölçün. Əgər test dəstindəki performans əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   train_score = model.score(X_train, y_train)\n",
    "   test_score = model.score(X_test, y_test)\n",
    "   \n",
    "   print(\"Train Score:\", train_score)\n",
    "   print(\"Test Score:\", test_score)\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train score çox yüksək, test score isə əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsidir.\n",
    "\n",
    "### 2. **Learning Curve**\n",
    "   - **Learning Curve:** Train və test məlumatları üzərində modelin performansını müxtəlif təlim məlumatları ilə vizuallaşdıran bir əyri çəkin. Əgər train əyrisi yuxarıda qalır və test əyrisi daha aşağıda qalırsa, bu da overfitting göstəricisidir.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import learning_curve\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "   plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "   plt.xlabel(\"Training examples\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Əgər train əyrisi yüksək performans göstərir, lakin test əyrisi daha aşağı qalırsa və aradakı fərq böyükdürsə, bu da overfitting-i göstərir.\n",
    "\n",
    "### 3. **Cross-Validation**\n",
    "   - **Cross-Validation:** Verilənlər dəstini bir neçə hissəyə bölərək modelin performansını müxtəlif hissələr üzərində yoxlayın. Əgər modelin performansı müxtəlif hissələr arasında çox dəyişirsə, bu da overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "   print(\"Cross-Validation Scores:\", scores)\n",
    "   print(\"Mean Score:\", np.mean(scores))\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Cross-validation nəticələri arasında böyük fərqlər varsa, bu, modelin müəyyən verilənlər dəstlərinə həddən artıq uyğunlaşdığını göstərə bilər.\n",
    "\n",
    "### 4. **Validation Curve**\n",
    "   - **Validation Curve:** Modelin müəyyən parametrləri üzrə performansını yoxlamaq üçün validation curve çəkə bilərsiniz. Əgər train performansı yüksəkdir, lakin validation performansı aşağıdırsa, overfitting ehtimalı var.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import validation_curve\n",
    "\n",
    "   param_range = np.logspace(-6, -1, 5)\n",
    "   train_scores, test_scores = validation_curve(model, X, y, param_name=\"alpha\", param_range=param_range, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "   plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "   plt.xlabel(\"Parameter\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train əyrisi yüksək, validation əyrisi isə daha aşağı qalırsa, overfitting ehtimalı var.\n",
    "\n",
    "### 5. **Regularization Yoxlamaq**\n",
    "   - **Regularization:** Regularization (L1, L2 cərimələndirmə) metodlarından istifadə etməklə modelin overfitting edilib-etmədiyini yoxlaya bilərsiniz. Regularization overfitting-in qarşısını almağa kömək edir.\n",
    "\n",
    "   - **Overfitting əlaməti:** Regularization tətbiq etdikdə modelin performansı daha sabit və balanslı olursa, bu, overfitting-in qarşısını almağa kömək edə bilər.\n",
    "\n",
    "### 6. **Modelin Kompleksliyi**\n",
    "   - **Modelin Kompleksliyi:** Çox kompleks modellər (çox dərin decision tree-lər, çox sayda hidden layer-ləri olan neyron şəbəkələr və s.) asanlıqla overfitting edə bilər. Sadə modellər istifadə edərək performansı müqayisə etmək də overfitting-i müəyyən etməyə kömək edə bilər.\n",
    "\n",
    "   - **Overfitting əlaməti:** Çox kompleks model çox yüksək train performansı və aşağı test performansı göstərə bilər.\n",
    "\n",
    "Bu metodlar və yanaşmalarla modelin overfitting edib-etmədiyini müəyyən edə bilərsiniz və ehtiyac olduqda daha yaxşı performans əldə etmək üçün modelin quruluşunu və parametrlərini tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c0dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a3f95",
   "metadata": {},
   "source": [
    "## Bu overfitting ucun kifayetdir?\n",
    "* Cross-Validation Scores: [0.81, 0.76, 0.82, 0.77, 0.79]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33d3bb",
   "metadata": {},
   "source": [
    "Verilən cross-validation nəticələrinə əsasən, modellərinizin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə göstəriciyə diqqət yetirmək lazımdır:\n",
    "\n",
    "1. **Dəyişkənlik (Variance):** Cross-validation skorları arasında çox böyük bir fərq yoxdursa, bu yaxşı bir göstəricidir. Verilən skorlar 0.76 ilə 0.82 arasında dəyişir, bu isə bir qədər dəyişkənlik göstərsə də, ciddi bir overfitting işarəsi deyil. \n",
    "\n",
    "2. **Test və Təlim Dəqiqlikləri (Train vs. Test Accuracy):** Əgər təlim dəstindəki dəqiqlik çox yüksək, amma test dəstindəki dəqiqlik nisbətən aşağıdırsa, bu overfitting işarəsi ola bilər. Siz bu məlumatı paylaşmamısınız, amma əgər test və train dəqiqlikləri arasında böyük bir fərq varsa, overfitting ehtimalı daha yüksəkdir.\n",
    "\n",
    "3. **Kompromis:** Skorların orta səviyyədə olması (məsələn, 0.79) modeli qismən balanslaşdırılmış göstərə bilər. Amma, modellərinizi daha da optimallaşdırmaq və overfitting-in qarşısını almaq üçün modeli sadələşdirmək və ya daha çox məlumat toplamaq barədə düşünə bilərsiniz.\n",
    "\n",
    "**Nəticə:** Bu skorlar çox böyük bir dəyişkənlik göstərmədiyi üçün ciddi bir overfitting işarəsi deyildir. Lakin, modelin təlim və test dəstində necə performans göstərdiyini daha dərindən analiz etmək daha dəqiq bir qərar verməyə kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a92e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b126fc89",
   "metadata": {},
   "source": [
    "### train_score = model.score(X_train, y_train)\n",
    "### test_score = model.score(X_test, y_test)\n",
    "## Bunlar accuracy dir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66317e3",
   "metadata": {},
   "source": [
    "Bəli, `model.score(X_train, y_train)` və `model.score(X_test, y_test)` metodları adətən **accuracy** dəyərini qaytarır, amma bu, yalnız təsnifat (classification) modelləri üçün keçərlidir. \n",
    "\n",
    "### Təfsilatlı izah:\n",
    "- **Classification Modelləri üçün:** \n",
    "  - `model.score(X_train, y_train)` və `model.score(X_test, y_test)` təlim və test dəstlərində modelin təsnifat dəqiqliyini (accuracy) qaytarır. Bu, düzgün proqnozların ümumi nümunələrə nisbətidir.\n",
    "\n",
    "  ```python\n",
    "  accuracy = correct_predictions / total_predictions\n",
    "  ```\n",
    "\n",
    "- **Regression Modelləri üçün:** \n",
    "  - Əgər model regresiya modelidirsə (məsələn, Linear Regression, Ridge, Lasso), `model.score()` metodunun qaytardığı dəyər **R² score** olacaq. R² score, modelin verilənlər dəstinə nə qədər yaxşı uyğun gəldiyini ölçür və proqnozların faktiki dəyərlərə nə qədər yaxın olduğunu göstərir.\n",
    "\n",
    "  ```python\n",
    "  R² = 1 - (SS_res / SS_tot)\n",
    "  ```\n",
    "  Burada:\n",
    "  - `SS_res`: Proqnozlaşdırılmış dəyərlərin və faktiki dəyərlərin kvadrat fərqlərinin cəmi.\n",
    "  - `SS_tot`: Faktiki dəyərlərin və onların ortalama dəyərinin kvadrat fərqlərinin cəmi.\n",
    "\n",
    "### Nəticə:\n",
    "Əgər təsnifat modeli istifadə edirsinizsə, `model.score()` metodu **accuracy** dəyərini, əgər regresiya modeli istifadə edirsinizsə, **R² score** qaytaracaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8df425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b7828c",
   "metadata": {},
   "source": [
    "## Bank datalari ucun ve elektronika satan sirket datalari ucun en cox hansi modeller istifade olunur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ba52d",
   "metadata": {},
   "source": [
    "Bank dataları və elektronika satan şirkətlərin məlumatları üçün ən uyğun modellər seçilərkən, bu dataların xarakteristikası və məqsəd nəzərə alınır. Hər bir sektorun özünəməxsus xüsusiyyətləri var, buna görə də istifadə olunan modellər fərqlənə bilər.\n",
    "\n",
    "### Bank Dataları Üçün\n",
    "Bank sektoru əsasən müştərilərin maliyyə məlumatları, kredit riski, fırıldaqçılıq aşkar edilməsi, müştəri seqmentasiyası kimi mövzulara yönəlir. Bu səbəbdən bank məlumatları üçün geniş istifadə olunan modellər:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - Əsasən kredit riski, müştəri davranışı və fırıldaqçılıq aşkar etmək üçün istifadə olunur.\n",
    "   - Məqsəd: İkili təsnifat (binary classification) məsələləri.\n",
    "\n",
    "2. **Random Forest:**\n",
    "   - Fərqli atributlar üzərində güclü performans göstərir və feature importance üçün istifadə edilir.\n",
    "   - Məqsəd: Təsnifat və regresiya məsələləri.\n",
    "\n",
    "3. **Gradient Boosting (XGBoost, LightGBM, CatBoost):**\n",
    "   - Bank dataları üçün xüsusilə effektivdir, çünki bu metodlar mürəkkəb əlaqələri və nümunələri öyrənə bilir.\n",
    "   - Məqsəd: Kredit skoring, risk analizi, müştəri churn təxminatı.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - Daha az məlumat üçün güclü performans göstərir, lakin böyük datalar üçün hesablama baxımından bahalı ola bilər.\n",
    "   - Məqsəd: Kredit riski və fırıldaqçılıq aşkar edilməsi.\n",
    "\n",
    "5. **K-Means Clustering:**\n",
    "   - Müştəri seqmentasiyası üçün istifadə olunur.\n",
    "   - Məqsəd: Müştəri qruplarını tapmaq və onlara fərqli maliyyə məhsulları təklif etmək.\n",
    "\n",
    "### Elektronika Satan Şirkətlərin Dataları Üçün\n",
    "Elektronika satan şirkətlər əsasən satış təxminatı, müştəri davranışı, stok idarəsi və qiymət optimallaşdırması üzərində işləyirlər. Bu sahə üçün istifadə olunan əsas modellər:\n",
    "\n",
    "1. **Linear Regression:**\n",
    "   - Satış proqnozları və qiymət optimallaşdırılması üçün istifadə olunur.\n",
    "   - Məqsəd: Kəmiyyət (quantitative) proqnozlar.\n",
    "\n",
    "2. **Time Series Analysis (ARIMA, SARIMA):**\n",
    "   - Satış və tələbat proqnozları üçün zaman seriyası analizləri geniş istifadə olunur.\n",
    "   - Məqsəd: Gələcək satışların və tələbin proqnozlaşdırılması.\n",
    "\n",
    "3. **Random Forest və Gradient Boosting:**\n",
    "   - Elektronika məhsullarının satışlarına təsir edən müxtəlif faktorları təhlil etmək üçün istifadə olunur.\n",
    "   - Məqsəd: Təsnifat və proqnozlaşdırma.\n",
    "\n",
    "4. **Collaborative Filtering (Recommendation Systems):**\n",
    "   - Məhsul təklifləri və müştəriyə fərdi təkliflərin hazırlanması üçün geniş istifadə olunur.\n",
    "   - Məqsəd: Müştəriyə uyğun məhsul təklifləri vermək.\n",
    "\n",
    "5. **Clustering (K-Means, DBSCAN):**\n",
    "   - Müştəri seqmentasiyası və marketinq strategiyalarının tərtibi üçün.\n",
    "   - Məqsəd: Müştəri qruplarının aşkar edilməsi və hədəfli reklam strategiyalarının inkişafı.\n",
    "\n",
    "### Nəticə\n",
    "Bank məlumatları üçün daha çox təsnifat modelləri (logistic regression, random forest, gradient boosting) istifadə olunur. Elektronika satan şirkətlər isə satış proqnozları və tələbat analizi üçün zaman seriyaları modelləri və regresiya modellərinə üstünlük verirlər. Hər iki sahədə də clustering müştəri seqmentasiyası üçün geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46597909",
   "metadata": {},
   "source": [
    "## CatBoost-da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba7ccd",
   "metadata": {},
   "source": [
    "CatBoost modeli daxilində işləyən spesifik modelləri dəyişdirmək mümkün deyil, çünki CatBoost özü qərar ağacları (decision trees) əsasında işləyən bir **Gradient Boosting** metodudur və daxili arxitekturası istifadəçilər üçün dəyişdirilə bilən deyil. CatBoost, xüsusilə kateqorik verilənlərlə effektiv işləmək üçün optimallaşdırılmışdır və daxili olaraq xüsusi bir **symmetrical tree** strukturu istifadə edir.\n",
    "\n",
    "Bununla belə, siz CatBoost modelinin performansını və davranışını təsir edən müxtəlif hiperparametrləri tənzimləməklə modelin necə işlədiyini dəyişə bilərsiniz. Məsələn:\n",
    "\n",
    "### 1. **`depth` (ağacın dərinliyi):**\n",
    "   - Modelin hər bir ağacının maksimum dərinliyini təyin edir. Daha böyük dərinlik daha kompleks modellər və overfitting riskini artırır.\n",
    "\n",
    "### 2. **`learning_rate`:**\n",
    "   - Modelin hər bir iterasiyada addım böyüklüyünü təyin edir. Daha aşağı qiymətlər daha sabit, amma daha yavaş öyrənməyə səbəb olur.\n",
    "\n",
    "### 3. **`iterations`:**\n",
    "   - Ümumi ağac sayını müəyyən edir. Daha çox iterasiya adətən daha yüksək dəqiqlik, amma overfitting riski ilə nəticələnə bilər.\n",
    "\n",
    "### 4. **`l2_leaf_reg`:**\n",
    "   - Ağacların budaqlanmasının qarşısını almaq üçün cərimələndirmə əmsalıdır (regularization term). Daha böyük dəyər daha az kompleks modellərə və overfitting riskinin azalmasına səbəb ola bilər.\n",
    "\n",
    "### 5. **`one_hot_max_size`:**\n",
    "   - Kateqorik dəyişənlər üçün one-hot encoding tətbiq ediləcək maksimum unikal dəyər sayını təyin edir. Bu, xüsusilə çox unikal kateqorik dəyərə malik atributlar üçün faydalıdır.\n",
    "\n",
    "### 6. **`boosting_type`:**\n",
    "   - CatBoost modeli əsasən **Ordered Boosting** və **Plain Boosting** kimi iki növ boosting metodu təklif edir:\n",
    "     - **Ordered Boosting**: Overfitting-in qarşısını almaq üçün daha yaxşıdır.\n",
    "     - **Plain Boosting**: Daha sürətli öyrənməyə səbəb ola bilər, amma overfitting riskini artıra bilər.\n",
    "\n",
    "Bu hiperparametrləri dəyişərək CatBoost modelinin performansını və davranışını təyin edə bilərsiniz, amma daxildəki modeli fərqli bir model ilə əvəz etmək mümkün deyil, çünki CatBoost özü bir boosting alqoritmi kimi çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69438d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d07da7",
   "metadata": {},
   "source": [
    "## XGBoost da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e74fbc",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) modelində də spesifik olaraq işləyən \"daxili model\" anlayışı yoxdur, çünki XGBoost özü qərar ağacları (decision trees) üzərində qurulmuş bir **Gradient Boosting** metodudur. Bununla belə, siz XGBoost modelində bəzi hiperparametrləri dəyişdirərək modelin necə işlədiyini təsir edə bilərsiniz. Amma bu, istifadə olunan modelin növünü deyil, qərar ağaclarının quruluşunu və iş rejimini dəyişdirir.\n",
    "\n",
    "### XGBoost Modelində Dəyişdirilə Bilən Əsas Hiperparametrlər:\n",
    "\n",
    "1. **`booster`:**\n",
    "   - **`gbtree`**: Standart qərar ağaclarına əsaslanan boosting (ən geniş yayılmış).\n",
    "   - **`gblinear`**: Qərar ağacları əvəzinə xətti modellər (linear boosters) istifadə edilir.\n",
    "   - **`dart`**: Dropouts təklif edir ki, bu da gbtree-də olan bəzi ağacları tərk edərək overfitting-i azalda bilər.\n",
    "\n",
    "   Yəni, XGBoost-da \"model\" əvəzinə, fərqli boosting metodlarını seçə bilərsiniz.\n",
    "\n",
    "2. **`max_depth`:**\n",
    "   - Hər bir ağacın maksimum dərinliyini təyin edir. Dərinlik nə qədər böyükdürsə, model o qədər kompleks və potensial olaraq overfitting riski böyükdür.\n",
    "\n",
    "3. **`learning_rate`:**\n",
    "   - Yeni ağacın verdiyi töhfənin ölçüsünü təyin edir. Aşağı dəyər daha stabillik gətirir, amma modelin öyrənmə sürətini azaldır.\n",
    "\n",
    "4. **`n_estimators`:**\n",
    "   - Ağacların ümumi sayını təyin edir. Daha çox ağac daha yaxşı performansa gətirib çıxara bilər, amma overfitting riski də artar.\n",
    "\n",
    "5. **`subsample`:**\n",
    "   - Hər bir ağac üçün istifadə edilən təlim məlumatlarının faizini təyin edir. Bu, modelin daha az məlumat üzərində öyrənməsinə və overfitting riskinin azalmasına kömək edir.\n",
    "\n",
    "6. **`colsample_bytree`, `colsample_bylevel`, `colsample_bynode`:**\n",
    "   - Hər bir ağac üçün (və ya səviyyə və ya düyün üçün) istifadə edilən atributların faizini təyin edir. Bu da overfitting riskini azaldır.\n",
    "\n",
    "7. **`gamma`:**\n",
    "   - Ağacların bölünməsinə nəzarət edən cərimələndirmə əmsalıdır. Yüksək dəyər daha az bölünmə və daha sadə modellərlə nəticələnir.\n",
    "\n",
    "### Yekun\n",
    "XGBoost-da da daxildə işləyən model dəyişdirilə bilməz, çünki o, Gradient Boosting-ə əsaslanan bir metoddur. Lakin siz modelin necə öyrəndiyini və performansını dəyişdirmək üçün müxtəlif hiperparametrləri tənzimləyə bilərsiniz. Məsələn, **`booster`** parametri ilə \"gbtree\", \"gblinear\", və \"dart\" arasında seçim edə bilərsiniz ki, bu da modelin davranışını xeyli dərəcədə dəyişə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c889867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7eee2b",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansinda daxili modeli deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13699f",
   "metadata": {},
   "source": [
    "Boosting modellərində ümumiyyətlə \"daxili model\" anlayışı yoxdur, çünki bu modellərin strukturu müəyyən bir əsas modelə, adətən qərar ağaclarına (decision trees), əsaslanır. Bu səbəbdən, boosting metodlarında daxili modeli dəyişdirmək mümkün olmur. \n",
    "\n",
    "### Əsas Boosting Modelləri və Daxili Model Seçimi:\n",
    "\n",
    "1. **XGBoost, LightGBM, və CatBoost:**\n",
    "   - Bu modellər qərar ağacları əsasında işləyir və istifadəçilər üçün daxili modelin (məsələn, qərar ağacları əvəzinə xətti modellər və ya başqa bir model) dəyişdirilməsi imkanı təqdim etmir.\n",
    "   - Yeganə istisna XGBoost-da **`booster`** parametridir, burada \"gbtree\" (qərar ağacları) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz. Lakin bu, daha çox modelin strukturu ilə əlaqədardır və tam mənada \"daxili model\" dəyişimi kimi qəbul edilmir.\n",
    "\n",
    "2. **AdaBoost:**\n",
    "   - AdaBoost-da istifadə olunan zəif modellər adətən qərar ağaclarıdır, lakin bu modelin təbiətinə görə zəif model olaraq fərqli algoritmlər (məsələn, xətti regresiya, SVM və ya başqa modellər) istifadə oluna bilər. Bu, AdaBoost-un digər boosting metodlarından fərqli olmasının əsas səbəbidir.\n",
    "   - Yəni, **AdaBoost** modelində istifadə olunan zəif modelləri dəyişdirmək mümkündür.\n",
    "\n",
    "### Nəticə\n",
    "Əgər daxili modeli dəyişmək imkanını istəyirsinizsə, **AdaBoost** istifadə etməlisiniz. AdaBoost-da zəif model olaraq fərqli algoritmlərdən istifadə edə bilərsiniz. Lakin digər məşhur boosting metodları olan XGBoost, LightGBM, və CatBoost-da daxili model dəyişdirilməsi mümkün deyil, çünki bunlar xüsusi olaraq qərar ağacları üzərində işləmək üçün dizayn olunub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eccd75",
   "metadata": {},
   "source": [
    "## Gradient Boosting-de daxili modeli deyismek olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b54636",
   "metadata": {},
   "source": [
    "Bəli, Gradient Boosting metodunda da daxili model dəyişdirilmir. Gradient Boosting, adətən qərar ağacları (decision trees) istifadə edərək işləyən bir boosting metodudur və bu, metodun əsas strukturunu təşkil edir. \n",
    "\n",
    "### Əsas Nöqtələr:\n",
    "- **Gradient Boosting** metodunun əsas ideyası, zəif modellərin (adətən qərar ağacları) ardıcıl olaraq qurulması və hər bir yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışmasıdır.\n",
    "- **Qərar ağacları** bu metodun əsasında durur və buna görə də daxili model olaraq dəyişdirilə bilməz.\n",
    "\n",
    "### İstisnalar və Seçimlər:\n",
    "- **XGBoost** və **LightGBM** kimi Gradient Boosting-in optimallaşdırılmış versiyaları da əsasən qərar ağacları üzərində işləyir. Bu modellərdə də daxili modeli dəyişdirmək mümkün deyil.\n",
    "- Yalnız **XGBoost** modelində `booster` parametri ilə \"gbtree\" (qərar ağacı) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz, amma bu tam olaraq daxili modelin dəyişdirilməsi deyil. Bu parametr sadəcə olaraq modelin xətti və ya ağac əsaslı olacağını təyin edir.\n",
    "\n",
    "### Nəticə:\n",
    "Gradient Boosting metodunda daxili model adətən dəyişdirilmir və əsas olaraq qərar ağacları istifadə olunur. Bu, metodun fundamental xüsusiyyətlərindən biridir və onun effektivliyini təmin edən əsas səbəblərdən biridir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938f93d4",
   "metadata": {},
   "source": [
    "## Robust Scale haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3a003",
   "metadata": {},
   "source": [
    "**Robust Scaler** məlumat dəstələrinin (datasets) miqyasını dəyişdirmək üçün istifadə olunan bir normalizasiya texnikasıdır. Bu metod ənənəvi miqyaslandırma üsulları (məsələn, Standart Scaler, Min-Max Scaler) ilə müqayisədə məlumat dəstələrindəki outlier-lərə (məlumatların ümumi tendensiyasından uzaq olan dəyərlərə) qarşı daha davamlıdır. \n",
    "\n",
    "### Robust Scaler-in Əsas Xüsusiyyətləri:\n",
    "\n",
    "1. **Median Və İQR Əsaslı Miqyaslandırma:**\n",
    "   - **Median**: Məlumat dəstəsinin ortasında olan dəyərdir və bu dəyər məlumat dəstəsindəki dəyərlərin yarısının bu dəyərdən az, yarısının isə çox olduğunu göstərir.\n",
    "   - **IQR (Interquartile Range)**: Məlumat dəstəsinin 25-ci və 75-ci percentilləri arasındakı fərqdir. Bu aralıq mərkəzləşmiş dəyərləri əhatə edir və outlier-lərdən daha az təsirlənir.\n",
    "   - Robust Scaler məlumat dəstəsini median ətrafında mərkəzləşdirir və dəyərləri IQR ilə miqyaslandırır. Yəni, hər bir dəyərdən median çıxılır və sonra IQR-ə bölünür:\n",
    "     $$ \n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{Median}(X)}{\\text{IQR}(X)}\n",
    "      $$\n",
    "   - Bu yanaşma məlumat dəstəsində outlier-lər olduğu halda belə, məlumatların düzgün miqyaslandırılmasını təmin edir.\n",
    "\n",
    "2. **Outlier-lərə Qarşı Dayanıqlıq:**\n",
    "   - Outlier-lər, məlumatların əksəriyyətinin uzaqında yerləşən ekstremal dəyərlərdir. Ənənəvi miqyaslandırma üsulları bu dəyərlərə həssas ola bilər və nəticədə miqyaslandırılmış dəyərlər qeyri-adekvat ola bilər.\n",
    "   - Robust Scaler, median və IQR istifadə etdiyi üçün outlier-lərin təsirini azaldır və məlumatların miqyaslandırılmasını daha sabit və dəqiq edir.\n",
    "\n",
    "3. **Tətbiq Sahələri:**\n",
    "   - **Maşın Öyrənməsi (Machine Learning):** Məlumat dəstələrinin outlier-lərlə zəngin olduğu vəziyyətlərdə xüsusən istifadə olunur. Bu, modellərin (məsələn, linear regression, SVM) performansını artırmağa kömək edə bilər.\n",
    "   - **Peyk Məlumatları:** Məsələn, peyk görüntüləri və ya uzaqdan zondlama (remote sensing) məlumatları çox sayda outlier-lərlə işləyə bilər.\n",
    "   - **Finans Məlumatları:** Finansal məlumatlarda outlier-lər tez-tez rast gəlinir. Robust Scaler bu məlumatları miqyaslandırmaq üçün əlverişlidir.\n",
    "\n",
    "### Nümunə: Python ilə Robust Scaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Random data with outliers\n",
    "data = np.array([[1, 2, 2], \n",
    "                 [3, 6, 4], \n",
    "                 [7, 18, 8], \n",
    "                 [9, 10, 12], \n",
    "                 [20, 50, 90]])\n",
    "\n",
    "# RobustScaler-in tətbiqi\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nScaled Data with RobustScaler:\\n\", scaled_data)\n",
    "```\n",
    "\n",
    "### Üstünlüklər:\n",
    "- **Outlier-lərə Dayanıqlıq:** Median və IQR istifadə edildiyi üçün məlumat dəstələrindəki outlier-lər bu miqyaslandırmaya az təsir edir.\n",
    "- **Daha Sabit Nəticələr:** Ənənəvi miqyaslandırma metodları ilə müqayisədə, outlier-lərin səbəb olduğu qeyri-stabilliklər azaldılır.\n",
    "\n",
    "### Çatışmazlıqlar:\n",
    "- **Küçük Məlumat Dəstələrində Problemlər:** Əgər məlumat dəstəsi çox kiçikdirsə və ya çox az sayda nümunədən ibarətdirsə, median və IQR düzgün hesablanmaya bilər.\n",
    "- **Yalnız Düzgün Paylanmış Məlumatlarda Effektivdir:** Məlumatların paylanışı çox qeyri-bərabərdirsə, Robust Scaler bəzən qeyri-adekvat miqyaslandırma apara bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Robust Scaler outlier-lərlə zəngin olan məlumat dəstələrinin miqyaslandırılması üçün ideal bir vasitədir. Median və IQR kimi statistik ölçülərdən istifadə edərək məlumatları miqyaslandırmaqla, bu metod outlier-lərə qarşı daha sabit və dəqiq nəticələr verir. Bu səbəbdən, Robust Scaler maşın öyrənməsi tətbiqlərində və digər məlumat təhlili proseslərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a586bc43",
   "metadata": {},
   "source": [
    "## Feature Importance Riyaziyyatda formulu? Hesablanma qaydasi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebb069",
   "metadata": {},
   "source": [
    "Decision Tree modellərində **feature importance** hər bir xüsusiyyətin (feature) modeldə nə qədər vacib olduğunu göstərir. Bu, xüsusiyyətin ağacın qurulmasında (tree-building process) nə qədər təsirli olduğunu qiymətləndirir. Decision Tree-də **feature importance** ümumiyyətlə impurity-based importance metodu ilə hesablanır.\n",
    "\n",
    "### Feature Importance Hesablanması\n",
    "\n",
    "1. **Impurity Measurement (Gini impurity, Entropy):** \n",
    "   - Hər bir düyündə (node) impurity (məsələn, Gini impurity, Entropy) hesablanır. Impurity, həmin düyündəki qarışıqlığın ölçüsüdür.\n",
    "\n",
    "2. **Impurity Reduction (Information Gain):**\n",
    "   - Hər bir xüsusiyyət üçün, həmin xüsusiyyətə əsaslanaraq ağacın düyünü bölündükdən sonra impurity azalması hesablanır. Bu azalma həmin xüsusiyyətin nə qədər məlumat verdiyini (information gain) göstərir.\n",
    "   \n",
    "   - Impurity reduction = impurity_before_split - impurity_after_split\n",
    "\n",
    "3. **Cumulative Importance:**\n",
    "   - Hər bir xüsusiyyət üçün, ağacın müxtəlif düyünlərindəki impurity reduction-lar toplanır. Bu, həmin xüsusiyyətin ümumi əhəmiyyətini (total importance) göstərir.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - Hər bir xüsusiyyətin əhəmiyyəti normallaşdırılır (0 ilə 1 arasında) və nəticədə hər bir xüsusiyyətin final feature importance dəyəri əldə edilir.\n",
    "\n",
    "### Riyazi Formula\n",
    "\n",
    "Feature importance $  FI_j  $ hər bir xüsusiyyət $ j $ üçün aşağıdakı kimi hesablanır:\n",
    "\n",
    "$$ \n",
    "FI_j = \\sum_{t=1}^{T} \\Delta i_t \\cdot 1 \\{v(t) = j\\}\n",
    " $$\n",
    "\n",
    "burada:\n",
    "- $  \\Delta i_t  $ - düyün $ t $-də impurity reduction-dur (impurity_before_split - impurity_after_split).\n",
    "- $ 1 \\{v(t) = j\\}  $ - düyün $ t $-nin xüsusiyyət $ j $-yə aid olub-olmamasını göstərən göstərici funksiyasıdır.\n",
    "- $ T $ - ümumi düyünlərin sayıdır.\n",
    "\n",
    "Bu formula ağacın bütün düyünlərində xüsusiyyətin nə qədər məlumat verdiyini toplamaqla xüsusiyyətin ümumi əhəmiyyətini hesablayır. Nəticədə, daha çox informasiya verən xüsusiyyətlərin əhəmiyyəti yüksək olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e04d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a46511d",
   "metadata": {},
   "source": [
    "* scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "* X_train = scaler.transform(X_train)\n",
    "* X_test = scaler.transform(X_test)\n",
    "\n",
    "### Muellif scale -i bu formada edib. Niye X_test-i de X_traine gore scale edib? Umimiyyetle  train test splitden evvel ya sonra yaxsidir scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cc696",
   "metadata": {},
   "source": [
    "`StandardScaler` və digər skalarlardan istifadə edərkən `train` və `test` setlərinin uyğunlaşdırılması (scaling) çox önəmlidir. Niyə `X_test` də `X_train`-ə görə standartlaşdırılır və bu prosesin ən doğru vaxtı haqqında izahat verəcəyəm.\n",
    "\n",
    "### Niyə `X_test` `X_train`-ə görə standartlaşdırılır?\n",
    "\n",
    "1. **Məlumat Sızması (Data Leakage) Əngəllənməsi:**\n",
    "   - **Qarşısını Almaq:** Test məlumatlarına birbaşa müdaxilə etmək modelin qiymətləndirilməsini səhv edə bilər. Test məlumatlarına əvvəlcədən baxmaq, modelin test dəstinə uyğunlaşmasına və nəticədə daha yaxşı nəticələr əldə etməsinə səbəb ola bilər. Bu, məlumat sızması (data leakage) olaraq adlandırılır.\n",
    "   - **Nə Üçün:** Test setini `X_train`-ə görə standartlaşdırmaq, test məlumatlarının yalnız təlim məlumatları əsasında miqyaslandırılması və beləliklə, test dəstinin düzgün qiymətləndirilməsi təmin edilir.\n",
    "\n",
    "2. **Hesablama Uyğunluğu:**\n",
    "   - **Standartlaşdırma:** Təlim məlumatları (`X_train`) üzərində fit edilmiş `StandardScaler`, müvafiq olaraq bu məlumatların ortalaması və standart sapması ilə `X_train` və `X_test`-i miqyaslandırır.\n",
    "   - **Nə Üçün:** Bu, modelin test məlumatlarının dəqiq və obyektiv qiymətləndirilməsi üçün lazımdır. `X_test`-i `X_train`-ə görə miqyaslandırmaq, test dəstinin eyni skala ilə işlənməsini təmin edir.\n",
    "\n",
    "### Train Test Splitdən Əvvəl Yoxsa Sonra Scale Etmək?\n",
    "\n",
    "- **Testdən Sonra:** Test və təlim məlumatlarını ayırdıqdan sonra, `StandardScaler` fit edildikdə, yalnız təlim məlumatlarına əsaslanaraq fit edilir və sonra bu transformasiyanı test məlumatlarına tətbiq etmək məsləhətdir. Bu, test məlumatlarının təlim məlumatlarının xüsusiyyətlərindən istifadə etmədən miqyaslandırılmasını təmin edir.\n",
    "\n",
    "### Məsələn\n",
    "\n",
    "**Train Test Splitdən Sonra Skalarlama:**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DataFrame və target dəyişənini ayırmaq\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler obyektini yaratmaq və fit etmək\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### Əsas Məqamlar\n",
    "\n",
    "1. **Train Test Splitdən Sonra Skalarlama:**\n",
    "   - `StandardScaler`-in `fit()` metodu yalnız `X_train` üzərində çalışır.\n",
    "   - `transform()` metodu həm `X_train`, həm də `X_test` üzərində tətbiq olunur.\n",
    "\n",
    "2. **Test Seti Üzərində Fit Etmək:**\n",
    "   - Test setinə fit edilməməlidir, çünki bu, test məlumatlarının məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "Bu yanaşma test dəstinin düzgün qiymətləndirilməsini təmin edir və modelin yalnız təlim məlumatlarına əsaslanaraq necə performans göstərdiyini daha doğru qiymətləndirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a1f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f552a7",
   "metadata": {},
   "source": [
    "## RobustScaler haqqinda genis melumat ver?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bc9aa",
   "metadata": {},
   "source": [
    "`RobustScaler`, xüsusilə outlier-lərlə zəngin məlumat dəstləri üçün nəzərdə tutulmuş bir miqyaslandırma metodudur. Bu metod, xüsusiyyətləri `median` və `interquartile range` (IQR) əsasında standartlaşdıraraq daha dayanıqlı və etibarlı nəticələr əldə etməyə imkan verir.\n",
    "\n",
    "### `RobustScaler` Nədir?\n",
    "\n",
    "`RobustScaler`, `sklearn.preprocessing` kitabxanasında olan bir vasitədir. Bu metod, xüsusiyyətləri aşağıdakı addımlar ilə miqyaslandırır:\n",
    "\n",
    "1. **Median ilə Mərkəzləşdirmə:**\n",
    "   - **Median**: Hər bir xüsusiyyət üçün orta dəyəri çıxarmaq məqsədilə istifadə olunur. Median outlier-lərdən təsirlənməz, çünki o, 50%-lik kvantildir və outlier-lərin təsirini azaltmağa kömək edir.\n",
    "\n",
    "2. **Interquartile Range (IQR) ilə Miqyaslandırma:**\n",
    "   - **IQR**: Bu metod, 25%-lik (Q1) və 75%-lik (Q3) kvantil arasındakı məsafəni ölçür. Bu məsafə xüsusiyyətlərin yayılmasını ölçmək üçün istifadə edilir və outlier-lərin təsirini azaltmaq məqsədilə miqyaslandırma prosesində tətbiq olunur.\n",
    "\n",
    "### Formula\n",
    "\n",
    "`RobustScaler`-in tətbiq etdiyi standartlaşdırma aşağıdakı kimi təyin olunur:\n",
    "\n",
    "- **Mərkəzləşdirmə (Mean subtraction):**\n",
    "  \n",
    "  \\[\n",
    "  X_{\\text{centered}} = X - \\text{Median}\n",
    "  \\]\n",
    "\n",
    "- **Miqyaslandırma (Scaling):**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{scaled}} = \\frac{X_{\\text{centered}}}{\\text{IQR}}\n",
    "  \\]\n",
    "\n",
    "  Burada:\n",
    "  - \\( X \\) - Xüsusiyyətlərin dəyərləri\n",
    "  - **Median** - Xüsusiyyətlərin medianı\n",
    "  - **IQR** - Interquartile range (Q3 - Q1)\n",
    "\n",
    "### İstifadə Misalı\n",
    "\n",
    "`RobustScaler`-dən istifadə edərkən adətən aşağıdakı addımlar izlənir:\n",
    "\n",
    "1. **DataFrame və target dəyişənini ayırmaq:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "   # Məsələn, DataFrame\n",
    "   data = {\n",
    "       'feature1': [1, 2, 3, 1000],  # Outlier daxil olan xüsusiyyətlər\n",
    "       'feature2': [4, 5, 6, 7]\n",
    "   }\n",
    "   df = pd.DataFrame(data)\n",
    "   X = df[['feature1', 'feature2']]\n",
    "   y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "   # Train və Test dəstini ayırmaq\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **`RobustScaler`-i tətbiq etmək:**\n",
    "\n",
    "   ```python\n",
    "   # RobustScaler obyektini yaratmaq və fit etmək\n",
    "   scaler = RobustScaler()\n",
    "   scaler.fit(X_train)\n",
    "\n",
    "   # Təlim və test məlumatlarını transformasiya etmək\n",
    "   X_train_scaled = scaler.transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "   # Dəyişdirilmiş məlumatları çap etmək\n",
    "   print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "   print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "   ```\n",
    "\n",
    "### Nə Zaman İstifadə Edilməlidir?\n",
    "\n",
    "- **Outlier-lərlə Mübarizə:** `RobustScaler` xüsusilə outlier-lərin olduğu məlumat dəstləri üçün uyğundur, çünki median və IQR outlier-lərdən təsirlənmir.\n",
    "- **Model Performansı:** Outlier-lərdən təsirlənməyən daha düzgün nəticələr və daha stabil model performansı təmin edir.\n",
    "\n",
    "### Müqayisə\n",
    "\n",
    "- **`StandardScaler` vs. `RobustScaler`:**\n",
    "  - `StandardScaler`-in istifadə etdiyi ortalama və standart sapma outlier-lərlə təsirlənə bilər, amma `RobustScaler`-in istifadə etdiyi median və IQR outlier-lərdən daha dayanıqlıdır.\n",
    "  - `StandardScaler` daha çox \"normal\" paylanma olan məlumat dəstləri üçün uyğundur, `RobustScaler` isə outlier-lərin olduğu hallarda daha uyğundur.\n",
    "\n",
    "Bu şəkildə, `RobustScaler` xüsusiyyətlərin daha dayanıqlı şəkildə standartlaşdırılmasını təmin edir və xüsusiyyətlərdəki outlier-lərin təsirini azaltmağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9397e4e4",
   "metadata": {},
   "source": [
    "## RobustScaler de train test split den sonra olmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7fc8f",
   "metadata": {},
   "source": [
    "Bəli, `RobustScaler` və digər miqyaslandırma metodları train-test splitdən sonra tətbiq olunmalıdır. Bu, məlumat sızmasının (data leakage) qarşısını almaq və modelin təlim və test mərhələlərində düzgün qiymətləndirilməsini təmin etmək üçün vacibdir.\n",
    "\n",
    "### Niyə Train-Test Splitdən Sonra Scalarlama?\n",
    "\n",
    "1. **Məlumat Sızmasının Qarşısını Almaq:**\n",
    "   - **Niyə:** Test məlumatlarına birbaşa təsir etmədən yalnız təlim məlumatlarından miqyaslandırma parametrləri (median və IQR) əldə etmək lazımdır. Bu, test dəstinin yalnız təlim məlumatları əsasında miqyaslandırıldığını təmin edir.\n",
    "   - **Nəticə:** Test məlumatlarına əvvəlcədən baxmaq və ya onları miqyaslandırmaq modelin nəticələrini təhrif edə bilər və məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "2. **Miqyaslandırma Parametrlərinin Düzgün Tətbiqi:**\n",
    "   - **Təlim Məlumatları:** Miqyaslandırma parametrləri yalnız təlim məlumatlarına əsaslanaraq fit edilir.\n",
    "   - **Test Məlumatları:** Eyni parametrlər (mediana və IQR) test məlumatlarına tətbiq edilir.\n",
    "\n",
    "### Misal Kodu\n",
    "\n",
    "**1.** **Train-Test Splitdən Sonra Miqyaslandırma:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Nümunə DataFrame\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 1000],  # Outlier olan xüsusiyyət\n",
    "    'feature2': [4, 5, 6, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RobustScaler obyektini yaratmaq və fit etmək\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)  # Yalnız X_train üzərində fit edilir\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dəyişdirilmiş məlumatları çap etmək\n",
    "print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "```\n",
    "\n",
    "**2.** **Nümunənin İslahatı:**\n",
    "\n",
    "- **DataFrame Yaratma:** Nümunə məlumatlar yaradılır.\n",
    "- **Train-Test Split:** `X` və `y` məlumatları train və test dəstlərinə bölünür.\n",
    "- **`RobustScaler` İstifadəsi:** `RobustScaler` yalnız təlim dəstində fit edilir, sonra həm təlim, həm də test dəstinə tətbiq edilir.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "`RobustScaler` və digər miqyaslandırma üsulları üçün düzgün təcrübə train-test splitdən sonra miqyaslandırma tətbiq etməkdir. Bu yanaşma, məlumat sızmasının qarşısını alır və modelin daha düzgün qiymətləndirilməsini təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15586d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61de99c",
   "metadata": {},
   "source": [
    "## Bagging parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5a4a",
   "metadata": {},
   "source": [
    "* bag = BaggingClassifier(estimator=svm,\n",
    "*                        n_estimators=10,\n",
    "*                        max_samples=0.4,\n",
    "*                        max_features=35,\n",
    "*                        bootstrap=True,\n",
    "*                        bootstrap_features=True,\n",
    "*                        random_state=123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61254f",
   "metadata": {},
   "source": [
    "`BaggingClassifier` parametrleri aşağıda izah olunur:\n",
    "\n",
    "1. **`estimator=svm`**: Burada `svm`, bagging ilə istifadə olunan əsas modelin növünü göstərir. SVM (Support Vector Machine) burada əsas model kimi istifadə olunur. `BaggingClassifier`, bu SVM modellərindən bir neçə nüsxə yaradaraq onları birləşdirir.\n",
    "\n",
    "2. **`n_estimators=10`**: Bu parametr, neçə dənə baza modelinin (bu halda SVM) yaradılacağını göstərir. `n_estimators=10` təyin edildikdə, `BaggingClassifier` 10 fərqli SVM modeli qurur və onları birləşdirir.\n",
    "\n",
    "3. **`max_samples=0.4`**: Bu parametr, hər bir baza modelinin təlimi üçün orijinal məlumat dəstindən seçilən nümunələrin faizi və ya sayı ilə bağlıdır. `max_samples=0.4` olduqda, hər baza modeli üçün orijinal məlumat dəstinin 40%-i təlim üçün istifadə olunur. Bu seçim prosesi \"bootstrapping\" metodu ilə həyata keçirilir.\n",
    "\n",
    "4. **`max_features=35`**: Bu parametr, hər bir baza modelinin qurulmasında istifadə olunan xüsusiyyətlərin maksimum sayını göstərir. `max_features=35` təyin edildikdə, hər baza modelinin təlimində 35 xüsusiyyət seçilir.\n",
    "\n",
    "5. **`bootstrap=True`**: Bu parametr, nümunələrin \"bootstrapping\" (yəni, geri qoyaraq nümunə götürmə) metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap=True` olduqda, hər bir baza modeli üçün təlim nümunələri geri qoyularaq seçilir, yəni eyni nümunə bir neçə dəfə seçilə bilər.\n",
    "\n",
    "6. **`bootstrap_features=True`**: Bu parametr, xüsusiyyətlərin \"bootstrapping\" metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap_features=True` olduqda, hər bir baza modeli üçün xüsusiyyətlər geri qoyularaq seçilir.\n",
    "\n",
    "7. **`random_state=123`**: Bu parametr, nəticələrin təkrarlanabilirliyini təmin etmək üçün istifadə olunur. `random_state=123` təyin edildikdə, modelin təsadüfi seçimləri eyni şəkildə həyata keçirilir və nəticələr hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `BaggingClassifier` modelinin müxtəlif aspektlərini nəzarət edir və modelin qurulması, nümunə götürülməsi və xüsusiyyətlərin seçilməsi prosesi üzərində tam nəzarət təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafb507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bfaed9",
   "metadata": {},
   "source": [
    "## XGBoost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31aee",
   "metadata": {},
   "source": [
    "* xgbt = XGBClassifier(max_depth = 2,\n",
    "*              learning_rate = 0.2,\n",
    "*              objective  = \"multi:softmax\",\n",
    "*              num_class = 2,\n",
    "*              booster = \"gbtree\",\n",
    "*              n_estimators = 10,\n",
    "*              random_state = 123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16db875",
   "metadata": {},
   "source": [
    "`XGBClassifier` modelinin parametrləri aşağıda izah olunur:\n",
    "\n",
    "1. **`max_depth = 2`**:\n",
    "   - Bu parametr hər bir fərdi ağacın maksimal dərinliyini təyin edir. `max_depth = 2` təyin edildikdə, hər ağac ən çox 2 dərinlik səviyyəsinə malik olacaq. Daha kiçik dərinlik overfitting-i azaldır, lakin çox kiçik olarsa, model kifayət qədər mürəkkəbliyi tuta bilməz.\n",
    "\n",
    "2. **`learning_rate = 0.2`**:\n",
    "   - Bu parametr, hər ağacın öyrənmə sürətini təyin edir. `learning_rate = 0.2` olduqda, hər bir ağacın modelə töhfəsi 0.2 faktorla azalır. Kiçik learning rate daha dəqiq və stabil model təlimi verir, amma daha çox iterasiya tələb edir.\n",
    "\n",
    "3. **`objective = \"multi:softmax\"`**:\n",
    "   - Bu parametr modelin məqsədini təyin edir. `multi:softmax` təyin edildikdə, model çoxlu sinifli təsnifat (multi-class classification) üçün istifadə edilir və proqnozlaşdırılan dəyər ən yüksək ehtimala malik sinif kimi verilir.\n",
    "\n",
    "4. **`num_class = 2`**:\n",
    "   - Bu parametr, modelin neçə sinif olduğunu təyin edir. `num_class = 2` təyin edildikdə, model iki sinifli (binary) təsnifat məsələsini həll edir. (Qeyd: `multi:softmax` ilə birlikdə istifadə olunduqda, bu parametr faktiki olaraq daha çox sinif sayı üçün istifadə olunur.)\n",
    "\n",
    "5. **`booster = \"gbtree\"`**:\n",
    "   - Bu parametr, modelin hansı boosting algoritmini istifadə edəcəyini təyin edir. `gbtree` təyin edildikdə, model qərar ağacları ilə qurulur. Alternativ olaraq `gblinear` (linear model) və ya `dart` (Dropouts meet Multiple Additive Regression Trees) kimi digər algoritmlər də istifadə edilə bilər.\n",
    "\n",
    "6. **`n_estimators = 10`**:\n",
    "   - Bu parametr, modeldə neçə dənə ağacın qurulacağını göstərir. `n_estimators = 10` təyin edildikdə, model 10 ağac qurur. Daha çox ağac modelin dəqiqliyini artıra bilər, lakin eyni zamanda təlim müddətini uzada bilər.\n",
    "\n",
    "7. **`random_state = 123`**:\n",
    "   - Bu parametr, təsadüfi proseslər üçün toxum (seed) təyin edir ki, nəticələr təkrarlanabilsin. `random_state = 123` təyin edildikdə, modelin nəticələri hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `XGBClassifier` modelinin qurulması və işləməsi zamanı onun davranışını təyin edir və modelin düzgün performans göstərməsi üçün müxtəlif aspektləri optimallaşdırmağa imkan verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5475dc",
   "metadata": {},
   "source": [
    "### XGBoots-da \n",
    "* scale_pos_weight=[default=1]\n",
    "### ???????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c25fe",
   "metadata": {},
   "source": [
    "`scale_pos_weight` parametri `XGBoost` modelində balanssız verilənlər dəstlərində (yəni, bir sinifin digərindən çox üstün olduğu hallarda) istifadə olunur. Bu parametr siniflər arasındakı balanssızlığı tənzimləmək və modelin daha yaxşı performans göstərməsini təmin etmək üçün istifadə edilir.\n",
    "\n",
    "### `scale_pos_weight` Parametrinin İzahı\n",
    "\n",
    "- **Məqsəd:** Əgər verilənlər dəstində siniflərdən biri (məsələn, pozitiv sinif) çox az sayda nümunə ilə təmsil olunursa, `scale_pos_weight` parametri bu sinifə daha çox əhəmiyyət vermək üçün istifadə olunur.\n",
    "- **Dəyər:** `scale_pos_weight = nöqativ_sinif_nümunələri / pozitiv_sinif_nümunələri` şəklində təyin edilir.\n",
    "\n",
    "### Necə İşləyir?\n",
    "\n",
    "- **Balanssızlıq Problemi:** Əgər siniflər arasında balanssızlıq varsa, model çoxluqda olan sinifi daha çox öyrənir və bu da nadir sinifin düzgün proqnozlaşdırılmasına mane ola bilər.\n",
    "- **`scale_pos_weight` Parametrinin Təyini:** Bu parametr, azlıqda olan sinifin təsirini artırmaq üçün istifadə olunur. Məsələn, pozitiv sinifdə 100 nümunə, neqativ sinifdə isə 1000 nümunə varsa, `scale_pos_weight = 1000/100 = 10` təyin etmək olar. Bu, modelin azlıqda olan sinifə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Misal:\n",
    "\n",
    "```python\n",
    "xgbt = XGBClassifier(max_depth=2,\n",
    "                     learning_rate=0.2,\n",
    "                     objective=\"binary:logistic\",\n",
    "                     n_estimators=10,\n",
    "                     scale_pos_weight=10,  # Siniflər arasındakı balanssızlıq nəzərə alınır\n",
    "                     random_state=123)\n",
    "```\n",
    "\n",
    "Burada `scale_pos_weight=10` təyin edilib. Bu, modelin pozitiv sinifə neqativ sinifdən 10 dəfə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Nəticə\n",
    "`scale_pos_weight` parametri balanssız verilənlər dəstlərində siniflər arasındakı balansı tənzimləyərək, modelin azlıqda olan sinifi düzgün proqnozlaşdırmasını təmin etmək üçün istifadə edilir. Bu parametr xüsusilə tibbi diaqnostika, fırıldaqçılığın aşkarlanması kimi sahələrdə faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e67694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0befd682",
   "metadata": {},
   "source": [
    "## Datani Understanding etmek ucun nelere baxim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b969",
   "metadata": {},
   "source": [
    "Bu məlumat dəstini yaxşı anlamaq üçün aşağıdakı addımları ata bilərsiniz:\n",
    "\n",
    "### 1. **Veri Tipini Yoxlayın**\n",
    "   - **Hər bir sütunun** (dəyişənin) tipini yoxlayın (məsələn, integer, float, object). Bu, hər bir dəyişənin necə işlənməli olduğunu anlamağa kömək edəcək. Məsələn, `LoanAmount` və `ApplicantIncome` sayısal dəyərlərdir, amma `Gender`, `Married` və `Loan_Status` kimi dəyişənlər kateqoriyalardır.\n",
    "\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "\n",
    "### 2. **Məlumatların Yoxluğu (Missing Data)**\n",
    "   - Məlumat dəstində boş dəyərlərin olub-olmadığını yoxlayın. Bu, model qurmaq üçün məlumatların keyfiyyətini artırmağa kömək edəcək. Boş dəyərlər varsa, onları doldurma (imputation) və ya silmə metodları tətbiq edə bilərsiniz.\n",
    "\n",
    "   ```python\n",
    "   df.isnull().sum()\n",
    "   ```\n",
    "\n",
    "### 3. **Məlumatların Dağılımı**\n",
    "   - Sayısal dəyişənlərin (məsələn, `ApplicantIncome`, `LoanAmount`) və kateqoriyal dəyişənlərin (`Gender`, `Education`) dağılımını yoxlayın. Bu, məlumatların normal paylanıb-paylanmadığını və ya bəzi dəyişənlərdə outlier-ların olub-olmadığını anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.describe()\n",
    "   df['ApplicantIncome'].hist()\n",
    "   ```\n",
    "\n",
    "### 4. **Kateqoriyal Dəyərlərin Paylanması**\n",
    "   - Kateqoriyal dəyişənlərin (məsələn, `Gender`, `Married`, `Education`) müxtəlif siniflər üzrə paylanmasını yoxlayın. Bu, məlumat dəstində hansı siniflərin dominant olduğunu və modelin necə performans göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df['Gender'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 5. **Hədəf Dəyişən (Target Variable)**\n",
    "   - `Loan_Status` dəyişəninin paylanmasını yoxlayın. Hədəf dəyişənin balanslı və ya balanssız olub-olmadığını bilmək vacibdir. Balanssızlıq varsa, xüsusi metodlar tətbiq etmək lazım ola bilər (məsələn, undersampling, oversampling).\n",
    "\n",
    "   ```python\n",
    "   df['Loan_Status'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 6. **Məlumatların Əlaqəsi (Correlation)**\n",
    "   - Sayısal dəyişənlər arasındakı əlaqəni yoxlayın. Bu, hansı dəyişənlərin bir-biri ilə daha çox əlaqəli olduğunu və hansı dəyişənlərin modelə daha çox təsir göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.corr()\n",
    "   ```\n",
    "\n",
    "### 7. **Pivotal Dəyişənlərin Təsiri**\n",
    "   - Hədəf dəyişən (`Loan_Status`) ilə digər dəyişənlər arasındakı əlaqəni yoxlayın. Məsələn, `ApplicantIncome`, `Credit_History` və `Education` dəyişənlərinin kredit təsdiqinə necə təsir etdiyini araşdırın.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "   ```\n",
    "\n",
    "### 8. **Outlier-ları Yoxlamaq**\n",
    "   - Sayısal dəyişənlərdə outlier-ları yoxlayın və onları necə idarə edəcəyinizi qərar verin. Outlier-lar modeli yanıltmaq riski daşıya bilər.\n",
    "\n",
    "   ```python\n",
    "   df.boxplot(column='LoanAmount')\n",
    "   ```\n",
    "\n",
    "Bu addımları yerinə yetirdikdən sonra məlumat dəstini daha yaxşı anlayacaq və model qurmağa başlamaq üçün yaxşı bir zəmin yaradacaqsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b084870e",
   "metadata": {},
   "source": [
    "## Datani Preparation usullari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564113cf",
   "metadata": {},
   "source": [
    "Loan eligibility prediction üçün data preparation mərhələsi çox vacibdir və müvəffəqiyyətli modellər qurmaq üçün düzgün hazırlıq tələb olunur. Aşağıdakı addımlar bu prosesdə sizə kömək edə bilər:\n",
    "\n",
    "### 1. **Veri Yükləmə və İnspeksiya**\n",
    "   - Veriləri yükləyin və ilk baxışda statistik göstəricilərə baxın.\n",
    "   - Datalarda hər hansı bir boş (missing) dəyərin olub olmadığını yoxlayın.\n",
    "\n",
    "### 2. **Boş Dəyərlərin İdarə Edilməsi**\n",
    "   - Boş dəyərləri necə idarə edəcəyinizə qərar verin. Ən çox istifadə olunan metodlar:\n",
    "     - **İlkin Təsnifat**: Boş dəyərləri təyin olunmuş bir dəyər ilə əvəzləmək (məsələn, median, mod, və s.)\n",
    "     - **Sıxlıqla Təyin Edilməsi**: Boş dəyərləri sıxlıqla təyin edilmiş dəyərlərlə əvəzləmək.\n",
    "\n",
    "### 3. **Kateqoriyalı Dəyərlərin Kodlaşdırılması**\n",
    "   - Kateqoriyalı dəyişənləri (Gender, Married, Education, Self_Employed, Property_Area) rəqəmlə ifadə etməlisiniz.\n",
    "     - **Label Encoding**: Hər bir kateqoriyanı təmsil edən bir rəqəm verilir.\n",
    "     - **One-Hot Encoding**: Hər bir kateqoriya üçün ayrılıqda sütun yaradılır və müvafiq sütunda 1, digərlərində isə 0 olur.\n",
    "\n",
    "### 4. **Miqdar Dəyərlərin Miqyaslandırılması**\n",
    "   - **Standardization**: Orta və standart sapma əsasında dəyərlərin normallaşdırılması.\n",
    "   - **Normalization**: Dəyərlərin [0, 1] intervalına gətirilməsi.\n",
    "\n",
    "### 5. **Boş Dəyərlərin Aşağı Səviyyədə Dəyərləndirilməsi**\n",
    "   - Verilərin doğruluğunu və əlaqəliliyini artırmaq üçün boş dəyərləri analiz edin.\n",
    "\n",
    "### 6. **Feature Engineering**\n",
    "   - **Yeni Dəyişənlərin Yaradılması**: Yeni dəyərləri yaratmaq üçün mövcud dəyişənləri birləşdirə bilərsiniz (məsələn, **TotalIncome** = **ApplicantIncome** + **CoapplicantIncome**).\n",
    "   - **Dəyişənlərin Seçilməsi**: Modeliniz üçün ən əhəmiyyətli olan dəyişənləri seçin.\n",
    "\n",
    "### 7. **Outlierların İdarə Edilməsi**\n",
    "   - **Outlier Analizi**: Outlierları aşkar edin və onların təsirini azaltmaq üçün uyğun üsullar tətbiq edin.\n",
    "\n",
    "### 8. **Target Variableın İdarə Edilməsi**\n",
    "   - **Label Encoding**: Kreditin təsdiqi dəyişənini (Loan_Status) 0 və 1 ilə təmsil edin.\n",
    "\n",
    "### 9. **Data Split**\n",
    "   - **Train-Test Split**: Veriləri təlim (training) və test (testing) qruplarına ayırın.\n",
    "\n",
    "### 10. **Cross-validation**\n",
    "   - Modelin gücünü qiymətləndirmək üçün cross-validation tətbiq edin.\n",
    "\n",
    "### 11. **Feature Importance Analizi**\n",
    "   - **Feature Importance**: Hansı xüsusiyyətlərin model üçün daha əhəmiyyətli olduğunu müəyyən edin və nəticələri nəzərə alın.\n",
    "\n",
    "Bu addımları izləyərək data preparation mərhələsini uğurla tamamlayaraq modelinizi hazırlamağa başlaya bilərsiniz. Ehtiyacınıza uyğun addımları tətbiq edərək nəticələrinizi inkişaf etdirə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab60505b",
   "metadata": {},
   "source": [
    "## Men burda bezi data preparation isleri gormisem.  ne yaddan cixib elemimisem sen onlari izah ele ve kodu yaz. Yani sonra men model quracam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d7cd",
   "metadata": {},
   "source": [
    "Mən notebook-dan aşağıdakı data preparation addımlarını aşkar etdim. Gördüyünüz işlər bunlardır:\n",
    "\n",
    "### 1. **Verilənlərin Yüklənməsi**\n",
    "   İlk addım olaraq, verilənlər `loan_prediction.csv` faylından yüklənib və `df` DataFrame-i yaradılıb.\n",
    "\n",
    "### 2. **Sütun Adlarının və Məlumatın Araşdırılması**\n",
    "   `df.columns` və `df.info()` metodları istifadə edilərək verilənlərin sütun adları və strukturu araşdırılıb.\n",
    "\n",
    "### 3. **Verilənlərin İzlənməsi**\n",
    "   - **Dəyərlərin sayılması**: `Dependents` və `Loan_Status` sütunlarında dəyərlərin sayı yoxlanılıb (`value_counts`).\n",
    "   - **Balansın yoxlanılması**: `Loan_Status` sütununda balansın olub-olmadığı yoxlanılıb. Görünür, 192/612 və 422/612 hesabatları göstərir ki, balans problemi yoxdur.\n",
    "\n",
    "### 4. **Boş Dəyərlərin Yoxlanılması və Doldurulması**\n",
    "   - **Boş Dəyərlərin Araşdırılması**: `df.isnull().sum()` metodu ilə dataset-də boş dəyərlər yoxlanılıb.\n",
    "   - **Boş Dəyərlərin Doldurulması**: Aşağıdakı sütunlarda boş dəyərlər doldurulub:\n",
    "     - `Gender`: Mod (ən çox təkrarlanan) dəyər ilə doldurulub.\n",
    "     - `Married`, `Dependents`, `Self_Employed`, `Credit_History`: Bu sütunlar da mod dəyəri ilə doldurulub.\n",
    "\n",
    "### 5. **Hələ Yadınızdan Çıxmış Ola Bilən Data Preparation Addımları**\n",
    "   - **Outlier Detection**: Məsələn, `ApplicantIncome` və `LoanAmount` üçün outlier-lərin yoxlanılması və ya uyğunlaşdırılması (log transformasiya və s.).\n",
    "   - **Sütunların Normalizasiyası/Standartizasiyası**: `ApplicantIncome`, `LoanAmount` kimi numeric dəyişənlər üçün normalizasiya və ya standartizasiya aparıla bilər.\n",
    "   - **Categorical Encoding**: Kategorik dəyişənlər (`Gender`, `Married`, `Education`, `Property_Area`, və s.) üçün one-hot encoding və ya label encoding tətbiq oluna bilər.\n",
    "   - **Feature Engineering**: Yeni dəyişənlərin yaradılması, məsələn, `TotalIncome` (müraciət edən və birgə müraciət edənin gəliri toplamı).\n",
    "\n",
    "Bu addımları tətbiq etmək üçün sizə uyğun kodları təqdim edə bilərəm. Hansı xüsusi addımlar haqqında məlumat əldə etmək istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133173aa",
   "metadata": {},
   "source": [
    "Aşağıda, qeyd etdiyiniz dataya əlavə olaraq həyata keçirə biləcəyiniz data preparation addımlarını təqdim edirəm:\n",
    "\n",
    "### 1. **Outlier-ların Yoxlanılması və Müxtəlifləşdirilməsi**\n",
    "Numeric dəyişənlər (`ApplicantIncome`, `LoanAmount`) üçün outlier-ların olub-olmadığını yoxlayıb, ehtiyac varsa log transformasiya tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ApplicantIncome üçün outlier-ların yoxlanılması\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['ApplicantIncome'])\n",
    "plt.title('ApplicantIncome Outliers')\n",
    "\n",
    "# Log transformasiya ilə outlier-ların uyğunlaşdırılması\n",
    "df['Log_ApplicantIncome'] = np.log1p(df['ApplicantIncome'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_ApplicantIncome'])\n",
    "plt.title('Log Transformed ApplicantIncome')\n",
    "plt.show()\n",
    "\n",
    "# LoanAmount üçün eyni proseduru həyata keçirə bilərsiniz\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)  # Boş dəyərləri median ilə doldurmaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['LoanAmount'])\n",
    "plt.title('LoanAmount Outliers')\n",
    "\n",
    "df['Log_LoanAmount'] = np.log1p(df['LoanAmount'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_LoanAmount'])\n",
    "plt.title('Log Transformed LoanAmount')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. **Categorical Encoding (One-Hot Encoding)**\n",
    "Kategorik dəyişənlər üçün one-hot encoding tətbiq edərək, onları model üçün istifadə edə biləcəyiniz numerik formaya çevirmək.\n",
    "\n",
    "```python\n",
    "# One-hot encoding tətbiq etmək\n",
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents'], drop_first=True)\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 3. **Yeni Dəyişənlər Yaratmaq (Feature Engineering)**\n",
    "Müraciət edən şəxsin və birgə müraciət edən şəxsin gəlirini toplam olaraq yeni bir dəyişən yaratmaq.\n",
    "\n",
    "```python\n",
    "# TotalIncome adlı yeni bir dəyişən yaratmaq\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "\n",
    "# TotalIncome üçün log transformasiya tətbiq etmək\n",
    "df['Log_TotalIncome'] = np.log1p(df['TotalIncome'])\n",
    "\n",
    "# Vizual olaraq log transformasiyanın təsirini yoxlamaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['TotalIncome'])\n",
    "plt.title('TotalIncome Outliers')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_TotalIncome'])\n",
    "plt.title('Log Transformed TotalIncome')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4. **Numeric Dəyərlərin Normalizasiyası və ya Standartizasiyası**\n",
    "Numeric dəyişənlər üçün normalizasiya (Min-Max scaling) və ya standartizasiya (Z-score normalization) tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Numeric dəyişənləri müəyyənləşdirin\n",
    "numeric_features = ['Log_ApplicantIncome', 'Log_LoanAmount', 'Log_TotalIncome']\n",
    "\n",
    "# Standartizasiya tətbiq etmək\n",
    "scaler = StandardScaler()\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Bu addımların hamısı datanızı model üçün hazır etmək məqsədi daşıyır. Bu addımlardan sonra model qurma mərhələsinə keçə bilərsiniz. Hansısa spesifik addım haqqında daha çox məlumat və ya əlavə izah istəsəniz, mənə bildirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa7140f",
   "metadata": {},
   "source": [
    "## Mene paired sample t test et bu iki sutunu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eab99b",
   "metadata": {},
   "source": [
    "Paired sample t-test, eyni subyektlərin iki müstəqil dəyişən üzərində ölçülən nəticələrinin statistik olaraq əhəmiyyətli fərqini yoxlamaq üçün istifadə olunur. Lakin sizin nümunədə `Loan_Status` (binary) və `ApplicantIncome` (numeric continuous) dəyişənləri arasında birbaşa `paired t-test` tətbiq etmək məntiqli olmayacaq, çünki `Loan_Status` müstəqil dəyişən, `ApplicantIncome` isə davamlı dəyişəndir. \n",
    "\n",
    "Bununla belə, eyni subyektlərdə iki ölçüm arasındakı fərqi təhlil etmək üçün `paired t-test` istifadə oluna bilər. Məsələn, iki müxtəlif zaman nöqtəsində eyni subyektlərdəki `ApplicantIncome` dəyişəni ölçülə bilər. Bu halda, `paired t-test` aşağıdakı şəkildə tətbiq oluna bilər:\n",
    "\n",
    "### Python Kod Nümunəsi\n",
    "\n",
    "1. **Bir Dataset Yaradın**: İki ölçüm arasındakı fərqi təsvir etmək üçün iki fərqli gəlir sütunu yaradın.\n",
    "2. **Paired Sample t-Test Tətbiq Edin**: `scipy.stats` kitabxanasını istifadə edərək.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset yaratmaq üçün funksiyalar\n",
    "def generate_data(num_samples):\n",
    "    np.random.seed(42)\n",
    "    # Təsadüfi numeric continuous dəyərlər\n",
    "    applicant_income_1 = np.random.rand(num_samples) * 10000  # 0-10000 arasında gəlir\n",
    "    applicant_income_2 = applicant_income_1 + np.random.randn(num_samples) * 2000  # ikinci ölçüm, biraz fərqli\n",
    "    \n",
    "    # DataFrame yaratmaq\n",
    "    df = pd.DataFrame({\n",
    "        'ApplicantIncome_1': applicant_income_1,\n",
    "        'ApplicantIncome_2': applicant_income_2\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Data yaratmaq\n",
    "df = generate_data(1000)\n",
    "\n",
    "# Paired sample t-test tətbiq etmək\n",
    "t_statistic, p_value = ttest_rel(df['ApplicantIncome_1'], df['ApplicantIncome_2'])\n",
    "\n",
    "print(f'T-statistic: {t_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Korrelyasiya matrisini hesablamaq\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Korrelyasiya matrisini vizualizasiya etmək\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Açıklama\n",
    "\n",
    "1. **Data Yaratmaq**: `generate_data` funksiyası iki fərqli `ApplicantIncome` sütunu ilə bir dataset yaradır.\n",
    "2. **Paired Sample t-Test**: `ttest_rel` funksiyasını istifadə edərək iki müstəqil gəlir sütunu arasındakı fərqi test edir.\n",
    "3. **Korrelyasiya Matrisini Hesablamaq**: Yaradılan data üzərində korrelyasiya matrisini və heatmap vizualizasiyasını təqdim edir.\n",
    "\n",
    "Bu kod nümunəsi `ApplicantIncome` dəyişəninin iki fərqli ölçüm arasında əhəmiyyətli bir fərqin olub olmadığını yoxlamaq üçün `paired sample t-test` tətbiq edir. Əgər siz `Loan_Status` ilə `ApplicantIncome` arasında fərq yoxlamaq istəyirsinizsə, fərqli statistik testlər (məsələn, t-test, ANOVA) və ya metodlar istifadə etməlisiniz, çünki bu iki dəyişən fərqli növlərdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9586093",
   "metadata": {},
   "source": [
    "## plt.hist(np.log(df['CoapplicantIncome'] + 1))\n",
    "* niye +1 edirik?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d96f9",
   "metadata": {},
   "source": [
    "`np.log(df['CoapplicantIncome'] + 1)` əməliyyatında `+1` etməyin səbəbi, logaritma funksiyasının tərifinə görə sıfır və mənfi dəyərlərin logaritmı tərif olunmur. Əgər `CoapplicantIncome` sütununda sıfır dəyəri varsa, `np.log(0)` undefined (müəyyən edilməmiş) olacaq və bu da səhvlərə səbəb ola bilər.\n",
    "\n",
    "`+1` əlavə etməklə, bütün dəyərlər ən azı 1 olacaq və logaritmik transformasiyanın tətbiqi mümkün olacaq. Bu həmçinin dəyərləri bir qədər dəyişir, lakin ümumi tendensiyanı və ya datanın strukturu əhəmiyyətli dərəcədə dəyişmir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3901c039",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansina random_state vermek lazimdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bcab5",
   "metadata": {},
   "source": [
    "Boosting modellərində `random_state` parametri modelin təkrarlanabilməsini təmin etmək üçün istifadə olunur. Bu, təlim zamanı təsadüfi elementlərin (məsələn, təlim məlumatlarının təsadüfi seçilməsi, təsadüfi başlanğıc ağırlıqları və s.) nəzarət edilməsi üçün bir toxum dəyəri (seed value) təmin edir.\n",
    "\n",
    "### Hər hansı bir `random_state` dəyəri təyin edərkən:\n",
    "\n",
    "- **Sabit bir dəyər təyin etmək** (məsələn, `random_state=42`): Təlim və test proseslərinin hər dəfə eyni nəticəni verməsini təmin edər. Bu, modeli təkrarlamaq və nəticələri yoxlamaq üçün faydalıdır.\n",
    "  \n",
    "- **`random_state`-i `None` olaraq buraxmaq**: Modelin hər dəfə fərqli bir təsadüfi dəyər seçməsinə səbəb olar. Bu da modelin hər dəfə fərqli nəticələr verməsi ilə nəticələnir.\n",
    "\n",
    "### Fərqli Boosting Modellərində `random_state`:\n",
    "\n",
    "1. **XGBoost (`xgboost.XGBClassifier` və ya `xgboost.XGBRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Sabit bir dəyər təyin edərək (məsələn, `random_state=42`), modeli təkrarlamaq mümkündür.\n",
    "\n",
    "2. **LightGBM (`lightgbm.LGBMClassifier` və ya `lightgbm.LGBMRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Burada da sabit bir dəyər təyin etməklə modeli təkrarlamaq mümkündür.\n",
    "\n",
    "3. **CatBoost (`catboost.CatBoostClassifier` və ya `catboost.CatBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Eyni təkrarlanabilirlik üçün sabit bir dəyər istifadə edilə bilər.\n",
    "\n",
    "4. **AdaBoost (`sklearn.ensemble.AdaBoostClassifier` və ya `sklearn.ensemble.AdaBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Təkrarlanabilirlik üçün sabit bir dəyər verilməsi məsləhətlidir.\n",
    "\n",
    "### Hansı `random_state` təyin edilməlidir?\n",
    "\n",
    "- **Sabit dəyər olaraq `42`**: Bu, elm sahəsində çox yaygın olaraq istifadə olunur və təkrarlanabilirlik təmin etmək üçün yaxşı bir seçimdir.\n",
    "- **Başqa bir sabit dəyər**: Əgər xüsusi olaraq fərqli bir toxum dəyəri istəyirsinizsə, istənilən ədəd seçilə bilər.\n",
    "- **`None` olaraq buraxmaq**: Hər dəfə təsadüfi dəyərlər istəyirsinizsə, `random_state=None` olaraq buraxıla bilər.\n",
    "\n",
    "Nəticədə, hansı `random_state` istifadə edəcəyiniz sizin proyektinizin tələblərindən asılıdır. Əgər nəticələrin hər dəfə təkrarlanmasını istəyirsinizsə, sabit bir `random_state` dəyəri təyin etmək yaxşı bir təcrübədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f53ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad97c34",
   "metadata": {},
   "source": [
    "## [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "\n",
    "## Nece edim bu warning olmasin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee933e3",
   "metadata": {},
   "source": [
    "Bu xəbərdarlıqlar, `LightGBM` modelini təlim etdirərkən yaranır və bu xəbərdarlıqların əsas səbəbi budur ki, model təlim zamanı bəzi node-lar üçün (qovşaqlar üçün) mənfi və ya sıfır dəyərində \"gain\" əldə edir. Bu halda, əlavə bir split (qovşaqda yeni bir bölmə) yaratmaq mənasız olur, çünki bu heç bir yaxşılaşma gətirməyəcək.\n",
    "\n",
    "### Bu xəbərdarlıqların səbəbləri:\n",
    "\n",
    "1. **Məlumatların Paylanması**: Məlumatların dəyər spektri çox dar ola bilər və ya bəzi dəyərlər çox nadir ola bilər. Bu halda, hər hansı bir split üçün müsbət \"gain\" əldə etmək çətin ola bilər.\n",
    "\n",
    "2. **Parametr Dəyərləri**: Modelin parametr dəyərləri (məsələn, `min_child_samples`, `min_split_gain`, `max_depth`) çox sərt olaraq təyin edilmiş ola bilər, bu da modelin çox sayda split yaratmasına maneə ola bilər.\n",
    "\n",
    "3. **Overfitting**: Model təlim zamanı çox sayda split etməyə çalışarkən overfitting-i önləmək üçün bu xəbərdarlıqları verir.\n",
    "\n",
    "### Çözüm Yolları:\n",
    "\n",
    "1. **Parametrləri Yenidən Dəyərləndirin**:\n",
    "   - `min_child_samples` dəyərini azaldaraq daha çox split imkanı verə bilərsiniz.\n",
    "   - `min_split_gain` dəyərini azaldaraq daha kiçik split-ləri icazə verə bilərsiniz.\n",
    "   - `max_depth` dəyərini artırmaq da bəzi hallarda kömək edə bilər.\n",
    "\n",
    "2. **Məlumatları Təkrardan Analiz Edin**:\n",
    "   - Məlumatlarınızı yoxlayın və fərqli qruplar arasında balansı təmin etməyə çalışın.\n",
    "   - Ehtiyac varsa, məlumatları transformasiya edərək (məsələn, log transformasiyası) modelin daha yaxşı öyrənməsi üçün şərait yaradın.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Məlumat dəstinizə əlavə xüsusiyyətlər əlavə etməyi və ya bəzi xüsusiyyətləri yenidən yaratmağı nəzərdən keçirə bilərsiniz. Bu, modelin daha yaxşı öyrənməsinə kömək edə bilər.\n",
    "\n",
    "Bu xəbərdarlıqlar modelin nəticələrinə birbaşa mənfi təsir göstərməyə bilər, ancaq məlumatların və parametrlərin diqqətlə incələnməsi modelin performansını artırmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082dd5a4",
   "metadata": {},
   "source": [
    "LightGBM modelini qurarkən bu xəbərdarlıqların qarşısını almaq üçün bəzi parametrləri aşağıdakı kimi tənzimləyə bilərsiniz:\n",
    "\n",
    "1. **`min_split_gain`** dəyərini azaldaraq daha kiçik split-ləri icazə vermək.\n",
    "2. **`min_child_samples`** dəyərini artırmaq, beləliklə model yalnız müəyyən sayda nümunələr üçün split edər.\n",
    "3. **`max_depth`** dəyərini artırmaq və ya başqa dəyərlərini tənzimləməklə modelin daha yaxşı performans göstərməsini təmin etmək.\n",
    "\n",
    "Aşağıda bir nümunə LightGBM kodu verilmişdir:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',  # Klassifikasiya üçün\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,  # Split üçün minimum \"gain\"\n",
    "    min_child_samples=20,  # Split üçün minimum nümunə sayı\n",
    "    max_depth=7,  # Maksimum dərinlik\n",
    "    random_state=42  # Reproducibility üçün\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### İpuçları:\n",
    "- **`min_split_gain`**: Bu dəyəri artırmaq splitlərin daha da mənalı olmasını təmin edəcək, beləliklə bu xəbərdarlıqların sayı azalacaq.\n",
    "- **`min_child_samples`**: Bu dəyəri artırmaq, çox kiçik nümunələr üçün split yaratmamaq üçün istifadə edilə bilər.\n",
    "- **`max_depth`**: Dərinliyi artırmaq, modelin daha çox split etməsinə icazə verəcək, ancaq overfitting riskini də nəzərə almaq lazımdır.\n",
    "\n",
    "Bu parametrlər xəbərdarlıqların qarşısını ala bilər və ya sayını azalda bilər. Ayrıca, bu parametrləri layihənizin spesifik tələblərinə uyğun tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13485373",
   "metadata": {},
   "source": [
    "## yene warning gelir...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492efc3",
   "metadata": {},
   "source": [
    "LightGBM-də xəbərdarlıqları azaltmaq üçün `min_split_gain`, `min_child_samples` və `max_depth` kimi parametrləri dəyişdirmək kifayət etmədikdə, digər variantları da yoxlamaq olar. Xəbərdarlıqları tamamilə aradan qaldırmaq üçün əlavə parametrlərdən istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`verbosity` və ya `verbose` Parametrindən İstifadə Edin:**\n",
    "LightGBM modellərində `verbosity` (və ya `verbose`) parametrini istifadə edərək xəbərdarlıqların səviyyəsini tənzimləyə bilərsiniz. Xəbərdarlıqların qarşısını almaq üçün `verbosity` dəyərini `-1` təyin edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1  # Warning-ları tamamilə söndürmək\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### 2. **`early_stopping_rounds` İstifadə Edərək Overfitting-i Azaltmaq:**\n",
    "`early_stopping_rounds` parametrini istifadə edərək, təlim zamanı modelin lazımsız splitlər etməsinin qarşısını ala bilərsiniz. Bu, xəbərdarlıqların azalmasına kömək edə bilər.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "```\n",
    "\n",
    "### 3. **`min_child_weight` Parametrini Dəyişdirin:**\n",
    "`min_child_weight` parametrini artırmaqla xəbərdarlıqların qarşısını almaq mümkündür. Bu, splitlərin daha da məntiqli olmasını təmin edəcək.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=1.0,  # Dəyəri artırmaqla xəbərdarlıqların qarşısını almaq\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri birləşdirərək və tənzimləyərək xəbərdarlıqların sayını azalda bilərsiniz. Ancaq `verbosity=-1` parametri bu xəbərdarlıqları tamamilə gizlədəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc75a",
   "metadata": {},
   "source": [
    "## SVC \n",
    "* kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’,  \n",
    "* C : float,  default=1.0,  \n",
    "* degree: int, default=3, \n",
    "* gamma: {‘scale’, ‘auto’} or float, default=’scale’  \n",
    "### Bu parameterleri haqqinda genis meulumat yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd3460",
   "metadata": {},
   "source": [
    "SVC (Support Vector Classifier) modelində **kernel** funksiyası, verilənlər arasındakı qeyri-xətti əlaqələri müəyyən etmək üçün istifadə olunur. Kernel funksiyası, verilənlərin daha yüksək ölçülü bir fəzaya proyeksiya edilməsini təmin edir, beləliklə verilənlər xətti olaraq ayrıla bilməyən hallarda belə effektiv bir ayrıcı xətt tapıla bilir.\n",
    "\n",
    "### Kernel Növləri\n",
    "\n",
    "#### 1. **Linear Kernel (‘linear’)**\n",
    "- **Təsvir:** Verilənlər xətti olaraq ayrılabilirsə, linear kernel istifadə olunur. Xətti kernel SVC modelində xüsusiyyətlərin (features) xətti birləşməsini hesablayır.\n",
    "- **Tətbiq Sahəsi:** Sadə və xətti olan verilənlər üçün effektivdir. Məsələn, eyni tipli verilənlər iki sinif arasında çoxluqda ayrılabilirsə, linear kernel uyğundur.\n",
    "- **Riyazi İzah:** Xətti kernel funksiyası \\( K(x_i, x_j) = x_i^T x_j \\) olaraq göstərilir. Burada \\( x_i \\) və \\( x_j \\) verilən nöqtələrdir.\n",
    "\n",
    "#### 2. **Polynomial Kernel (‘poly’)**\n",
    "- **Təsvir:** Polinomial kernel, verilənlər arasında qeyri-xətti əlaqələri təyin etmək üçün istifadə olunur. Bu kernel funksiyası, xüsusiyyətlərin çoxsahəli kombinasiya edilərək daha kompleks bir model yaratmağa imkan verir.\n",
    "- **Tətbiq Sahəsi:** Verilənlərdə dərəcəli bir münasibət varsa (məsələn, bir sinifin xüsusiyyətləri digərindən polinomial olaraq asılıdırsa), bu kernel effektivdir.\n",
    "- **Riyazi İzah:** Polinomial kernel funksiyası \\( K(x_i, x_j) = (x_i^T x_j + c)^d \\) olaraq göstərilir. Burada \\( d \\) polinomial dərəcədir, \\( c \\) isə sabitdir.\n",
    "\n",
    "#### 3. **RBF Kernel (Radial Basis Function, ‘rbf’)**\n",
    "- **Təsvir:** RBF kernel funksiyası, xüsusən verilənlər arasında kompleks və qeyri-xətti əlaqələr varsa, çox güclü bir seçimdir. Bu kernel funksiyası, hər bir verilən nöqtənin məsafəsini nəzərə alaraq, daha yüksək ölçülü fəzada onların təsir sahəsini genişləndirir.\n",
    "- **Tətbiq Sahəsi:** Çoxlu xüsusiyyətləri olan və onların arasında əlaqənin mürəkkəb olduğu verilənlərdə RBF kernel çox istifadə edilir.\n",
    "- **Riyazi İzah:** RBF kernel funksiyası \\( K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\) olaraq göstərilir. Burada \\( \\gamma \\) kernelin təsir sahəsini təyin edən parametrdir.\n",
    "\n",
    "#### 4. **Sigmoid Kernel (‘sigmoid’)**\n",
    "- **Təsvir:** Sigmoid kernel, sinir şəbəkələrindəki aktivasiya funksiyasına bənzər bir funksiyadır və iki verilən nöqtə arasındakı məsafəni qeyri-xətti olaraq modelləşdirir.\n",
    "- **Tətbiq Sahəsi:** Sigmoid kernel, xüsusən logistik əlaqələr olan verilənlərdə effektivdir. Bu kernel logistik reqlamentasiya və sinir şəbəkələri ilə oxşarlıq göstərir.\n",
    "- **Riyazi İzah:** Sigmoid kernel funksiyası \\( K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c) \\) olaraq göstərilir. Burada \\( \\alpha \\) və \\( c \\) tənzimləmə parametrləridir.\n",
    "\n",
    "#### 5. **Precomputed Kernel (‘precomputed’)**\n",
    "- **Təsvir:** Bu seçimdə kernel funksiyası əvvəlcədən hesablanır və SVC modelinə ötürülür. Bu, xüsusən xüsusi kernel funksiyaları istifadə edildikdə və ya verilənlərin çox olduğu hallarda istifadə olunur.\n",
    "- **Tətbiq Sahəsi:** Kernel matrisini əvvəldən hesablamağa ehtiyac olduğu hallarda və ya xüsusi kernel funksiyaları tətbiq edilən vəziyyətlərdə istifadə edilir.\n",
    "\n",
    "### Kernel Funksiyalarının Seçimi\n",
    "\n",
    "- **Linear kernel** verilənlərdə sadə və xətti bir əlaqə varsa uyğundur.\n",
    "- **Polynomial kernel** qeyri-xətti, lakin mürəkkəb olmayan münasibətləri modelləşdirmək üçün istifadə olunur.\n",
    "- **RBF kernel** daha mürəkkəb və qeyri-xətti verilənlərdə optimal performans göstərir.\n",
    "- **Sigmoid kernel** logistik münasibətləri modelləşdirmək üçün yaxşı seçimdir.\n",
    "- **Precomputed kernel** xüsusi kernel funksiyaları istifadə edilərkən və ya kernel matrixini əvvəlcədən hesablamaq istədikdə istifadə olunur.\n",
    "\n",
    "### Qrafik Təsviri\n",
    "Kernel funksiyalarının hər birinin təsirini göstərən qrafiklərlə izah edə bilərəm. Məsələn, verilənlərə tətbiq edilən linear, polynomial və RBF kernel funksiyalarının qərar sərhədlərini necə dəyişdiyini göstərə bilərəm. Bu, müxtəlif kernel növlərinin verilənlər üzərində necə işlədiyini və fərqli təsir göstərdiyini daha aydın şəkildə başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb9972",
   "metadata": {},
   "source": [
    "## C parametri\n",
    "C parametri, Support Vector Classifier (SVC) modelində cəza tənzimləyicisi (regularization parameter) olaraq istifadə olunur. Bu parametr modelin margin (ayırıcı xəttin yaxınlığında olan nümunələr) genişliyi ilə səhvlər arasında bir balans yaradır. C parametri aşağıdakı iki əsas amilə təsir edir:\n",
    "\n",
    "### 1. **Margin Genişliyi və Klassifikasiya Səhvləri**\n",
    "C parametri marginin genişliyini və modelin səhvlərə olan dözümlülüyünü tənzimləyir:\n",
    "\n",
    "- **Kiçik C Dəyəri (C < 1):** \n",
    "  - Model daha geniş margin yaratmağa çalışır. Bu, daha çox səhv qəbul edilməsinə səbəb olur, lakin model daha ümumiləşdirilmiş (generalized) olur. Yəni, model overfitting (həddindən artıq uyğunlaşma) riskini azaldır.\n",
    "  - Məsələn, qərar sərhədi çox yaxın olan iki fərqli sinif varsa, kiçik C dəyəri ilə model bu sərhədin hər iki tərəfindəki bəzi səhvləri göz ardı edərək daha hamar bir ayrıcı xətt çəkər.\n",
    "\n",
    "- **Böyük C Dəyəri (C > 1):**\n",
    "  - Model səhvləri minimuma endirmək üçün daha dar margin yaratmağa çalışır. Bu, modelin training setinə çox uyğunlaşmasına və overfitting etməsinə səbəb ola bilər.\n",
    "  - Böyük C dəyərində model hər bir səhvi ciddi şəkildə cəzalandırır və səhvləri aradan qaldırmaq üçün qərar sərhədini həmin səhvlərə uyğunlaşdırır.\n",
    "\n",
    "### 2. **Overfitting və Underfitting**\n",
    "- **Kiçik C Dəyəri:** Underfitting riski artır, yəni model verilənlərə kifayət qədər uyğunlaşmır və bəzi məlumat nümunələrini düzgün klassifikasiya edə bilmir.\n",
    "- **Böyük C Dəyəri:** Overfitting riski artır, yəni model verilənlərə həddindən artıq uyğunlaşır və yeni verilənlərə ümumiləşdirməkdə çətinlik çəkir.\n",
    "\n",
    "### Qrafik İzahı\n",
    "Qrafikdə C parametri ilə margin genişliyi arasındakı əlaqəni belə göstərə bilərik:\n",
    "\n",
    "- **Kiçik C dəyərində** margin daha geniş olacaq, lakin bəzi nümunələr marginin içində qalacaq (yəni səhvlər göz ardı ediləcək).\n",
    "- **Böyük C dəyərində** margin daha dar olacaq, nümunələr marginə çox yaxın olacaq və model hər bir nümunəni düzgün klassifikasiya etməyə çalışacaq.\n",
    "\n",
    "Bu izahı qrafiklə də göstərməyimi istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d7378",
   "metadata": {},
   "source": [
    "## Margin genisliyi\n",
    "**Margin**, Support Vector Classifier (SVC) modelində verilənlər sinifləri arasındakı ən yaxın nöqtələrə qədər olan məsafədir. Daha dəqiq desək, margin iki sinif arasında çəkilmiş ayrıcı xətt (decision boundary) ilə həmin siniflərə aid olan ən yaxın nümunələr (support vectors) arasındakı məsafəni ifadə edir. SVC modelində əsas məqsəd bu margin-i maksimuma çatdırmaqdır, çünki daha böyük bir margin ümumiləşdirilmənin daha yaxşı olmasına və modelin daha yaxşı performans göstərməsinə səbəb ola bilər.\n",
    "\n",
    "### Margin-in Əsas Məqamları:\n",
    "\n",
    "1. **Maksimum Margin:** \n",
    "   - SVC modelinin əsas məqsədi verilən iki sinif arasındakı margin-i maksimuma çatdırmaqdır. Yəni, ayrıcı xətt bu sinifləri mümkün qədər uzaq tutmalıdır. Bu, modelin daha çox ümumiləşdirmə qabiliyyətinə sahib olmasına kömək edir.\n",
    "   - Məsələn, aşağıdakı şəkildə iki sinif arasındakı ayrıcı xətt ən geniş margin-ə malikdir və bu xətt margin-i maksimum edən ayrıcı xətdir.\n",
    "\n",
    "2. **Support Vectors:** \n",
    "   - Margin-i təyin edən verilən nöqtələrə **support vectors** deyilir. Bu nöqtələr margin-in sərhədlərini müəyyənləşdirir. SVC modeli bu nöqtələr üzərində işləyir və digər nöqtələr margin-in genişliyini təsir etmir.\n",
    "   - Əgər support vectors modelləşdirilmə zamanı dəyişərsə, margin və ayrıcı xətt də dəyişə bilər.\n",
    "\n",
    "3. **Margin-in Genişliyi və Performans:**\n",
    "   - Daha geniş bir margin, verilənləri daha yaxşı ayırır və bu, overfitting riskini azaldır, çünki model daha ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "   - Əgər margin çox dardırsa, model verilənlərə çox yaxınlaşır və overfitting riski artır, yəni model training setində yaxşı performans göstərsə də, yeni verilənlərdə zəif performans göstərə bilər.\n",
    "\n",
    "4. **Margin və `C` Parametri:**\n",
    "   - `C` parametri margin-in ölçüsünü idarə edir. Kiçik `C` dəyəri daha geniş margin-i təşviq edir və daha az səhv nümunələrə icazə verir. Yüksək `C` dəyəri isə margin-i daraldır və modelin verilənlərə daha çox uyğunlaşmasına səbəb olur, lakin overfitting riski artır.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "Bir qrafikdə margin-i təsvir etmək olar:\n",
    "- İki sinif nöqtələrinin olduğu bir qrafikdə, bir ayrıcı xətt (decision boundary) çəkilir və bu xəttin hər iki tərəfində margin göstərilir.\n",
    "- Ayrıcı xəttə ən yaxın olan nöqtələr support vectors olaraq göstərilir.\n",
    "- Margin-in ölçüsü bu nöqtələr ilə ayrıcı xətt arasındakı məsafə ilə təyin olunur.\n",
    "\n",
    "İstəyirsinizsə, margin və support vectors-i göstərən belə bir qrafik hazırlaya bilərəm. Bu, margin-in necə işlədiyini və onun model performansına necə təsir etdiyini daha aydın izah edəcək."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad67c5",
   "metadata": {},
   "source": [
    "## degree parametri\n",
    "`degree` parametri, Support Vector Classifier (SVC) modelində yalnız **polinomial kernel** (`'poly'`) üçün istifadə edilir və polinomial kernelin dərəcəsini (degree) təyin edir. Bu parametr verilənlərin xüsusiyyətlərini daha yüksək dərəcədə olan bir fəzaya proyeksiya etməyə imkan verir, beləliklə verilənlərdəki qeyri-xətti əlaqələri daha yaxşı modelləşdirmək mümkündür.\n",
    "\n",
    "### `degree` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Polinomial Kernelin Kompleksliyi**\n",
    "- **Kiçik dərəcə (d = 2, 3):**\n",
    "  - Nisbətən sadə, lakin qeyri-xətti əlaqələri modelləşdirmək üçün uyğundur.\n",
    "  - Məsələn, iki sinif arasında sadə, lakin qeyri-xətti bir sərhəd varsa, dərəcə 2 və ya 3 istifadə edilə bilər. Bu vəziyyətdə polinomial kernel ikinci və ya üçüncü dərəcəli polinomlara əsaslanacaq.\n",
    "\n",
    "- **Böyük dərəcə (d > 3):**\n",
    "  - Daha kompleks və mürəkkəb əlaqələri modelləşdirmək üçün istifadə olunur.\n",
    "  - Dərəcənin artırılması, kernel funksiyasının daha yüksək dərəcəli polinomlara əsaslanmasını təmin edir və beləliklə, çoxsaylı xüsusiyyətlər arasındakı mürəkkəb əlaqələri modelləşdirir.\n",
    "\n",
    "#### 2. **Overfitting Riski**\n",
    "- **Yüksək `degree` (dərəcə) dəyəri:**\n",
    "  - Overfitting riski artır, çünki model training datasetindəki hər bir nümunəni düzgün uyğunlaşdırmağa çalışır. Bu, modelin training setinə çox yaxşı uyğunlaşması, lakin yeni verilənlərdə ümumiləşdirmədə çətinlik çəkməsi deməkdir.\n",
    "  - Məsələn, çox yüksək dərəcəli polinomial kernel hər bir nümunə üçün fərdi qərar sərhədləri çəkə bilər, lakin bu, modelin daha sonra səhvlərlə üzləşməsinə səbəb ola bilər.\n",
    "\n",
    "- **Aşağı `degree` (dərəcə) dəyəri:**\n",
    "  - Underfitting riski artır, çünki model verilənlərdəki daha mürəkkəb əlaqələri düzgün modelləşdirə bilmir. Bu halda, model siniflər arasındakı daha kompleks münasibətləri nəzərə almadan sadə bir sərhəd çəkə bilər.\n",
    "\n",
    "#### 3. **Dərəcənin Seçilməsi**\n",
    "- `degree` parametrini seçərkən verilənlərin xüsusiyyətlərinə və onların arasındakı əlaqələrə diqqət yetirmək vacibdir.\n",
    "- Nəzərə alınmalıdır ki, dərəcənin artırılması modelin kompleksliyini və hesablama tələblərini artıracaq.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Aşağı dərəcə** (d = 2): Polinomial kernel, iki sinif arasında nisbətən sadə və əyri bir sərhəd çəkəcək.\n",
    "- **Orta dərəcə** (d = 3): Kernel funksiyası daha mürəkkəb, lakin hələ də idarə edilə bilən bir sərhəd çəkəcək.\n",
    "- **Yüksək dərəcə** (d > 3): Kernel funksiyası çox mürəkkəb bir sərhəd çəkəcək, bu da overfittingə səbəb ola bilər.\n",
    "\n",
    "Bu təsiri qrafiklərlə göstərmək üçün, müxtəlif dərəcələr üçün polinomial kernelin necə işlədiyini qrafiklərlə göstərə bilərəm. Bu cür qrafiklər dərəcə parametri ilə decision boundary-nin necə dəyişdiyini daha yaxşı başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9557d7f",
   "metadata": {},
   "source": [
    "## gamma parametri\n",
    "`gamma` parametri, Support Vector Classifier (SVC) modelində **RBF (Radial Basis Function)**, **polinomial**, və **sigmoid** kernel funksiyaları üçün istifadə edilir. Bu parametr, verilənlər arasında məsafənin təsirini tənzimləyir və modelin qərar sərhədlərinin nə qədər yaxın və ya geniş olacağını müəyyən edir.\n",
    "\n",
    "### `gamma` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Verilənlər Arasındakı Məsafə**\n",
    "- **Yüksək `gamma` dəyəri:**\n",
    "  - Verilənlərdəki hər bir nöqtənin təsir sahəsini daraldır, yəni hər bir nöqtənin təsiri çox yaxın məsafədə olur. Bu, modelin verilənlərə çox yaxşı uyğunlaşmasına və beləliklə, overfitting etməsinə səbəb ola bilər.\n",
    "  - Bu halda, model hər bir nümunəni çox yaxından izləyir və qərar sərhədləri çox qısa məsafələrdə dəyişir.\n",
    "  \n",
    "- **Aşağı `gamma` dəyəri:**\n",
    "  - Verilənlərdəki nöqtələrin təsir sahəsi genişlənir, yəni hər bir nöqtənin təsiri daha uzaq məsafədə hiss olunur. Bu, modelin daha hamar və ümumiləşdirilmiş bir sərhəd yaratmasına imkan verir.\n",
    "  - Bu vəziyyətdə, model fərdi nümunələrə çox yaxınlaşmır və daha geniş, ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "\n",
    "#### 2. **Overfitting və Underfitting**\n",
    "- **Yüksək `gamma`:** \n",
    "  - Model verilənlərə çox uyğunlaşır (overfitting), bu da training setində yüksək dəqiqlik əldə edə bilər, lakin yeni verilənlərə qarşı zəif performans göstərə bilər.\n",
    "  - Qərar sərhədi çox qarmaqarışıq və mürəkkəb olur, belə ki, hətta kiçik fərqlər də sərhədi dəyişə bilər.\n",
    "  \n",
    "- **Aşağı `gamma`:** \n",
    "  - Model verilənlərə kifayət qədər uyğunlaşmır (underfitting), bu da training setində aşağı dəqiqlik, lakin yeni verilənlərə qarşı daha yaxşı ümumiləşdirmə ilə nəticələnə bilər.\n",
    "  - Qərar sərhədi daha sadə və hamar olur.\n",
    "\n",
    "### `gamma` Parametrinin Dəyərləri\n",
    "\n",
    "#### 1. **‘scale’ (default dəyəri):**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına və məlumatların yayılmasına əsasən avtomatik olaraq təyin olunur.\n",
    "  - Riyazi olaraq, `scale` dəyəri $  \\frac{1}{n \\times \\text{X.var}}  $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayı, `X.var` isə məlumatın dispersiyasını (variance) ifadə edir.\n",
    "  - Bu, orta bir gamma dəyəri verir və modelin çox uyğunlaşmasının (overfitting) qarşısını almaq üçün yaxşı balanslaşdırılmışdır.\n",
    "\n",
    "#### 2. **‘auto’:**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına (n_features) əsasən təyin olunur.\n",
    "  - Riyazi olaraq, `auto` dəyəri $ \\frac{1}{n} $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayıdır.\n",
    "  - Bu, daha az xüsusiyyətli verilənlər üçün uyğundur və daha geniş təsir sahəsi verir. Lakin böyük datasetlərdə underfitting riski artır.\n",
    "\n",
    "#### 3. **Float dəyəri:**\n",
    "  - `gamma` dəyərini özünüz təyin edə bilərsiniz. Məsələn, `gamma=0.1` və ya `gamma=1.0` kimi.\n",
    "  - Bu, verilənlərə xüsusi uyğunlaşdırılmış gamma dəyəri ilə daha dəqiq idarə etmə imkanı verir.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Yüksək `gamma` dəyəri:** Qərar sərhədi çox mürəkkəb və dar olacaq, hər bir nümunəyə uyğunlaşaraq sərhədin tez-tez dəyişməsinə səbəb olacaq.\n",
    "- **Aşağı `gamma` dəyəri:** Qərar sərhədi daha geniş və hamar olacaq, fərdi nümunələrin təsirindən daha az asılı olacaq.\n",
    "\n",
    "Bu təsirləri qrafiklərlə də göstərə bilərəm. `gamma` parametrinin fərqli dəyərləri üçün decision boundary-lərin necə dəyişdiyini qrafiklərlə təsvir edə bilərəm. Bu, `gamma` parametrinin modelin performansına necə təsir etdiyini daha aydın başa düşməyinizə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1eae8ac",
   "metadata": {},
   "source": [
    "## Cross validation-a X, Y verilmelidir, yoxsa X_train, Y_train ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe845f",
   "metadata": {},
   "source": [
    "Cross-validation (çarpaz doğrulama) prosesində modelin performansını qiymətləndirmək üçün **orijinal verilənlər dəsti** (`X` və `Y`) istifadə olunur. Bu prosesdə, orijinal verilənlər dəsti bir neçə alt dəstəyə bölünür, daha sonra bu alt dəstələrdən biri test dəsti kimi istifadə edilir, qalanları isə train (öyrədici) dəsti kimi istifadə olunur.\n",
    "\n",
    "### Ətraflı İzahetmə\n",
    "\n",
    "- **X və Y:** Bu, sizin bütün verilənlər dəstinizdir. `X` bağımsız dəyişənləri (features) və `Y` isə bağımlı dəyişəni (target, label) təmsil edir.\n",
    "\n",
    "- **Cross-Validation Prosesində:**\n",
    "  - Verilənlər `k` alt dəstəyə bölünür (`k`-fold cross-validation).\n",
    "  - Hər dövrədə (`fold`) bir alt dəstə test dəsti kimi istifadə olunur, qalan `k-1` alt dəstə isə train dəsti kimi istifadə olunur.\n",
    "  - Bu dövrə `k` dəfə təkrarlanır və hər dəfə fərqli bir alt dəstə test dəsti kimi istifadə edilir.\n",
    "  - Bu prosesin nəticəsində modelin performansı hər bir fold üçün ölçülür və son nəticə olaraq bu performans ölçülərinin ortalaması götürülür.\n",
    "\n",
    "**Nəticə:**\n",
    "Cross-validation tətbiq edərkən, bütün verilənlər dəstinizi (`X` və `Y`) funksiyaya ötürməlisiniz. Model bu verilənlər dəstini daxildə train və test dəstlərinə böləcək və cross-validation prosesini icra edəcək. Beləliklə, siz train/test bölünməsini özünüz etmirsiniz; bunu cross-validation avtomatik olaraq edir.\n",
    "\n",
    "### İstifadə Nümunəsi:\n",
    "\n",
    "Python-da `sklearn` kitabxanasından `cross_val_score` funksiyasını istifadə edərkən, verilənlər dəstini aşağıdakı kimi daxil edirsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Bütün verilənlər dəstinizi istifadə edin\n",
    "model = SVC(kernel='linear')\n",
    "scores = cross_val_score(model, X, Y, cv=5)\n",
    "```\n",
    "\n",
    "Burada `X` və `Y` tam verilənlər dəstidir və `cv=5` isə 5-fold cross-validation həyata keçirdiyinizi göstərir. Bu funksiyada `cross_val_score` verilənlər dəstini avtomatik olaraq train və test dəstlərinə böləcək və cross-validation-u icra edəcək.\n",
    "\n",
    "**Diqqət:** Əgər `X_train` və `Y_train` artıq bölünmüş verilənlər dəstinə sahibsinizsə və cross-validation tətbiq etmək istəyirsinizsə, o zaman həmin verilənlər dəstini də `X` və `Y` kimi istifadə edə bilərsiniz, lakin bu halda yalnız həmin bölünmüş train dəstiniz üzərində cross-validation icra ediləcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64e706b",
   "metadata": {},
   "source": [
    "## Code izah ele?\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88052caa",
   "metadata": {},
   "source": [
    "Bu kod parçası bir korelyasiya heatmap-i (istilik xəritəsi) yaratmaq üçün istifadə olunur. İndi hər sətirə nəzər salaq:\n",
    "\n",
    "### 1. Şəkil ölçüsünü təyin edir:\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "```\n",
    "- `figsize` parametri vasitəsilə çıxan şəkilin ölçüsünü (en və hündürlük) təyin edir. Burada 16x12 ölçülərində bir şəkil yaradır.\n",
    "\n",
    "### 2. Üst üçbucağı gizlətmək üçün maska yaradılır:\n",
    "```python\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "```\n",
    "- Bu sətir korelyasiya matrisinin üst üçbucaqlı hissəsini gizlətmək üçün bir maska yaradır. \n",
    "- `np.ones_like(df.corr())` matrisin bütün elementlərini 1 ilə doldurur.\n",
    "- `np.triu` funksiyası isə bu 1-lik matrisin yuxarı üçbucağını saxlayır (alt üçbucağını 0 edir).\n",
    "- Bu maska, korelyasiya matrisinin yarısını (üst üçbucağı) gizlədərək yalnız alt yarısını görməyə imkan verir.\n",
    "\n",
    "### 3. Korelyasiya heatmap-i yaradılır:\n",
    "```python\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "```\n",
    "- `df.corr()` DataFrame-in sütunları arasındakı korelyasiya matrisini qaytarır.\n",
    "- `round(df.corr(),1)` korelyasiya dəyərlərini 1 ondalık rəqəmlə yuvarlayır.\n",
    "- `mask=mask` parametri ilə yaratdığınız maska tətbiq edilir, beləliklə üst üçbucaqdakı dəyərlər gizlədilir.\n",
    "- `vmin` və `vmax` parametrləri heatmap-də minimum və maksimum dəyərləri təyin edir (burada -1 və 1 arasındadır).\n",
    "- `annot=True` parametri hüceyrələrin içində dəyərlərin göstərilməsini təmin edir.\n",
    "- `cmap='BrBG'` rəng xəritəsini təyin edir. Burada \"BrBG\" palitrası istifadə olunur.\n",
    "\n",
    "### 4. Heatmap başlığı və formatlama:\n",
    "```python\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```\n",
    "- `set_title` funksiyası heatmap-in başlığını \"Correlation Heatmap\" olaraq təyin edir.\n",
    "- `fontdict={'fontsize':18}` başlığın şrift ölçüsünü 18 olaraq təyin edir.\n",
    "- `pad=16` başlıq ilə heatmap arasında boşluğun ölçüsünü təyin edir.\n",
    "\n",
    "Bu kod ilə, DataFrame-in sütunları arasındakı korelyasiyaları asanlıqla görsəl formada təqdim edə bilərsiniz. Korelyasiya heatmap-ləri, xüsusən dəyişənlər arasındakı əlaqələri təhlil etmək üçün faydalıdır. Yuxarı üçbucağı maskalamaq isə məlumatın təkrarlanmasını və vizual qarışıqlığı azaldır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b884cb07",
   "metadata": {},
   "source": [
    "## iki feature arasinda corr 1 dir. bu ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48708488",
   "metadata": {},
   "source": [
    "İki dəyişən (feature) arasındakı korelyasiyanın (corr) 1 olması onların tam müsbət əlaqədə olduğunu göstərir. Bu vəziyyətin mənası aşağıdakılardır:\n",
    "\n",
    "1. **Tam Müsbət Xətti Əlaqə:** Korelyasiya əmsalı 1 olan iki dəyişən bir-birinə tam xətti şəkildə bağlıdır. Yəni, bir dəyişən artdıqca, digəri də eyni nisbətdə artır. Məsələn, bir dəyişən 10% artarsa, digəri də mütləq şəkildə 10% artacaq.\n",
    "\n",
    "2. **Biri Digərini Tamamilə Təyin Edir:** Əgər iki dəyişən arasında korelyasiya 1-dirsə, bu o deməkdir ki, bir dəyişənin dəyərləri digər dəyişənin dəyərlərindən tamamilə asılıdır. Bir dəyişən digərini eyni şəkildə təkrarlayır və ya onun sadə bir xətti funksiyasıdır.\n",
    "\n",
    "3. **Multikollinearlıq Problemi:** Əgər bir modeldə iki dəyişən arasında korelyasiya 1-dirsə, bu multikollinearlıq problemi yarada bilər. Multikollinearlıq modelin performansını və etibarlılığını mənfi təsir edə bilər, çünki dəyişənlərdən biri digərini tam əvəz edir, və nəticədə modeldə lazımsız təkrarlanma yaranır.\n",
    "\n",
    "### Nəticə:\n",
    "Korelyasiyanın 1 olması iki dəyişənin eyni məlumatı daşıdığı mənasına gəlir. Birini modeldə saxlamaq kifayətdir; ikincisi isə redundant ola bilər və modelin performansını yaxşılaşdırmaq üçün çıxarılmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e951b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4c22e4",
   "metadata": {},
   "source": [
    "## Bes categoryc deyerler arasinda Multikollinearlıq nece teyin etmek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4935c",
   "metadata": {},
   "source": [
    "Kategorik (category) dəyişənlər üçün tam uyğunluq və ya çox oxşarlığı müəyyən etmək numeric (rəqəmsal) dəyişənlərdəki kimi sadə deyil, lakin müəyyən üsullar və yanaşmalar mövcuddur:\n",
    "\n",
    "### 1. **Crosstab (Çapraz cədvəl) və ya Contingency Table:**\n",
    "   - İki və ya daha çox kategorik dəyişəni analiz etmək üçün `pd.crosstab()` funksiyasını istifadə edə bilərsiniz. Bu, dəyişənlər arasındakı əlaqələri görmək üçün faydalıdır.\n",
    "   - Əgər iki kategorik dəyişənin crosstab cədvəli eynidirsə, bu onların bir-birinə çox oxşar olduğunu göstərir.\n",
    "   - Məsələn, eyni dəyərləri ehtiva edən iki dəyişənin bütün crosstab dəyərləri (yəni sıxlıqları) eyni olacaq.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   ```\n",
    "\n",
    "### 2. **Khi-kvadrat Testi (Chi-Square Test):**\n",
    "   - Khi-kvadrat testi iki kategorik dəyişənin bir-birindən asılı olub-olmadığını yoxlamaq üçün istifadə olunur. Əgər iki dəyişənin arasında çox güclü bir əlaqə varsa (yüksək Chi-square dəyəri və çox aşağı p-value), bu onların bir-birinə çox oxşar olduğuna işarə edə bilər.\n",
    "   \n",
    "   ```python\n",
    "   from scipy.stats import chi2_contingency\n",
    "   contingency_table = pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "   ```\n",
    "\n",
    "### 3. **Mutual Information (Özəl İnformasiya):**\n",
    "   - Mutual Information metodu kategorik dəyişənlər arasında nə qədər informasiya paylaşdığını ölçür. Yüksək mutual information dəyəri onların güclü bir əlaqəyə malik olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.feature_selection import mutual_info_classif\n",
    "   mi = mutual_info_classif(df[['category_feature1']], df['category_feature2'])\n",
    "   print(mi)\n",
    "   ```\n",
    "\n",
    "### 4. **Label Encoding və Korelyasiya:**\n",
    "   - Kategorik dəyişənləri öncə `Label Encoding` və ya `One-Hot Encoding` ilə rəqəmsal formaya çevirərək, daha sonra onların arasında korelyasiya təhlili apara bilərsiniz. Bu, xüsusilə ordinal (sıralı) kategorik dəyişənlər üçün daha mənalı ola bilər.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "   le = LabelEncoder()\n",
    "   df['encoded_feature1'] = le.fit_transform(df['category_feature1'])\n",
    "   df['encoded_feature2'] = le.fit_transform(df['category_feature2'])\n",
    "   \n",
    "   correlation = df[['encoded_feature1', 'encoded_feature2']].corr()\n",
    "   ```\n",
    "\n",
    "### 5. **Jaccard Similarity:**\n",
    "   - İki kategorik dəyişənin kəsişməsini və birləşməsini müqayisə etmək üçün Jaccard bənzərlik metrikasını istifadə edə bilərsiniz. Əgər Jaccard bənzərliyi 1-dirsə, bu, iki dəyişənin tam eyni olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.metrics import jaccard_score\n",
    "   similarity = jaccard_score(df['category_feature1'], df['category_feature2'], average='macro')\n",
    "   ```\n",
    "\n",
    "### 6. **Mode və ya Value Counts:**\n",
    "   - Hər bir kategorik dəyişənin `mode` (ən çox təkrarlanan dəyər) və ya `value_counts` statistikalarını müqayisə edərək oxşarlığı təhlil edə bilərsiniz. Əgər iki dəyişənin dəyərləri və onların sıxlıqları çox bənzərdirsə, bu oxşarlıq göstəricisi ola bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Numeric dəyişənlərdəki kimi, kategorik dəyişənlər arasında da tam uyğunluq və güclü oxşarlığı müəyyən etmək üçün müxtəlif statistik metodlardan istifadə edilə bilər. Bu yanaşmalarla kategorik dəyişənlərin redundant (təkrarlanan) olub-olmadığını müəyyənləşdirə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217e6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b96ad99d",
   "metadata": {},
   "source": [
    "## VIf score hem numeric,  hem categoryc ucun isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ffe37",
   "metadata": {},
   "source": [
    "VIF (Variance Inflation Factor) skoru yalnız numeric (rəqəmsal) dəyişənlər üçün tətbiq olunur. Bu metod multikollinearlığı, yəni bir rəqəmsal dəyişənin digər rəqəmsal dəyişənlərlə nə qədər əlaqəli olduğunu qiymətləndirir. VIF, rəqəmsal dəyişənlər arasında xətti asılılıqların olub-olmamasını təhlil etmək üçün istifadə olunur.\n",
    "\n",
    "### Niyə VIF yalnız numeric dəyişənlər üçün işləyir?\n",
    "\n",
    "1. **Linear Regression Əsaslı:** \n",
    "   - VIF, bir dəyişəni digər rəqəmsal dəyişənlərə qarşı bir lineer regresiya modeli ilə proqnozlaşdırmaqla hesablanır. Kategorik dəyişənlər birbaşa linear regressiyada mənalı bir şəkildə istifadə edilə bilməz, çünki onlar saylı deyil.\n",
    "\n",
    "2. **Multikollinearlıq:**\n",
    "   - Multikollinearlıq problemi xətti modellərdə yaranır, çünki bu modellər dəyişənlər arasında xətti əlaqələrdən asılıdır. Kategorik dəyişənlər isə qeyri-xətti xüsusiyyətlərə malikdir, buna görə də multikollinearlıq VIF ilə qiymətləndirilmir.\n",
    "\n",
    "### Kategorik dəyişənlər üçün nə istifadə etmək olar?\n",
    "\n",
    "Kategorik dəyişənlər üçün VIF-ə bənzər bir üsul yoxdur, lakin onların redundant olub olmadığını müəyyənləşdirmək üçün aşağıdakı metodlar istifadə edilə bilər:\n",
    "\n",
    "1. **Chi-Square Testi:**\n",
    "   - Kategorik dəyişənlər arasında asılılıq dərəcəsini ölçmək üçün Chi-Square testi istifadə edilə bilər. Bu, kategorik dəyişənlərin bir-birinə nə qədər bağlı olduğunu göstərir.\n",
    "\n",
    "2. **Mutual Information:**\n",
    "   - Mutual Information kategorik dəyişənlər arasında nə qədər məlumatın paylaşıldığını ölçür. Yüksək mutual information dəyəri onların bir-birinə çox oxşar və ya əlaqəli olduğunu göstərir.\n",
    "\n",
    "3. **Label Encoding ilə Korelyasiya:**\n",
    "   - Kategorik dəyişənlər Label Encoding və ya One-Hot Encoding ilə numeric formaya çevrilə bilər, daha sonra bu dəyişənlər arasında korelyasiya hesablanıb redundant olanlar müəyyən edilə bilər.\n",
    "\n",
    "### Nəticə:\n",
    "VIF yalnız numeric dəyişənlər üçün istifadə edilir. Kategorik dəyişənlər üçün isə digər statistik metodlardan istifadə edərək, redundantlıq və oxşarlığı qiymətləndirmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad8228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdbb49d9",
   "metadata": {},
   "source": [
    "* from sklearn.model_selection import train_test_split\n",
    "* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "## stratify=y  ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67be01f",
   "metadata": {},
   "source": [
    "`stratify=y` parametri, `train_test_split` funksiyasında verilən məlumatların sınıflar üzrə balanslı şəkildə bölünməsini təmin edir. Yəni, orijinal `y` target (hədəf) dəyişənindəki sınıfların nisbəti necədirsə, həm təlim, həm də test dəstində də həmin sınıf nisbətləri qorunacaq.\n",
    "\n",
    "### Misal:\n",
    "Tutaq ki, sizin `y` hədəf dəyişəninizdə iki sınıf (məsələn, 0 və 1) var və bunların nisbəti belədir:\n",
    "- `0` sınıfı: 80%\n",
    "- `1` sınıfı: 20%\n",
    "\n",
    "Əgər siz `stratify=y` parametri ilə bölmə aparsanız, həm təlim dəstində, həm də test dəstində bu sınıfların nisbəti eyni qalacaq (80% və 20%). Bu, modelin hər iki dəstdə məlumatların təmsilçiliyini qoruması üçün faydalıdır.\n",
    "\n",
    "Əgər `stratify=y` olmasa, bölünmə təsadüfi olar və sınıfların nisbəti təlim və test dəstlərində qeyri-bərabər ola bilər, bu da modelin performansını mənfi təsir edə bilər, xüsusilə də sınıflar arasında balanssızlıq varsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082402e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513f1e35",
   "metadata": {},
   "source": [
    "## XGBoost esas parameterleri ve o parameterlere hyperparameter tuning ucun hansi deyerler vermek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe8fdd",
   "metadata": {},
   "source": [
    "XGBoost modellərində əsas parametrlər və onların hyperparameter tuning (hiperparametr tənzimlənməsi) üçün dəyərləri aşağıda göstərilmişdir. Bu dəyərlər `GridSearchCV` və ya `BayesSearchCV` kimi alətlərlə tuning üçün istifadə oluna bilər.\n",
    "\n",
    "### Əsas XGBoost Parametrləri\n",
    "\n",
    "1. **`n_estimators`**:\n",
    "   - Açıklama: Modelin istifadə etdiyi ağacların sayı.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `100, 200, 500, 1000`\n",
    "\n",
    "2. **`learning_rate`** (və ya `eta`):\n",
    "   - Açıklama: Modelin öyrənmə sürəti. Kiçik dəyərlər öyrənməni yavaşladır, ancaq nəticəni yaxşılaşdırır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.01, 0.05, 0.1, 0.2`\n",
    "\n",
    "3. **`max_depth`**:\n",
    "   - Açıklama: Hər bir qərar ağacının maksimum dərinliyi. Daha böyük dərinliklər modelin mürəkkəbliyini artırır, lakin overfitting-ə səbəb ola bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `3, 5, 7, 10`\n",
    "\n",
    "4. **`min_child_weight`**:\n",
    "   - Açıklama: Bir yarpaq qovşağının ən az nə qədər müşahidəyə sahib olmalı olduğunu təyin edir. Böyük dəyərlər underfitting yarada bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `1, 3, 5, 7`\n",
    "\n",
    "5. **`subsample`**:\n",
    "   - Açıklama: Təlim üçün istifadə edilən məlumatların nisbəti. Overfitting-i azaltmaq üçün faydalıdır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "6. **`colsample_bytree`**:\n",
    "   - Açıklama: Hər bir ağac üçün təsadüfi seçilən xüsusiyyətlərin nisbəti.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "7. **`gamma`**:\n",
    "   - Açıklama: Ağacın budaqlanmasına qərar verən minimal itki azalma dəyəri. Dəyər nə qədər yüksəkdirsə, budaqlanma daha sərt olur.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.1, 0.2, 0.5`\n",
    "\n",
    "8. **`reg_alpha`** (L1 tənzimlənməsi):\n",
    "   - Açıklama: L1 regularization-u. Daha yüksək dəyərlər sadə modellərə gətirib çıxara bilər və overfitting-i azalda bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "9. **`reg_lambda`** (L2 tənzimlənməsi):\n",
    "   - Açıklama: L2 regularization-u. Overfitting-in qarşısını almağa kömək edir.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "### Hyperparameter Tuning Misalı (`GridSearchCV`):\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "Bu tuning prosesini modelinizə uyğun olaraq dəyişə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4458f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33468f9c",
   "metadata": {},
   "source": [
    "## GridSearchCV, RandomizedSearchCV və BayesSearchCV Parameters cedveli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f25710",
   "metadata": {},
   "source": [
    "Əlbəttə! Aşağıda `GridSearchCV`, `RandomizedSearchCV` və `BayesSearchCV` metodlarının əsas parametrlərini Jupyter Notebook üçün uyğun şəkildə cədvəl şəklində təqdim edirəm.\n",
    "\n",
    "| **Parameter**               | **GridSearchCV (scikit-learn)**                                   | **RandomizedSearchCV (scikit-learn)**                     | **BayesSearchCV (scikit-optimize)**                        |\n",
    "|-----------------------------|-------------------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **estimator**               | Optimallaşdırılacaq maşın öyrənmə modeli                          | Optimallaşdırılacaq maşın öyrənmə modeli                    | Optimallaşdırılacaq maşın öyrənmə modeli                     |\n",
    "| **param_grid / param_distributions / search_spaces** | `param_grid`: dict – Parametrlərin tam qrafiki                | `param_distributions`: dict və ya list – Parametrlərin paylanması | `search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər |\n",
    "| **n_iter**                  | Tətbiq olunmur                                                    | `n_iter`: int – Nümunə parametrlərin sayı                  | `n_iter`: int – Optimallaşdırma üçün iterasiya sayı         |\n",
    "| **scoring**                 | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası |\n",
    "| **cv**                      | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası |\n",
    "| **n_jobs**                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı            | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı             |\n",
    "| **refit**                   | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma      | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma |\n",
    "| **verbose**                 | `verbose`: int – Verbosity səviyyəsi                              | `verbose`: int – Verbosity səviyyəsi                      | `verbose`: int – Verbosity səviyyəsi                        |\n",
    "| **pre_dispatch**            | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı  | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı | Tətbiq olunmur, implementasiyaya bağlı                      |\n",
    "| **random_state**            | Tətbiq olunmur                                                    | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması |\n",
    "| **error_score**             | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | Implementasiyaya bağlı ola bilər                              |\n",
    "| **return_train_score**      | `return_train_score`: bool – Təlim skorlarını daxil etmək         | `return_train_score`: bool – Təlim skorlarını daxil etmək   | `return_train_score`: bool – Təlim skorlarını daxil etmək     |\n",
    "| **optimizer**               | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `optimizer`: str – Optimallaşdırma alqoritmi, məsələn \"auto\", \"GP\" və s. |\n",
    "| **n_points**                | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı |\n",
    "\n",
    "### Qısa İzahat\n",
    "\n",
    "- **GridSearchCV**:\n",
    "  - Bütün mümkün parametrlərin kombinasyonlarını sınayır.\n",
    "  - `param_grid` vasitəsilə tam qrafik təyin olunur.\n",
    "  - Daha çox hesablama vaxtı tələb edir, xüsusilə parametrlərin sayı çox olduqda.\n",
    "\n",
    "- **RandomizedSearchCV**:\n",
    "  - Parametrlərin müəyyən paylanmalardan təsadüfi seçilmiş kombinasiyalarını sınayır.\n",
    "  - `n_iter` ilə sınanacaq kombinasiyaların sayını təyin edə bilərsiniz.\n",
    "  - Hesablama baxımından daha səmərəlidir, xüsusilə geniş parametrlər sahəsində.\n",
    "\n",
    "- **BayesSearchCV**:\n",
    "  - Bayes optimallaşdırması istifadə edərək daha ağıllı axtarış aparır.\n",
    "  - `search_spaces` ilə axtarış sahəsi təyin olunur.\n",
    "  - Daha az iterasiya ilə optimal nəticələr əldə etməyə çalışır.\n",
    "\n",
    "### Jupyter Notebook-da Görüntüləmək\n",
    "\n",
    "Bu cədvəli Jupyter Notebook-da gözəl şəkildə görüntüləmək üçün Markdown hüceyrəsində yuxarıdakı cədvəl formatını istifadə edə bilərsiniz. Alternativ olaraq, `pandas` kitabxanasından istifadə edərək DataFrame şəklində də təqdim edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Parameter\": [\n",
    "        \"estimator\",\n",
    "        \"param_grid / param_distributions / search_spaces\",\n",
    "        \"n_iter\",\n",
    "        \"scoring\",\n",
    "        \"cv\",\n",
    "        \"n_jobs\",\n",
    "        \"refit\",\n",
    "        \"verbose\",\n",
    "        \"pre_dispatch\",\n",
    "        \"random_state\",\n",
    "        \"error_score\",\n",
    "        \"return_train_score\",\n",
    "        \"optimizer\",\n",
    "        \"n_points\"\n",
    "    ],\n",
    "    \"GridSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_grid`: dict – Parametrlərin tam qrafiki\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"RandomizedSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_distributions`: dict və ya list – Parametrlərin paylanması\",\n",
    "        \"`n_iter`: int – Nümunə parametrlərin sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"BayesSearchCV (scikit-optimize)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər\",\n",
    "        \"`n_iter`: int – Optimallaşdırma üçün iterasiya sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"Tətbiq olunmur, implementasiyaya bağlı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"Implementasiyaya bağlı ola bilər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"`optimizer`: str – Optimallaşdırma alqoritmi, məsələn 'auto', 'GP' və s.\",\n",
    "        \"`n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "```\n",
    "\n",
    "Bu kodu işlədərək daha interaktiv və oxunaqlı bir cədvəl əldə edə bilərsiniz.\n",
    "\n",
    "Ümid edirəm ki, bu məlumatlar sizin üçün faydalı oldu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "034bc5a1",
   "metadata": {},
   "source": [
    "## imbalance problemi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa70613",
   "metadata": {},
   "source": [
    "Imbalance problemi, xüsusilə təsnifat (classification) problemlərində rast gəlinən vəziyyətdir. Bu zaman siniflər arasında böyük fərqlilik olur, yəni bir sinif çox az, digəri isə çox sayda nümunə ehtiva edir. Məsələn, xəstəlik diaqnozu ilə bağlı bir datasetdə xəstəlik olan insanların sayı çox az, sağlam olan insanların sayı isə çox ola bilər.\n",
    "\n",
    "Imbalance problemi modelin performansını mənfi təsir edir, çünki model çox vaxt dominant sinifin nümunələrini düzgün təsnif etməyə meyilli olur. Bunun nəticəsində, model azlıqda olan sinifin nümunələrini düzgün öyrənə bilmir və bu da overall performansı zəiflədir.\n",
    "\n",
    "Bu problemi həll etmək üçün bir neçə metod istifadə olunur:\n",
    "\n",
    "### 1. **Resampling (Nümunələrin Dəyişdirilməsi):**\n",
    "   - **Oversampling (Çoxaldma):** Azlıqda olan sinifin nümunələrini çoxaltmaqla balans yaratmaq olar. `SMOTE (Synthetic Minority Over-sampling Technique)` kimi metodlar istifadə olunur.\n",
    "   - **Undersampling (Azaldılma):** Çoxluqda olan sinifin nümunələrini azaldaraq balans təmin etmək olar. Lakin bu üsul datanın bir hissəsini itirmək riskini yaradır.\n",
    "\n",
    "### 2. **Class Weights (Sinif Çəki Dəyərləri):**\n",
    "   Təsnifat modellərində (məsələn, `RandomForest`, `SVM`, `XGBoost`) sinif çəki dəyərlərini tənzimləməklə azlıqda olan sinifin daha vacib olduğunu göstərmək olar. Bununla model azlıq sinifə daha çox diqqət yetirir.\n",
    "\n",
    "### 3. **Anomaly Detection (Anomaliya Aşkarlanması):**\n",
    "   Əgər azlıq sinifi anomaliyaya bənzəyirsə, anomaly detection metodları ilə bu sinifi tapmaq mümkün ola bilər.\n",
    "\n",
    "### 4. **Evaluation Metrics (Qiymətləndirmə Metodları):**\n",
    "   Klassik ölçüm metrikləri (məsələn, accuracy) imbalance datada yanıltıcı ola bilər. Bunun yerinə `precision`, `recall`, `F1-score`, `ROC-AUC` kimi metriklərdən istifadə etmək daha uyğundur.\n",
    "\n",
    "Imbalance problemi ilə qarşılaşanda bu metodlardan birini və ya bir neçəsini sınamaq balanslı və daha performanslı bir model yaratmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d354699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786294d8",
   "metadata": {},
   "source": [
    "## Nece faiz nisbetden sonra imblance sayilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2bbeb",
   "metadata": {},
   "source": [
    "Imbalance problemi adətən siniflər arasında əhəmiyyətli fərqlər olduqda meydana çıxır. Ümumiyyətlə, siniflər arasında 1:1.5 və ya 1:2 nisbəti qədər fərq olduqda balanslı (balanced) hesab edilir. Əgər bir sinif digərindən 1:4 və ya daha çox dəfə böyükdürsə, bu artıq imbalance problemi kimi qiymətləndirilə bilər.\n",
    "\n",
    "Daha dəqiq desək, bəzi tədqiqatçılar siniflər arasında 70% və ya daha çox fərqin (yəni bir sinifin nümunələrinin 70%-dən çoxunu təşkil etməsi) imbalance problemi yaratdığını deyirlər. Məsələn, bir datasetdə bir sinif 90% və digər sinif 10% təşkil edirsə, bu ciddi bir imbalance olaraq qəbul edilir.\n",
    "\n",
    "Əsas qayda odur ki, əgər azlıqda olan sinifin modellə düzgün öyrənilməsi çətinləşirsə və model çoxluqda olan sinifə üstünlük verirsə, bu zaman imbalance problemi var deməkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372da750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d5322b",
   "metadata": {},
   "source": [
    "## Butun feature sutunlari etsem ne bas verecek ki?  1 ve 0 olanlar deyise biler. yoxsa oldugu kimi saxlayacaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e99c66",
   "metadata": {},
   "source": [
    "Əgər datada categorical dəyişənləri artıq `1` və `0` olaraq çevirmisinizsə, həmin dəyişənlər artıq binary formatdadır və onları yenidən scale etmək lazım deyil. Bu tip binary dəyişənlər onsuz da məhdud aralıqda olur (0 və 1), ona görə də scale etməyə ehtiyac yoxdur.\n",
    "\n",
    "Numeric dəyişənləri isə scale etməniz məqsədəuyğundur, çünki onlar fərqli böyüklükdə ola bilər və bəzi modellər (məsələn, SVM, KNN və ya linear regression modelləri) fərqli miqyaslarda olan dəyişənlərdən mənfi təsirlənə bilər.\n",
    "\n",
    "Yəni, ümumi tövsiyə belədir:\n",
    "- Numeric dəyişənləri scale edin.\n",
    "- Binary (0 və 1 olan categorical) dəyişənləri scale etməyin.\n",
    "\n",
    "Bu addımları tətbiq etməklə datanızı daha optimal şəkildə modelə təqdim edə bilərsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357584b",
   "metadata": {},
   "source": [
    "Əgər bütün feature sütunlarını (həm numeric, həm də binary olanları) scale etsəniz, 0 və 1 olan binary sütunlar da dəyişəcək. Məsələn, `StandardScaler` və ya `MinMaxScaler` tətbiq edildikdə bu binary sütunlar 0 və 1 arasında başqa qiymətlər alacaq, çünki scale prosesi bütün dəyişənləri yeni miqyasda sıxışdırmağa çalışır.\n",
    "\n",
    "Məsələn, əgər `MinMaxScaler` tətbiq edirsinizsə:\n",
    "- `0` olan dəyərlər dəyişməz qalacaq.\n",
    "- `1` olan dəyərlər isə `min` və `max` aralığında yeni dəyər ala bilər (məsələn, 0.5 və ya başqa bir qiymət).\n",
    "\n",
    "Bu, modelin davranışını mənfi təsir edə bilər, çünki bəzi modellər binary dəyişənləri orijinal 0 və 1 aralığında gözləyir. Ona görə də binary sütunları scale etməmək daha məqsədəuyğundur.\n",
    "\n",
    "Əgər ümumi datanı scale etmək istəyirsinizsə, binary sütunları istisna etmək üçün kodunuzda bu dəyişənləri seçmədən yalnız numeric sütunlara scale tətbiq etməniz daha doğru olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34302a69",
   "metadata": {},
   "source": [
    "## Bes nece edim axi. nece kod yazim ki binary olanalra toxunmasin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0765452",
   "metadata": {},
   "source": [
    "Binary sütunlara toxunmadan yalnız numeric sütunları scale etmək üçün əvvəlcə numeric və binary sütunları ayırmalısınız. Python və `pandas` istifadə edərək bunu etmək üçün aşağıdakı addımları izləyə bilərsiniz:\n",
    "\n",
    "1. **Binary və numeric sütunları ayırın.**\n",
    "2. **Yalnız numeric sütunlara `scaler` tətbiq edin.**\n",
    "3. **Yenidən datafraimi birləşdirin.**\n",
    "\n",
    "Məsələn, aşağıdakı kimi bir kod yaza bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Məsələn, data frame-iniz olsun\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# Target və features sütunlarını müəyyən edin (məsələn, 'target' adlı bir sütun)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split edin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numeric sütunları seçin\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Binary sütunları müəyyən edin\n",
    "binary_cols = [col for col in numeric_cols if X_train[col].nunique() == 2 and sorted(X_train[col].unique()) == [0, 1]]\n",
    "\n",
    "# Binary olmayan numeric sütunları tapın\n",
    "numeric_cols_to_scale = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Scaler tətbiq edin yalnız numeric sütunlara\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Yalnız train datasında fit edin, amma həm train, həm də test datasında transform edin\n",
    "X_train[numeric_cols_to_scale] = scaler.fit_transform(X_train[numeric_cols_to_scale])\n",
    "X_test[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
    "\n",
    "# Artıq yalnız numeric sütunlar scale olundu, binary sütunlara toxunulmadı\n",
    "```\n",
    "\n",
    "### Açıqlama:\n",
    "- `select_dtypes(include=['float64', 'int64'])`: Numeric sütunları seçir.\n",
    "- `df[col].nunique() == 2`: Sütunda yalnız 2 unikal dəyər (0 və 1) olduğunu yoxlayır.\n",
    "- `sorted(df[col].unique()) == [0, 1]`: Sütundakı dəyərlərin həqiqətən 0 və 1 olmasını yoxlayır.\n",
    "- Son olaraq, yalnız numeric sütunlar scale olunur, binary sütunlara toxunulmur.\n",
    "\n",
    "Beləliklə, yalnız numeric sütunları scale edə bilərsiniz, binary sütunlar isə dəyişməz qalacaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eccd2960",
   "metadata": {},
   "source": [
    "## Outlier sixmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f32fe",
   "metadata": {},
   "source": [
    "Outlier-ləri sıxmaq üçün bir neçə fərqli metod var. Bu metodlar datasetin növündən, təbiətindən və məqsədlərinizdən asılı olaraq dəyişir. Aşağıda outlier-ləri sıxmaq üçün ən çox istifadə olunan üsullar və onların Python-da tətbiqi nümunələri ilə tanış ola bilərsiniz.\n",
    "\n",
    "### 1. **Z-Score Metodu ilə Sıxmaq**\n",
    "Z-score, hər bir dəyərin ortalamadan neçə standart sapma uzaqda olduğunu göstərir. Əgər bir dəyər müəyyən bir həddən böyükdürsə (məsələn, 3 standart sapmadan uzaqdırsa), onu həmin həddə sıxmaq olar.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "df\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score hesablayın (sayısal sütunlar üçün)\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# Z-scores hesablayın\n",
    "z_scores = zscore(df_)\n",
    "\n",
    "# Threshold təyin edin\n",
    "threshold = 3\n",
    "\n",
    "# Z-scores dəyəri 3-dən böyük olanları 3-ə sıxın, -3-dən kiçik olanları isə -3-ə sıxın\n",
    "z_scores_compressed = np.where(z_scores > threshold, threshold, \n",
    "                               np.where(z_scores < -threshold, -threshold, z_scores))\n",
    "\n",
    "# Z-scores əsasında sıxılmış dəyərləri orijinal df_-dəki dəyərlərlə əvəz edin\n",
    "df_compressed = df_.copy()\n",
    "\n",
    "for i, col in enumerate(df_.columns):\n",
    "    # Hər sütun üçün sıxılmış Z-scores dəyərlərini yenidən orijinal miqyasına çevirin\n",
    "    df_compressed[col] = (z_scores_compressed[:, i] * df_[col].std()) + df_[col].mean()\n",
    "\n",
    "df_compressed\n",
    "```\n",
    "\n",
    "### 2. **IQR (Interquartile Range) Metodu ilə Sıxmaq**\n",
    "IQR metodu, dəyərləri 1-ci və 3-cü kvartillər (Q1 və Q3) əsasında müəyyənləşdirir. Outlier-lər Q1 - 1.5 * IQR və Q3 + 1.5 * IQR intervalından kənarda olan dəyərlərdir. Bu hədləri aşan dəyərləri həmin həddə sıxışdıra bilərsiniz.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CSV faylını oxuyun\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "\n",
    "# Sayısal sütunları seçin və lazımsız sütunları silin\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "\n",
    "# Nəmissing dəyərləri sütun ortalamaları ilə doldurun\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# IQR istifadə edərək outlier-ləri sıxın\n",
    "df_clipped = df_.copy()\n",
    "\n",
    "for col in df_clipped.columns:\n",
    "    Q1 = df_clipped[col].quantile(0.25)\n",
    "    Q3 = df_clipped[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Outlier sərhədlərini müəyyən edin\n",
    "    lower_fence = Q1 - 1.5 * IQR\n",
    "    upper_fence = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Outlier-ləri sıxın\n",
    "    df_clipped[col] = df_clipped[col].clip(lower=lower_fence, upper=upper_fence)\n",
    "\n",
    "df_clipped\n",
    "```\n",
    "\n",
    "### 3. **Winsorization (Sərhəd Dəyərlərə Sıxmaq)**\n",
    "Winsorization metodu, outlier-ləri müəyyən bir həddə sıxışdırır, yəni outlier-lər alt və üst limitlərdən kənara çıxa bilmir.\n",
    "\n",
    "```python\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize tətbiq edin (1% alt və 1% üst həddə sıxışdırma)\n",
    "df_winsorized = winsorize(df, limits=[0.01, 0.01])\n",
    "\n",
    "# Bu dəyərləri müəyyən bir həddə sıxışdırmış olursunuz\n",
    "```\n",
    "\n",
    "### 4. **Percentile Metodu ilə Sıxmaq**\n",
    "Bəzi hallarda, outlier-ləri percentil əsaslı metodlarla müəyyən etmək faydalı ola bilər. 1%-dən aşağı və ya 99%-dən yuxarı olan dəyərləri sıxmaq üçün aşağıdakı koddan istifadə edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "# Alt və üst hədləri təyin edin\n",
    "lower_percentile = df.quantile(0.01)\n",
    "upper_percentile = df.quantile(0.99)\n",
    "\n",
    "# Outlier-ləri sıxın\n",
    "df_clipped = np.where(df < lower_percentile, lower_percentile, df)\n",
    "df_clipped = np.where(df > upper_percentile, upper_percentile, df)\n",
    "```\n",
    "\n",
    "### 5. **Log Transformasiya**\n",
    "Əgər outlier-lər data massivində çox geniş fərqlənirsə, log transformasiya dəyərləri daha kiçik miqyasda sıxmağa kömək edə bilər.\n",
    "\n",
    "```python\n",
    "df_log_transformed = np.log1p(df)  # 1 əlavə edərək log transformasiya edin\n",
    "```\n",
    "\n",
    "### 6. **Clipping (Sərhədli Dəyərlərlə Əvəz Etmək)**\n",
    "Bir çox hallarda dəyərləri müəyyən bir hədd ilə sıxmaq üçün birbaşa `numpy`-ın `clip()` funksiyasından istifadə edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "df_clipped = np.clip(df, lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "### Hansı Metodu Seçməli?\n",
    "- **Z-score**: Daha çox normal paylanma üçün uyğundur.\n",
    "- **IQR**: Paylanma asimmetrikdirsə və outlier-lər çox genişdirsə istifadə edilir.\n",
    "- **Winsorization və Percentile**: Daha stabildir və sərhəd dəyərləri təyin etmək asandır.\n",
    "- **Log Transformasiya**: Exponential (eksponential) və ya geniş miqyaslı paylanmalar üçün uyğundur.\n",
    "\n",
    "Hansı metoddan istifadə edəcəyiniz datasetin təbiətindən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf446d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2677df6d",
   "metadata": {},
   "source": [
    "## Missing deyerleri doldurmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1816a7",
   "metadata": {},
   "source": [
    "Missing dəyərləri doldurmaq (imputation) analitik və maşın öyrənməsi işlərində tez-tez qarşılaşılan problemlərdən biridir. Aşağıda bir neçə effektiv yanaşma və onların Python vasitəsilə həyata keçirilməsi yollarını görə bilərsiniz.\n",
    "\n",
    "### 1. **Ortalama (Mean) ilə Doldurma**\n",
    "Bu üsul sadədir və əsasən **numerik** verilənlər üçün tətbiq olunur. Missing dəyərləri həmin sütunun ortalama (mean) dəyəri ilə əvəz edirsiniz.\n",
    "\n",
    "```python\n",
    "# Numerik sütundakı missing dəyərləri ortalama ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mean())\n",
    "```\n",
    "\n",
    "### 2. **Median (Orta Əsaslı) Doldurma**\n",
    "Median üsulu, xüsusilə outlier-lərin çox olduğu datasetlərdə daha sabit nəticələr verir. Bu üsul da əsasən **numerik** verilənlər üçün istifadə olunur.\n",
    "\n",
    "```python\n",
    "# Missing dəyərləri median ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].median())\n",
    "```\n",
    "\n",
    "### 3. **Mod (Modus) ilə Doldurma**\n",
    "**Kategorik** (category) verilənlər üçün daha çox istifadə edilən üsuldur. Mod üsulu, sütundakı ən çox təkrarlanan dəyəri missing dəyərlərə əlavə edir.\n",
    "\n",
    "```python\n",
    "# Kategorik sütunu mod ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mode()[0])\n",
    "```\n",
    "\n",
    "### 4. **Sabit Dəyər ilə Doldurma**\n",
    "Bəzi hallarda missing dəyərləri sabit bir dəyərlə əvəz etmək lazım ola bilər (məsələn, 0 və ya müəyyən kateqoriya).\n",
    "\n",
    "```python\n",
    "# Sabit dəyərlə doldurmaq (məsələn, 0 ilə)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(0)\n",
    "\n",
    "# Kategorik verilənlər üçün sabit kateqoriya təyin etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna('Unknown')\n",
    "```\n",
    "\n",
    "### 5. **Linear İnteqrasiya ilə Doldurma (Interpolation)**\n",
    "Linear inteqrasiya metodu zaman seriyası məlumatları və ya ardıcıl numerik məlumatlar üçün uyğundur. Məlumatın əvvəlki və sonrakı dəyərləri əsasında missing dəyərləri interpolasiya edərək hesablayır.\n",
    "\n",
    "```python\n",
    "# Linear interpolation tətbiq etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].interpolate()\n",
    "```\n",
    "\n",
    "### 6. **K-NN (K-Nearest Neighbors) İmputation**\n",
    "K-NN imputation hər bir missing dəyəri ən yaxın qonşu dəyərlər əsasında tamamlayır. Bu üsul həm numerik, həm də kategorik dəyərlər üçün istifadə oluna bilər.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)  # 5 ən yaxın qonşu ilə doldurmaq\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 7. **MICE (Multiple Imputation by Chained Equations)**\n",
    "MICE çoxlu itkin dəyərlərin olduğu datasetlər üçün istifadə edilən bir üsuldur. Bu metod hər bir missing dəyəri digər dəyişənlərdən istifadə edərək təkrarlanmış şəkildə təxmin edir.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# MICE ilə doldurma\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 8. **Fərqli Sütunlar Əsasında Doldurma**\n",
    "Bəzi hallarda missing dəyərləri doldurmaq üçün başqa bir sütunun məlumatından istifadə etmək lazım ola bilər.\n",
    "\n",
    "```python\n",
    "# Əgər 'sütun_adi1' missingdirsə, onu 'sütun_adi2' ilə doldur\n",
    "df['sütun_adi1'] = df['sütun_adi1'].fillna(df['sütun_adi2'])\n",
    "```\n",
    "\n",
    "### 9. **Foward Fill və Backward Fill (Zaman Seriyasında)**\n",
    "Zaman seriyasında əvvəlki və ya sonrakı dəyərləri missing dəyərlə əvəz etməyə imkan verir.\n",
    "\n",
    "```python\n",
    "# Əvvəlki dəyərlə doldurmaq (forward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='ffill')\n",
    "\n",
    "# Sonrakı dəyərlə doldurmaq (backward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='bfill')\n",
    "```\n",
    "\n",
    "### 10. **Model Əsaslı Doldurma**\n",
    "Daha kompleks yanaşma kimi, missing dəyərləri təxmin etmək üçün maşın öyrənməsi modellərindən istifadə edə bilərsiniz. Məsələn, regression və ya classification modelləri ilə missing dəyərləri təxmin etmək mümkündür.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Missing dəyərləri təxmin etmək üçün model istifadə edin\n",
    "model = LinearRegression()\n",
    "X_train = df.dropna(subset=['sütun_adi'])  # Missing olmayan dəyərlərlə modeli öyrədin\n",
    "y_train = X_train.pop('sütun_adi')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Missing dəyərləri təxmin edin\n",
    "X_missing = df[df['sütun_adi'].isnull()].drop('sütun_adi', axis=1)\n",
    "df.loc[df['sütun_adi'].isnull(), 'sütun_adi'] = model.predict(X_missing)\n",
    "```\n",
    "\n",
    "### Hansı Metodun Seçilməsi:\n",
    "- **Ortalama və Median**: Numerik sütunlar üçün tez-tez istifadə olunur.\n",
    "- **Modus və Sabit Dəyər**: Kategorik sütunlar üçün uyğundur.\n",
    "- **K-NN və MICE**: Çox dəyişkənli imputation tələb edən datasetlərdə daha dəqiqdir.\n",
    "- **Interpolation**: Zaman seriyaları üçün ideal metodlardan biridir.\n",
    "\n",
    "Hansı metodu seçməyiniz datasetin növündən və analiz məqsədinizdən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617523e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d9866c",
   "metadata": {},
   "source": [
    "## XGBOOST esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e625c",
   "metadata": {},
   "source": [
    "XGBoost-un əsas parametrləri və onların təklif olunan dəyərləri aşağıdakılardır:\n",
    "\n",
    "### 1. `booster`\n",
    "- **Izah**: Modelin öyrənmə üsulunu təyin edir. \n",
    "- **Dəyərlər**:\n",
    "  - `'gbtree'`: Ən çox istifadə edilən ağac əsaslı modeldir. Çoxsaylı qərar ağacları yaradır.\n",
    "  - `'gblinear'`: Xətti modeldir. Xətti birbaşa nəticələr təxmin edir.\n",
    "  - `'dart'`: Dropouts ilə ağac modelləri, daha çox tənzimləmə təmin edir.\n",
    "\n",
    "### 2. `learning_rate` (və ya `eta`)\n",
    "- **Izah**: Hər iterasiyada öyrənmə sürətini müəyyən edir. Kiçik dəyərlər daha yavaş öyrənmə sürəti təmin edir və daha çox iterasiya tələb edə bilər.\n",
    "- **Dəyərlər**: 0.01 ilə 0.3 arasında, məsələn, 0.01, 0.1, 0.2.\n",
    "\n",
    "### 3. `n_estimators`\n",
    "- **Izah**: Təlimdə istifadə olunan ağacların sayını göstərir.\n",
    "- **Dəyərlər**: 50 ilə 1000 arasında, məsələn, 100, 200, 500.\n",
    "\n",
    "### 4. `max_depth`\n",
    "- **Izah**: Hər bir qərar ağacının maksimum dərinliyini müəyyən edir. Daha böyük dəyərlər daha kompleks ağaclar yaradır.\n",
    "- **Dəyərlər**: 3 ilə 15 arasında, məsələn, 3, 6, 10.\n",
    "\n",
    "### 5. `min_child_weight`\n",
    "- **Izah**: Bir düyündəki minimum nümunə çəkisini təyin edir. Bu, bir düyünün bölünməsi üçün tələb olunan minimal nümunə çəkisini göstərir.\n",
    "- **Dəyərlər**: 1 ilə 10 arasında, məsələn, 1, 3, 5.\n",
    "\n",
    "### 6. `subsample`\n",
    "- **Izah**: Hər iterasiyada istifadə olunan nümunələrin hissəsini müəyyən edir. Kiçik dəyərlər daha çox tənzimləmə təmin edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 7. `colsample_bytree`\n",
    "- **Izah**: Hər ağac üçün istifadə olunan xüsusiyyətlərin hissəsini müəyyən edir. Bu parametr xüsusiyyətlərin təsadüfi alt qruplarını istifadə edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 8. `lambda` (və ya `reg_lambda`)\n",
    "- **Izah**: L2 (Ridge) cərimələmə parametridir. Modelin sadələşdirilməsinə kömək edir və aşırı uyğunlaşmanı azaldır.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 9. `alpha` (və ya `reg_alpha`)\n",
    "- **Izah**: L1 (Lasso) cərimələmə parametridir. Bu, bəzi xüsusiyyətlərin çəkilərini sıfıra endirərək modelin sadələşdirilməsinə kömək edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 10. `gamma`\n",
    "- **Izah**: Bölünmə üçün tələb olunan minimal qazanma dəyərini təyin edir. Daha yüksək dəyərlər ağacların bölünməsini daha sərt edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 5.\n",
    "\n",
    "### 11. `scale_pos_weight`\n",
    "- **Izah**: Sinif balansı məsələlərində istifadə olunur, çünki modelin siniflərin tarazlığına uyğunlaşdırılmasına kömək edir.\n",
    "- **Dəyərlər**: 1 ilə (nöqtələr arasındakı fərqə əsasən dəyişir), məsələn, 1, 10, 20.\n",
    "\n",
    "### 12. `objective`\n",
    "- **Izah**: Məqsəd funksiyasını müəyyən edir. Problemin növünə görə dəyişir, məsələn, `reg:squarederror` (regressiya üçün), `binary:logistic` (ikili təsnifat üçün).\n",
    "- **Dəyərlər**:\n",
    "  - `'reg:squarederror'`: Regressiya üçün.\n",
    "  - `'binary:logistic'`: İkili təsnifat üçün.\n",
    "  - `'multi:softmax'`: Çoxsinifli təsnifat üçün (proqnozlar sinif etiketləri şəklində verilir).\n",
    "\n",
    "### 13. `eval_metric`\n",
    "- **Izah**: Modelin qiymətləndirilməsi üçün istifadə olunan ölçü vahidini müəyyən edir.\n",
    "- **Dəyərlər**:\n",
    "  - `'rmse'`: Kök orta kvadrat səhv (regressiya üçün).\n",
    "  - `'logloss'`: Loqaritmik itki (ikili təsnifat üçün).\n",
    "  - `'mlogloss'`: Çoxsinifli loqaritmik itki (çoxsinifli təsnifat üçün).\n",
    "\n",
    "Parametrləri tənzimləyərkən, ən yaxşı nəticələri əldə etmək üçün çapraz təsdiqləmə (cross-validation) istifadə edərək, hər parametrin müxtəlif dəyərlərini yoxlamaq tövsiyə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664514d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06f0c06",
   "metadata": {},
   "source": [
    "## GridSearchCV esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eea088",
   "metadata": {},
   "source": [
    "`GridSearchCV`, verilən parametrlər üçün bütün mümkün kombinasiyaları sınayaraq ən yaxşı nəticəni tapmaq üçün istifadə olunur. Bu alqoritm hər bir parametri müxtəlif dəyərlərlə sınayır və bu, daha dəqiq bir optimallaşdırma aparmağa imkan verir. Lakin, çox sayda parametr və dəyərlər olduqda bu proses çox vaxt ala bilər.\n",
    "\n",
    "### Əsas Parametrlər və Onların İzahı\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **Açıqlama**: Parametrlərini optimallaşdırmaq istədiyiniz modeldir. Bu model, `fit` və `predict` metodlarına sahib olmalıdır.\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()`, `LogisticRegression()`, və s.\n",
    "\n",
    "2. **`param_grid`**:\n",
    "   - **Açıqlama**: Modelin optimallaşdırılacaq hiperparametrlərinin dəyərlərini təyin edən grid (tor) strukturdur. Bütün parametrlərin dəyərlərini sınamaq üçün istifadə olunur.\n",
    "   - **Dəyərlər**: Parametrlərin mümkün dəyərlərini ehtiva edən `dict` formatında olmalıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     param_grid = {\n",
    "         'n_estimators': [50, 100, 200],\n",
    "         'learning_rate': [0.01, 0.1, 0.2],\n",
    "         'max_depth': [3, 5, 7],\n",
    "         'subsample': [0.6, 0.8, 1.0],\n",
    "         'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün istifadə olunan qiymətləndirmə kriteriyasıdır.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`\n",
    "\n",
    "4. **`cv`** (Cross-validation):\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə prosesində məlumatları neçə qrupa bölmək lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri (məsələn, 3, 5, 10).\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "5. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir. Bütün nüvələri istifadə etməklə paralelləşdirilmiş hesablama mümkündür.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "6. **`verbose`**:\n",
    "   - **Açıqlama**: Axtarış prosesi zamanı nə qədər detallı geri bildirim göstəriləcəyini təyin edir.\n",
    "   - **Dəyərlər**: 0 (səssiz rejim), 1 (minimum məlumat), 2 və daha yüksək (daha çox məlumat).\n",
    "   - **Nümunə**: `verbose=2`\n",
    "\n",
    "7. **`refit`**:\n",
    "   - **Açıqlama**: Ən yaxşı tapılan parametrlərlə modelin yenidən uyğunlaşdırılıb-uyğunlaşdırılmayacağını təyin edir.\n",
    "   - **Dəyərlər**: `True` (model refit ediləcək), `False` (model refit edilməyəcək).\n",
    "   - **Nümunə**: `refit=True`\n",
    "\n",
    "8. **`pre_dispatch`**:\n",
    "   - **Açıqlama**: Paralelləşdirilmiş hesablama zamanı nə qədər iş tapşırığının əvvəlcədən göndərilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri və ya `'2*n_jobs'` kimi nisbət.\n",
    "   - **Nümunə**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "9. **`error_score`**:\n",
    "   - **Açıqlama**: Modelin bir parametri ilə səhv baş verərsə, o vəziyyətdə neçə xal verilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `float` və ya `np.nan` dəyəri (nəticə olmaması).\n",
    "   - **Nümunə**: `error_score=np.nan`\n",
    "\n",
    "### GridSearchCV ilə XGBClassifier tətbiqi\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr grid-i\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# XGBClassifier modeli\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# GridSearchCV obyektinin yaradılması\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Modelin uyğunlaşdırılması (fit)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və nəticəni çap etmək\n",
    "print(f\"Ən yaxşı parametrlər: {grid_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {grid_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### Parametrlərin Dəyərlər Aralıqları\n",
    "\n",
    "1. **`n_estimators`** (Ağac sayı):\n",
    "   - Aralıq: [50, 100, 200, 500]\n",
    "   \n",
    "2. **`max_depth`** (Ağacların dərinliyi):\n",
    "   - Aralıq: [3, 5, 7, 10]\n",
    "   \n",
    "3. **`learning_rate`** (Öyrənmə sürəti):\n",
    "   - Aralıq: [0.01, 0.05, 0.1, 0.2]\n",
    "   \n",
    "4. **`subsample`** (Nümunə payı):\n",
    "   - Aralıq: [0.5, 0.7, 1.0]\n",
    "   \n",
    "5. **`colsample_bytree`** (Xüsusiyyət payı):\n",
    "   - Aralıq: [0.6, 0.8, 1.0]\n",
    "\n",
    "6. **`gamma`** (Bölünmə üçün minimum itki azaldılması):\n",
    "   - Aralıq: [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "Bu aralıqlar verilən problemə görə genişlədilə və ya daraldıla bilər. `GridSearchCV` bütün bu aralıqlardakı mümkün kombinasiyaları sınayaraq ən optimal parametrləri təyin edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889bfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bace169f",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1859a5",
   "metadata": {},
   "source": [
    "`RandomizedSearchCV` istifadə edərkən, müxtəlif parametrlər var və hər birinin müəyyən funksiyası var. Hər parametrin də nə dəyər ala biləcəyini və bu dəyərlərin aralıqlarını başa düşmək vacibdir. Gəlin, əsas parametrləri və onların izahını nəzərdən keçirək.\n",
    "\n",
    "### `RandomizedSearchCV` əsas parametrləri və izahı:\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **İzah**: Axtarılacaq model. Məsələn, `XGBClassifier`, `SVC`, `RandomForestClassifier`, və s.\n",
    "   - **Dəyər aralığı**: Bir machine learning modeli (`estimator`) obyektini alır.\n",
    "\n",
    "2. **`param_distributions`**:\n",
    "   - **İzah**: Hiperparametrlərin təsadüfi axtarışı üçün parametr dəyərlərinin aralıqları.\n",
    "   - **Dəyər aralığı**: Hiperparametrlərin mümkün dəyərlərinin siyahısını və ya numpy array-i kimi obyektlər alır.\n",
    "   - **Misal**:\n",
    "     ```python\n",
    "     param_dist = {\n",
    "         'n_estimators': np.arange(50, 400, 50),\n",
    "         'max_depth': [3, 4, 5, 6],\n",
    "         'learning_rate': np.linspace(0.01, 0.3, 10)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`n_iter`**:\n",
    "   - **İzah**: Axtarış zamanı təsadüfi olaraq neçə kombinasiya yoxlanılacaq.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (adətən 10-100 arası). Axtarışın sürəti və keyfiyyəti arasında balans yaradılır.\n",
    "   - **Misal**: `n_iter=50`\n",
    "\n",
    "4. **`cv`**:\n",
    "   - **İzah**: Çarpaz təsdiqləmə qatlarının sayı (neçə fold ilə model yoxlanılacaq).\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (məsələn, `cv=3`, `cv=5`). Daha çox qat dəqiq nəticələr verə bilər, lakin təlim müddətini uzadır.\n",
    "   - **Misal**: `cv=5` (beş qatlı çarpaz təsdiqləmə)\n",
    "\n",
    "5. **`scoring`**:\n",
    "   - **İzah**: Modelin performansını qiymətləndirmək üçün ölçü.\n",
    "   - **Dəyər aralığı**: `accuracy`, `f1`, `roc_auc`, `precision`, `recall`, və s.\n",
    "   - **Misal**: `scoring='accuracy'`, `scoring='roc_auc'`\n",
    "\n",
    "6. **`random_state`**:\n",
    "   - **İzah**: Təkrarlanabilirlik üçün təsadüfi toxum. Nəticələrin təkrarlanabilməsi üçün istifadə edilir.\n",
    "   - **Dəyər aralığı**: İstənilən tam ədəd (məsələn, `random_state=42`).\n",
    "   - **Misal**: `random_state=42`\n",
    "\n",
    "7. **`verbose`**:\n",
    "   - **İzah**: Təlim prosesi haqqında ekrana nə qədər məlumat çıxarılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: `0`, `1`, `2`, və ya daha yüksək (daha yüksək dəyərlər daha çox məlumat göstərir).\n",
    "   - **Misal**: `verbose=1` (orta dərəcədə məlumat)\n",
    "\n",
    "8. **`n_jobs`**:\n",
    "   - **İzah**: Paralel işləmə üçün istifadə ediləcək CPU nüvələrinin sayı.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd, `-1` bütün nüvələri istifadə etmək deməkdir.\n",
    "   - **Misal**: `n_jobs=-1` (bütün mövcud CPU nüvələrini istifadə etmək üçün)\n",
    "\n",
    "9. **`pre_dispatch`**:\n",
    "   - **İzah**: Paralel axtarış üçün neçə iş prosesinin əvvəlcədən yaradılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: Tam ədəd və ya `all` kimi dəyərlər ala bilər.\n",
    "   - **Misal**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "### XGBClassifier ilə `RandomizedSearchCV` misalı:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr aralığı\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 400, 50),       # Ağac sayı\n",
    "    'max_depth': np.arange(3, 10),                # Ağacların dərinliyi\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Öyrənmə sürəti\n",
    "    'subsample': np.linspace(0.5, 1.0, 5),        # Nümunə payı\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 5), # Xüsusiyyət payı\n",
    "    'gamma': np.linspace(0, 0.5, 5),              # Bölünmə üçün minimum itki azaldılması\n",
    "    'reg_alpha': np.linspace(0, 1, 5),            # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': np.linspace(1, 2, 5),           # L2 cəriməsi (Ridge)\n",
    "    'booster': ['gbtree', 'dart']                 # Booster növü\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV obyektinin yaradılması\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50,           # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=5,                 # Çarpaz təsdiqləmə qatları\n",
    "    verbose=1,            # Geri bildirim səviyyəsi\n",
    "    random_state=42,      # Təsadüfi toxum\n",
    "    n_jobs=-1             # Bütün CPU nüvələrini istifadə et\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {random_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {random_search.best_score_}\")\n",
    "```\n",
    "\n",
    "Bu kod `RandomizedSearchCV` istifadə edərək `XGBClassifier` modelinin hiperparametrlərini optimallaşdırır və ən yaxşı parametrləri çıxarır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9486f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3063f1",
   "metadata": {},
   "source": [
    "## BayesSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07a07e",
   "metadata": {},
   "source": [
    "`BayesSearchCV` modeli, hiperparametr optimallaşdırması üçün Bayesian Optimization metodundan istifadə edən bir alqoritmdir. Bu metod, hiperparametr arama prosesini daha ağıllı və sürətli edir, çünki hər bir axtarış addımı əvvəlki nəticələrə əsaslanaraq həyata keçirilir.\n",
    "\n",
    "### 1. **`estimator`**:\n",
    "   - **Açıqlama**: Optimallaşdırmaq istədiyiniz model obyektidir (məsələn, `XGBClassifier`).\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()` və s.\n",
    "\n",
    "### 2. **`search_spaces`**:\n",
    "   - **Açıqlama**: Bu parametr, optimallaşdırılacaq hiperparametrləri və onların dəyərlər aralığını təyin edir. `BayesSearchCV` bu aralığa əsaslanaraq axtarış həyata keçirir.\n",
    "   - **Dəyərlər**: Müxtəlif parametrlərin dəyərlər aralığıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     search_spaces = {\n",
    "         'n_estimators': (50, 400),\n",
    "         'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "         'max_depth': (3, 10),\n",
    "         'subsample': (0.5, 1.0),\n",
    "         'colsample_bytree': (0.5, 1.0),\n",
    "         'gamma': (0, 0.5),\n",
    "         'reg_alpha': (0, 1),\n",
    "         'reg_lambda': (1, 2)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### 3. **`n_iter`**:\n",
    "   - **Açıqlama**: Axtarış zamanı edilməsi planlaşdırılan iterasiya sayı. Bu dəyər Bayesian Optimization ilə təyin olunan fərqli hiperparametr dəstlərinin sayını bildirir.\n",
    "   - **Dəyərlər**: Müxtəlif iterasiya sayı.\n",
    "   - **Nümunə**: `n_iter=30` (30 iterasiya).\n",
    "\n",
    "### 4. **`cv`**:\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə (Cross-validation) qatlarının sayını təyin edir. Məlumatları neçə qrupa böləcəyini göstərir.\n",
    "   - **Dəyərlər**: 3, 5 və ya 10.\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "### 5. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün hansı metrikdən istifadə olunacağını təyin edir.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`.\n",
    "\n",
    "### 6. **`n_points`**:\n",
    "   - **Açıqlama**: Hər iterasiyada eyni anda neçə fərqli parametri sınamaq lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `n_points=1` (hər iterasiyada bir parametr dəsti sınanır), `n_points=2` (hər iterasiyada iki parametr dəsti sınanır).\n",
    "   - **Nümunə**: `n_points=3` (hər iterasiyada 3 fərqli parametr dəsti sınanır).\n",
    "\n",
    "### 7. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "### 8. **`random_state`**:\n",
    "   - **Açıqlama**: Nəticələrin təkrar istehsal edilə bilməsi üçün təsadüfi toxum dəyəri təyin edir.\n",
    "   - **Dəyərlər**: İstənilən integer dəyəri.\n",
    "   - **Nümunə**: `random_state=42`.\n",
    "\n",
    "### 9. **`optimizer_kwargs`**:\n",
    "   - **Açıqlama**: Bayesian Optimization alqoritmi ilə bağlı əlavə seçimləri idarə edir (məsələn, akvizisiya funksiyası).\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     optimizer_kwargs = {\n",
    "         'base_estimator': 'GP',  # Gaussian Process (GP) ilə optimizasiya edəcək\n",
    "         'acq_func': 'EI'         # Expected Improvement (EI) akvizisiya funksiyası\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### XGBClassifier ilə `BayesSearchCV` tətbiqi nümunəsi:\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Hiperparametr aralığı\n",
    "search_spaces = {\n",
    "    'n_estimators': (50, 400),                # Ağac sayı\n",
    "    'learning_rate': (0.01, 0.3, 'log-uniform'), # Öyrənmə sürəti\n",
    "    'max_depth': (3, 10),                     # Maksimum dərinlik\n",
    "    'subsample': (0.5, 1.0),                  # Nümunə payı\n",
    "    'colsample_bytree': (0.5, 1.0),           # Xüsusiyyət payı\n",
    "    'gamma': (0, 0.5),                        # Minimum itki azaldılması\n",
    "    'reg_alpha': (0, 1),                      # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': (1, 2)                      # L2 cəriməsi (Ridge)\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# BayesSearchCV obyektinin yaradılması\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb, \n",
    "    search_spaces=search_spaces, \n",
    "    n_iter=30,            # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=3,                 # Çarpaz təsdiqləmə qatları\n",
    "    n_jobs=-1,            # Bütün CPU nüvələrini istifadə et\n",
    "    random_state=42       # Təsadüfi toxum\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {bayes_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {bayes_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### BayesSearchCV-nin əsas üstünlükləri:\n",
    "- **Təsadüfi olmaması**: Bayesian Optimization əvvəlki nəticələrə əsaslanaraq növbəti parametri ağıllı şəkildə seçir.\n",
    "- **Daha az iterasiya**: RandomizedSearchCV ilə müqayisədə daha az iterasiya ilə daha optimal nəticələr əldə etmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df419cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
