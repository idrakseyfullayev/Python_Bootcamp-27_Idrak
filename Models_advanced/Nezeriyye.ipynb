{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816ef60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d82ebba",
   "metadata": {},
   "source": [
    "## Normal paylanma nədir bəs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d720a",
   "metadata": {},
   "source": [
    "**Normal paylanma** statistikada çox mühüm bir anlayışdır və bir çox real dünya hadisələri təxmini normal paylanmaya uyğun gəlir. Bu paylanma **Gauss paylanması** və ya **zəng əyrisi** (bell curve) kimi də tanınır. Normal paylanmanın əsas xüsusiyyəti onun simmetrik olmasıdır və ortalama (mean), median (orta dəyər) və moda (ən çox təkrarlanan dəyər) eyni nöqtədə yerləşir.\n",
    "\n",
    "### Normal paylanmanın xüsusiyyətləri:\n",
    "1. **Simmetrik əyri**: Paylanmanın sol və sağ tərəfləri tamamilə simmetrikdir.\n",
    "2. **Ortalamaya yaxınlıq**: Dəyərlərin böyük bir hissəsi ortalama ətrafında cəmləşir. Dəyərlər ortalamadan uzaqlaşdıqca azalmağa başlayır.\n",
    "3. **Zəng əyrisi (bell-shaped curve)**: Qrafik olaraq normal paylanmanın əyrisi zəngə bənzəyir.\n",
    "4. **Standart sapma**: Normal paylanmada dəyərlərin 68%-i bir standart sapma (σ) daxilində, 95%-i iki standart sapma daxilində, 99.7%-i isə üç standart sapma daxilində olur. Bu qayda **68-95-99.7 qaydası** kimi tanınır.\n",
    "\n",
    "### Normal paylanmanın qrafiki:\n",
    "- **Orta** (mean) mərkəzdədir.\n",
    "- Əyrinin hər iki tərəfi simmetrik olaraq bərabərdir.\n",
    "- Paylanmanın böyük hissəsi orta qiymətə yaxın olan dəyərlərdir.\n",
    "- Hər iki ucda ekstremal dəyərlər çox nadir hallarda baş verir.\n",
    "\n",
    "### Normal paylanmanın formulu:\n",
    "Normal paylanmanın sıxlıq funksiyası aşağıdakı kimi göstərilir:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Burada:\n",
    "- **μ**: Paylanmanın orta qiyməti,\n",
    "- **σ**: Standart sapma,\n",
    "- **x**: Təsadüfi dəyər,\n",
    "- **e**: Təbii logaritm əsası (təxminən 2.718).\n",
    "\n",
    "### Normal paylanmanın əhəmiyyəti:\n",
    "1. **Təbiət və sosial hadisələr**: Çoxlu fiziki və sosial hadisələr normal paylanmaya yaxındır. Məsələn, insanların boyu, çəkisi, test nəticələri və s. adətən normal paylanmaya uyğundur.\n",
    "   \n",
    "2. **Fərziyyə testləri və statistika**: Bir çox statistik metod və fərziyyə testləri, məsələn, Z-test və t-test, normal paylanmaya əsaslanır.\n",
    "   \n",
    "3. **Qeyri-müəyyənliyin ölçülməsi**: Normal paylanma qeyri-müəyyənliyi qiymətləndirmək üçün istifadə olunur və bir çox inferensial statistika üsulları onun əsasında qurulub.\n",
    "\n",
    "### 68-95-99.7 qaydası:\n",
    "Bu qayda normal paylanmada dəyərlərin nə qədərinin orta qiymətə yaxın olacağını göstərir:\n",
    "- Təxminən **68%** dəyər bir standart sapma (σ) daxilində (yəni ortalama ± 1σ).\n",
    "- Təxminən **95%** dəyər iki standart sapma (yəni ortalama ± 2σ).\n",
    "- Təxminən **99.7%** dəyər üç standart sapma (yəni ortalama ± 3σ).\n",
    "\n",
    "### Normal paylanmanın tətbiqi:\n",
    "- **Təhsil**: Tələbələrin imtahan nəticələri normal paylanmaya yaxın ola bilər.\n",
    "- **Tibb**: Müxtəlif tibbi parametrlər (məsələn, qan təzyiqi, bədən istiliyi) əhalidə normal paylanmaya uyğunlaşır.\n",
    "- **Maliyyə**: Qiymət dəyişiklikləri və səhmlərin gəlirləri tez-tez normal paylanma qəbul edilərək analiz edilir.\n",
    "\n",
    "Yekun olaraq, **normal paylanma** dəyərlərin ortalama ətrafında cəmləşdiyi, müəyyən qaydalara uyğun olaraq simmetrik paylanmadır və statistik analizdə geniş tətbiq olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6974d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5eb778",
   "metadata": {},
   "source": [
    "## Datanın normal paylanması nə deməkdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e868ad7",
   "metadata": {},
   "source": [
    "**Datanın normal paylanması**, verilmiş məlumatların **normal paylanmaya** uyğun olaraq bölündüyü anlamına gəlir. Yəni, məlumatların böyük bir hissəsi orta qiymət (mean) ətrafında cəmlənir və məlumatların dəyərləri simmetrik olaraq ortalamadan uzaqlaşdıqca azalmağa başlayır. Bu, məlumatların zəng əyrisi (bell-shaped curve) formasında qrafikləşməsinə səbəb olur.\n",
    "\n",
    "Datanın normal paylanması deməkdir ki:\n",
    "\n",
    "1. **Ortalamaya yaxın dəyərlər daha çoxdur**: Məlumat dəstəsinin böyük hissəsi ortalama ətrafında yerləşir. Məsələn, insanların boylarını ölçən bir dataset varsa, insanların çoxunun boyu orta boy ətrafında olacaq.\n",
    "   \n",
    "2. **Simmetrik paylanma**: Ortalamanın solunda və sağında dəyərlər bərabər sayda olur. Başqa sözlə, ortalamadan nə qədər uzaqlaşırsanız, həm kiçik, həm də böyük dəyərlərin tezliyi eyni dərəcədə azalır.\n",
    "\n",
    "3. **Outlier-lər azdır**: Ekstremal dəyərlər (çox kiçik və ya çox böyük dəyərlər) çox nadir hallarda olur. Normal paylanmada outlier-lər ortalamadan ən uzaq dəyərlər olur və nadir hallarda rast gəlinir.\n",
    "\n",
    "### Məlumatların normal paylanmasını anlamaq üçün bir neçə əsas nöqtə:\n",
    "\n",
    "#### 1. **Ortalamalar və standart sapmalar**:\n",
    "- **Orta (mean)**, **median** və **moda** normal paylanmada eyni nöqtədə olur.\n",
    "- Dəyərlərin təxminən 68%-i bir standart sapma daxilində (yəni, ortalamadan ±1 standart sapma məsafədə), 95%-i iki standart sapma daxilində, 99.7%-i isə üç standart sapma daxilində olur (68-95-99.7 qaydası).\n",
    "\n",
    "#### 2. **Qrafik təsviri**:\n",
    "Normal paylanmanı təsvir etmək üçün histogram və ya **Q-Q plot** (Quantile-Quantile plot) istifadə olunur. Histogramda zəng şəklində əyri varsa, bu, məlumatların normal paylanmaya uyğun olduğunu göstərir.\n",
    "\n",
    "#### 3. **Normal paylanmanın yoxlanılması**:\n",
    "Datanın normal paylanması olub-olmadığını yoxlamaq üçün bir neçə statistik test və qrafik metodu mövcuddur:\n",
    "- **Shapiro-Wilk testi** və ya **Kolmogorov-Smirnov testi**: Bu testlər datanın normal paylanmadan nə qədər sapdığını ölçmək üçün istifadə olunur.\n",
    "- **Histogram** və **Q-Q Plot**: Qrafik olaraq normal paylanmanı vizual şəkildə təsdiqləmək üçün istifadə olunur.\n",
    "\n",
    "#### 4. **Normal paylanmanın əhəmiyyəti**:\n",
    "Normal paylanmanın olduğu məlumat dəstələri statistik analizlər üçün uyğundur. Bir çox statistik metodlar (məsələn, t-test, ANOVA, regresyon analizləri) normal paylanmaya əsaslanır. Əgər məlumat normal paylanmırsa, bu analizlər düzgün nəticə verməyə bilər, buna görə də datanı normal paylanmaya yaxınlaşdırmaq üçün transformasiya metodlarından istifadə edilir (məsələn, log transformasiya).\n",
    "\n",
    "#### 5. **Normal paylanmanın real həyatda nümunələri**:\n",
    "- İnsanların boyu, çəkisi, qan təzyiqi kimi biometrik ölçülər adətən normal paylanmaya uyğun gəlir.\n",
    "- Bir çox sosial və iqtisadi göstəricilər də normal paylanmaya yaxındır, məsələn, test balları və ya məhsulun ölçüləri.\n",
    "\n",
    "### Datanın normal paylanmaması halları:\n",
    "Məlumatlar normal paylanmaya uyğun gəlmirsə, bu, müxtəlif hallarda baş verə bilər:\n",
    "- **Çox yüksək və ya aşağı outlier-lər** olduqda.\n",
    "- **Skewness** (asimetriya) olduqda: Sağ və ya sol tərəfə doğru qrafikdə əyilmə müşahidə oluna bilər.\n",
    "- **Kurtosis**: Paylanmanın zirvəsi normala nisbətən daha geniş və ya dar ola bilər.\n",
    "\n",
    "Yəni, **datanın normal paylanması**, məlumatların standart bir forma malik olduğu, böyük bir qisminin ortalama ətrafında cəmləşdiyi və ortalamadan uzaqlaşdıqca tezliyin azaldığı paylanma deməkdir. Bu, statistik analizlər üçün vacib şərtlərdən biridir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdcc9295",
   "metadata": {},
   "source": [
    "## Z-score haqqında geniş anlayış ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da445b",
   "metadata": {},
   "source": [
    "**Z-score** (və ya standart skor) statistikada müəyyən bir dəyərin ortalama ilə müqayisədə nə qədər uzaqda olduğunu ölçmək üçün istifadə olunur. Z-score, həmin dəyərin orta qiymətdən neçə standart sapma uzaqlıqda olduğunu göstərir. Bu üsul xüsusilə fərqli vahidlər və ölçülərdən istifadə edən məlumatları müqayisə etmək üçün əhəmiyyətlidir, çünki bütün məlumatları standartlaşdırılmış vahidlərə (z-skorlara) çevirir.\n",
    "\n",
    "### Z-Score-nu hesablamaq üçün formula:\n",
    "$$\n",
    "Z = \\frac{(X - \\mu)}{\\sigma}\n",
    "$$\n",
    "Burada:\n",
    "- **Z** – Z-score,\n",
    "- **X** – müşahidə olunan dəyər,\n",
    "- **$\\mu$** – orta dəyər (mean),\n",
    "- **$\\sigma$** – standart sapma.\n",
    "\n",
    "### Z-score-un mənası:\n",
    "- **Z-score = 0**: Dəyər tam ortalamaya bərabərdir.\n",
    "- **Z-score > 0**: Dəyər ortalamadan yuxarıdır (pozitiv sapma).\n",
    "- **Z-score < 0**: Dəyər ortalamadan aşağıdır (neqativ sapma).\n",
    "- **Z-score = 1**: Dəyər orta dəyərdən bir standart sapma yuxarıdır.\n",
    "- **Z-score = -2**: Dəyər orta dəyərdən iki standart sapma aşağıdır.\n",
    "\n",
    "### Z-score-un əhəmiyyəti:\n",
    "1. **Outlier-lərin tapılması**: Z-score böyük və ya çox kiçik olarsa, bu, dəyərin məlumat dəstəsindəki digər dəyərlərdən çox uzaq olduğunu göstərə bilər. Məsələn, Z-score -3 və ya +3-dən böyükdürsə, bu dəyər outlier kimi qəbul oluna bilər.\n",
    "   \n",
    "2. **Müqayisə etmək**: Z-score müxtəlif ölçü vahidlərində olan dəyərləri müqayisə etməyə imkan verir. Məsələn, bir tələbənin iki fərqli fəndən aldığı balları müqayisə etmək üçün Z-score istifadə oluna bilər.\n",
    "\n",
    "3. **Normal paylanma**: Normal paylanma üçün, müşahidələrin təxminən 68%-i ortalama ətrafında bir standart sapma daxilində, 95%-i isə iki standart sapma daxilində yerləşir. Z-score bu paylanmanı vizual olaraq görməyə kömək edir.\n",
    "\n",
    "### Z-Score istifadəsi:\n",
    "- **Hypothesis testing (təsadüfi fərziyyə testi)**: Z-score normallıq testlərində və fərziyyə testlərində istifadə olunur.\n",
    "- **Standartlaşdırma**: Fərqli vahidlərdəki dəyişənləri eyni ölçü vahidinə çevirir.\n",
    "- **Skorların təhlili**: Təhsildə və ya digər sahələrdə fərqli qiymətləndirmələr arasında müqayisə üçün istifadə olunur.\n",
    "\n",
    "Yəni, Z-score, məlumatlar içində bir dəyərin yerini standart sapmalar ilə müqayisə edərək izah edir və məlumatların təhlilində geniş istifadə edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a221c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e11b3f",
   "metadata": {},
   "source": [
    "## T-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e25b5f",
   "metadata": {},
   "source": [
    "**T-score** (və ya **t-skore**), statistikada kiçik nümunə ölçülərində istifadə olunan Z-score-un bir variantıdır. Xüsusilə, nümunə ölçüsü kiçik olduqda və ya populyasiyanın standart sapması bilinmədikdə, Z-score əvəzinə t-skordan istifadə olunur. T-skore **t-test** və digər statistik testlər zamanı paylanmanın normallığını yoxlamaq və fərziyyə testləri aparmaq üçün istifadə edilir.\n",
    "\n",
    "### T-score ilə Z-score fərqləri:\n",
    "- **Z-score** adətən böyük nümunə ölçüləri üçün (n > 30) və ya populyasiyanın standart sapması bilindiyi zaman istifadə olunur.\n",
    "- **T-score** isə kiçik nümunə ölçülərində (n < 30) və ya populyasiyanın standart sapması məlum olmadığı hallarda tətbiq edilir.\n",
    "\n",
    "### T-score-un formulu:\n",
    "T-score hesablamaq üçün aşağıdakı formula istifadə olunur:\n",
    "$$\n",
    "T = \\frac{(X - \\mu)}{s/\\sqrt{n}}\n",
    "$$\n",
    "Burada:\n",
    "- **T** – T-score,\n",
    "- **X** – müşahidə olunan dəyər (nümunə ortası),\n",
    "- **μ** – populyasiya ortası,\n",
    "- **s** – nümunə standart sapması,\n",
    "- **n** – nümunənin ölçüsü (müşahidə sayı).\n",
    "\n",
    "### T-score-un tətbiq sahələri:\n",
    "1. **Kiçik nümunələrdə fərziyyə testi**: T-score, xüsusən də kiçik nümunələr üçün populyasiya ortalaması ilə müqayisə aparmaq üçün istifadə edilir.\n",
    "2. **T-testlər**: İki qrupun ortalamalarını müqayisə etmək üçün **t-test** aparılarkən istifadə olunur. Bu test iki növ ola bilər:\n",
    "   - **One-sample t-test**: Nümunənin ortasını verilmiş populyasiya ortası ilə müqayisə etmək.\n",
    "   - **Two-sample t-test**: İki fərqli nümunənin ortalarını bir-biri ilə müqayisə etmək.\n",
    "3. **Nəticələrin güvən aralığının hesablanması**: Kiçik nümunələrdən əldə olunan nəticələrin nə qədər güvənlə populyasiyaya tətbiq oluna biləcəyini göstərmək üçün t-distribusiya istifadə olunur.\n",
    "\n",
    "### T-score-un əhəmiyyəti:\n",
    "- **Kiçik nümunələrdə dəqiqlik**: Nümunə ölçüsü kiçik olduqda və standart sapma məlum olmadıqda, Z-score istifadə etmək dəqiq nəticə verməyə bilər. T-score bu hallarda daha dəqiq təxminlər verir.\n",
    "- **Fərziyyə testləri**: Kiçik nümunə ölçüləri ilə işləyərkən, t-test və t-score, iki qrup arasında əhəmiyyətli bir fərqin olub-olmadığını müəyyənləşdirməyə kömək edir.\n",
    "\n",
    "Yəni, t-score, xüsusən kiçik nümunələrlə işləyərkən statistik analiz aparmaq üçün vacibdir və nəticələrin daha dəqiq olmasını təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03fe51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0667bac8",
   "metadata": {},
   "source": [
    "## Supervised Learning Regression Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31acd3f",
   "metadata": {},
   "source": [
    "### R-squared: This measures the variation of a regression model. R-squared either increases or remains the same when new predictors are added to the model. ( R-kvadrat: Bu, reqressiya modelinin dəyişməsini ölçür. Modelə yeni proqnozlaşdırıcılar əlavə edildikdə R-kvadrat ya artır, ya da eyni qalır.)\n",
    "### Adjusted R-squared: This measures the variation for a multiple regression model, and helps you determine goodness of fit. Unlike R-squared, adjusted R-squared only adds new predictors to its model if it improves the model’s predicting power. (Düzəliş edilmiş R-kvadrat: Bu, çoxlu reqressiya modeli üçün variasiyanı ölçür və uyğunluğun yaxşılığını müəyyən etməyə kömək edir. R-kvadratdan fərqli olaraq, düzəliş edilmiş R-kvadrat modeli yalnız modelin proqnozlaşdırma gücünü yaxşılaşdırdığı halda modelinə yeni proqnozlaşdırıcılar əlavə edir.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76550179",
   "metadata": {},
   "source": [
    "#### https://images.app.goo.gl/Qgeqz1LPsGkEAfoD9\n",
    "#### https://images.app.goo.gl/xRogYdJjCpqKPc937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0b212c",
   "metadata": {},
   "source": [
    "## SST, SSR, SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d2871",
   "metadata": {},
   "source": [
    "**SST, SSR, və SSE** statistika və regresiya analizində istifadə olunan əsas ölçülərdir. Bu ölçülər modelin nə dərəcədə yaxşı uyğunlaşdığını və modeldəki variasiyaların nə qədərinin izah edildiyini qiymətləndirmək üçün istifadə olunur.\n",
    "\n",
    "**1. SST (Total Sum of Squares):** Ümumi kvadratlar cəmi (Total Sum of Squares) \n",
    "- Bu, `y` dəyişəninin ümumi dəyişkənliyini ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $ (y_i) $ həqiqi müşahidə olunan dəyərlər, $( \\bar{y})$ isə müşahidə olunan dəyərlərin orta qiymətidir.\n",
    "\n",
    "**2. SSR (Regression Sum of Squares):** Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- Bu, modelin nə dərəcədə uyğun gəldiyini ölçür, yəni modelin izah etdiyi variasiyadır.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $( \\hat{y}_i )$ model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**3. SSE (Error Sum of Squares):** Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- Bu, modelin izah edə bilmədiyi, qalıq variasiyanı ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\hat{y}_i )$ isə model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**Əlaqə**:\n",
    "Bu üç ölçü arasında belə bir əlaqə var:\n",
    "$$\n",
    "\\text{SST} = \\text{SSR} + \\text{SSE}\n",
    "$$\n",
    "Bu formula, ümumi variasiyanın izah edilən variasiyaya (SSR) və izah edilməmiş qalıq variasiyaya (SSE) bölünməsini göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345e34bc",
   "metadata": {},
   "source": [
    "## R2 and Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619e63b",
   "metadata": {},
   "source": [
    "**R-squared (R²)** və **Adjusted R-squared** regresiya modelinin uyğunluğunu qiymətləndirmək üçün istifadə olunan iki əsas ölçüdür. \n",
    "\n",
    "### 1. **R-squared (R²)**\n",
    "R², modelin nə qədər yaxşı izah etdiyini, yəni müstəqil dəyişənlərin asılı dəyişən üzərindəki variasiyanı nə qədər izah etdiyini göstərir. \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **SSR** - Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- **SSE** - Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- **SST** - Ümumi kvadratlar cəmi (Total Sum of Squares)\n",
    "\n",
    "R² dəyəri həmişə 0 ilə 1 arasında olur. 1-ə yaxın olduqda modelin yüksək uyğunluğunu, 0-a yaxın olduqda isə zəif uyğunluğu göstərir.\n",
    "\n",
    "### 2. **Adjusted R-squared**\n",
    "**Adjusted R²** (Düzəldilmiş R²) modelə əlavə edilən dəyişənlərin sayını nəzərə alır və modelin sadəcə dəyişənlərin sayına görə şişirdilməməsini təmin edir. Əgər modelə faydalı olmayan bir dəyişən əlavə edilərsə, Adjusted R² dəyəri azalacaq. Bu xüsusiyyət Adjusted R²-i daha etibarlı edir.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **R²** - R-squared dəyəri\n",
    "- **n** - Nümunə ölçüsü (müşahidələrin sayı)\n",
    "- **p** - Müstəqil dəyişənlərin sayı\n",
    "\n",
    "**Adjusted R²** dəyəri hər zaman **R²**-dən kiçik və ya ona bərabərdir. Bu, modelin sadəcə əlavə dəyişənlərin sayı ilə deyil, onların əhəmiyyəti ilə də əlaqəli olduğunu göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e31207",
   "metadata": {},
   "source": [
    "## R2 ve Corellation ferqi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707ebeb",
   "metadata": {},
   "source": [
    "**R² (R-squared)**, həm də determinasiya əmsalı kimi tanınır, statistikada bir modelin izah gücünü ölçən bir göstəricidir. O, asılı (hədəf) dəyişənin dəyərlərinin neçə faizinin müstəqil dəyişənlər tərəfindən izah olunduğunu göstərir.\n",
    "\n",
    "R² ilə **korrelyasiya** arasında əlaqə var, amma onlar tam eyni məna daşımır:\n",
    "\n",
    "- **Korrelyasiya** (r) iki dəyişən arasındakı xətti əlaqəni ölçür və dəyərləri -1 ilə 1 arasında olur. Pozitiv korrelyasiya (r > 0) müsbət əlaqəni, mənfi korrelyasiya (r < 0) isə mənfi əlaqəni göstərir.\n",
    "  \n",
    "- **R²** isə korrelyasiya əmsalının kvadratı kimi hesablanır və dəyərləri 0 ilə 1 arasında olur. Bu, müstəqil dəyişənlərin modeldə asılı dəyişəni nə qədər yaxşı izah etdiyini göstərir. Məsələn, R² = 0.7 olarsa, bu o deməkdir ki, dəyişənlərin 70%-i model tərəfindən izah edilir.\n",
    "\n",
    "### R² necə işləyir?\n",
    "\n",
    "Məsələn, bir xətti regresiya modelində:\n",
    "\n",
    "- Əgər **R² = 1**, bu o deməkdir ki, model mükəmməl proqnozlaşdırır, bütün məlumat nöqtələri xəttin üzərindədir.\n",
    "- **R² = 0** olarsa, model heç bir proqnozlaşdırma dəqiqliyinə malik deyil və asılı dəyişən müstəqil dəyişənlər tərəfindən izah olunmur.\n",
    "\n",
    "Bunu belə də ifadə edə bilərik: R² korrelyasiya əmsalının kvadratı olsa da, korrelyasiya dəyişənlərin yalnız bir-biri ilə əlaqəsini göstərir, R² isə modelin nə qədər yaxşı uyğunlaşdığını ölçür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adbad7",
   "metadata": {},
   "source": [
    "Həm **korrelyasiya əmsalı (r)**, həm də **R² (determinasiya əmsalı)** fərqli düsturlarla hesablanır.\n",
    "\n",
    "### 1. **Korrelyasiya (r)**\n",
    "\n",
    "Korrelyasiya iki dəyişən arasında əlaqəni ölçən statistik göstəricidir. Korrelyasiya əmsalı (r) -1 ilə 1 arasında dəyişir.\n",
    "\n",
    "Düstur belədir:\n",
    "$$\n",
    "r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n\\sum x^2 - (\\sum x)^2][n\\sum y^2 - (\\sum y)^2]}}\n",
    "$$\n",
    "\n",
    "- \\(x\\) və \\(y\\) — dəyişənlərin dəyərləri.\n",
    "- \\(n\\) — müşahidələrin sayı.\n",
    "- $\\sum xy$ — iki dəyişənin dəyərlərinin hasilinin cəmi.\n",
    "- $\\sum x^2 $ və $\\sum y^2$ — dəyişənlərin dəyərlərinin kvadratlarının cəmi.\n",
    "\n",
    "Bu düstur iki dəyişən arasında xətti əlaqənin gücünü və istiqamətini ölçür. Pozitiv \\(r\\) müsbət xətti əlaqəni, mənfi \\(r\\) mənfi xətti əlaqəni göstərir. \\(r = 0\\) isə əlaqənin olmadığını göstərir.\n",
    "\n",
    "Bəli, tamamilə doğrudur! **Korrelyasiya əmsalının (r)** başqa bir daha sadə və məşhur formulu da var, o da kovariasiya ilə standart sapmaların istifadəsidir:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- $\\text{cov}(x, y)$ — \\(x\\) və \\(y\\) dəyişənlərinin kovariasiyasıdır.\n",
    "- $\\sigma_x$ — \\(x\\) dəyişəninin standart sapmasıdır.\n",
    "- $\\sigma_y$ — \\(y\\) dəyişəninin standart sapmasıdır.\n",
    "\n",
    "Bu düstur iki dəyişənin bir-biri ilə necə əlaqəli olduğunu ölçür:\n",
    "- Əgər **kovariasiya** müsbətdirsə, bu, \\(x\\) və \\(y\\)-nin birlikdə artdığını göstərir.\n",
    "- Əgər kovariasiya mənfidirsə, \\(x\\) artdıqca \\(y\\) azalır və ya əksinə.\n",
    "  \n",
    "Standart sapmalar isə dəyişənlərin öz dəyişkənliyini ölçür. Korrelyasiya bu iki statistik göstəricini istifadə edərək, dəyişənlər arasındakı əlaqənin gücünü normalizasiya edir və nəticəni -1 ilə 1 arasında bir əmsal şəklində göstərir.\n",
    "\n",
    "Beləliklə, korrelyasiyanı kovariasiya üzərindən də hesablamaq mümkündür:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) \\cdot \\sigma_x \\cdot \\sigma_y}\n",
    "$$\n",
    "\n",
    "Bu düstur kovariasiya ilə korrelyasiya arasındakı əlaqəni daha yaxşı izah edir.\n",
    "\n",
    "\n",
    "### 2. **R² (Determinasiya əmsalı)**\n",
    "\n",
    "**R²**, xətti regresiya modelində modelin nə qədər yaxşı uyğunlaşdığını ölçən göstəricidir. O, korrelyasiya əmsalının kvadratı kimi də hesablana bilər.\n",
    "\n",
    "Düstur:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- $y_i$ — faktiki müşahidələr (real dəyərlər).\n",
    "- $\\hat{y}_i$ — proqnozlaşdırılan dəyərlər (modelin verdiyi nəticələr).\n",
    "- $\\bar{y}$ — faktiki müşahidələrin orta qiyməti.\n",
    "- $\\sum (y_i - \\hat{y}_i)^2$ — modelin səhvlərinin kvadratlarının cəmi (residual sum of squares, SSR).\n",
    "- $\\sum (y_i - \\bar{y})^2$ — ümumi fərqlənmənin kvadratlarının cəmi (total sum of squares, SST).\n",
    "\n",
    "Bu düstur göstərir ki, model nə qədər yaxşı izah edir. \\(R^2\\) modelin proqnozlaşdırdığı və faktiki müşahidələr arasındakı fərqlənməni nə qədər yaxşı izah etdiyini ölçür.\n",
    "\n",
    "**Korrelyasiya və R² əlaqəsi:** Əgər yalnız bir müstəqil dəyişənli sadə xətti regresiya modeliniz varsa, o zaman R² korrelyasiya əmsalının kvadratı kimi də hesablana bilər:\n",
    "\n",
    "$$\n",
    "R^2 = r^2\n",
    "$$\n",
    "\n",
    "Bu yalnız sadə xətti regresiya modellərində doğrudur, çoxlu müstəqil dəyişənlərin olduğu hallarda isə bu düstur daha mürəkkəbləşir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a00ea63",
   "metadata": {},
   "source": [
    "## R, R2, Adjutsed R2  bunun hansi menfi ola biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaac43f",
   "metadata": {},
   "source": [
    "**R (korrelyasiya əmsalı)**, **R² (determinasiya əmsalı)** və **Adjusted R² (tənzimlənmiş determinasiya əmsalı)** göstəriciləri fərqli mənalar daşıyır və onların hansı hallarda mənfi olma ehtimalını izah etmək vacibdir:\n",
    "\n",
    "### 1. **R (korrelyasiya əmsalı)**\n",
    "- **Aralıq:** -1 ilə 1 arasında dəyişir.\n",
    "- **Mənfi ola bilərmi?**: Bəli, **R** mənfi ola bilər. Əgər iki dəyişən arasında mənfi xətti əlaqə varsa, yəni bir dəyişən artdıqca digəri azalırsa, onda **R** mənfi olacaq. **R = -1** tam mənfi xətti əlaqəni, **R = 1** isə tam müsbət xətti əlaqəni göstərir.\n",
    "\n",
    "### 2. **R² (determinasiya əmsalı)**\n",
    "- **Aralıq:** 0 ilə 1 arasında olur.\n",
    "- **Mənfi ola bilərmi?**: **Xeyr, R² mənfi ola bilməz.** Çünki o, korrelyasiya əmsalının kvadratı kimi hesablanır (r²), və kvadrat heç vaxt mənfi ola bilməz. **R² = 0** modelin heç bir izah gücü olmadığını göstərir, **R² = 1** isə mükəmməl uyğunlaşmanı göstərir.\n",
    "\n",
    "### 3. **Adjusted R² (tənzimlənmiş R²)**\n",
    "- **Aralıq:** Mənfi dəyərlər ala bilər və 1-ə qədər yüksələ bilər.\n",
    "- **Mənfi ola bilərmi?**: **Bəli, Adjusted R² mənfi ola bilər.** Adjusted R², modelin izah gücünü dəyişənlərin sayını nəzərə alaraq tənzimləyir. Əgər model çox zəifdirsə və modelin quraşdırdığı dəyişənlər hədəf dəyişəni yaxşı izah etmirsə, Adjusted R² mənfi ola bilər. Bu, modelin, hətta təsadüfi proqnozlaşdırmadan daha pis olduğunu göstərə bilər.\n",
    "\n",
    "Adjusted R² belə hesablansa da, əlavə dəyişənlərin sayını da nəzərə alır:\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "Burada:\n",
    "- \\( n \\) — müşahidələrin sayı,\n",
    "- \\( p \\) — müstəqil dəyişənlərin sayı.\n",
    "\n",
    "Əgər əlavə dəyişənlər modeli əhəmiyyətli dərəcədə yaxşılaşdırmazsa, Adjusted R² mənfi ola bilər.\n",
    "\n",
    "### Qısaca:\n",
    "- **R** mənfi ola bilər (-1 ilə 1 arasında).\n",
    "- **R² mənfi ola bilməz** (0 ilə 1 arasında).\n",
    "- **Adjusted R²** mənfi ola bilər, əgər model zəifdirsə və faydasız dəyişənlər əlavə edilibsə."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9a9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21ed24b",
   "metadata": {},
   "source": [
    "### R2 = SSregression / SST = 1 - SSresiduals / SST\n",
    "### R2 adjusted = 1 - (1 - R2) * (n - 1) / ( n - p - 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5052549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7111220768354529\n",
      "Adjusted R^2: 0.7110611578301462\n",
      "MAE: 2.15\n",
      "MSE: 7.17\n",
      "RMSE: 2.68\n",
      "Pearson Korelyasiya Əmsalı: 0.8434059453729525\n",
      "Pearson Korelyasiya Əmsalı (scipy ilə): 0.8434059453729523\n",
      "Intercept (kəsik): 7.603902211896262\n",
      "Slope (meyl əmsalı): 0.9982101225691038\n",
      "Xətti tənlik: y = 0.9982101225691038 * x + 7.603902211896262\n",
      "Yeni feature x_1 = 23\n",
      "Predict: y = 30.56273503098565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week4\\day1\\Weather.csv\")\n",
    "dataset = data.copy()\n",
    "\n",
    "# Data\n",
    "x = dataset['MinTemp'].values.reshape(-1,1)\n",
    "y = dataset['MaxTemp'].values.reshape(-1,1)\n",
    "\n",
    "# Train-test bölünməsi\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "# Model təlimi\n",
    "regressor = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Metriklərin hesablamaq\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "print('R^2:', R2)\n",
    "\n",
    "n = x_test.shape[0]\n",
    "p = x_test.shape[1]\n",
    "adjusted_R2 = 1 - (1-R2)*(n-1)/(n-p-1)\n",
    "print('Adjusted R^2:', adjusted_R2)\n",
    "\n",
    "MAE = round(mean_absolute_error(y_test, y_pred), 2)\n",
    "print(\"MAE:\", MAE)\n",
    "\n",
    "MSE = round(mean_squared_error(y_test, y_pred), 2)\n",
    "print(\"MSE:\", MSE)\n",
    "\n",
    "RMSE = round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Pearson korelyasiya əmsalını hesablamaq\n",
    "correlation_matrix = np.corrcoef(y_test.flatten(), y_pred.flatten())\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print('Pearson Korelyasiya Əmsalı:', pearson_corr)\n",
    "\n",
    "# Alternativ olaraq, scipy istifadə etməklə\n",
    "pearson_corr, _ = pearsonr(y_test.flatten(), y_pred.flatten())\n",
    "print('Pearson Korelyasiya Əmsalı (scipy ilə):', pearson_corr)\n",
    "\n",
    "# Intercept və Slope tapmaq\n",
    "intercept = regressor.intercept_[0]\n",
    "slope = regressor.coef_[0][0]\n",
    "print(\"Intercept (kəsik):\", intercept)\n",
    "print(\"Slope (meyl əmsalı):\", slope)\n",
    "\n",
    "# Xətti tənliyi qurmaq\n",
    "print(f\"Xətti tənlik: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Yeni feature\n",
    "x_1 = 23\n",
    "print(f'Yeni feature x_1 = {x_1}')\n",
    "\n",
    "# Predict\n",
    "print(f\"Predict: y = {slope * x_1 + intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e761747",
   "metadata": {},
   "source": [
    "## Recall ve Precision nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e03ff",
   "metadata": {},
   "source": [
    "**Recall** və **Precision**, təsnifat (classification) modellərində nəticələrin keyfiyyətini ölçmək üçün istifadə olunan iki əsas metrikdir. Bu anlayışlar, xüsusən balanssız datasetlərdə modelin performansını qiymətləndirmək üçün çox vacibdir.\n",
    "\n",
    "Bu, confusion matrix-i ən sadə formada göstərən cədvəl variantıdır.\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "\n",
    "### Recall (Həssaslıq)\n",
    "Recall, modelin gerçək pozitiv halların nə qədərini düzgün tapdığını göstərir. Başqa sözlə, gerçək \"pozitiv\" olan nümunələrin neçə faizinin düzgün şəkildə \"pozitiv\" kimi təsnif olunduğunu ölçür. Recall aşağıdakı formula ilə hesablanır:\n",
    "\n",
    "$$  \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}  $$\n",
    "\n",
    "- **True Positive (TP)**: Modelin düzgün şəkildə pozitiv olaraq təxmin etdiyi hallar.\n",
    "- **False Negative (FN)**: Modelin səhv şəkildə negativ olaraq təxmin etdiyi, əslində pozitiv olan hallar.\n",
    "\n",
    "Recall yüksəkdirsə, model əksər pozitiv halları düzgün təyin edir. Lakin bu zaman model yanlış pozitivlərə qarşı həssas ola bilər.\n",
    "\n",
    "### Precision (Dəqiqlik)\n",
    "Precision, modelin pozitiv olaraq təxmin etdiyi hallar arasından neçə faizinin həqiqətən də pozitiv olduğunu ölçür. Başqa sözlə, modelin \"pozitiv\" olaraq təsnif etdiyi nümunələrin düzgün olub-olmadığını göstərir. Precision aşağıdakı formula ilə hesablanır:\n",
    "\n",
    "$$  \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}} $$  \n",
    "\n",
    "- **True Positive (TP)**: Modelin düzgün şəkildə pozitiv olaraq təxmin etdiyi hallar.\n",
    "- **False Positive (FP)**: Modelin səhv şəkildə pozitiv olaraq təxmin etdiyi, əslində negativ olan hallar.\n",
    "\n",
    "Precision yüksəkdirsə, model pozitiv olaraq təyin etdiyi halların çoxunu düzgün təyin edir, yəni model daha \"dəqiq\"dir.\n",
    "\n",
    "### Recall və Precision arasındakı fərq\n",
    "- **Recall**: Gerçək pozitivlərin nə qədərini düzgün tapır.\n",
    "- **Precision**: Modelin pozitiv olaraq təsnif etdiyi halların nə qədərini düzgün tapır.\n",
    "\n",
    "İki metrik arasında ticarət əlaqəsi var. Recall-u artırmaq bəzən Precision-un azalmasına səbəb ola bilər və əksinə. Əgər model yüksək recall əldə etməyə çalışırsa, daha çox pozitiv təxmin edəcək və yanlış pozitivlər də arta bilər. Yüksək precision üçün isə model yalnız ən güvənli pozitivləri təyin edəcək, bu da bəzi gerçək pozitivlərin qaçırılmasına (FN artmasına) səbəb ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f47b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b065a",
   "metadata": {},
   "source": [
    "## Supervised Learning Classification Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2b083",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "#### Accuracy, modelin düzgün təsnif etdiyi nümunələrin ümumi nümunələrə olan nisbətini göstərir.\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394276f",
   "metadata": {},
   "source": [
    "#### AUC (Area Under the Curve):\n",
    "#### AUC, ROC (Receiver Operating Characteristic) əyrisinin altında qalan sahəni ölçür. ROC əyrisi modelin True Positive Rate (TPR) və False Positive Rate (FPR) göstəricilərini müxtəlif qərar hədlərinə (threshold) görə təyin edir. AUC dəyəri 0 ilə 1 arasında dəyişir və modelin təsnifat qabiliyyətini qiymətləndirir. Yüksək AUC dəyəri modelin müsbət və mənfi sinifləri yaxşı ayırdığını göstərir.\n",
    "#### TPR = TP / (TP+FN)\n",
    "#### FPR = FP / (FP+TN)\n",
    "\n",
    "#### AUC dəyərinin formulu birbaşa verilmir, amma AUC əsasən ROC əyrisinin altında qalan sahədir və inteqrasiya vasitəsilə hesablanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1310546",
   "metadata": {},
   "source": [
    "#### https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)\n",
    "#### PRECISON = TP / TP + FP\n",
    "#### RECALL = TP / TP + FN\n",
    "### F1 = 2 * ( (precision *  recall) / (precision+  recall) )\n",
    "### F0.5 = 1.25 * ( (precision * recall) / (0.25 * precision + recall) )\n",
    "### F2 = 5 * ( (precision * recall) / (4 * precision + recall) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e35012",
   "metadata": {},
   "source": [
    "### F1 score, modelin həm dəqiqlik, həm də recall göstəricilərində yaxşı performans göstərib-göstərmədiyini dəyərləndirir və balanslı bir metrikdir. Bir modelin F1 score-u yüksəkdirsə, demək olar ki, model həm yanlış müsbət, həm də yanlış mənfi nəticələri minimallaşdırmağa nail olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf5b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                     Predicted\n",
    "#                     *******************************************     \n",
    "#                     *  Positive        *     Negative         *\n",
    "#         ***************************************************************************************\n",
    "#         * Positive  *   True Positive  *    False Negative    *    Sensitivity                *\n",
    "#         *           *     (TP)         *  (FN)Type II error   *  TP / (TP + FN)               *  \n",
    "# Actual  ***************************************************************************************   \n",
    "#         * Negative  *  False Positive  *   True Negative      *    Specificity                *  \n",
    "#         *           * (FP)Type I error *      (TN)            *  TN / (TN + FP)               * \n",
    "#         *************************************************************************************** \n",
    "#         *           *  Precision       *  Negative Predictive *    Accuracy                   *   \n",
    "#         *           * TP / (TP + FP)   *    TN / (TN + FN)    * TP + TN / (TP + TN + FP + FN) *\n",
    "#         *************************************************************************************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20679742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b46ddda",
   "metadata": {},
   "source": [
    "## type error 1,  yoxsa type error 2 hansinda sehv etsek daha pisdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f85170",
   "metadata": {},
   "source": [
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "\n",
    "**Type I error** (False Positive) və **Type II error** (False Negative) arasında hansı daha pis olduğuna qərar vermək, konkret vəziyyətdən asılıdır, çünki hər bir vəziyyətdə bu səhvlərin təsirləri fərqli ola bilər. Gəlin hər iki səhvi izah edək:\n",
    "\n",
    "- **Type I error (False Positive)**: Əslində negativ olan bir halın səhvən pozitiv kimi təsnif edilməsi. Bu, \"yanlış alarm\" olaraq da bilinir.\n",
    "  - Məsələn: Səhvən bir şəxsi kredit riski yüksək olduğu halda kredit almağa layiq göstərmək.\n",
    "\n",
    "- **Type II error (False Negative)**: Əslində pozitiv olan bir halın səhvən negativ kimi təsnif edilməsi. Bu, \"qaçırılan fürsət\" olaraq bilinir.\n",
    "  - Məsələn: Əslində kredit almağa layiq bir şəxsin səhvən kredit riski yüksək olduğu üçün rədd edilməsi.\n",
    "\n",
    "### Hansı daha pisdir?\n",
    "Bu, kontekstdən asılıdır:\n",
    "\n",
    "1. **Tibb (xərçəng testi)**\n",
    "   - **Type I error**: Səhvən xəstəyə \"xərçəng var\" deyilir (həqiqətdə yoxdur).\n",
    "   - **Type II error**: Səhvən xəstəyə \"xərçəng yoxdur\" deyilir (həqiqətdə var).\n",
    "   - **Daha pis olan**: Type II error (çünki xəstə diaqnoz qoyulmadığı üçün müalicə olunmaya bilər, bu isə ölümcül ola bilər).\n",
    "\n",
    "2. **Maliyyə (kredit riski)**\n",
    "   - **Type I error**: Səhvən bir şəxsə kredit verilir (əslində ödəmə qabiliyyəti yoxdur).\n",
    "   - **Type II error**: Səhvən kredit verilməyən bir şəxsə (əslində ödəmə qabiliyyəti var).\n",
    "   - **Daha pis olan**: Type I error (çünki kredit qaytarılmadığı halda bank itkiyə məruz qala bilər).\n",
    "\n",
    "### Nəticə\n",
    "Hansı səhvin daha pis olduğunu qiymətləndirmək, tətbiq olunan sahəyə bağlıdır. Kritik səhvlərin daha ciddi təsir edə biləcəyi hallarda, bu səhvlərin qarşısını almaq üçün müxtəlif yanaşmalar istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfdd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f146d27",
   "metadata": {},
   "source": [
    "## Bes ne vaxt Recall ve ya Precison onemlidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0075171",
   "metadata": {},
   "source": [
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "Recall və precision fərqli hallarda önəmli ola bilər, və bunlar konkret problemlərə bağlı olaraq müxtəlif dərəcədə əhəmiyyətli ola bilər. \n",
    "\n",
    "1. **Recall** (Həqiqi müsbətlərin hissəsi) çox önəmlidir:\n",
    "   - **Sağlamlıq diaqnostikası**: Tibbi testlərdə, məsələn, xərçəng aşkar edən testlərdə recall çox önəmlidir, çünki əgər xəstəlik varsa, onu qaçırmamaq üçün testin mümkün qədər çox müsbət nəticə verməsi lazımdır.\n",
    "   - **Kiber təhlükəsizlik**: Həssas məlumatların sızdırılması və ya hücumları aşkarlama kimi vəziyyətlərdə, recall yüksək olmalıdır ki, mümkün olan bütün hücumları müəyyənləşdirmək mümkün olsun.\n",
    "\n",
    "2. **Precision** (Həqiqi müsbətlərin ümumi müsbət nəticələrə nisbəti) çox önəmlidir:\n",
    "   - **Məhsul təklifləri**: Təkliflər sistemlərində, məsələn, müştəriyə xüsusi məhsul tövsiyə edərkən, yalnız müştərinin maraqlandığı məhsulları təklif etmək daha yaxşıdır. Burada precision yüksək olmalıdır ki, təkliflər daha uyğun olsun.\n",
    "   - **Spam filtrləri**: E-poçt spam filtrləri üçün precision yüksək olmalıdır ki, istifadəçilərin əhəmiyyətli e-poçtları səhvən spam olaraq qeyd olunmasın.\n",
    "\n",
    "   - **Kredit başvurularının təsdiqlənməsi**: Kredit başvurularını təsdiqləyərkən, precision yüksək olmalıdır ki, yalnız əslində etibarlı müştərilərə kredit verilsin. Yüksək precision kreditlərin daha etibarlı müştərilərə verilməsini təmin edir, beləliklə qeyri-ödənilən kreditlərin sayı azalır.\n",
    "\n",
    "**F1 score** isə recall və precision arasındakı ticarət balansını təmin etmək üçün istifadə edilə bilər. İki metrik arasında tarazlığı təmin etmək istəyirsinizsə, F1 score hesablamaq faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487ad4d7",
   "metadata": {},
   "source": [
    "## True Positive Rate and  False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518be03",
   "metadata": {},
   "source": [
    "#### True Positive Rate (TPR) və False Positive Rate (FPR) təsnifat modellərinin performansını qiymətləndirmək üçün istifadə edilən iki mühüm göstəricidir. Onlar modelin müsbət və mənfi təsnifatlarla necə başa çıxdığını izah edirlər.\n",
    "\n",
    "#### 1. True Positive Rate (TPR)\n",
    "#### True Positive Rate (TPR), digər adı ilə Recall və ya Sensitivity (Sensitivlik), həqiqətən müsbət olan nümunələrin neçə faizinin model tərəfindən düzgün olaraq müsbət kimi təsnif edildiyini göstərir.\n",
    "\n",
    "#### TPR formulu:\n",
    "\n",
    "#### TPR = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n",
    "\n",
    " \n",
    "#### True Positives (TP): Modelin müsbət olaraq təsnif etdiyi və həqiqətən müsbət olan nümunələr.\n",
    "#### False Negatives (FN): Modelin mənfi olaraq təsnif etdiyi, lakin əslində müsbət olan nümunələr.\n",
    "#### TPR yüksəkdirsə, bu o deməkdir ki, model həqiqətən müsbət olanları yaxşı tanıyır və az sayda yanlış mənfi nəticə verir.\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "#### False Positive Rate (FPR), həqiqətən mənfi olan nümunələrin neçə faizinin model tərəfindən səhvən müsbət kimi təsnif edildiyini göstərir. FPR, modelin yanlış müsbət nəticələr vermə tendensiyasını göstərir.\n",
    "\n",
    "#### FPR formulu:\n",
    "#### FPR = 1 - Specificity = 1 - TN / (TN + FP)\n",
    "#### FPR = False Positives (FP) /  (False Positives (FP) + True Negatives (TN))\n",
    " \n",
    "#### False Positives (FP): Modelin müsbət olaraq təsnif etdiyi, lakin əslində mənfi olan nümunələr.\n",
    "#### True Negatives (TN): Modelin mənfi olaraq təsnif etdiyi və həqiqətən mənfi olan nümunələr.\n",
    "#### FPR yüksəkdirsə, bu, modelin çox sayda yanlış müsbət nəticə verdiyini göstərir.\n",
    "\n",
    "#### Nəticə:\n",
    "#### TPR yüksəkdirsə, bu, modelin həqiqətən müsbət olanları yaxşı tanıdığını göstərir.\n",
    "#### FPR aşağıdırsa, bu, modelin yanlış müsbət nəticələri azaldığını göstərir.\n",
    "#### Bu iki göstərici adətən ROC (Receiver Operating Characteristic) əyrisi ilə birlikdə istifadə edilir və AUC (Area Under the Curve) dəyəri ilə modelin performansı qiymətləndirilir. TPR və FPR dəyərlərinin tarazlaşdırılması modelin ümumi effektivliyini təmin edir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abfc11",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f40ab1",
   "metadata": {},
   "source": [
    "#### https://www.saedsayad.com/images/MLR_r2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7a43a",
   "metadata": {},
   "source": [
    " Ridge :  (yi − ŷi)^2 + λ ∗ Slope^2  \n",
    " Lasso: (yi − ŷi)^2 + λ ∗ |Slope|\n",
    "\n",
    "Bu ifadə, ümumiyyətlə, cərimə terminləri (penalization terms) ilə birlikdə istifadə olunan optimallaşdırma məqsəd funksiyalarından biridir. Buradakı simvollar və ifadələrin mənası belədir:\n",
    "\n",
    "- `(yi − ŷi)^2`: Bu, müşahidə olunan dəyərlər (yi) ilə proqnozlaşdırılan dəyərlər (ŷi) arasındakı fərqlərin kvadratıdır. Bu fərq, adətən xəta və ya qalıq (residual) adlanır. Kvadrat halında ifadə edilməsinin səbəbi fərqlərin müsbət və mənfi olmasının fərqini aradan qaldırmaq və daha böyük fərqləri cəza olaraq daha çox nəzərə almaqdır.\n",
    "  \n",
    "- `λ ∗ Slope`: Bu, cərimə terminidir. Burada `λ` (lambda) cərimə əmsalıdır. Bu, modelin sadəliyinə nə qədər əhəmiyyət verildiyini göstərir. `Slope` isə modelin meyli və ya meyl əmsalının (məsələn, xətti reqressiyada meyl əmsalı beta) qiymətidir. Cərimə termini modelin çox mürəkkəb olmasının qarşısını almaq üçün əlavə olunur.\n",
    "\n",
    "Bu cür cərimə terminləri Ridge və ya Lasso kimi reqressiya modellərində istifadə olunur. Məqsəd, sadəcə modelin dəqiqliyini artırmaq deyil, həm də modelin sadəliyini və ümumiləşdirmə qabiliyyətini (overfitting-i aradan qaldırmaq) təmin etməkdir. Lambda (`λ`) çox böyük olarsa, model həddindən artıq sadə ola bilər; əksinə, çox kiçik olarsa, model mürəkkəb və ya overfit ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d74db131",
   "metadata": {},
   "source": [
    "## Ridge an Lasso haqqinda bilmek isteyirem. ne is gorurler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16efde",
   "metadata": {},
   "source": [
    "Ridge və Lasso reqressiyası, xətti reqressiya modellərində dəyişənlərin seçilməsi və modelin sadələşdirilməsi məqsədilə istifadə olunan iki məşhur cərimələndirmə (penalization) metodudur. Hər ikisi overfitting-in qarşısını almaq üçün reqressiya əmsallarına cərimə tətbiq edir, lakin bunu fərqli yollarla edirlər.\n",
    "\n",
    "### 1. **Ridge Regression**\n",
    "   - **Nədir?**\n",
    "     Ridge reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların kvadratlarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ (βj^2)` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Ridge reqressiyası, əmsalların böyük olmasının qarşısını almaq üçün onları kiçikləşdirir. Bu metod xüsusilə yüksək kollinear (bir-birinə bağlı) dəyişənlər olduğu hallarda effektivdir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L2 norması (kvadrat cərimə).\n",
    "     - **Əsas üstünlüyü:** Overfitting-in qarşısını alır və modelin stabilliyini artırır.\n",
    "     - **Dəyişənlər:** Bütün dəyişənlər saxlanılır, lakin əmsalları kiçikləşdirilir.\n",
    "\n",
    "### 2. **Lasso Regression**\n",
    "   - **Nədir?**\n",
    "     Lasso reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların modullarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ |βj|` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Lasso reqressiyası, bəzi əmsalların tamamilə sıfıra bərabər olmasına səbəb ola bilər. Bu, modeldə əhəmiyyətsiz olan dəyişənlərin tamamilə çıxarılmasına imkan verir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L1 norması (modul cərimə).\n",
    "     - **Əsas üstünlüyü:** Dəyişənlərin seçilməsi və modelin sadələşdirilməsi.\n",
    "     - **Dəyişənlər:** Bəzi dəyişənlər sıfırlanır və modeldən çıxarılır.\n",
    "\n",
    "### **Ridge və Lasso Arasındakı Fərqlər:**\n",
    "   - **Cərimə növü:** Ridge L2 cəriməsi istifadə edərək əmsalları kiçikləşdirir, Lasso isə L1 cəriməsi ilə bəzi əmsalları sıfıra çevirir.\n",
    "   - **Dəyişənlərin seçilməsi:** Ridge bütün dəyişənləri saxlayır, lakin Lasso bəzi dəyişənləri tamamilə çıxarır.\n",
    "   - **Yüksək korrelyasiya:** Ridge yüksək korrelyasiyalı dəyişənlər üçün daha uyğundur, çünki o, bu dəyişənləri bir arada saxlayır, Lasso isə yüksək korrelyasiyalı dəyişənlərin bir qismini sıfırlaya bilər.\n",
    "\n",
    "### **Ridge və Lasso'nun İstifadəsi:**\n",
    "   - Ridge reqressiyası daha çox dəyişən olduğu və onların əhəmiyyətinin hamısının qorunması lazım olduğu hallarda istifadə olunur.\n",
    "   - Lasso reqressiyası isə dəyişənlərin seçilməsi və daha sadə modellər qurmaq lazım olduğu hallarda istifadə olunur.\n",
    "\n",
    "Bu metodlar, xüsusilə böyük verilənlər dəstləri və yüksək miqdarda dəyişən olan problemlər üçün faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7e9fc0b",
   "metadata": {},
   "source": [
    "## Cross_Validation haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312bdd9",
   "metadata": {},
   "source": [
    "Cross-validation, maşın öyrənməsində modelin performansını qiymətləndirmək üçün istifadə olunan bir texnikadır. Bu üsul, modelin overfitting (modelin təlim məlumatlarına həddən artıq uyğunlaşması) və underfitting (modelin həm təlim, həm də test məlumatlarını yaxşı öyrənməməsi) probleminin qarşısını almağa kömək edir. Cross-validation, modeli müxtəlif alt qruplar üzərində təlim edərək və test edərək modelin ümumiləşdirmə qabiliyyətini qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Nədir?\n",
    "Cross-validation, ümumiyyətlə, məlumat dəstəsini bir neçə hissəyə (fold) bölmək və bu hissələrdən birini test dəsti kimi, qalanlarını isə təlim dəsti kimi istifadə etmək üsuludur. Bu proses bir neçə dəfə təkrarlanır və nəticədə hər bir hissə bir dəfə test dəsti kimi istifadə olunur. Bu təkrarlamalar nəticəsində əldə edilən nəticələr ortalanır və modelin performansı bu ortalama üzərindən qiymətləndirilir.\n",
    "\n",
    "### Növləri:\n",
    "1. **K-Fold Cross-Validation:**\n",
    "   - **Ən çox istifadə olunan üsuldur.** Məlumat dəstəsi K sayda hissəyə bölünür (məsələn, K=5 və ya K=10).\n",
    "   - **Proses:** Məlumat dəstəsi K hissəyə bölünür və model K dəfə təlim edilir. Hər dəfə bir hissə test dəsti kimi istifadə olunur, qalan K-1 hissə isə təlim üçün istifadə edilir. Nəticədə K dəfə modelin performansı ölçülür və bu nəticələr ortalanır.\n",
    "   - **Üstünlükləri:** Bütün məlumatlar həm təlim, həm də test üçün istifadə olunduğundan, bu üsul daha dəqiq və ümumi nəticələr verir.\n",
    "\n",
    "2. **Stratified K-Fold Cross-Validation:**\n",
    "   - K-Fold Cross-Validation-a bənzərdir, amma burada məlumat dəstəsi siniflərin paylanmasına uyğun olaraq bölünür. Bu, xüsusilə balanssız siniflər olan məlumat dəstələrində faydalıdır.\n",
    "   - Siniflərin hər fold içində təmsil olunmasını təmin edir.\n",
    "\n",
    "3. **Leave-One-Out Cross-Validation (LOOCV):**\n",
    "   - **Hər dəfə bir nümunə çıxarılaraq test üçün istifadə edilir.** Qalan bütün nümunələr təlim üçün istifadə olunur.\n",
    "   - **Proses:** Məlumat dəstəsinin hər bir nümunəsi bir dəfə test dəsti kimi istifadə olunur, qalan nümunələr isə təlim üçün istifadə edilir. Bu üsul çox vaxt aparsa da, daha dəqiq nəticələr verir.\n",
    "   - **Üstünlükləri:** Xüsusilə kiçik məlumat dəstələrində modelin performansını dəqiq ölçmək üçün yaxşıdır.\n",
    "\n",
    "4. **Time Series Cross-Validation:**\n",
    "   - Zaman sırasına görə məlumatları böldüyü üçün, zaman sıralı məlumatlarla işləyən modellər üçün istifadə olunur.\n",
    "   - Məlumatlar zamanla əlaqəli olduqda, gələcək məlumatların proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "5. **Nested Cross-Validation:**\n",
    "   - Bu üsul hiperparametr tənzimləməsi ilə model performansını qiymətləndirmək üçün istifadə olunur.\n",
    "   - Birinci dərəcəli cross-validation modeli tənzimləmək üçün, ikinci dərəcəli cross-validation isə performansı qiymətləndirmək üçün istifadə edilir.\n",
    "\n",
    "### Cross-Validation Üstünlükləri:\n",
    "- **Ümumiləşdirmə:** Modelin məlumatlara həddən artıq uyğunlaşmasını (overfitting) qarşısını almağa kömək edir.\n",
    "- **Dəqiqlik:** Məlumat dəstəsinin bütün hissələri həm təlim, həm də test üçün istifadə olunduğundan, modelin performansını daha dəqiq ölçür.\n",
    "- **Stabil qiymətləndirmə:** Bir çox təkrarlama ilə ortalama nəticələr əldə olunur, bu da modelin performansını daha stabil şəkildə qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Zamanı Dikkat Edilmesi Gerekenler:\n",
    "- **Məlumatın Bərabər Bölünməsi:** Məlumat dəstəsinin fold-lara bərabər şəkildə bölündüyündən əmin olunmalıdır.\n",
    "- **Zaman Əlaqəsi:** Zamanla əlaqəli məlumatlar varsa, zaman sırası gözlənilməlidir.\n",
    "- **Hesablama Məsrəfləri:** Daha çox fold daha dəqiq nəticələr verir, lakin hesablama vaxtını artırır.\n",
    "\n",
    "Cross-validation, maşın öyrənməsində model performansını qiymətləndirmək və tənzimləmək üçün əsas vasitələrdən biridir. Bu texnika, modelin ümumiləşdirmə qabiliyyətini artırır və həm akademik, həm də sənaye tətbiqlərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655fb9a",
   "metadata": {},
   "source": [
    "Cross-validation prosesini cədvəl şəklində göstərmək üçün hər bir fold-un təlim və test dəstlərinə necə bölündüyünü və nəticələrin ortalanmasını cədvəl formasında təsvir edə bilərik.\n",
    "\n",
    "### 5-Fold Cross-Validation Cədvəli\n",
    "\n",
    "| Fold  | Təlim Dəsti                   | Test Dəsti      | F1-score |\n",
    "|-------|--------------------------------|-----------------|----------|\n",
    "| 1     | Fold 2, 3, 4, 5                | Fold 1          | F1-score_1|\n",
    "| 2     | Fold 1, 3, 4, 5                | Fold 2          | F1-score_2|\n",
    "| 3     | Fold 1, 2, 4, 5                | Fold 3          | F1-score_3|\n",
    "| 4     | Fold 1, 2, 3, 5                | Fold 4          | F1-score_4|\n",
    "| 5     | Fold 1, 2, 3, 4                | Fold 5          | F1-score_5|\n",
    "\n",
    "### Ümumi F1-score:\n",
    "$$ \n",
    "\\text{Ortalama F1-score} = \\frac{\\text{F1-score}_1 + \\text{F1-score}_2 + \\text{F1-score}_3 + \\text{F1-score}_4 + \\text{F1-score}_5}{5}\n",
    " $$\n",
    "\n",
    "Bu ifadə hər bir fold üçün hesablanmış F1-score dəyərlərinin ortalamasını verir.\n",
    "\n",
    "Bu cədvəl hər fold üçün təlim və test dəstlərinin necə bölündüyünü və nəticələrin (məsələn, F1-score) necə toplandığını göstərir. Nəticədə, bütün fold-ların performans göstəriciləri ortalanaraq modelin ümumi performansı müəyyən edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218735b9",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "accuracies = cross_validate(estimator=NB_model, X=X_train_scaled, \n",
    "                            y=y_train, cv=4, scoring = ['f1', 'roc_auc', 'recall', 'precision', 'accuracy'])\n",
    "accuracies\n",
    "\n",
    "output\n",
    "{'fit_time': array([0.004, 0.004, 0.004, 0.004]),\n",
    " 'score_time': array([0.033, 0.031, 0.03 , 0.048]),\n",
    " 'test_f1': array([0.8136, 0.8333, 0.8308, 0.8276]),\n",
    " 'test_roc_auc': array([0.866 , 0.8983, 0.8308, 0.9161]),\n",
    " 'test_recall': array([0.7742, 0.8065, 0.9   , 0.7742]),\n",
    " 'test_precision': array([0.8571, 0.8621, 0.7714, 0.8889]),\n",
    " 'test_accuracy': array([0.807 , 0.8246, 0.8036, 0.8214])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982d185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db5fba0",
   "metadata": {},
   "source": [
    "## bes λ nece tapilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c67a8",
   "metadata": {},
   "source": [
    "Lambda (`λ`) dəyəri, modelin cərimə əmsalıdır və onun optimal dəyəri modelin performansını yaxşılaşdırmaq üçün seçilir. Lambda-nı təyin etmək üçün bir neçə üsul mövcuddur:\n",
    "\n",
    "### 1. **Cross-Validation (Çapraz Doğrulama)**\n",
    "   - **Addım 1:** Verilənlər toplusunu bir neçə hissəyə bölmək (məsələn, 5 və ya 10 fold cross-validation).\n",
    "   - **Addım 2:** Müxtəlif lambda dəyərləri üçün modelin performansını hər bir hissədə qiymətləndirmək.\n",
    "   - **Addım 3:** Hər bir fold üçün modelin performansını ölçmək və ortalama performansı hesablayaraq ən yaxşı nəticə verən lambda dəyərini seçmək.\n",
    "\n",
    "### 2. **Grid Search**\n",
    "   - Müəyyən bir aralıqda lambda dəyərləri təyin olunur (məsələn, 0.001-dən 10-a qədər).\n",
    "   - Hər bir lambda dəyəri üçün model qurulur və performans ölçülür.\n",
    "   - Ən yaxşı performansı verən lambda seçilir.\n",
    "\n",
    "### 3. **Regularization Paths**\n",
    "   - Bu üsulda, müxtəlif lambda dəyərləri üçün reqressiya əmsallarının dəyişməsi analiz edilir.\n",
    "   - Bu qrafik üzərində, reqressiya əmsallarının stabilləşdiyi nöqtədəki lambda dəyəri seçilir.\n",
    "\n",
    "### 4. **Information Criteria (AIC, BIC)**\n",
    "   - Akaike Information Criterion (AIC) və ya Bayesian Information Criterion (BIC) kimi informasiya meyarları ilə lambda seçilə bilər. Bu meyarlar modeli uyğunlaşdırma keyfiyyətini və onun mürəkkəbliyini qiymətləndirir.\n",
    "\n",
    "Bu üsulların hər biri fərqli vəziyyətlər üçün uyğun ola bilər. Ən çox istifadə olunan metod isə Cross-Validation üsuludur, çünki bu metod həm modelin performansını yaxşı ölçməyə, həm də overfitting-in qarşısını almağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8eba0b",
   "metadata": {},
   "source": [
    "## penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0 bu paramertler haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4199",
   "metadata": {},
   "source": [
    "Bu parametrlər, `sklearn.linear_model.LogisticRegression` sinfində ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə olunur. Aşağıda hər bir parametrin detallı izahı verilmişdir:\n",
    "\n",
    "### 1. **`penalty='elasticnet'`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsini tətbiq edir. ElasticNet cərimələndirməsi həm L1 (Lasso) həm də L2 (Ridge) normlarını birləşdirir. Yəni, ElasticNet modeli həm bəzi əmsalları sıfıra yaxınlaşdırır (Lasso kimi), həm də digər əmsalları kiçikləşdirir (Ridge kimi).\n",
    "   - **Necə işləyir?**: Bu parametr təkbaşına işləməz, onu aktivləşdirmək üçün `l1_ratio` parametri və uyğun `solver` lazımdır.\n",
    "\n",
    "### 2. **`solver='saga'`**\n",
    "   - **İşlevi:** `solver` parametri Logistic Regression modelinin təlimi üçün istifadə edilən optimizasiya alqoritmini təyin edir. `saga`, həm L1, həm də ElasticNet cərimələndirmələrini dəstəkləyən bir solverdir. Bu alqoritm böyük verilənlər dəstlərində və sparse (sıx olmayan) xüsusiyyətlərə malik olan modellərdə çox effektivdir.\n",
    "   - **Nə üçün vacibdir?**: ElasticNet cərimələndirməsi üçün `saga` solverini seçmək məcburidir, çünki yalnız `saga` (və `liblinear`) L1 normunu dəstəkləyir, ElasticNet isə həm L1, həm də L2 normlarını birləşdirdiyindən, onun üçün də uyğundur.\n",
    "\n",
    "### 3. **`l1_ratio=0.5`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsi daxilində L1 və L2 cərimələri arasındakı balansı təyin edir. \n",
    "     - **`l1_ratio=0`** olduqda, ElasticNet yalnız L2 (yəni, Ridge) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=1`** olduqda, ElasticNet yalnız L1 (yəni, Lasso) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=0.5`** dəyəri həm L1, həm də L2 cərimələndirmələrinin qarışığını tətbiq edir, yəni hər ikisini bərabər şəkildə qarışdırır.\n",
    "   - **İstifadə məqsədi:** Bu parametr L1 və L2 normları arasında uyğun bir balans yaratmaq üçündür. Verilən problemə uyğun optimal balansı tapmaq üçün `l1_ratio` dəyərini dəyişdirərək təcrübə etmək mümkündür.\n",
    "\n",
    "### 4. **`C=1.0`**\n",
    "   - **İşlevi:** `C` parametri modelin cərimələndirmə əmsalını təyin edir. `C` dəyəri cərimələndirmənin tərs əmsalıdır (yəni, `C` nə qədər böyükdürsə, cərimə bir o qədər kiçikdir və əksinə).\n",
    "   - **Nə üçün vacibdir?**: \n",
    "     - **Kiçik `C` dəyəri:** Daha böyük cərimələndirməyə səbəb olur, yəni modelin sadələşdirilməsinə və əmsalların kiçikləşdirilməsinə (və ya sıfıra yaxınlaşmasına) gətirib çıxarır.\n",
    "     - **Böyük `C` dəyəri:** Daha az cərimələndirməyə səbəb olur və model daha çox uyğunlaşır (overfitting riskini artırır).\n",
    "   - **Standart dəyər:** `C=1.0` balanslı bir dəyərdir, lakin onu artırıb azaltmaqla modelin performansını dəyişdirə bilərsiniz.\n",
    "\n",
    "### **Yekun:**\n",
    "Bu parametrlər birlikdə ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə edilir. `penalty='elasticnet'` ElasticNet cərimələndirməsini tətbiq edir, `solver='saga'` bu cərimələndirməni dəstəkləyən optimizasiya alqoritmidir, `l1_ratio=0.5` L1 və L2 normları arasında balansı təyin edir, və `C=1.0` isə cərimələndirmənin dərəcəsini müəyyən edir. Bu parametrlər sizin modelinizin kompleksliyini idarə etmək və overfitting riskini azaldaraq daha dəqiq proqnozlar vermək üçün istifadə edilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab96bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe20b15",
   "metadata": {},
   "source": [
    "## solver nedir umumiyyetle ve imkan varsa formulasini yaz ve ya adi dilde izah ele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd31be",
   "metadata": {},
   "source": [
    "`Solver`, statistik modellərin və optimizasiya problemlərinin həlli üçün istifadə olunan bir alqoritmdir. Xüsusən, `sklearn.linear_model.LogisticRegression` sinifində olduğu kimi, xətti modellərdə (xətti reqressiya, logistika reqressiyası və s.) əmsalları tapmaq üçün bu alqoritmlərdən istifadə olunur. \n",
    "\n",
    "### **Solver Nədir?**\n",
    "\n",
    "- **Nə iş görür?**: Solver, optimizasiya problemini həll edərək modelin əmsallarını tapır. Bu prosesdə məqsəd, itki funksiyasını (loss function) minimallaşdırmaq və ən uyğun model parametrlərini tapmaqdır.\n",
    "- **İş prinsipi:** Solver modelin parametrlərini iterativ şəkildə dəyişdirir və itki funksiyasını tənzimləyərək ən yaxşı nəticəyə çatmağa çalışır. Hər bir iterasiyada modelin parametrləri yenilənir və daha az itki ilə nəticə əldə etməyə çalışılır.\n",
    "\n",
    "### **Populyar Solver-lər və İş Prinsipləri**\n",
    "\n",
    "1. **`liblinear`**\n",
    "   - **Xüsusiyyətləri:** Çoxlu miqdarda xüsusiyyətə malik olan (feature) kiçik və orta miqdarda verilənlər üçün yaxşıdır. L1 və L2 cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Koordinat Descent alqoritmindən istifadə edir. L1 və L2 cərimələndirməsi ilə səmərəlidir.\n",
    "\n",
    "2. **`newton-cg`**\n",
    "   - **Xüsusiyyətləri:** Daha böyük verilənlər dəstləri üçün uyğundur. L2 cərimələndirməsini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Newton-Raphson metodundan istifadə edir. Hər iterasiyada ikinci dərəcəli diferensial məlumatlardan istifadə edərək əmsalları yeniləyir.\n",
    "\n",
    "3. **`saga`**\n",
    "   - **Xüsusiyyətləri:** Böyük verilənlər dəstləri və sıx olmayan (sparse) verilənlər üçün yaxşıdır. L1, L2 və ElasticNet cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Stochastic Average Gradient Descent (SAG) metodunun inkişaf etdirilmiş formasıdır. Bu, gradient descend metodu ilə əmsalları iterativ şəkildə yeniləyir.\n",
    "\n",
    "4. **`lbfgs`**\n",
    "   - **Xüsusiyyətləri:** L2 cərimələndirməsini dəstəkləyir və böyük verilənlər üçün yaxşı nəticələr verir.\n",
    "   - **İş Prinsipi:** Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) alqoritmindən istifadə edir. İkinci dərəcəli optimizasiya üsulu olaraq parametrlərin optimallaşdırılmasına kömək edir.\n",
    "\n",
    "### **Solver-lərin İstifadəsi**\n",
    "\n",
    "Solver-lər müxtəlif optimizasiya alqoritmləri ilə modelin parametrlərini optimallaşdırmağa kömək edir. Onların əsas məqsədi itki funksiyasını minimallaşdırmaqdır.\n",
    "\n",
    "### **Formulalar**\n",
    "\n",
    "Solver-lərin istifadə etdiyi əsas formulalar ümumiyyətlə optimizasiya və gradient descend metodlarına əsaslanır. Burada ən çox istifadə olunan metodlardan biri olan gradient descent formulunu təqdim edirəm:\n",
    "\n",
    "**Gradient Descent Formula:**\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} $$\n",
    "\n",
    "- **$\\theta_{j}$**: Modelin parametridir.\n",
    "- **$\\alpha$**: Öğrenmə sürətidir (learning rate).\n",
    "- **$J(\\theta)$**: İtki funksiyasıdır (loss function).\n",
    "- **$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$**: İtki funksiyasının $\\theta_{j}$ parametrinə görə gradientidir.\n",
    "\n",
    "### **Adi Dildə İzahetmə**\n",
    "\n",
    "Solver-lər modelin \"öyrənmə\" mərhələsində əmsalları tapmağa kömək edir. Məsələn, Logistic Regression modelini qurduğunuz zaman, solver modelin düzgün parametrlərini tapmaq üçün müxtəlif üsullardan istifadə edir. Hər iterasiyada solver əmsalları dəyişdirərək itki funksiyasını (modelin səhvini) azaldır və nəticədə ən yaxşı parametrləri tapmağa çalışır. \n",
    "\n",
    "Solver-lərin seçimləri, verilənlərin ölçüsünə, cərimələndirmə növünə və modelin tələblərinə görə fərqlənə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82674380",
   "metadata": {},
   "source": [
    "## itki funksiayasi nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d078b70",
   "metadata": {},
   "source": [
    "İtki funksiyası (loss function), statistik modelləşdirmədə modelin nə qədər düzgün proqnoz verdiyini ölçmək üçün istifadə olunan bir funksiyadır. Bu funksiya modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqi kvantlaşdırır. İtki funksiyasının məqsədi modelin performansını qiymətləndirmək və modelin parametrlərini optimallaşdırmaq üçün istifadə olunur.\n",
    "\n",
    "İtki funksiyası müxtəlif modellərdə müxtəlif formullarla təyin olunur. Burada iki əsas növ itki funksiyasını təqdim edirəm: **Xətti Reqressiya** və **Logistik Reqressiya** üçün.\n",
    "\n",
    "### **1. Xətti Reqressiya üçün İtki Funksiyası (Mean Squared Error - MSE)**\n",
    "\n",
    "Xətti reqressiya modelində itki funksiyası adətən **Mean Squared Error (MSE)** ilə təyin olunur. MSE, modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqin kvadratlarının ortalamasını ölçür. \n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı (datanın ölçüsü).\n",
    "- **$y_i$**: Həqiqi dəyər (ground truth).\n",
    "- **$\\hat{y_i}$**: Modelin proqnozu (predicted value).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- MSE, modelin nə qədər yanıldığını ölçür. Kiçik MSE dəyərləri modelin daha yaxşı nəticə verdiyini göstərir.\n",
    "- Kvadratlı fərqlərin ortalaması olduğu üçün, böyük səhvləri daha çox vurğulayır.\n",
    "\n",
    "### **2. Logistik Reqressiya üçün İtki Funksiyası (Log Loss / Binary Cross-Entropy Loss)**\n",
    "\n",
    "Logistik reqressiya modelində itki funksiyası adətən **Log Loss** (və ya Binary Cross-Entropy Loss) ilə təyin olunur. Bu itki funksiyası modelin proqnozlarının doğru etiketlərlə nə qədər uyğun olduğunu ölçür.\n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı.\n",
    "- **$y_i$**: Həqiqi etiket (1 və ya 0).\n",
    "- **$\\hat{y_i}$**: Modelin ehtimal proqnozu (0 ilə 1 arasında).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- Log Loss, proqnozların həqiqi etiketlərlə nə qədər uyumlu olduğunu qiymətləndirir.\n",
    "- Əgər modelin proqnozu həqiqi etiketə yaxınsa, Log Loss dəyəri kiçik olacaqdır.\n",
    "- Log Loss, ehtimal dəyərləri ilə işləyir və aşağı ehtimallar üçün yüksək cəzalar verir.\n",
    "\n",
    "### **Əlavə Nümunələr:**\n",
    "\n",
    "- **Hinge Loss**: SVM (Support Vector Machine) modellərində istifadə olunur. Formulu belədir: \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y_i})\n",
    "  $$\n",
    "  burada **$y_i$** doğru etiketlərdir (1 və ya -1), **$\\hat{y_i}$** isə proqnoz edilən etiketlərdir.\n",
    "\n",
    "İtki funksiyası modelin öyrənmə prosesində əsas rol oynayır, çünki modelin parametrləri itki funksiyasını minimuma endirmək üçün optimallaşdırılır. Hər bir model və problem üçün itki funksiyası fərqli ola bilər və bu, modelin məqsədinə və xüsusiyyətlərinə əsaslanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcb53f6",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LogisticRegression daxilinde ridge lasso elasticnet tetbiqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987371e",
   "metadata": {},
   "source": [
    "`sklearn.linear_model.LogisticRegression` sinfi daxilində Ridge, Lasso və ElasticNet cərimələndirmələrini tətbiq etmək mümkündür. Bu cərimələndirmələr `penalty` parametri vasitəsilə seçilir. Aşağıda hər bir cərimələndirmə üsulunun necə tətbiq edildiyini göstərən nümunələr təqdim edirəm.\n",
    "\n",
    "### 1. **Ridge Regression (L2 Penalty)**\n",
    "Ridge cərimələndirməsi `penalty='l2'` parametri ilə həyata keçirilir. Bu, `LogisticRegression` sinfinin standart cərimələndirmə üsuludur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Məlumat dəstini bölmək\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli L2 cəriməsi ilə\n",
    "ridge_model = LogisticRegression(penalty='l2', C=1.0)  # C parametri lambda-nın tərsidir\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ridge Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed563de",
   "metadata": {},
   "source": [
    "### 2. **Lasso Regression (L1 Penalty)**\n",
    "Lasso cərimələndirməsi `penalty='l1'` parametri ilə həyata keçirilir. Bunun üçün `solver` parametrini də uyğun olaraq `liblinear` və ya `saga` seçmək lazımdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)  # SAGA və ya liblinear solver\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Lasso Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4951",
   "metadata": {},
   "source": [
    "### 3. **ElasticNet Regression**\n",
    "ElasticNet cərimələndirməsi üçün həm `l1_ratio`, həm də `penalty='elasticnet'` parametrləri istifadə olunur. Burada da `solver` olaraq `saga` istifadə edilməlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d461ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)\n",
    "elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = elasticnet_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ElasticNet Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb3bb",
   "metadata": {},
   "source": [
    "### Parametrlərin İzahı:\n",
    "- **`penalty`**: Cərimələndirmə növünü təyin edir (`l1`, `l2`, `elasticnet`).\n",
    "- **`C`**: Cərimə əmsalını təyin edir. Daha böyük `C` daha az cərimələndirmə deməkdir (`C` dəyəri lambda-nın tərsidir).\n",
    "- **`l1_ratio`**: ElasticNet üçün L1 və L2 arasında balans yaradır. `l1_ratio=0` yalnız Ridge, `l1_ratio=1` isə yalnız Lasso cəriməsi deməkdir.\n",
    "- **`solver`**: Problemi həll etmək üçün istifadə edilən alqoritmdir. `liblinear` və `saga` solverləri L1 və ElasticNet cərimələndirməsini dəstəkləyir.\n",
    "\n",
    "Bu nümunələrdə Logistic Regression daxilində Ridge, Lasso və ElasticNet cərimələndirmə üsullarını tətbiq edə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555e2d88",
   "metadata": {},
   "source": [
    "## Ridge, Lasso ve Elasticnet  svm, KNN, Decision tree-de de isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa227",
   "metadata": {},
   "source": [
    "Ridge, Lasso və ElasticNet kimi cərimələndirmə üsulları xətti modellərdə, xüsusilə xətti reqressiya və logistika reqressiya modellərində tətbiq olunan texnikalardır. Bu cərimələndirmə metodları, xətti modellərin daha yaxşı ümumiləşdirilməsini təmin etmək üçün nəzərdə tutulmuşdur. Ancaq digər maşın öyrənməsi alqoritmlərində (məsələn, SVM, KNN, Decision Tree) bu cərimələndirmə üsullarının eyni formada işlədilməsi mümkün deyil, çünki bu alqoritmlər fərqli optimizasiya və iş prinsiplərinə malikdir.\n",
    "\n",
    "### **Ridge, Lasso və ElasticNet Cərimələndirmələri**\n",
    "- **Ridge (L2 Penalty):** Əmsalların böyüklüyünü cərimələyir və onların kiçik olmasına səbəb olur. Overfitting riskini azaldır.\n",
    "- **Lasso (L1 Penalty):** Bəzi əmsalları tam sıfıra bərabər edir, yəni xüsusiyyət seçimi (feature selection) rolunu oynayır.\n",
    "- **ElasticNet:** Həm L1, həm də L2 cərimələndirməsini birləşdirir və xüsusiyyətlər arasındakı əlaqələri idarə edir.\n",
    "\n",
    "### **Bu Texnikalar Aşağıdakı Alqoritmlərdə Tətbiq Edilə Bilərmi?**\n",
    "\n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - **SVM Cərimələndirməsi:** SVM-də **cərimə parametri (C)** Ridge Regression-a bənzər şəkildə işləyir. Bu parametr səhv təsnifatları cərimələndirərək marjanı böyütməyə çalışır. Yəni, SVM-də L1 və L2 cəriməsi istifadə edilməsə də, itki funksiyası və cərimələndirmə prinsipləri cərimə parametri `C` ilə idarə olunur.\n",
    "   - **L1 və L2 Cərimələndirməsi:** SVM-də **LinearSVC** sinfi daxilində `penalty='l1'` və `penalty='l2'` parametrləri ilə L1 və L2 cərimələri tətbiq oluna bilər.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "   - **KNN-də Cərimələndirmə:** KNN alqoritmində cərimələndirmə tətbiq olunmur, çünki bu alqoritm xüsusiyyət əmsalları ilə deyil, məsafə metrikləri ilə işləyir. KNN sadəcə verilənlərdəki ən yaxın nöqtələrə əsaslanaraq qərar verir və əmsalların böyüklüyü kimi anlayışlar burada mövcud deyil.\n",
    "   \n",
    "3. **Decision Tree**\n",
    "   - **Decision Tree Cərimələndirməsi:** Decision Tree alqoritmlərində də Ridge və Lasso kimi cərimələndirmələr tətbiq edilmir, çünki bu alqoritm əmsallarla işləmək əvəzinə, verilənləri ağac strukturu şəklində bölür.\n",
    "   - **Overfitting-i İdarə Etmək:** Overfitting riskini azaltmaq üçün Decision Tree alqoritmində **ağacın dərinliyini məhdudlaşdırmaq** və ya **minimum nümunə sayını** təyin etmək kimi texnikalardan istifadə olunur.\n",
    "\n",
    "### **Nəticə**\n",
    "- **Ridge, Lasso və ElasticNet** xətti modellərə aid texnikalardır və onlar SVM-də müəyyən formada tətbiq oluna bilər.\n",
    "- **KNN** və **Decision Tree** kimi qeyri-xətti modellərdə isə bu texnikalar tətbiq edilmir, çünki bu alqoritmlər əmsallarla deyil, fərqli prinsiplərlə işləyirlər.\n",
    "\n",
    "Bu cərimələndirmə metodlarının işlədiyi modelləri və digər modellərdə alternativ üsulları başa düşmək maşın öyrənməsində doğru texnikaları seçmək üçün vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3321d294",
   "metadata": {},
   "source": [
    "## Bes Random Forest-de Ridge, Lasso ve Elasticnet isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e03f2f",
   "metadata": {},
   "source": [
    "**Random Forest** alqoritmində **Ridge**, **Lasso** və **ElasticNet** kimi cərimələndirmə üsulları birbaşa tətbiq olunmur. Bunun səbəbi Random Forest-in təməlində **Decision Tree** alqoritmi dayanmasıdır və Decision Tree əmsallarla işləmək əvəzinə, verilənləri qaydalara görə bölən bir alqoritmdir.\n",
    "\n",
    "### **Niyə Random Forest-də Ridge, Lasso və ElasticNet İşləmir?**\n",
    "\n",
    "1. **Decision Tree ilə İşləmə Prinsipi:**\n",
    "   - Random Forest bir çox decision tree-dən ibarət olan ansambl metodudur. Hər bir decision tree verilənlər dəstini qaydalara görə bölərək təsnifat və ya reqressiya işləri görür.\n",
    "   - Decision Tree-lər verilənləri əmsallarla təhlil etmədiyi üçün, Ridge, Lasso və ElasticNet cərimələri ilə işləmirlər. Bu cərimələr xətti modellərdə əmsalları tənzimləmək üçün istifadə olunur, lakin Decision Tree-lərdə belə bir əmsal yoxdur.\n",
    "\n",
    "2. **Overfitting-lə Mübarizə:**\n",
    "   - Random Forest alqoritmi decision tree-lərin təbii overfitting meylini azaldır. Bir çox ağacın nəticələrini birləşdirərək (və fərqli verilənlər dəstləri və xüsusiyyətlər üzərində hər bir ağacı təlim etdirərək), Random Forest daha sabit və ümumiləşdirilmiş nəticələr verir.\n",
    "   - Random Forest modelində overfitting-in qarşısını almaq üçün ağacın dərinliyini məhdudlaşdırmaq, minimum nümunə sayını təyin etmək və ağaclar arasında müxtəliflik yaratmaq kimi metodlardan istifadə olunur. Bu üsullar Ridge, Lasso və ElasticNet cərimələndirmələrinin funksiyasını əvəz edir.\n",
    "\n",
    "### **Random Forest üçün Alternativlər:**\n",
    "Ridge, Lasso və ElasticNet kimi cərimələndirmələr xətti modellərdə xüsusiyyətlərin seçimi və modelin sadələşdirilməsi üçün istifadə olunur. Random Forest-də isə xüsusiyyətlərin seçimi avtomatik şəkildə edilir, çünki hər ağac yalnız bir hissə xüsusiyyəti istifadə edir. \n",
    "\n",
    "Əgər xüsusiyyət seçimi və ya əmsallar üzərində müəyyən bir cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərdən istifadə etməlisiniz, məsələn:\n",
    "\n",
    "- **Linear Regression** üçün Ridge, Lasso və ElasticNet.\n",
    "- **Logistic Regression** üçün L1, L2 və ElasticNet.\n",
    "\n",
    "Random Forest-də belə bir ehtiyac yoxdur, çünki o, ansambl metodlarının təbiəti ilə artıq overfitting-ə qarşı dayanır və xüsusiyyət seçimi prosesini təbii olaraq həyata keçirir.\n",
    "\n",
    "### **Nəticə:**\n",
    "Random Forest-də Ridge, Lasso və ElasticNet kimi cərimələndirmələr işləməz, çünki bu cərimələndirmələr xətti modellər üçün nəzərdə tutulmuşdur və Random Forest-in iş prinsipinə uyğun gəlmir. Əgər modeldə xüsusiyyət seçimi və cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərə baxmaq daha uyğun olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a911b4c3",
   "metadata": {},
   "source": [
    "## Types of Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3c35f",
   "metadata": {},
   "source": [
    "Ensemble learning metodları, bir neçə zəif modelin birləşdirilərək daha güclü və sabit nəticələr əldə edilməsinə imkan verir. Bu metodlar fərqli modelləri və ya eyni modelin fərqli versiyalarını istifadə edərək nəticələrin ortalamasını alır və ya səs çoxluğunu təyin edir. Aşağıda Ensemble learning-in əsas növləri haqqında geniş məlumat verilir:\n",
    "\n",
    "### 1. **Voting**\n",
    "Voting, müxtəlif modellərin proqnozlarının birləşdirilməsi üsuludur. Burada əsas məqsəd bir neçə fərqli modelin proqnozlarını birləşdirərək daha dəqiq nəticə əldə etməkdir.\n",
    "\n",
    "- **Hard Voting**: Hər modelin proqnozları nəzərə alınır və ən çox səs verilən sinif seçilir. Yəni, hansı sinif daha çox model tərəfindən seçilirsə, o sinif yekun proqnoz kimi təyin edilir.\n",
    "- **Soft Voting**: Hər modelin ehtimal proqnozları nəzərə alınır və ortalama ehtimallar əsasında yekun proqnoz verilir.\n",
    "\n",
    "### 2. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging, Bootstrap metodunu istifadə edərək təkrarlı olaraq verilənlərdən müxtəlif alt dəstələr seçir və hər bir alt dəstədə fərqli modellər qurur. Bu metod əsasən yüksək dəyişkənliyə malik modellərin stabilləşdirilməsi üçün istifadə edilir.\n",
    "\n",
    "- **Random Forest**: Bagging-in ən məşhur tətbiqi olan Random Forest, müxtəlif decision tree-lərdən ibarət bir ansambl yaradır. Hər bir decision tree fərqli verilənlər dəstəsində qurulur və nəticələr səs çoxluğuna əsasən təyin edilir.\n",
    "\n",
    "### 3. **Boosting**\n",
    "Boosting, ardıcıl olaraq zəif modellər qurur və hər yeni model əvvəlki modelin düzgün proqnoz verə bilmədiyi nümunələri düzəltməyə çalışır. Bu metod zəif modelləri güclü modelə çevirmək üçün nəzərdə tutulub.\n",
    "\n",
    "- **AdaBoost**: AdaBoost, hər modelin düzgün və ya səhv proqnozlarına görə ağırlıqlarını dəyişdirərək zəif modelləri ardıcıl qurur. Səhv proqnozlar daha çox ağırlıq alır, doğru proqnozlar isə daha az.\n",
    "- **Gradient Boosting**: Gradient Boosting, qalıqların üzərində işləyən modellər yaradır. Hər yeni model əvvəlki modelin səhvlərini düzəltməyə çalışır.\n",
    "- **XGBoost**: Gradient Boosting-in optimallaşdırılmış versiyasıdır. Hesablama performansını artırmaq və overfitting-i azaltmaq üçün müxtəlif optimallaşdırmalar təqdim edir.\n",
    "- **CatBoost**: KatBoost, əsasən kateqorik verilənlərlə işləmək üçün optimallaşdırılmış bir Boosting metodudur. Bu metod xüsusilə kateqorik dəyişənlərin avtomatik olaraq kodlaşdırılmasını və optimal performans təmin etməsini asanlaşdırır.\n",
    "- **LightGBM**: LightGBM, Gradient Boosting-in daha yüngül və sürətli bir variantıdır. Bu metod böyük verilənlər dəstələri üzərində effektiv şəkildə işləyir və hesablama müddətini əhəmiyyətli dərəcədə azaldır.\n",
    "\n",
    "### 4. **Model Stacking**\n",
    "Model Stacking, fərqli modellərin proqnozlarını birləşdirərək ikinci səviyyəli bir model qurur. Bu ikinci səviyyəli model ilkin modellərin proqnozlarına əsaslanaraq yekun proqnoz verir.\n",
    "\n",
    "- **Base Models**: İlk səviyyədə bir neçə müxtəlif model istifadə olunur və hər biri verilənlər dəstəsində təlim olunur.\n",
    "- **Meta-Model**: İkinci səviyyədə, ilkin modellərin proqnozlarını input olaraq istifadə edən bir meta-model qurulur. Bu meta-model yekun proqnozu təyin edir.\n",
    "\n",
    "### Nəticə\n",
    "Bu metodlar Ensemble learning-in müxtəlif tətbiq üsullarıdır və hər biri fərqli üstünlüklərə malikdir. Bagging yüksək dəyişkənlikli modelləri stabilləşdirmək üçün istifadə edilir, Boosting isə zəif modelləri güclü modellərə çevirmək üçün. Voting sadə birləşdirmə üsuludur, Stacking isə daha kompleks və güclü ansambl modellər yaratmağa imkan verir.\n",
    "\n",
    "CatBoost və LightGBM kimi spesifik alqoritmlər isə Boosting texnikalarının optimallaşdırılmış formalarını təklif edirlər və müxtəlif problem sahələrində yüksək performans təmin edə bilirlər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ab1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e37dec7",
   "metadata": {},
   "source": [
    "## 3. Boosting modellerin daha etrafli izahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869bd18",
   "metadata": {},
   "source": [
    "Boosting metodları maşın öyrənməsində güclü və effektiv modellər qurmaq üçün geniş istifadə olunur. AdaBoost, Gradient Boosting, XGBoost, CatBoost və LightGBM kimi müxtəlif Boosting alqoritmləri mövcuddur. Hər birinin özünəməxsus xüsusiyyətləri, üstünlükləri və məhdudiyyətləri vardır. Aşağıda bu modellərin detallı fərqlərini və hansı vəziyyətdə hansı modelin daha uyğun olacağını izah edirəm.\n",
    "\n",
    "## 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni zəif modelin əvvəlki modellərin səhvlərinə fokuslanaraq qurulmasını təmin edir. Yəni, səhv proqnoz verilən nümunələrə daha çox ağırlıq verilir.\n",
    "- **Ağırlıq Yeniləmə:** Hər iterasiyada nümunələrin ağırlıqları yenilənir. Səhv proqnoz verilən nümunələrin ağırlığı artırılır, doğru proqnoz verilənlərin isə azaldılır.\n",
    "- **Zəif Modellər:** Adətən, sadə qərar ağacları (decision stumps) kimi zəif modellər istifadə olunur.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sadə və asan tətbiq edilə biləndir.\n",
    "- Overfitting riskini azaldır.\n",
    "- Sürətli təlim prosesinə malikdir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Məsələn, çoxlu sayda outliers olan verilənlərdə performansı zəif ola bilər.\n",
    "- Kompleks verilənlər üçün optimal olmaya bilər.\n",
    "\n",
    "## 2. Gradient Boosting\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** Gradient Boosting, hər yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışdığı iterativ bir prosesdir.\n",
    "- **Qalıq Əsasında Təkmilləşdirmə:** Hər iterasiyada əvvəlki modellərin səhvləri üzərində işləyərək ümumi dəqiqliyi artırır.\n",
    "- **Loss Funksiyaları:** Müxtəlif itki funksiyalarını dəstəkləyir, bu da onu müxtəlif problemlərə uyğunlaşdırır.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Yüksək dəqiqlik və performans.\n",
    "- Çeşidli problemlərə tətbiq oluna bilir (regressiya, təsnifat və s.).\n",
    "- Parametrlərin geniş konfiqurasiyası ilə optimallaşdırma imkanı.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Hesablama baxımından bahalı ola bilər.\n",
    "- Overfitting riski yüksəkdir, xüsusilə çox sayda iterasiya ilə.\n",
    "\n",
    "## 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Optimallaşdırılmış Gradient Boosting:** Hesablama sürətini və yaddaş istifadəsini optimallaşdırır.\n",
    "- **Regularizasiya:** L1 və L2 regularizasiyası vasitəsilə overfitting-i azaldır.\n",
    "- **Paralel Təlim:** Paralel hesablama imkanları ilə təlim prosesini sürətləndirir.\n",
    "- **Tree Pruning:** Daha effektiv ağac kəsimi alqoritmləri istifadə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sürətli və effektiv, böyük verilənlər üzərində yaxşı işləyir.\n",
    "- Yüksək performans və dəqiqlik.\n",
    "- Çoxlu sayda parametr tuning imkanları.\n",
    "- Kaggle kimi yarışmalarda geniş istifadə olunur.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Parametr tuning mürəkkəb ola bilər.\n",
    "- Daha çox resurs tələb edir.\n",
    "\n",
    "## 4. CatBoost\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Kateqorik Dəyişənlərin Avtomatik Emalı:** Kateqorik verilənləri avtomatik olaraq kodlaşdırır, əlavə ön emal tələb etmir.\n",
    "- **Order Boosting:** Overfitting-i azaltmaq üçün xüsusi təlim strategiyası istifadə edir.\n",
    "- **Default Parametrlər:** Yaxşı performans üçün az parametr tuning tələb edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Kateqorik verilənlərlə asan işləyir.\n",
    "- Overfitting riski aşağıdır.\n",
    "- Performansı yüksək, xüsusən kateqorik xüsusiyyətlər çox olan verilənlər üçün.\n",
    "- Minimal ön emal tələb edir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Bəzi hallarda digər modellərə nisbətən daha yavaş ola bilər.\n",
    "- Bəzi platformalarda performans fərqləri ola bilər.\n",
    "\n",
    "## 5. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Sürətli Təlim:** Gradient Boosting metodunu daha sürətli və yüngül etmək üçün optimallaşdırılmışdır.\n",
    "- **Histogram-based:** Histogram əsasında təlim alqoritmi daha az yaddaş istifadə edir və sürətli təlim təmin edir.\n",
    "- **Leaf-wise Tree Growth:** Dərin ağaclar quraraq daha yaxşı dəqiqlik əldə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Böyük verilənlər dəstləri üçün idealdır.\n",
    "- Yüksək sürət və effektivlik.\n",
    "- Daha az yaddaş tələb edir.\n",
    "- Yaxşı performans və dəqiqlik.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Overfitting riski, xüsusilə dərin ağaclar istifadə edildikdə.\n",
    "- Kateqorik xüsusiyyətlər üçün əlavə ön emal tələb edə bilər (CatBoost-dan fərqli olaraq).\n",
    "\n",
    "## Hansını Necə Tətbiq Etmək?\n",
    "\n",
    "### **AdaBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Sadə problemlər və kiçik verilənlər üçün idealdır.\n",
    "- **Misal:** Klassik təsnifat problemləri, məsələn, spam filtrləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riski azaldılmışdır, lakin çoxlu outliers olan verilənlər üçün uyğun deyil.\n",
    "\n",
    "### **Gradient Boosting:**\n",
    "- **Uyğun Vəziyyətlər:** Yüksək dəqiqlik tələb edən müxtəlif problemlər üçün geniş istifadə olunur.\n",
    "- **Misal:** Proqnozlaşdırma modelləri, satış təhlili.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Optimal parametr tuning ilə yüksək performans əldə etmək mümkündür, amma hesablama baxımından bahalı ola bilər.\n",
    "\n",
    "### **XGBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Böyük verilənlər və yüksək performans tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Kaggle yarışmaları, böyük biznes məlumat analizi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Parametr tuning mürəkkəb ola bilər, amma yüksək performans təmin edir.\n",
    "\n",
    "### **CatBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Kateqorik xüsusiyyətlər çox olan verilənlər üçün ən yaxşı seçimdir.\n",
    "- **Misal:** Marketinq analizi, müştəri seqmentləşdirməsi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Minimal ön emal tələb edir və kateqorik verilənlərlə asan işləyir.\n",
    "\n",
    "### **LightGBM:**\n",
    "- **Uyğun Vəziyyətlər:** Çox böyük verilənlər və yüksək sürət tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Real-time təhlil sistemləri, böyük verilənlər dəstləri ilə işləyən biznes tətbiqləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riskinə diqqət edilməli, xüsusən dərin ağaclar istifadə edildikdə.\n",
    "\n",
    "## Nəticə\n",
    "\n",
    "Hansı Boosting metodunu seçməyiniz layihənizin tələblərinə və verilənlərinizin xüsusiyyətlərinə bağlıdır. Aşağıdakı tövsiyələrdən istifadə edə bilərsiniz:\n",
    "\n",
    "- **Kateqorik verilənlər çoxdursa:** **CatBoost** ən yaxşı seçimdir.\n",
    "- **Böyük verilənlər üzərində sürətli və effektiv performans istəyirsinizsə:** **LightGBM** uyğun olar.\n",
    "- **Optimal performans və geniş optimallaşdırma imkanları üçün:** **XGBoost** istifadə edilə bilər.\n",
    "- **Sadə və kiçik verilənlər üçün:** **AdaBoost** və ya **Gradient Boosting** istifadə edilə bilər.\n",
    "\n",
    "Əlavə olaraq, müxtəlif modelləri sınayıb onların performansını müqayisə etmək, layihəniz üçün ən uyğun olanını müəyyənləşdirməyə kömək edəcəkdir. Modellərin hər biri müxtəlif vəziyyətlərdə üstünlük göstərə bilər, buna görə də verilənlərinizin xüsusiyyətlərini və layihənizin tələblərini diqqətlə qiymətləndirmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b062ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd571f2",
   "metadata": {},
   "source": [
    "## Voting de regressor tetbiqi haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6587a",
   "metadata": {},
   "source": [
    "**Voting Regressor** də mövcuddur və bu metod, müxtəlif regresiya modellərinin nəticələrini birləşdirərək yekun proqnoz çıxarmaq üçün istifadə olunur. Voting Regressor, modellərin birləşdirilməsində fərqli strategiyalar istifadə edərək, bir neçə fərqli regressoru birləşdirir. \n",
    "\n",
    "### **Voting Regressor**\n",
    "Voting Regressor, bir neçə fərqli regressoru birləşdirərək yekun proqnoz əldə edir. Burada əsas ideya, müxtəlif modellərin proqnozlarını birləşdirərək ümumi nəticəni yaxşılaşdırmaqdır. Voting Regressor iki növ ola bilər:\n",
    "\n",
    "1. **Averaging (Orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır və onların ortalaması çıxarılır.\n",
    "   - Məsələn, üç fərqli model (Model 1, Model 2, Model 3) bir verilənlər dəstəsi üzərində tətbiq edildikdə, hər bir modelin proqnozu alınır və bu proqnozların arifmetik ortalaması nəticə kimi qəbul edilir.\n",
    "\n",
    "2. **Weighted Averaging (Çəkili orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır, lakin hər bir modelin proqnozuna müəyyən bir çəki verilir. Daha güclü performansa sahib modellərə daha çox çəki verilir.\n",
    "   - Bu metod, daha yaxşı performans göstərən modellərin nəticələrə daha çox təsir etməsini təmin edir.\n",
    "\n",
    "### **Voting Regressor-un üstünlükləri**\n",
    "- **Model müxtəlifliyi**: Fərqli modellərin birləşdirilməsi ilə, hər bir modelin zəif cəhətləri digər modellər tərəfindən kompensasiya edilə bilər.\n",
    "- **Təkmilləşdirilmiş performans**: Model müxtəlifliyi nəticəsində Voting Regressor daha sabit və dəqiq nəticələr təmin edə bilər.\n",
    "\n",
    "### **Nümunə: Voting Regressor**\n",
    "Tutaq ki, siz Linear Regression, Decision Tree Regressor və Random Forest Regressor modellərindən istifadə edirsiniz. Bu modellərin proqnozlarını Voting Regressor ilə birləşdirə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Ayrı-ayrı modelləri təyin edək\n",
    "model1 = LinearRegression()\n",
    "model2 = DecisionTreeRegressor()\n",
    "model3 = RandomForestRegressor()\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('lr', model1), \n",
    "    ('dt', model2), \n",
    "    ('rf', model3)\n",
    "])\n",
    "\n",
    "# Modeli təlim etdirin və proqnoz verin\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "```\n",
    "\n",
    "Bu yanaşma, modellərin hər birinin güclü tərəflərindən faydalanmağa imkan verir və beləliklə, yekun proqnoz daha dəqiq ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec88396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e038e07b",
   "metadata": {},
   "source": [
    "## bagging de regressor modelleri tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129672",
   "metadata": {},
   "source": [
    "Bəli, Bagging yalnız klassifikatorlar deyil, həm də regressor modelləri üçün tətbiq edilə bilər. Bagging metodunun əsas məqsədi, modelin təlim edildiyi verilənlər dəstələrini müxtəlif alt dəstələrə bölərək hər bir alt dəstədə fərqli modellər qurmaqdır. Bu metod, həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Bagging Regressor**\n",
    "- **Bagging Regressor**: Əsas model olaraq hər hansı bir regresor seçilə bilər, məsələn, Decision Tree Regressor, Linear Regression, və s. Bagging Regressor, müxtəlif verilənlər alt dəstələri üzərində bu əsas modelləri qurur və nəticələrin ortalamasını alaraq yekun proqnoz verir.\n",
    "  \n",
    "- **Random Forest Regressor**: Bagging-in spesifik bir tətbiqi olan Random Forest, Decision Tree Regressor-larının bir ansamblından ibarət olan bir regresor modeldir. Random Forest Regressor, verilənlər dəstəsindən təkrarən seçilmiş alt dəstələr üzərində bir neçə decision tree qurur və nəticədə bütün ağacların proqnozlarının ortalamasını alaraq yekun proqnoz verir.\n",
    "\n",
    "Bagging metodunun regressorlar üzərində istifadəsi, modelin variansını azaldır və daha stabil proqnozlar əldə etməyə kömək edir. Bu, xüsusilə decision tree kimi yüksək dəyişkənlikli modellərdə faydalıdır, çünki Bagging bu dəyişkənliyi stabilləşdirir və overfitting-in qarşısını alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe3b13a",
   "metadata": {},
   "source": [
    "## bes boosting de regressor tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845b0ce",
   "metadata": {},
   "source": [
    "Bəli, Boosting metodları da regressor modelləri üçün tətbiq oluna bilər. Boosting, əsas modelin ardıcıl olaraq qurulması ilə işləyir, burada hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu metod da həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Boosting Regressor**\n",
    "- **AdaBoost Regressor**: AdaBoost metodunun regresiya üçün tətbiqi. Burada hər bir ardıcıl model əvvəlki modelin proqnoz səhvlərinə daha çox diqqət yetirir. Nəticədə, modelin performansı ardıcıl olaraq yaxşılaşır.\n",
    "\n",
    "- **Gradient Boosting Regressor**: Gradient Boosting, hər yeni modelin qalıqları (yəni əvvəlki modellərin proqnoz səhvləri) üzərində qurulması ilə işləyir. Bu metod çox populyardır və yüksək dəqiqlik təmin edir. Scikit-learn kitabxanasında Gradient Boosting Regressor olaraq tətbiq edilir.\n",
    "\n",
    "- **XGBoost, LightGBM, və CatBoost**: Bunlar Boosting-in daha inkişaf etmiş versiyalarıdır və böyük verilənlər üzərində sürətli və effektiv təlim etmə qabiliyyəti ilə tanınır. Hər üç metod da regresiya problemləri üçün tətbiq oluna bilər və xüsusilə böyük ölçülü və mürəkkəb verilənlər dəstələri üzərində çox yaxşı nəticələr verir.\n",
    "\n",
    "Boosting metodları regressor modellərində də çox uğurlu ola bilər, xüsusən də verilənlər dəstəsində qeyri-xətti əlaqələr və mürəkkəb strukturlar olduqda. Bu metodlar modelin performansını artırmaq üçün ardıcıl olaraq səhvləri azaldır və nəticədə daha yaxşı proqnozlar təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8580f2f1",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96744d1",
   "metadata": {},
   "source": [
    "Aşağıdakı cədvəldə Voting, Bagging və Boosting metodlarının əsas fərqləri göstərilib:\n",
    "\n",
    "| **Xüsusiyyət**                | **Voting**                                      | **Bagging**                                   | **Boosting**                                      |\n",
    "|-------------------------------|-------------------------------------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **İş prinsipi**               | Fərqli modellərin nəticələrini birləşdirir      | Eyni modelin bir neçə varyantını təlim edir   | Modellər ardıcıl olaraq təlim edilir, səhvləri düzəldir |\n",
    "| **Modellərin birləşdirilməsi**| Bir neçə fərqli model (ən çox istifadə edilənlər) | Eyni modelin fərqli alt nümunələr üzərində təlimi | Hər bir növbəti model əvvəlkilərin səhvlərini düzəldir |\n",
    "| **İşləmə tərzi**              | Paralel                                         | Paralel                                        | Sequential (ardıcıl)                               |\n",
    "| **Overfitting riski**         | Aşağı                                           | Aşağı                                          | Daha yüksək, amma düzgün parametrlərlə idarə edilə bilər |\n",
    "| **Çeviklik**                  | Çox müxtəlif modelləri birləşdirə bilir         | Əsasən eyni modelin təkrarlanması               | Modellər daha çox özünü tənzimləyir, çətin nümunələrə fokuslanır |\n",
    "| **Hesablama mürəkkəbliyi**    | Orta                                            | Aşağı                                          | Yüksək, çünki ardıcıl təlim tələb olunur          |\n",
    "| **Məşhur alqoritmlər**        | VotingClassifier, VotingRegressor               | Random Forest, BaggingClassifier               | AdaBoost, XGBoost, Gradient Boosting, LightGBM, CatBoost |\n",
    "| **Paralel İşləmə**            | Bəli                                            | Bəli                                           | Xeyr                                              |\n",
    "| **Stabillik**                 | Artırır, modelləri birləşdirir                  | Artırır, varyansı azaldır                      | Riskli, ancaq yaxşı idarə olunarsa dəqiqlik artır  |\n",
    "| **Tətbiq sahələri**           | Hər iki klassifikasiya və reqressiya problemləri | Hər iki klassifikasiya və reqressiya problemləri | Əsasən klassifikasiya, bəzən reqressiya            |\n",
    "\n",
    "Bu cədvəl Voting, Bagging və Boosting metodlarının əsas xüsusiyyətlərini və fərqlərini ümumi şəkildə göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167de2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60b3719",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1071",
   "metadata": {},
   "source": [
    "| **Xüsusiyyət**                      | **Voting**                                      | **Bagging**                                      | **Boosting**                                  |\n",
    "|-------------------------------------|-------------------------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **Əsas İdeya**                      | Müxtəlif modellərin nəticələrinin birləşdirilməsi | Eyni modelin müxtəlif nümunələr və xüsusiyyətlər üzərində təlim edilməsi | Modellərin ardıcıl şəkildə təlim edilməsi və səhvlərin düzəldilməsi |\n",
    "| **Model Birləşdirmə**               | Fərqli modellər (məs. SVM, DT, LR)               | Eyni modelin fərqli versiyaları (məs. bir neçə decision tree) | Hər bir model əvvəlki modelin səhvlərini düzəldir |\n",
    "| **Nəticə Birləşdirmə**              | Səs çoxluğu və ya ortalama ilə                   | Orta/median (regressiya) və ya səs çoxluğu (klassifikasiya) | Ardıcıl modellərin nəticələri birləşdirilir |\n",
    "| **Paralel/Sequential İşləmə**       | Paralel                                         | Paralel                                          | Sequential, yəni ardıcıl |\n",
    "| **Overfitting Risk**                | Orta səviyyədə                                  | Aşağı                                            | Yüksək (əgər düzgün təlim edilməzsə)        |\n",
    "| **Əsas Məqsəd**                     | Modellərin ümumi dəqiqliyini artırmaq            | Varyansı azaltmaq və stabil nəticələr əldə etmək | Səhvləri azaltmaq və dəqiqliyi artırmaq     |\n",
    "| **İstifadə Edilən Məşhur Algoritmlər** | VotingClassifier                                | Random Forest, BaggingClassifier                 | AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost |\n",
    "| **Parametr Sayı**                   | Əsasən modellərin sayına bağlıdır               | Estimator, n_estimators, max_samples, max_features, bootstrap | Learning rate, n_estimators, base_estimator |\n",
    "| **Hesablama Çətinliyi**             | Aşağı və ya orta                                 | Orta                                             | Yüksək                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5443ad3",
   "metadata": {},
   "source": [
    "### X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size= 0.3, stratify = y, random_state=42) \n",
    "* stratify nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d473ff",
   "metadata": {},
   "source": [
    "`stratify` parametri, `train_test_split` funksiyasında verilən dataset-i təlim və test dəstlərinə bölərkən, target dəyişəninin (`y`) siniflərinin nisbətini qorumaq üçün istifadə edilir. Bu, dataset-dəki siniflərin balanslı qalmasını təmin edir.\n",
    "\n",
    "### Nümunə:\n",
    "Tutaq ki, dataset-də iki sinif var: `0` və `1`. Əgər dataset-inizdə `0` sinfi 70% və `1` sinfi 30% təşkil edirsə, `stratify=y` olaraq `train_test_split` funksiyasına verildikdə, həm təlim, həm də test dəstlərində bu nisbətlər təxminən qorunacaq. Bu, modelin hər iki sinifi daha yaxşı öyrənməsinə kömək edir, xüsusilə də sinif balansı qeyri-bərabər olduqda.\n",
    "\n",
    "Əgər `stratify` parametrini verməsəniz və dataset sinifləri qeyri-bərabərdirsə, təlim və test dəstlərində siniflərin nisbəti fərqli ola bilər, bu da modelin performansına mənfi təsir göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bb581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7202b8ed",
   "metadata": {},
   "source": [
    "## confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad768cf",
   "metadata": {},
   "source": [
    "`confusion_matrix` funksiyasının Sklearn-də qaytardığı qarışıqlıq matrisi aşağıdakı formada olur:\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | True Negative (TN)         | False Positive (FP)        |\n",
    "| **Əsl Sinif 1** | False Negative (FN)        | True Positive (TP)         |\n",
    "\n",
    "### Açıklama:\n",
    "- **True Negative (TN):** Əslində 0 olan nümunələr və düzgün 0 olaraq proqnozlaşdırılanlar.\n",
    "- **False Positive (FP):** Əslində 0 olan nümunələr, amma səhvən 1 olaraq proqnozlaşdırılanlar.\n",
    "- **False Negative (FN):** Əslində 1 olan nümunələr, amma səhvən 0 olaraq proqnozlaşdırılanlar.\n",
    "- **True Positive (TP):** Əslində 1 olan nümunələr və düzgün 1 olaraq proqnozlaşdırılanlar.\n",
    "\n",
    "### Confusion Matrix-dəki Hüceyrələr\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | TN (True Negative)         | FP (False Positive)        |\n",
    "| **Əsl Sinif 1** | FN (False Negative)        | TP (True Positive)         |\n",
    "\n",
    "Bu cədvəl confusion matrix-dəki hüceyrələrin yerləşməsini göstərir. Sklearn-də `confusion_matrix` funksiyasını istifadə edərək əldə etdiyiniz matrisi bu qaydada oxuya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b4016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1d0a",
   "metadata": {},
   "source": [
    "## xgbt.score(X_train, y_train)\n",
    "* bu neyin score dur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52720c",
   "metadata": {},
   "source": [
    "`xgbt.score(X_train, y_train)` metodu, `xgbt` modelinin (`XGBClassifier` obyektinin) təlim məlumat dəstində (`X_train`, `y_train`) əldə etdiyi dəqiqliyi (`accuracy`) qaytarır.\n",
    "\n",
    "### Ətraflı:\n",
    "- **`X_train`:** Təlim üçün istifadə olunan xüsusiyyətlər (features).\n",
    "- **`y_train`:** Təlim üçün istifadə olunan hədəf dəyişəni (labels).\n",
    "- **`xgbt.score`:** Dəqiqlik hesabını qaytarır, yəni təlim dəstində proqnozların neçə faizinin düzgün olduğunu göstərir.\n",
    "\n",
    "Əgər modelin proqnozları `y_train` ilə uyğun gəlirsə, `score` metodu təlim dəstindəki ümumi düzgün proqnozların nisbətini verir. Dəqiqlik hesabı 0 ilə 1 arasında dəyişir, burada 1 tam dəqiqliyi (bütün proqnozlar doğrudur) göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce0fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7da861e",
   "metadata": {},
   "source": [
    " xgbt = XGBClassifier( max_depth = 2,\n",
    "*             learning_rate = 0.2,\n",
    "*             objective  = \"multi:softmax\",\n",
    "*             num_class = 2,\n",
    "*             booster = \"gbtree\",\n",
    "*             n_estimators = 10,\n",
    "*             random_state = 123)\n",
    "\n",
    "### Parametrler haqqinda melumet ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fef5f",
   "metadata": {},
   "source": [
    "`XGBClassifier` üçün istifadə olunan parametrler aşağıdakılardır:\n",
    "\n",
    "### **Parametrlər və Təsvirləri:**\n",
    "\n",
    "1. **`max_depth`**\n",
    "   - **Təsvir:** Hər bir decision tree-nin maksimum dərinliyi.\n",
    "   - **Qeyd:** Dərinlik artdıqca model daha kompleks olur, amma overfitting riski də artır.\n",
    "\n",
    "2. **`learning_rate` (və ya `eta`)**\n",
    "   - **Təsvir:** Hər bir tree-nin təsirini azaldır. Kiçik dəyərlər daha yaxşı nəticələr verə bilər, amma daha çox tree tələb edə bilər.\n",
    "   - **Qeyd:** Düşük learning rate modelin öyrənməsini yavaşlatır və bu, daha çox `n_estimators` tələb edir.\n",
    "\n",
    "3. **`objective`**\n",
    "   - **Təsvir:** Məqsəd funksiyasını təyin edir. `multi:softmax` çox sinifli təsnifat üçün istifadə olunur.\n",
    "   - **Qeyd:** Məqsəd funksiyası, modelin öyrənmə məqsədini müəyyən edir. `multi:softmax` birbaşa siniflərin proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "4. **`num_class`**\n",
    "   - **Təsvir:** Siniflərin sayı. `objective=\"multi:softmax\"` istifadə edilərkən təyin edilir.\n",
    "   - **Qeyd:** Bu parametr çox sinifli təsnifat problemini həll edərkən mütləq olaraq təyin edilməlidir.\n",
    "\n",
    "5. **`booster`**\n",
    "   - **Təsvir:** Hansı növ boosterlə (tree-based, linear) modelləşdiriləcəyini müəyyən edir. `gbtree`, `gblinear` və `dart` seçimləri var.\n",
    "   - **Qeyd:** `gbtree` ağac əsaslı boosterlə daha kompleks problemləri həll edə bilər, `gblinear` isə xətləri modelləşdirmək üçün istifadə olunur.\n",
    "\n",
    "6. **`n_estimators`**\n",
    "   - **Təsvir:** Modeldə istifadə olunan tree-lərin sayı.\n",
    "   - **Qeyd:** Daha çox tree əlavə edildikdə model daha yaxşı öyrənə bilər, amma daha uzun təlim vaxtı və overfitting riski ilə birlikdə gəlir.\n",
    "\n",
    "7. **`random_state`**\n",
    "   - **Təsvir:** Modelin təsadüfi işlərini təyin edən ədədi dəyər.\n",
    "   - **Qeyd:** Təyin edilmiş dəyər modelin hər dəfə eyni nəticələr verməsini təmin edir, bu da nəticələrin təkrarolunmasını asanlaşdırır.\n",
    "\n",
    "### Parametrlərin Tətbiqi\n",
    "\n",
    "Yuxarıdakı parametr dəyərləri ilə XGBoost modeli qurarkən:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbt = XGBClassifier(\n",
    "    max_depth=2,                 # Tree dərinliyi\n",
    "    learning_rate=0.2,           # Təsir dərəcəsi\n",
    "    objective=\"multi:softmax\",   # Çox sinifli təsnifat\n",
    "    num_class=2,                 # İki sinif\n",
    "    booster=\"gbtree\",            # Tree-based boosting\n",
    "    n_estimators=10,             # 10 tree\n",
    "    random_state=123             # Təkrar nəticələr üçün təsadüfi dəyər\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri öz modelinizdə tənzimləməklə, performansını artırmaq və modelin tələblərinə uyğunlaşdırmaq mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e05a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173a3fd2",
   "metadata": {},
   "source": [
    "## Regression ucun en yaxsi model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff926d",
   "metadata": {},
   "source": [
    "Regressiya problemləri üçün ən yaxşı model müxtəlif amillərə, o cümlədən verilənlərin təbiətinə, modelin interpretasiya qabiliyyətinə və performansına bağlı olaraq dəyişir. Aşağıda ən çox istifadə olunan və yüksək performans verən regressiya modellərinin qısa izahı verilmişdir:\n",
    "\n",
    "### 1. **Linear Regression**\n",
    "   - **İstifadə Sahəsi:** Sadə, xətti əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Asan tətbiq edilir və interpretasiya olunur.\n",
    "   - **Məhdudiyyətləri:** Non-linear (qeyri-xətti) əlaqələri yaxşı tuta bilmir; outliers-ə həssasdır.\n",
    "\n",
    "### 2. **Ridge və Lasso Regression**\n",
    "   - **İstifadə Sahəsi:** Yüksək ölçülü verilənlər və ya multicollinearity olan problemlər.\n",
    "   - **Üstünlükləri:** Regularization (cərimələndirmə) istifadə edərək modelin sadələşdirilməsi və overfitting-in azaldılması.\n",
    "   - **Məhdudiyyətləri:** Non-linear əlaqələr üçün optimal deyil.\n",
    "\n",
    "### 3. **Polynomial Regression**\n",
    "   - **İstifadə Sahəsi:** Non-linear əlaqələr.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Model çox kompleks ola bilər və overfitting riski yüksəkdir.\n",
    "\n",
    "### 4. **Decision Tree Regressor**\n",
    "   - **İstifadə Sahəsi:** Həm xətti, həm də xətti olmayan əlaqələr üçün istifadə olunur.\n",
    "   - **Üstünlükləri:** Asan interpretasiya olunur və outliers-lərə daha az həssasdır.\n",
    "   - **Məhdudiyyətləri:** Overfitting riski yüksəkdir, xüsusilə ağac çox dərin olduqda.\n",
    "\n",
    "### 5. **Random Forest Regressor**\n",
    "   - **İstifadə Sahəsi:** Decision Tree-dən daha stabil və dəqiq proqnozlar tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Overfitting riski azdır; yaxşı ümumi performans verir.\n",
    "   - **Məhdudiyyətləri:** Ağacların sayı çox olduqda daha çox hesablama resursu tələb edir.\n",
    "\n",
    "### 6. **Gradient Boosting Regressor**\n",
    "   - **İstifadə Sahəsi:** Yüksək dəqiqlik tələb edən non-linear əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Yüksək performans; yaxşı ümumi dəqiqlik.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və parametr tuning mürəkkəbdir.\n",
    "\n",
    "### 7. **XGBoost, LightGBM, və CatBoost Regressor**\n",
    "   - **İstifadə Sahəsi:** Böyük verilənlər, kompleks əlaqələr və yüksək dəqiqlik tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Yüksək sürət və performans; müxtəlif optimallaşdırmalar və avtomatik kateqorik xüsusiyyətlərin emalı.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning mürəkkəb ola bilər.\n",
    "\n",
    "### 8. **Support Vector Regression (SVR)**\n",
    "   - **İstifadə Sahəsi:** Xətti olmayan və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri yaxşı tuta bilir və margin-based regressiya təmin edir.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning çətin ola bilər və böyük verilənlər dəstləri üçün yavaş ola bilər.\n",
    "\n",
    "### 9. **Neural Networks (MLP Regressor)**\n",
    "   - **İstifadə Sahəsi:** Çox böyük və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan kompleks əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və interpretasiya etmək çətindir.\n",
    "\n",
    "### Hansı Modeli Seçmək?\n",
    "- **Sadə, xətti əlaqələr:** Linear Regression, Ridge və ya Lasso Regression.\n",
    "- **Xətti olmayan əlaqələr:** Polynomial Regression, Decision Tree Regressor, SVR.\n",
    "- **Yüksək performans və kompleks modellər:** Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "- **Böyük və çox ölçülü verilənlər:** LightGBM, CatBoost, Neural Networks.\n",
    "\n",
    "Verilənlərinizin xüsusiyyətlərinə uyğun olaraq modelləri sınamaq və onların performansını müqayisə etmək daha yaxşı nəticələr əldə etməyə kömək edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e6dea1",
   "metadata": {},
   "source": [
    "## Polynomial Regression model qur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26e2bc",
   "metadata": {},
   "source": [
    "Aşağıda Polynomial Regression modelini qurmaq üçün tam kod nümunəsi verilmişdir:\n",
    "\n",
    "### Adım 1: Lazımi kitabxanaların yüklənməsi\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "\n",
    "### Adım 2: Verilənlərin yüklənməsi və hazırlanması\n",
    "```python\n",
    "# Verilənləri yükləyin\n",
    "credit = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week6\\day1\\CreditCardDefault.csv\")\n",
    "credit.drop([\"ID\"], axis=1, inplace=True)\n",
    "\n",
    "# X və y dəyişənlərini təyin edin\n",
    "X = credit.iloc[:, 0:23]\n",
    "y = credit.iloc[:, -1]\n",
    "\n",
    "# Verilənləri təlim və test dəstlərinə bölün\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### Adım 3: Polynomial Features yaratmaq\n",
    "```python\n",
    "# Polynomial features yaratmaq üçün PolynomialFeatures sinifini çağırın\n",
    "degree = 2  # Polynomial dərəcəsi\n",
    "poly = PolynomialFeatures(degree)\n",
    "\n",
    "# Təlim və test dəstləri üçün polynomial xüsusiyyətlər yaradın\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "```\n",
    "\n",
    "### Adım 4: Linear Regression modeli ilə polynomial regression tətbiqi\n",
    "```python\n",
    "# Linear Regression modelini qurun\n",
    "model = LinearRegression()\n",
    "\n",
    "# Polynomial verilənlər üzərində modeli təlim edin\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Test dəsti üzərində proqnozlar edin\n",
    "y_pred = model.predict(X_test_poly)\n",
    "```\n",
    "\n",
    "### Adım 5: Model performansının qiymətləndirilməsi\n",
    "```python\n",
    "# MSE və R2 dəyərlərini hesablayın\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "```\n",
    "\n",
    "### İcra\n",
    "Bu kod parçalarını addım-addım icra etdikdən sonra Polynomial Regression modelini qurmuş olacaqsınız. Kodun sonunda modelin performansını `Mean Squared Error (MSE)` və `R-squared (R2)` dəyərləri ilə ölçə biləcəksiniz.\n",
    "\n",
    "**Qeyd:** `degree` parametrini dəyişdirərək polynomial modelin dərəcəsini tənzimləyə bilərsiniz. Çox yüksək dərəcələr overfitting-ə səbəb ola bilər, buna görə dərəcəni diqqətlə seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a4509e",
   "metadata": {},
   "source": [
    "## Modellerde threshold-u nece deyisim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2571ed",
   "metadata": {},
   "source": [
    "Threshold (məhdud) dəyərini dəyişdirmək, xüsusilə təsnifat modelləri üçün vacibdir, çünki default olaraq modellər çox vaxt 0.5 threshold ilə işləyir. Threshold dəyərini dəyişdirməklə, modelin proqnoz nəticələrinin \"positive\" və \"negative\" olaraq təsnif edilmə meyarını dəyişdirə bilərsiniz.\n",
    "\n",
    "### Adımlar\n",
    "\n",
    "1. **Model proqnozlarını ehtimallar kimi əldə edin.**\n",
    "   - Təsnifat modellərində `.predict_proba()` metodu istifadə edərək ehtimal dəyərlərini ala bilərsiniz.\n",
    "\n",
    "2. **Threshold-a əsasən proqnozları təsnif edin.**\n",
    "   - İstədiyiniz threshold dəyərinə əsasən ehtimalları `positive` və ya `negative` olaraq təsnif edəcəksiniz.\n",
    "\n",
    "### Kod nümunəsi\n",
    "\n",
    "Məsələn, bir təsnifat modeli üçün threshold-u necə dəyişdirəcəyinizi göstərən bir nümunə:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Model proqnozları ehtimal kimi alındı\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Positive class üçün ehtimal\n",
    "\n",
    "# Threshold dəyərini təyin edin\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "# Threshold-a əsasən proqnozları təsnif edin\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### İzahat:\n",
    "- **`predict_proba`:** Bu metod modelin hər bir sinif üçün ehtimallarını qaytarır. `[:, 1]` pozitiv sinifin ehtimallarını seçir.\n",
    "- **`threshold = 0.3`:** İstədiyiniz threshold dəyərini təyin edirsiniz. Burada 0.3 olaraq seçilib.\n",
    "- **`np.where(y_probs > threshold, 1, 0)`:** Ehtimalları seçilən threshold dəyərinə əsasən 1 (positive) və ya 0 (negative) olaraq təsnif edir.\n",
    "\n",
    "Bu yanaşma, threshold dəyərini dəyişdirərək modelin performansını müxtəlif threshold dəyərləri üçün optimallaşdırmağa və balanslaşdırmağa imkan verir. Bu, xüsusilə sinif disbalansı olan verilənlər dəstlərində faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e4b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb3e2f3",
   "metadata": {},
   "source": [
    "## Butun modellerde threshold-u eyni cur deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6350af",
   "metadata": {},
   "source": [
    "Bəli, threshold dəyərini dəyişdirmə üsulu demək olar ki, bütün təsnifat modellərində tətbiq edilə bilər. Lakin bu proses yalnız ehtimal əsaslı təsnifat modelləri üçün keçərlidir. Aşağıda bəzi ümumi modellər üçün bu yanaşmanı necə tətbiq etmək barədə izahatlar verilmişdir:\n",
    "\n",
    "### 1. **Logistic Regression, Random Forest, Gradient Boosting və s.**  \n",
    "Bu modellər ehtimal proqnozları qaytarır və threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 2. **Support Vector Machine (SVM)**\n",
    "SVM modelləri default olaraq ehtimal vermir, lakin `probability=True` olaraq modeli qurmaqla ehtimal əsaslı proqnozlar əldə edə bilərsiniz. Sonra threshold-u dəyişdirmək mümkündür.\n",
    "\n",
    "### 3. **Neural Networks (MLPClassifier)**\n",
    "Sinif ehtimalları `.predict_proba()` metodu ilə əldə edilir və threshold-a əsasən dəyişdirilə bilər.\n",
    "\n",
    "### 4. **Naive Bayes**\n",
    "Naive Bayes modelləri də ehtimallar qaytarır, buna görə threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 5. **K-Nearest Neighbors (KNN)**\n",
    "KNN də `.predict_proba()` metodu ilə ehtimallar qaytarır və bu ehtimallara əsasən threshold dəyişdirilə bilər.\n",
    "\n",
    "### Tətbiq etmək üçün Ümumi Yanaşma:\n",
    "```python\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Pozitiv sinif ehtimalları\n",
    "\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### Qeyd:\n",
    "- **Decision Tree** kimi bəzi modellərdə ehtimal dəyərləri, leaf düyünlərinin təxminləri ilə hesablanır və `.predict_proba()` metodu vasitəsilə əldə edilə bilər.\n",
    "- **Threshold dəyərini dəyişdirmək**, modelin təsnifat balansını dəyişdirməyə imkan verir, bu da sinif disbalansı olan verilənlərdə xüsusilə faydalıdır.\n",
    "\n",
    "Beləliklə, ehtimal əsaslı təsnifat modellərində threshold dəyərini dəyişdirmək mümkün və çox vaxt faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a600c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6ead7e",
   "metadata": {},
   "source": [
    "## Boosting modellerinde, meselen XGBoost goturek. Bu modeli tetbiq ederken ozu arxa plnada hansi modeller qurur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab38e",
   "metadata": {},
   "source": [
    "Boosting modellərində, məsələn, XGBoost istifadə edərkən, modelin arxa planda qurduğu zəif modelləri (base learners) başa düşmək vacibdir. Boosting metodları ardıcıl olaraq zəif modelləri birləşdirərək daha güclü bir model qurur. XGBoost, Gradient Boosting metodunun optimallaşdırılmış bir versiyasıdır və bu arxa planda aşağıdakı addımları həyata keçirir:\n",
    "\n",
    "### XGBoost-un Arxa Planında Qurulan Modellər\n",
    "\n",
    "1. **Zəif Modellərin (Base Learners) Qurulması:**\n",
    "   - **Decision Trees:** XGBoost, ən çox Decision Tree-lərdən istifadə edir. Bu ağaclar, çox vaxt \"stumps\" olaraq da adlandırılan çox sadə qərar ağaclarıdır. XGBoost-da bu ağaclar çox vaxt qısa və çox dərin olmur; bu, hər bir ağacın çox sadə olduğunu bildirir.\n",
    "\n",
    "2. **Ardıcıl Tətbiq:**\n",
    "   - **Gradient Descent:** XGBoost, hər yeni ağacı əvvəlki modellərin səhvlərini düzəltmək üçün qurur. Bu, gradient descent optimallaşdırma metodu vasitəsilə həyata keçirilir. Hər bir ağacın yaradılması, əvvəlki ağacların səhvlərinə (residuals) əsaslanır.\n",
    "\n",
    "3. **Öyrənmə Sürəti (Learning Rate):**\n",
    "   - **Learning Rate:** Hər yeni ağacın təsirini tənzimləyən bir öyrənmə sürəti (learning rate) istifadə edilir. Bu, hər bir ağacın proqnozlarına əlavə ediləcək ağırlığı müəyyən edir. Daha kiçik öyrənmə sürəti ilə daha çox ağac tələb olunur.\n",
    "\n",
    "4. **Regularization:**\n",
    "   - **Regularization:** XGBoost-da modelin overfitting riskini azaltmaq üçün cərimələndirmə (regularization) tətbiq edilir. Bu, ağacların böyüməsini tənzimləmək və modelin kompleksliyini azaltmaq məqsədini güdür. XGBoost L1 (Lasso) və L2 (Ridge) cərimələndirmə üsullarını dəstəkləyir.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **Early Stopping:** XGBoost, modelin performansını qiymətləndirmək üçün validasiya dəstini istifadə edir və performansın yaxşılaşmadığı yerə qədər ağacları qurmağa davam edir. Bu, overfitting-i azaltmağa kömək edir.\n",
    "\n",
    "### XGBoost-un İş Prinsipi:\n",
    "\n",
    "1. **Başlanğıc Model:** Əvvəlcə bütün verilənlər üçün proqnozları təsadüfi bir başlanğıc dəyəri ilə başlatır.\n",
    "2. **Residual Hesablanması:** Təxmin edilmiş dəyərlərlə real dəyərlər arasındakı fərqi hesablamaqla qalıqları (residuals) tapır.\n",
    "3. **Yeni Ağacın Qurulması:** Qalıqları (residuals) düzəltmək üçün yeni bir Decision Tree qurur. Bu ağac qalıqların düzəldilməsinə kömək edir.\n",
    "4. **Proqnozlara Əlavə:** Yeni ağacın təxminləri əvvəlki proqnozlara əlavə edilir.\n",
    "5. **Yenidən Hesablamalar:** Bu proses təkrar olunur, hər dəfə yeni ağac əvvəlki ağacların səhvlərini düzəldir.\n",
    "\n",
    "Bu üsulla, XGBoost bir neçə zəif modelin (Decision Trees) ardıcıl olaraq birləşdirilməsi ilə güclü və dəqiq bir model qurur. Bu yanaşma, modelin öyrənmə qabiliyyətini artırır və daha yaxşı nəticələr əldə etməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdcfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a3e09c",
   "metadata": {},
   "source": [
    "## Boosting modelleri arxa planda hansi modelleri qurur. Bir bir hamisi haqqinda yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c83a6",
   "metadata": {},
   "source": [
    "Boosting modelləri arxa planda zəif modellər (base learners) qurur. Ən çox istifadə olunan boosting modelləri bunlardır: AdaBoost, Gradient Boosting, XGBoost, LightGBM, və CatBoost. Aşağıda hər birinin arxa planda qurduğu modellərin xüsusiyyətlərini izah edirəm:\n",
    "\n",
    "### 1. **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** Ən çox **Decision Trees** (adətən \"stumps\" adlanan çox sadə ağaclar) istifadə edilir. Hər bir ağac, əvvəlki ağacların səhvlərinə daha çox diqqət yetirir.\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni ağacın əvvəlki ağacların səhvlərini düzəltməyə çalışır. Səhv təsnif edilən nümunələrə daha çox ağırlıq verilir, beləliklə, yeni ağaclar bu nümunələrə daha çox diqqət yetirir.\n",
    "\n",
    "### 2. **Gradient Boosting**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** istifadə edilir. Bu ağaclar adətən daha dərin və daha kompleks ola bilər, lakin hələ də çox sadə modellərdir.\n",
    "- **İş Prinsipi:** Hər yeni ağac, əvvəlki ağacların qalıqları üzərində işləyərək səhvləri düzəltməyə çalışır. Hər ağac gradient descent metodunu istifadə edərək əvvəlki ağacların səhvlərini azaldır.\n",
    "\n",
    "### 3. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha kiçik və daha sadə).\n",
    "- **İş Prinsipi:** XGBoost, Gradient Boosting metodunun optimallaşdırılmış versiyasıdır. Bu metod eyni zamanda L1 (Lasso) və L2 (Ridge) cərimələndirmə tətbiq edir, və ağacların böyüməsini tənzimləyərək overfitting-i azaldır. Hər ağac əvvəlki ağacların səhvlərinə əsaslanaraq yaradılır və öyrənmə sürəti (learning rate) ilə tənzimlənir.\n",
    "\n",
    "### 4. **LightGBM (Light Gradient Boosting Machine)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha yüngül və daha sürətli öyrənən ağaclar).\n",
    "- **İş Prinsipi:** LightGBM, Gradient Boosting-in daha sürətli və daha az resurs tələb edən variantıdır. Ağacları \"leaf-wise\" bölmələrdən istifadə edərək qurur, bu da daha dərin və daha təkmil ağaclara gətirib çıxarır. Bu yanaşma, daha sürətli və daha yaxşı performans verir, xüsusilə böyük verilənlər dəstələrində.\n",
    "\n",
    "### 5. **CatBoost (Categorical Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (daha optimallaşdırılmış və kateqorik dəyişənləri avtomatik emal edən ağaclar).\n",
    "- **İş Prinsipi:** CatBoost, Gradient Boosting-in xüsusilə kateqorik verilənlərlə işləməyə optimallaşdırılmış versiyasıdır. Bu model kateqorik dəyişənlərin avtomatik kodlaşdırılmasını təmin edir və şumun qarşısını almaq üçün müxtəlif texnikalardan istifadə edir. Ağaclar adətən daha qısa və effektiv olur, modelin öyrənmə sürəti yüksəkdir.\n",
    "\n",
    "### Ümumi Yanaşma:\n",
    "Bütün bu boosting modelləri, zəif modelləri (adətən Decision Trees) bir-birinin ardınca quraraq, əvvəlki modellərin səhvlərini düzəltməyə çalışır. Hər yeni model əvvəlki modellərin qalıqları üzərində işləyir və modelin ümumi performansını artırır.\n",
    "\n",
    "Bu yanaşmaların hər biri, müxtəlif optimallaşdırma və cərimələndirmə üsulları ilə modelin performansını artırmağa və overfitting-i azaltmağa çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5b5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24406bd5",
   "metadata": {},
   "source": [
    "## Bagging-de eyni model qurulur? Boosting-de eyni model qurulur yoxsa bir nece model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a180d",
   "metadata": {},
   "source": [
    "Bagging və Boosting metodları arasında əsas fərq ondan ibarətdir ki, hər biri modelləri necə qurur və birləşdirir.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** metodunda bir neçə eyni model (həmin növ model) qurulur, lakin bu modellər fərqli verilənlər dəstləri üzərində öyrədilir. \n",
    "\n",
    "- **Eyni Model:** Bəli, **bagging** metodunda eyni növ modeldən bir neçə dəfə istifadə edilir. Məsələn, əgər bagging metodunda Decision Tree istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Verilənlər Dəstləri:** Bagging, orijinal verilənlər dəstinin təkrar nümunələri ilə yaradılan fərqli verilənlər dəstlərini (bootstrap samples) istifadə edərək hər bir modeli öyrədir.\n",
    "- **Final Nəticə:** Hər bir modelin proqnozları birləşdirilir. Təsnifat üçün bu, adətən səsvermə (majority voting) üsulu ilə, regresiyada isə ortalama (averaging) ilə həyata keçirilir.\n",
    "\n",
    "**Nəticə:** Bagging metodunda bir neçə eyni model yaradılır, lakin bu modellər fərqli verilənlər üzərində öyrədilir və final nəticə bütün modellərin nəticələrinin birləşdirilməsi ilə alınır.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Boosting** metodunda isə modellər ardıcıl olaraq qurulur, və hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu prosesdə modellərin hər biri eyni növ olsa da, onların hər biri müxtəlif verilənlər üzərində və fərqli səhvlər üzərində öyrədilir.\n",
    "\n",
    "- **Eyni Model:** Bəli, **boosting** metodunda da eyni növ model istifadə edilir. Məsələn, Gradient Boosting-də Decision Tree-lər istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Modellərin Qurulması:** Lakin burada fərq ondan ibarətdir ki, hər bir yeni model əvvəlki modellərin səhvlərini düzəltmək üçün qurulur, yəni hər model fərqli şəkildə optimallaşdırılmış bir variant kimi fəaliyyət göstərir.\n",
    "- **Final Nəticə:** Hər bir modelin nəticələri birləşdirilir, lakin bu birləşmə hər bir modelin səhvlərini düzəltmək üçün fərqli çəkilərdən istifadə edir.\n",
    "\n",
    "**Nəticə:** Boosting metodunda eyni növ model bir neçə dəfə qurulur, lakin bu modellər bir-birindən fərqlidir, çünki hər biri əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu modellər ardıcıl olaraq qurulur və nəticələr birləşdirilərək daha güclü bir model yaradılır.\n",
    "\n",
    "### Ümumi Qısa Xülasə:\n",
    "\n",
    "- **Bagging:** Eyni növ model, fərqli verilənlər dəstləri üzərində paralel olaraq öyrədilir və nəticələr birləşdirilir.\n",
    "- **Boosting:** Eyni növ model, ardıcıl olaraq qurulur və hər bir yeni model əvvəlki modellərin səhvlərini düzəldir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365bb8e2",
   "metadata": {},
   "source": [
    "## Modelin overfitting edib etmediyini nece bilim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0322b2",
   "metadata": {},
   "source": [
    "Modelin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə üsul və göstəricidən istifadə etmək mümkündür. Overfitting, modelin təlim məlumatlarına həddindən artıq uyğunlaşdığı, lakin test və ya yeni məlumatlarla yaxşı performans göstərmədiyi bir vəziyyətdir. Overfitting olub-olmadığını müəyyən etmək üçün aşağıdakı yanaşmaları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Train-Test Split və Performans Müqayisəsi**\n",
    "   - **Train Score:** Modelin təlim (train) məlumatlarındakı performansını ölçün. Yüksək dəqiqlik (accuracy) və ya digər performans metrikləri (məsələn, R² score) modelin təlim məlumatlarına çox yaxşı uyğunlaşdığını göstərə bilər.\n",
    "   - **Test Score:** Modelin test (validation) məlumatlarındakı performansını ölçün. Əgər test dəstindəki performans əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   train_score = model.score(X_train, y_train)\n",
    "   test_score = model.score(X_test, y_test)\n",
    "   \n",
    "   print(\"Train Score:\", train_score)\n",
    "   print(\"Test Score:\", test_score)\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train score çox yüksək, test score isə əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsidir.\n",
    "\n",
    "### 2. **Learning Curve**\n",
    "   - **Learning Curve:** Train və test məlumatları üzərində modelin performansını müxtəlif təlim məlumatları ilə vizuallaşdıran bir əyri çəkin. Əgər train əyrisi yuxarıda qalır və test əyrisi daha aşağıda qalırsa, bu da overfitting göstəricisidir.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import learning_curve\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "   plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "   plt.xlabel(\"Training examples\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Əgər train əyrisi yüksək performans göstərir, lakin test əyrisi daha aşağı qalırsa və aradakı fərq böyükdürsə, bu da overfitting-i göstərir.\n",
    "\n",
    "### 3. **Cross-Validation**\n",
    "   - **Cross-Validation:** Verilənlər dəstini bir neçə hissəyə bölərək modelin performansını müxtəlif hissələr üzərində yoxlayın. Əgər modelin performansı müxtəlif hissələr arasında çox dəyişirsə, bu da overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "   print(\"Cross-Validation Scores:\", scores)\n",
    "   print(\"Mean Score:\", np.mean(scores))\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Cross-validation nəticələri arasında böyük fərqlər varsa, bu, modelin müəyyən verilənlər dəstlərinə həddən artıq uyğunlaşdığını göstərə bilər.\n",
    "\n",
    "### 4. **Validation Curve**\n",
    "   - **Validation Curve:** Modelin müəyyən parametrləri üzrə performansını yoxlamaq üçün validation curve çəkə bilərsiniz. Əgər train performansı yüksəkdir, lakin validation performansı aşağıdırsa, overfitting ehtimalı var.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import validation_curve\n",
    "\n",
    "   param_range = np.logspace(-6, -1, 5)\n",
    "   train_scores, test_scores = validation_curve(model, X, y, param_name=\"alpha\", param_range=param_range, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "   plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "   plt.xlabel(\"Parameter\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train əyrisi yüksək, validation əyrisi isə daha aşağı qalırsa, overfitting ehtimalı var.\n",
    "\n",
    "### 5. **Regularization Yoxlamaq**\n",
    "   - **Regularization:** Regularization (L1, L2 cərimələndirmə) metodlarından istifadə etməklə modelin overfitting edilib-etmədiyini yoxlaya bilərsiniz. Regularization overfitting-in qarşısını almağa kömək edir.\n",
    "\n",
    "   - **Overfitting əlaməti:** Regularization tətbiq etdikdə modelin performansı daha sabit və balanslı olursa, bu, overfitting-in qarşısını almağa kömək edə bilər.\n",
    "\n",
    "### 6. **Modelin Kompleksliyi**\n",
    "   - **Modelin Kompleksliyi:** Çox kompleks modellər (çox dərin decision tree-lər, çox sayda hidden layer-ləri olan neyron şəbəkələr və s.) asanlıqla overfitting edə bilər. Sadə modellər istifadə edərək performansı müqayisə etmək də overfitting-i müəyyən etməyə kömək edə bilər.\n",
    "\n",
    "   - **Overfitting əlaməti:** Çox kompleks model çox yüksək train performansı və aşağı test performansı göstərə bilər.\n",
    "\n",
    "Bu metodlar və yanaşmalarla modelin overfitting edib-etmədiyini müəyyən edə bilərsiniz və ehtiyac olduqda daha yaxşı performans əldə etmək üçün modelin quruluşunu və parametrlərini tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c0dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a3f95",
   "metadata": {},
   "source": [
    "## Bu overfitting ucun kifayetdir?\n",
    "* Cross-Validation Scores: [0.81, 0.76, 0.82, 0.77, 0.79]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33d3bb",
   "metadata": {},
   "source": [
    "Verilən cross-validation nəticələrinə əsasən, modellərinizin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə göstəriciyə diqqət yetirmək lazımdır:\n",
    "\n",
    "1. **Dəyişkənlik (Variance):** Cross-validation skorları arasında çox böyük bir fərq yoxdursa, bu yaxşı bir göstəricidir. Verilən skorlar 0.76 ilə 0.82 arasında dəyişir, bu isə bir qədər dəyişkənlik göstərsə də, ciddi bir overfitting işarəsi deyil. \n",
    "\n",
    "2. **Test və Təlim Dəqiqlikləri (Train vs. Test Accuracy):** Əgər təlim dəstindəki dəqiqlik çox yüksək, amma test dəstindəki dəqiqlik nisbətən aşağıdırsa, bu overfitting işarəsi ola bilər. Siz bu məlumatı paylaşmamısınız, amma əgər test və train dəqiqlikləri arasında böyük bir fərq varsa, overfitting ehtimalı daha yüksəkdir.\n",
    "\n",
    "3. **Kompromis:** Skorların orta səviyyədə olması (məsələn, 0.79) modeli qismən balanslaşdırılmış göstərə bilər. Amma, modellərinizi daha da optimallaşdırmaq və overfitting-in qarşısını almaq üçün modeli sadələşdirmək və ya daha çox məlumat toplamaq barədə düşünə bilərsiniz.\n",
    "\n",
    "**Nəticə:** Bu skorlar çox böyük bir dəyişkənlik göstərmədiyi üçün ciddi bir overfitting işarəsi deyildir. Lakin, modelin təlim və test dəstində necə performans göstərdiyini daha dərindən analiz etmək daha dəqiq bir qərar verməyə kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a92e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b126fc89",
   "metadata": {},
   "source": [
    "### train_score = model.score(X_train, y_train)\n",
    "### test_score = model.score(X_test, y_test)\n",
    "## Bunlar accuracy dir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66317e3",
   "metadata": {},
   "source": [
    "Bəli, `model.score(X_train, y_train)` və `model.score(X_test, y_test)` metodları adətən **accuracy** dəyərini qaytarır, amma bu, yalnız təsnifat (classification) modelləri üçün keçərlidir. \n",
    "\n",
    "### Təfsilatlı izah:\n",
    "- **Classification Modelləri üçün:** \n",
    "  - `model.score(X_train, y_train)` və `model.score(X_test, y_test)` təlim və test dəstlərində modelin təsnifat dəqiqliyini (accuracy) qaytarır. Bu, düzgün proqnozların ümumi nümunələrə nisbətidir.\n",
    "\n",
    "  ```python\n",
    "  accuracy = correct_predictions / total_predictions\n",
    "  ```\n",
    "\n",
    "- **Regression Modelləri üçün:** \n",
    "  - Əgər model regresiya modelidirsə (məsələn, Linear Regression, Ridge, Lasso), `model.score()` metodunun qaytardığı dəyər **R² score** olacaq. R² score, modelin verilənlər dəstinə nə qədər yaxşı uyğun gəldiyini ölçür və proqnozların faktiki dəyərlərə nə qədər yaxın olduğunu göstərir.\n",
    "\n",
    "  ```python\n",
    "  R² = 1 - (SS_res / SS_tot)\n",
    "  ```\n",
    "  Burada:\n",
    "  - `SS_res`: Proqnozlaşdırılmış dəyərlərin və faktiki dəyərlərin kvadrat fərqlərinin cəmi.\n",
    "  - `SS_tot`: Faktiki dəyərlərin və onların ortalama dəyərinin kvadrat fərqlərinin cəmi.\n",
    "\n",
    "### Nəticə:\n",
    "Əgər təsnifat modeli istifadə edirsinizsə, `model.score()` metodu **accuracy** dəyərini, əgər regresiya modeli istifadə edirsinizsə, **R² score** qaytaracaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8df425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b7828c",
   "metadata": {},
   "source": [
    "## Bank datalari ucun ve elektronika satan sirket datalari ucun en cox hansi modeller istifade olunur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ba52d",
   "metadata": {},
   "source": [
    "Bank dataları və elektronika satan şirkətlərin məlumatları üçün ən uyğun modellər seçilərkən, bu dataların xarakteristikası və məqsəd nəzərə alınır. Hər bir sektorun özünəməxsus xüsusiyyətləri var, buna görə də istifadə olunan modellər fərqlənə bilər.\n",
    "\n",
    "### Bank Dataları Üçün\n",
    "Bank sektoru əsasən müştərilərin maliyyə məlumatları, kredit riski, fırıldaqçılıq aşkar edilməsi, müştəri seqmentasiyası kimi mövzulara yönəlir. Bu səbəbdən bank məlumatları üçün geniş istifadə olunan modellər:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - Əsasən kredit riski, müştəri davranışı və fırıldaqçılıq aşkar etmək üçün istifadə olunur.\n",
    "   - Məqsəd: İkili təsnifat (binary classification) məsələləri.\n",
    "\n",
    "2. **Random Forest:**\n",
    "   - Fərqli atributlar üzərində güclü performans göstərir və feature importance üçün istifadə edilir.\n",
    "   - Məqsəd: Təsnifat və regresiya məsələləri.\n",
    "\n",
    "3. **Gradient Boosting (XGBoost, LightGBM, CatBoost):**\n",
    "   - Bank dataları üçün xüsusilə effektivdir, çünki bu metodlar mürəkkəb əlaqələri və nümunələri öyrənə bilir.\n",
    "   - Məqsəd: Kredit skoring, risk analizi, müştəri churn təxminatı.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - Daha az məlumat üçün güclü performans göstərir, lakin böyük datalar üçün hesablama baxımından bahalı ola bilər.\n",
    "   - Məqsəd: Kredit riski və fırıldaqçılıq aşkar edilməsi.\n",
    "\n",
    "5. **K-Means Clustering:**\n",
    "   - Müştəri seqmentasiyası üçün istifadə olunur.\n",
    "   - Məqsəd: Müştəri qruplarını tapmaq və onlara fərqli maliyyə məhsulları təklif etmək.\n",
    "\n",
    "### Elektronika Satan Şirkətlərin Dataları Üçün\n",
    "Elektronika satan şirkətlər əsasən satış təxminatı, müştəri davranışı, stok idarəsi və qiymət optimallaşdırması üzərində işləyirlər. Bu sahə üçün istifadə olunan əsas modellər:\n",
    "\n",
    "1. **Linear Regression:**\n",
    "   - Satış proqnozları və qiymət optimallaşdırılması üçün istifadə olunur.\n",
    "   - Məqsəd: Kəmiyyət (quantitative) proqnozlar.\n",
    "\n",
    "2. **Time Series Analysis (ARIMA, SARIMA):**\n",
    "   - Satış və tələbat proqnozları üçün zaman seriyası analizləri geniş istifadə olunur.\n",
    "   - Məqsəd: Gələcək satışların və tələbin proqnozlaşdırılması.\n",
    "\n",
    "3. **Random Forest və Gradient Boosting:**\n",
    "   - Elektronika məhsullarının satışlarına təsir edən müxtəlif faktorları təhlil etmək üçün istifadə olunur.\n",
    "   - Məqsəd: Təsnifat və proqnozlaşdırma.\n",
    "\n",
    "4. **Collaborative Filtering (Recommendation Systems):**\n",
    "   - Məhsul təklifləri və müştəriyə fərdi təkliflərin hazırlanması üçün geniş istifadə olunur.\n",
    "   - Məqsəd: Müştəriyə uyğun məhsul təklifləri vermək.\n",
    "\n",
    "5. **Clustering (K-Means, DBSCAN):**\n",
    "   - Müştəri seqmentasiyası və marketinq strategiyalarının tərtibi üçün.\n",
    "   - Məqsəd: Müştəri qruplarının aşkar edilməsi və hədəfli reklam strategiyalarının inkişafı.\n",
    "\n",
    "### Nəticə\n",
    "Bank məlumatları üçün daha çox təsnifat modelləri (logistic regression, random forest, gradient boosting) istifadə olunur. Elektronika satan şirkətlər isə satış proqnozları və tələbat analizi üçün zaman seriyaları modelləri və regresiya modellərinə üstünlük verirlər. Hər iki sahədə də clustering müştəri seqmentasiyası üçün geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46597909",
   "metadata": {},
   "source": [
    "## CatBoost-da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba7ccd",
   "metadata": {},
   "source": [
    "CatBoost modeli daxilində işləyən spesifik modelləri dəyişdirmək mümkün deyil, çünki CatBoost özü qərar ağacları (decision trees) əsasında işləyən bir **Gradient Boosting** metodudur və daxili arxitekturası istifadəçilər üçün dəyişdirilə bilən deyil. CatBoost, xüsusilə kateqorik verilənlərlə effektiv işləmək üçün optimallaşdırılmışdır və daxili olaraq xüsusi bir **symmetrical tree** strukturu istifadə edir.\n",
    "\n",
    "Bununla belə, siz CatBoost modelinin performansını və davranışını təsir edən müxtəlif hiperparametrləri tənzimləməklə modelin necə işlədiyini dəyişə bilərsiniz. Məsələn:\n",
    "\n",
    "### 1. **`depth` (ağacın dərinliyi):**\n",
    "   - Modelin hər bir ağacının maksimum dərinliyini təyin edir. Daha böyük dərinlik daha kompleks modellər və overfitting riskini artırır.\n",
    "\n",
    "### 2. **`learning_rate`:**\n",
    "   - Modelin hər bir iterasiyada addım böyüklüyünü təyin edir. Daha aşağı qiymətlər daha sabit, amma daha yavaş öyrənməyə səbəb olur.\n",
    "\n",
    "### 3. **`iterations`:**\n",
    "   - Ümumi ağac sayını müəyyən edir. Daha çox iterasiya adətən daha yüksək dəqiqlik, amma overfitting riski ilə nəticələnə bilər.\n",
    "\n",
    "### 4. **`l2_leaf_reg`:**\n",
    "   - Ağacların budaqlanmasının qarşısını almaq üçün cərimələndirmə əmsalıdır (regularization term). Daha böyük dəyər daha az kompleks modellərə və overfitting riskinin azalmasına səbəb ola bilər.\n",
    "\n",
    "### 5. **`one_hot_max_size`:**\n",
    "   - Kateqorik dəyişənlər üçün one-hot encoding tətbiq ediləcək maksimum unikal dəyər sayını təyin edir. Bu, xüsusilə çox unikal kateqorik dəyərə malik atributlar üçün faydalıdır.\n",
    "\n",
    "### 6. **`boosting_type`:**\n",
    "   - CatBoost modeli əsasən **Ordered Boosting** və **Plain Boosting** kimi iki növ boosting metodu təklif edir:\n",
    "     - **Ordered Boosting**: Overfitting-in qarşısını almaq üçün daha yaxşıdır.\n",
    "     - **Plain Boosting**: Daha sürətli öyrənməyə səbəb ola bilər, amma overfitting riskini artıra bilər.\n",
    "\n",
    "Bu hiperparametrləri dəyişərək CatBoost modelinin performansını və davranışını təyin edə bilərsiniz, amma daxildəki modeli fərqli bir model ilə əvəz etmək mümkün deyil, çünki CatBoost özü bir boosting alqoritmi kimi çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69438d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d07da7",
   "metadata": {},
   "source": [
    "## XGBoost da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e74fbc",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) modelində də spesifik olaraq işləyən \"daxili model\" anlayışı yoxdur, çünki XGBoost özü qərar ağacları (decision trees) üzərində qurulmuş bir **Gradient Boosting** metodudur. Bununla belə, siz XGBoost modelində bəzi hiperparametrləri dəyişdirərək modelin necə işlədiyini təsir edə bilərsiniz. Amma bu, istifadə olunan modelin növünü deyil, qərar ağaclarının quruluşunu və iş rejimini dəyişdirir.\n",
    "\n",
    "### XGBoost Modelində Dəyişdirilə Bilən Əsas Hiperparametrlər:\n",
    "\n",
    "1. **`booster`:**\n",
    "   - **`gbtree`**: Standart qərar ağaclarına əsaslanan boosting (ən geniş yayılmış).\n",
    "   - **`gblinear`**: Qərar ağacları əvəzinə xətti modellər (linear boosters) istifadə edilir.\n",
    "   - **`dart`**: Dropouts təklif edir ki, bu da gbtree-də olan bəzi ağacları tərk edərək overfitting-i azalda bilər.\n",
    "\n",
    "   Yəni, XGBoost-da \"model\" əvəzinə, fərqli boosting metodlarını seçə bilərsiniz.\n",
    "\n",
    "2. **`max_depth`:**\n",
    "   - Hər bir ağacın maksimum dərinliyini təyin edir. Dərinlik nə qədər böyükdürsə, model o qədər kompleks və potensial olaraq overfitting riski böyükdür.\n",
    "\n",
    "3. **`learning_rate`:**\n",
    "   - Yeni ağacın verdiyi töhfənin ölçüsünü təyin edir. Aşağı dəyər daha stabillik gətirir, amma modelin öyrənmə sürətini azaldır.\n",
    "\n",
    "4. **`n_estimators`:**\n",
    "   - Ağacların ümumi sayını təyin edir. Daha çox ağac daha yaxşı performansa gətirib çıxara bilər, amma overfitting riski də artar.\n",
    "\n",
    "5. **`subsample`:**\n",
    "   - Hər bir ağac üçün istifadə edilən təlim məlumatlarının faizini təyin edir. Bu, modelin daha az məlumat üzərində öyrənməsinə və overfitting riskinin azalmasına kömək edir.\n",
    "\n",
    "6. **`colsample_bytree`, `colsample_bylevel`, `colsample_bynode`:**\n",
    "   - Hər bir ağac üçün (və ya səviyyə və ya düyün üçün) istifadə edilən atributların faizini təyin edir. Bu da overfitting riskini azaldır.\n",
    "\n",
    "7. **`gamma`:**\n",
    "   - Ağacların bölünməsinə nəzarət edən cərimələndirmə əmsalıdır. Yüksək dəyər daha az bölünmə və daha sadə modellərlə nəticələnir.\n",
    "\n",
    "### Yekun\n",
    "XGBoost-da da daxildə işləyən model dəyişdirilə bilməz, çünki o, Gradient Boosting-ə əsaslanan bir metoddur. Lakin siz modelin necə öyrəndiyini və performansını dəyişdirmək üçün müxtəlif hiperparametrləri tənzimləyə bilərsiniz. Məsələn, **`booster`** parametri ilə \"gbtree\", \"gblinear\", və \"dart\" arasında seçim edə bilərsiniz ki, bu da modelin davranışını xeyli dərəcədə dəyişə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c889867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7eee2b",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansinda daxili modeli deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13699f",
   "metadata": {},
   "source": [
    "Boosting modellərində ümumiyyətlə \"daxili model\" anlayışı yoxdur, çünki bu modellərin strukturu müəyyən bir əsas modelə, adətən qərar ağaclarına (decision trees), əsaslanır. Bu səbəbdən, boosting metodlarında daxili modeli dəyişdirmək mümkün olmur. \n",
    "\n",
    "### Əsas Boosting Modelləri və Daxili Model Seçimi:\n",
    "\n",
    "1. **XGBoost, LightGBM, və CatBoost:**\n",
    "   - Bu modellər qərar ağacları əsasında işləyir və istifadəçilər üçün daxili modelin (məsələn, qərar ağacları əvəzinə xətti modellər və ya başqa bir model) dəyişdirilməsi imkanı təqdim etmir.\n",
    "   - Yeganə istisna XGBoost-da **`booster`** parametridir, burada \"gbtree\" (qərar ağacları) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz. Lakin bu, daha çox modelin strukturu ilə əlaqədardır və tam mənada \"daxili model\" dəyişimi kimi qəbul edilmir.\n",
    "\n",
    "2. **AdaBoost:**\n",
    "   - AdaBoost-da istifadə olunan zəif modellər adətən qərar ağaclarıdır, lakin bu modelin təbiətinə görə zəif model olaraq fərqli algoritmlər (məsələn, xətti regresiya, SVM və ya başqa modellər) istifadə oluna bilər. Bu, AdaBoost-un digər boosting metodlarından fərqli olmasının əsas səbəbidir.\n",
    "   - Yəni, **AdaBoost** modelində istifadə olunan zəif modelləri dəyişdirmək mümkündür.\n",
    "\n",
    "### Nəticə\n",
    "Əgər daxili modeli dəyişmək imkanını istəyirsinizsə, **AdaBoost** istifadə etməlisiniz. AdaBoost-da zəif model olaraq fərqli algoritmlərdən istifadə edə bilərsiniz. Lakin digər məşhur boosting metodları olan XGBoost, LightGBM, və CatBoost-da daxili model dəyişdirilməsi mümkün deyil, çünki bunlar xüsusi olaraq qərar ağacları üzərində işləmək üçün dizayn olunub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eccd75",
   "metadata": {},
   "source": [
    "## Gradient Boosting-de daxili modeli deyismek olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b54636",
   "metadata": {},
   "source": [
    "Bəli, Gradient Boosting metodunda da daxili model dəyişdirilmir. Gradient Boosting, adətən qərar ağacları (decision trees) istifadə edərək işləyən bir boosting metodudur və bu, metodun əsas strukturunu təşkil edir. \n",
    "\n",
    "### Əsas Nöqtələr:\n",
    "- **Gradient Boosting** metodunun əsas ideyası, zəif modellərin (adətən qərar ağacları) ardıcıl olaraq qurulması və hər bir yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışmasıdır.\n",
    "- **Qərar ağacları** bu metodun əsasında durur və buna görə də daxili model olaraq dəyişdirilə bilməz.\n",
    "\n",
    "### İstisnalar və Seçimlər:\n",
    "- **XGBoost** və **LightGBM** kimi Gradient Boosting-in optimallaşdırılmış versiyaları da əsasən qərar ağacları üzərində işləyir. Bu modellərdə də daxili modeli dəyişdirmək mümkün deyil.\n",
    "- Yalnız **XGBoost** modelində `booster` parametri ilə \"gbtree\" (qərar ağacı) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz, amma bu tam olaraq daxili modelin dəyişdirilməsi deyil. Bu parametr sadəcə olaraq modelin xətti və ya ağac əsaslı olacağını təyin edir.\n",
    "\n",
    "### Nəticə:\n",
    "Gradient Boosting metodunda daxili model adətən dəyişdirilmir və əsas olaraq qərar ağacları istifadə olunur. Bu, metodun fundamental xüsusiyyətlərindən biridir və onun effektivliyini təmin edən əsas səbəblərdən biridir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938f93d4",
   "metadata": {},
   "source": [
    "## Robust Scale haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3a003",
   "metadata": {},
   "source": [
    "**Robust Scaler** məlumat dəstələrinin (datasets) miqyasını dəyişdirmək üçün istifadə olunan bir normalizasiya texnikasıdır. Bu metod ənənəvi miqyaslandırma üsulları (məsələn, Standart Scaler, Min-Max Scaler) ilə müqayisədə məlumat dəstələrindəki outlier-lərə (məlumatların ümumi tendensiyasından uzaq olan dəyərlərə) qarşı daha davamlıdır. \n",
    "\n",
    "### Robust Scaler-in Əsas Xüsusiyyətləri:\n",
    "\n",
    "1. **Median Və İQR Əsaslı Miqyaslandırma:**\n",
    "   - **Median**: Məlumat dəstəsinin ortasında olan dəyərdir və bu dəyər məlumat dəstəsindəki dəyərlərin yarısının bu dəyərdən az, yarısının isə çox olduğunu göstərir.\n",
    "   - **IQR (Interquartile Range)**: Məlumat dəstəsinin 25-ci və 75-ci percentilləri arasındakı fərqdir. Bu aralıq mərkəzləşmiş dəyərləri əhatə edir və outlier-lərdən daha az təsirlənir.\n",
    "   - Robust Scaler məlumat dəstəsini median ətrafında mərkəzləşdirir və dəyərləri IQR ilə miqyaslandırır. Yəni, hər bir dəyərdən median çıxılır və sonra IQR-ə bölünür:\n",
    "     $$ \n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{Median}(X)}{\\text{IQR}(X)}\n",
    "      $$\n",
    "   - Bu yanaşma məlumat dəstəsində outlier-lər olduğu halda belə, məlumatların düzgün miqyaslandırılmasını təmin edir.\n",
    "\n",
    "2. **Outlier-lərə Qarşı Dayanıqlıq:**\n",
    "   - Outlier-lər, məlumatların əksəriyyətinin uzaqında yerləşən ekstremal dəyərlərdir. Ənənəvi miqyaslandırma üsulları bu dəyərlərə həssas ola bilər və nəticədə miqyaslandırılmış dəyərlər qeyri-adekvat ola bilər.\n",
    "   - Robust Scaler, median və IQR istifadə etdiyi üçün outlier-lərin təsirini azaldır və məlumatların miqyaslandırılmasını daha sabit və dəqiq edir.\n",
    "\n",
    "3. **Tətbiq Sahələri:**\n",
    "   - **Maşın Öyrənməsi (Machine Learning):** Məlumat dəstələrinin outlier-lərlə zəngin olduğu vəziyyətlərdə xüsusən istifadə olunur. Bu, modellərin (məsələn, linear regression, SVM) performansını artırmağa kömək edə bilər.\n",
    "   - **Peyk Məlumatları:** Məsələn, peyk görüntüləri və ya uzaqdan zondlama (remote sensing) məlumatları çox sayda outlier-lərlə işləyə bilər.\n",
    "   - **Finans Məlumatları:** Finansal məlumatlarda outlier-lər tez-tez rast gəlinir. Robust Scaler bu məlumatları miqyaslandırmaq üçün əlverişlidir.\n",
    "\n",
    "### Nümunə: Python ilə Robust Scaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Random data with outliers\n",
    "data = np.array([[1, 2, 2], \n",
    "                 [3, 6, 4], \n",
    "                 [7, 18, 8], \n",
    "                 [9, 10, 12], \n",
    "                 [20, 50, 90]])\n",
    "\n",
    "# RobustScaler-in tətbiqi\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nScaled Data with RobustScaler:\\n\", scaled_data)\n",
    "```\n",
    "\n",
    "### Üstünlüklər:\n",
    "- **Outlier-lərə Dayanıqlıq:** Median və IQR istifadə edildiyi üçün məlumat dəstələrindəki outlier-lər bu miqyaslandırmaya az təsir edir.\n",
    "- **Daha Sabit Nəticələr:** Ənənəvi miqyaslandırma metodları ilə müqayisədə, outlier-lərin səbəb olduğu qeyri-stabilliklər azaldılır.\n",
    "\n",
    "### Çatışmazlıqlar:\n",
    "- **Küçük Məlumat Dəstələrində Problemlər:** Əgər məlumat dəstəsi çox kiçikdirsə və ya çox az sayda nümunədən ibarətdirsə, median və IQR düzgün hesablanmaya bilər.\n",
    "- **Yalnız Düzgün Paylanmış Məlumatlarda Effektivdir:** Məlumatların paylanışı çox qeyri-bərabərdirsə, Robust Scaler bəzən qeyri-adekvat miqyaslandırma apara bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Robust Scaler outlier-lərlə zəngin olan məlumat dəstələrinin miqyaslandırılması üçün ideal bir vasitədir. Median və IQR kimi statistik ölçülərdən istifadə edərək məlumatları miqyaslandırmaqla, bu metod outlier-lərə qarşı daha sabit və dəqiq nəticələr verir. Bu səbəbdən, Robust Scaler maşın öyrənməsi tətbiqlərində və digər məlumat təhlili proseslərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a586bc43",
   "metadata": {},
   "source": [
    "## Feature Importance Riyaziyyatda formulu? Hesablanma qaydasi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebb069",
   "metadata": {},
   "source": [
    "Decision Tree modellərində **feature importance** hər bir xüsusiyyətin (feature) modeldə nə qədər vacib olduğunu göstərir. Bu, xüsusiyyətin ağacın qurulmasında (tree-building process) nə qədər təsirli olduğunu qiymətləndirir. Decision Tree-də **feature importance** ümumiyyətlə impurity-based importance metodu ilə hesablanır.\n",
    "\n",
    "### Feature Importance Hesablanması\n",
    "\n",
    "1. **Impurity Measurement (Gini impurity, Entropy):** \n",
    "   - Hər bir düyündə (node) impurity (məsələn, Gini impurity, Entropy) hesablanır. Impurity, həmin düyündəki qarışıqlığın ölçüsüdür.\n",
    "\n",
    "2. **Impurity Reduction (Information Gain):**\n",
    "   - Hər bir xüsusiyyət üçün, həmin xüsusiyyətə əsaslanaraq ağacın düyünü bölündükdən sonra impurity azalması hesablanır. Bu azalma həmin xüsusiyyətin nə qədər məlumat verdiyini (information gain) göstərir.\n",
    "   \n",
    "   - Impurity reduction = impurity_before_split - impurity_after_split\n",
    "\n",
    "3. **Cumulative Importance:**\n",
    "   - Hər bir xüsusiyyət üçün, ağacın müxtəlif düyünlərindəki impurity reduction-lar toplanır. Bu, həmin xüsusiyyətin ümumi əhəmiyyətini (total importance) göstərir.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - Hər bir xüsusiyyətin əhəmiyyəti normallaşdırılır (0 ilə 1 arasında) və nəticədə hər bir xüsusiyyətin final feature importance dəyəri əldə edilir.\n",
    "\n",
    "### Riyazi Formula\n",
    "\n",
    "Feature importance $  FI_j  $ hər bir xüsusiyyət $ j $ üçün aşağıdakı kimi hesablanır:\n",
    "\n",
    "$$ \n",
    "FI_j = \\sum_{t=1}^{T} \\Delta i_t \\cdot 1 \\{v(t) = j\\}\n",
    " $$\n",
    "\n",
    "burada:\n",
    "- $  \\Delta i_t  $ - düyün $ t $-də impurity reduction-dur (impurity_before_split - impurity_after_split).\n",
    "- $ 1 \\{v(t) = j\\}  $ - düyün $ t $-nin xüsusiyyət $ j $-yə aid olub-olmamasını göstərən göstərici funksiyasıdır.\n",
    "- $ T $ - ümumi düyünlərin sayıdır.\n",
    "\n",
    "Bu formula ağacın bütün düyünlərində xüsusiyyətin nə qədər məlumat verdiyini toplamaqla xüsusiyyətin ümumi əhəmiyyətini hesablayır. Nəticədə, daha çox informasiya verən xüsusiyyətlərin əhəmiyyəti yüksək olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e04d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a46511d",
   "metadata": {},
   "source": [
    "* scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "* X_train = scaler.transform(X_train)\n",
    "* X_test = scaler.transform(X_test)\n",
    "\n",
    "### Muellif scale -i bu formada edib. Niye X_test-i de X_traine gore scale edib? Umimiyyetle  train test splitden evvel ya sonra yaxsidir scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cc696",
   "metadata": {},
   "source": [
    "`StandardScaler` və digər skalarlardan istifadə edərkən `train` və `test` setlərinin uyğunlaşdırılması (scaling) çox önəmlidir. Niyə `X_test` də `X_train`-ə görə standartlaşdırılır və bu prosesin ən doğru vaxtı haqqında izahat verəcəyəm.\n",
    "\n",
    "### Niyə `X_test` `X_train`-ə görə standartlaşdırılır?\n",
    "\n",
    "1. **Məlumat Sızması (Data Leakage) Əngəllənməsi:**\n",
    "   - **Qarşısını Almaq:** Test məlumatlarına birbaşa müdaxilə etmək modelin qiymətləndirilməsini səhv edə bilər. Test məlumatlarına əvvəlcədən baxmaq, modelin test dəstinə uyğunlaşmasına və nəticədə daha yaxşı nəticələr əldə etməsinə səbəb ola bilər. Bu, məlumat sızması (data leakage) olaraq adlandırılır.\n",
    "   - **Nə Üçün:** Test setini `X_train`-ə görə standartlaşdırmaq, test məlumatlarının yalnız təlim məlumatları əsasında miqyaslandırılması və beləliklə, test dəstinin düzgün qiymətləndirilməsi təmin edilir.\n",
    "\n",
    "2. **Hesablama Uyğunluğu:**\n",
    "   - **Standartlaşdırma:** Təlim məlumatları (`X_train`) üzərində fit edilmiş `StandardScaler`, müvafiq olaraq bu məlumatların ortalaması və standart sapması ilə `X_train` və `X_test`-i miqyaslandırır.\n",
    "   - **Nə Üçün:** Bu, modelin test məlumatlarının dəqiq və obyektiv qiymətləndirilməsi üçün lazımdır. `X_test`-i `X_train`-ə görə miqyaslandırmaq, test dəstinin eyni skala ilə işlənməsini təmin edir.\n",
    "\n",
    "### Train Test Splitdən Əvvəl Yoxsa Sonra Scale Etmək?\n",
    "\n",
    "- **Testdən Sonra:** Test və təlim məlumatlarını ayırdıqdan sonra, `StandardScaler` fit edildikdə, yalnız təlim məlumatlarına əsaslanaraq fit edilir və sonra bu transformasiyanı test məlumatlarına tətbiq etmək məsləhətdir. Bu, test məlumatlarının təlim məlumatlarının xüsusiyyətlərindən istifadə etmədən miqyaslandırılmasını təmin edir.\n",
    "\n",
    "### Məsələn\n",
    "\n",
    "**Train Test Splitdən Sonra Skalarlama:**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DataFrame və target dəyişənini ayırmaq\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler obyektini yaratmaq və fit etmək\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### Əsas Məqamlar\n",
    "\n",
    "1. **Train Test Splitdən Sonra Skalarlama:**\n",
    "   - `StandardScaler`-in `fit()` metodu yalnız `X_train` üzərində çalışır.\n",
    "   - `transform()` metodu həm `X_train`, həm də `X_test` üzərində tətbiq olunur.\n",
    "\n",
    "2. **Test Seti Üzərində Fit Etmək:**\n",
    "   - Test setinə fit edilməməlidir, çünki bu, test məlumatlarının məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "Bu yanaşma test dəstinin düzgün qiymətləndirilməsini təmin edir və modelin yalnız təlim məlumatlarına əsaslanaraq necə performans göstərdiyini daha doğru qiymətləndirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a1f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f552a7",
   "metadata": {},
   "source": [
    "## RobustScaler haqqinda genis melumat ver?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bc9aa",
   "metadata": {},
   "source": [
    "`RobustScaler`, xüsusilə outlier-lərlə zəngin məlumat dəstləri üçün nəzərdə tutulmuş bir miqyaslandırma metodudur. Bu metod, xüsusiyyətləri `median` və `interquartile range` (IQR) əsasında standartlaşdıraraq daha dayanıqlı və etibarlı nəticələr əldə etməyə imkan verir.\n",
    "\n",
    "### `RobustScaler` Nədir?\n",
    "\n",
    "`RobustScaler`, `sklearn.preprocessing` kitabxanasında olan bir vasitədir. Bu metod, xüsusiyyətləri aşağıdakı addımlar ilə miqyaslandırır:\n",
    "\n",
    "1. **Median ilə Mərkəzləşdirmə:**\n",
    "   - **Median**: Hər bir xüsusiyyət üçün orta dəyəri çıxarmaq məqsədilə istifadə olunur. Median outlier-lərdən təsirlənməz, çünki o, 50%-lik kvantildir və outlier-lərin təsirini azaltmağa kömək edir.\n",
    "\n",
    "2. **Interquartile Range (IQR) ilə Miqyaslandırma:**\n",
    "   - **IQR**: Bu metod, 25%-lik (Q1) və 75%-lik (Q3) kvantil arasındakı məsafəni ölçür. Bu məsafə xüsusiyyətlərin yayılmasını ölçmək üçün istifadə edilir və outlier-lərin təsirini azaltmaq məqsədilə miqyaslandırma prosesində tətbiq olunur.\n",
    "\n",
    "### Formula\n",
    "\n",
    "`RobustScaler`-in tətbiq etdiyi standartlaşdırma aşağıdakı kimi təyin olunur:\n",
    "\n",
    "- **Mərkəzləşdirmə (Mean subtraction):**\n",
    "  \n",
    "  \\[\n",
    "  X_{\\text{centered}} = X - \\text{Median}\n",
    "  \\]\n",
    "\n",
    "- **Miqyaslandırma (Scaling):**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{scaled}} = \\frac{X_{\\text{centered}}}{\\text{IQR}}\n",
    "  \\]\n",
    "\n",
    "  Burada:\n",
    "  - \\( X \\) - Xüsusiyyətlərin dəyərləri\n",
    "  - **Median** - Xüsusiyyətlərin medianı\n",
    "  - **IQR** - Interquartile range (Q3 - Q1)\n",
    "\n",
    "### İstifadə Misalı\n",
    "\n",
    "`RobustScaler`-dən istifadə edərkən adətən aşağıdakı addımlar izlənir:\n",
    "\n",
    "1. **DataFrame və target dəyişənini ayırmaq:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "   # Məsələn, DataFrame\n",
    "   data = {\n",
    "       'feature1': [1, 2, 3, 1000],  # Outlier daxil olan xüsusiyyətlər\n",
    "       'feature2': [4, 5, 6, 7]\n",
    "   }\n",
    "   df = pd.DataFrame(data)\n",
    "   X = df[['feature1', 'feature2']]\n",
    "   y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "   # Train və Test dəstini ayırmaq\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **`RobustScaler`-i tətbiq etmək:**\n",
    "\n",
    "   ```python\n",
    "   # RobustScaler obyektini yaratmaq və fit etmək\n",
    "   scaler = RobustScaler()\n",
    "   scaler.fit(X_train)\n",
    "\n",
    "   # Təlim və test məlumatlarını transformasiya etmək\n",
    "   X_train_scaled = scaler.transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "   # Dəyişdirilmiş məlumatları çap etmək\n",
    "   print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "   print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "   ```\n",
    "\n",
    "### Nə Zaman İstifadə Edilməlidir?\n",
    "\n",
    "- **Outlier-lərlə Mübarizə:** `RobustScaler` xüsusilə outlier-lərin olduğu məlumat dəstləri üçün uyğundur, çünki median və IQR outlier-lərdən təsirlənmir.\n",
    "- **Model Performansı:** Outlier-lərdən təsirlənməyən daha düzgün nəticələr və daha stabil model performansı təmin edir.\n",
    "\n",
    "### Müqayisə\n",
    "\n",
    "- **`StandardScaler` vs. `RobustScaler`:**\n",
    "  - `StandardScaler`-in istifadə etdiyi ortalama və standart sapma outlier-lərlə təsirlənə bilər, amma `RobustScaler`-in istifadə etdiyi median və IQR outlier-lərdən daha dayanıqlıdır.\n",
    "  - `StandardScaler` daha çox \"normal\" paylanma olan məlumat dəstləri üçün uyğundur, `RobustScaler` isə outlier-lərin olduğu hallarda daha uyğundur.\n",
    "\n",
    "Bu şəkildə, `RobustScaler` xüsusiyyətlərin daha dayanıqlı şəkildə standartlaşdırılmasını təmin edir və xüsusiyyətlərdəki outlier-lərin təsirini azaltmağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9397e4e4",
   "metadata": {},
   "source": [
    "## RobustScaler de train test split den sonra olmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7fc8f",
   "metadata": {},
   "source": [
    "Bəli, `RobustScaler` və digər miqyaslandırma metodları train-test splitdən sonra tətbiq olunmalıdır. Bu, məlumat sızmasının (data leakage) qarşısını almaq və modelin təlim və test mərhələlərində düzgün qiymətləndirilməsini təmin etmək üçün vacibdir.\n",
    "\n",
    "### Niyə Train-Test Splitdən Sonra Scalarlama?\n",
    "\n",
    "1. **Məlumat Sızmasının Qarşısını Almaq:**\n",
    "   - **Niyə:** Test məlumatlarına birbaşa təsir etmədən yalnız təlim məlumatlarından miqyaslandırma parametrləri (median və IQR) əldə etmək lazımdır. Bu, test dəstinin yalnız təlim məlumatları əsasında miqyaslandırıldığını təmin edir.\n",
    "   - **Nəticə:** Test məlumatlarına əvvəlcədən baxmaq və ya onları miqyaslandırmaq modelin nəticələrini təhrif edə bilər və məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "2. **Miqyaslandırma Parametrlərinin Düzgün Tətbiqi:**\n",
    "   - **Təlim Məlumatları:** Miqyaslandırma parametrləri yalnız təlim məlumatlarına əsaslanaraq fit edilir.\n",
    "   - **Test Məlumatları:** Eyni parametrlər (mediana və IQR) test məlumatlarına tətbiq edilir.\n",
    "\n",
    "### Misal Kodu\n",
    "\n",
    "**1.** **Train-Test Splitdən Sonra Miqyaslandırma:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Nümunə DataFrame\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 1000],  # Outlier olan xüsusiyyət\n",
    "    'feature2': [4, 5, 6, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RobustScaler obyektini yaratmaq və fit etmək\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)  # Yalnız X_train üzərində fit edilir\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dəyişdirilmiş məlumatları çap etmək\n",
    "print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "```\n",
    "\n",
    "**2.** **Nümunənin İslahatı:**\n",
    "\n",
    "- **DataFrame Yaratma:** Nümunə məlumatlar yaradılır.\n",
    "- **Train-Test Split:** `X` və `y` məlumatları train və test dəstlərinə bölünür.\n",
    "- **`RobustScaler` İstifadəsi:** `RobustScaler` yalnız təlim dəstində fit edilir, sonra həm təlim, həm də test dəstinə tətbiq edilir.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "`RobustScaler` və digər miqyaslandırma üsulları üçün düzgün təcrübə train-test splitdən sonra miqyaslandırma tətbiq etməkdir. Bu yanaşma, məlumat sızmasının qarşısını alır və modelin daha düzgün qiymətləndirilməsini təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15586d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61de99c",
   "metadata": {},
   "source": [
    "## Bagging parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5a4a",
   "metadata": {},
   "source": [
    "* bag = BaggingClassifier(estimator=svm,\n",
    "*                        n_estimators=10,\n",
    "*                        max_samples=0.4,\n",
    "*                        max_features=35,\n",
    "*                        bootstrap=True,\n",
    "*                        bootstrap_features=True,\n",
    "*                        random_state=123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61254f",
   "metadata": {},
   "source": [
    "`BaggingClassifier` parametrleri aşağıda izah olunur:\n",
    "\n",
    "1. **`estimator=svm`**: Burada `svm`, bagging ilə istifadə olunan əsas modelin növünü göstərir. SVM (Support Vector Machine) burada əsas model kimi istifadə olunur. `BaggingClassifier`, bu SVM modellərindən bir neçə nüsxə yaradaraq onları birləşdirir.\n",
    "\n",
    "2. **`n_estimators=10`**: Bu parametr, neçə dənə baza modelinin (bu halda SVM) yaradılacağını göstərir. `n_estimators=10` təyin edildikdə, `BaggingClassifier` 10 fərqli SVM modeli qurur və onları birləşdirir.\n",
    "\n",
    "3. **`max_samples=0.4`**: Bu parametr, hər bir baza modelinin təlimi üçün orijinal məlumat dəstindən seçilən nümunələrin faizi və ya sayı ilə bağlıdır. `max_samples=0.4` olduqda, hər baza modeli üçün orijinal məlumat dəstinin 40%-i təlim üçün istifadə olunur. Bu seçim prosesi \"bootstrapping\" metodu ilə həyata keçirilir.\n",
    "\n",
    "4. **`max_features=35`**: Bu parametr, hər bir baza modelinin qurulmasında istifadə olunan xüsusiyyətlərin maksimum sayını göstərir. `max_features=35` təyin edildikdə, hər baza modelinin təlimində 35 xüsusiyyət seçilir.\n",
    "\n",
    "5. **`bootstrap=True`**: Bu parametr, nümunələrin \"bootstrapping\" (yəni, geri qoyaraq nümunə götürmə) metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap=True` olduqda, hər bir baza modeli üçün təlim nümunələri geri qoyularaq seçilir, yəni eyni nümunə bir neçə dəfə seçilə bilər.\n",
    "\n",
    "6. **`bootstrap_features=True`**: Bu parametr, xüsusiyyətlərin \"bootstrapping\" metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap_features=True` olduqda, hər bir baza modeli üçün xüsusiyyətlər geri qoyularaq seçilir.\n",
    "\n",
    "7. **`random_state=123`**: Bu parametr, nəticələrin təkrarlanabilirliyini təmin etmək üçün istifadə olunur. `random_state=123` təyin edildikdə, modelin təsadüfi seçimləri eyni şəkildə həyata keçirilir və nəticələr hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `BaggingClassifier` modelinin müxtəlif aspektlərini nəzarət edir və modelin qurulması, nümunə götürülməsi və xüsusiyyətlərin seçilməsi prosesi üzərində tam nəzarət təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafb507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bfaed9",
   "metadata": {},
   "source": [
    "## XGBoost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31aee",
   "metadata": {},
   "source": [
    "* xgbt = XGBClassifier(max_depth = 2,\n",
    "*              learning_rate = 0.2,\n",
    "*              objective  = \"multi:softmax\",\n",
    "*              num_class = 2,\n",
    "*              booster = \"gbtree\",\n",
    "*              n_estimators = 10,\n",
    "*              random_state = 123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16db875",
   "metadata": {},
   "source": [
    "`XGBClassifier` modelinin parametrləri aşağıda izah olunur:\n",
    "\n",
    "1. **`max_depth = 2`**:\n",
    "   - Bu parametr hər bir fərdi ağacın maksimal dərinliyini təyin edir. `max_depth = 2` təyin edildikdə, hər ağac ən çox 2 dərinlik səviyyəsinə malik olacaq. Daha kiçik dərinlik overfitting-i azaldır, lakin çox kiçik olarsa, model kifayət qədər mürəkkəbliyi tuta bilməz.\n",
    "\n",
    "2. **`learning_rate = 0.2`**:\n",
    "   - Bu parametr, hər ağacın öyrənmə sürətini təyin edir. `learning_rate = 0.2` olduqda, hər bir ağacın modelə töhfəsi 0.2 faktorla azalır. Kiçik learning rate daha dəqiq və stabil model təlimi verir, amma daha çox iterasiya tələb edir.\n",
    "\n",
    "3. **`objective = \"multi:softmax\"`**:\n",
    "   - Bu parametr modelin məqsədini təyin edir. `multi:softmax` təyin edildikdə, model çoxlu sinifli təsnifat (multi-class classification) üçün istifadə edilir və proqnozlaşdırılan dəyər ən yüksək ehtimala malik sinif kimi verilir.\n",
    "\n",
    "4. **`num_class = 2`**:\n",
    "   - Bu parametr, modelin neçə sinif olduğunu təyin edir. `num_class = 2` təyin edildikdə, model iki sinifli (binary) təsnifat məsələsini həll edir. (Qeyd: `multi:softmax` ilə birlikdə istifadə olunduqda, bu parametr faktiki olaraq daha çox sinif sayı üçün istifadə olunur.)\n",
    "\n",
    "5. **`booster = \"gbtree\"`**:\n",
    "   - Bu parametr, modelin hansı boosting algoritmini istifadə edəcəyini təyin edir. `gbtree` təyin edildikdə, model qərar ağacları ilə qurulur. Alternativ olaraq `gblinear` (linear model) və ya `dart` (Dropouts meet Multiple Additive Regression Trees) kimi digər algoritmlər də istifadə edilə bilər.\n",
    "\n",
    "6. **`n_estimators = 10`**:\n",
    "   - Bu parametr, modeldə neçə dənə ağacın qurulacağını göstərir. `n_estimators = 10` təyin edildikdə, model 10 ağac qurur. Daha çox ağac modelin dəqiqliyini artıra bilər, lakin eyni zamanda təlim müddətini uzada bilər.\n",
    "\n",
    "7. **`random_state = 123`**:\n",
    "   - Bu parametr, təsadüfi proseslər üçün toxum (seed) təyin edir ki, nəticələr təkrarlanabilsin. `random_state = 123` təyin edildikdə, modelin nəticələri hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `XGBClassifier` modelinin qurulması və işləməsi zamanı onun davranışını təyin edir və modelin düzgün performans göstərməsi üçün müxtəlif aspektləri optimallaşdırmağa imkan verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5475dc",
   "metadata": {},
   "source": [
    "### XGBoots-da \n",
    "* scale_pos_weight=[default=1]\n",
    "### ???????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c25fe",
   "metadata": {},
   "source": [
    "`scale_pos_weight` parametri `XGBoost` modelində balanssız verilənlər dəstlərində (yəni, bir sinifin digərindən çox üstün olduğu hallarda) istifadə olunur. Bu parametr siniflər arasındakı balanssızlığı tənzimləmək və modelin daha yaxşı performans göstərməsini təmin etmək üçün istifadə edilir.\n",
    "\n",
    "### `scale_pos_weight` Parametrinin İzahı\n",
    "\n",
    "- **Məqsəd:** Əgər verilənlər dəstində siniflərdən biri (məsələn, pozitiv sinif) çox az sayda nümunə ilə təmsil olunursa, `scale_pos_weight` parametri bu sinifə daha çox əhəmiyyət vermək üçün istifadə olunur.\n",
    "- **Dəyər:** `scale_pos_weight = nöqativ_sinif_nümunələri / pozitiv_sinif_nümunələri` şəklində təyin edilir.\n",
    "\n",
    "### Necə İşləyir?\n",
    "\n",
    "- **Balanssızlıq Problemi:** Əgər siniflər arasında balanssızlıq varsa, model çoxluqda olan sinifi daha çox öyrənir və bu da nadir sinifin düzgün proqnozlaşdırılmasına mane ola bilər.\n",
    "- **`scale_pos_weight` Parametrinin Təyini:** Bu parametr, azlıqda olan sinifin təsirini artırmaq üçün istifadə olunur. Məsələn, pozitiv sinifdə 100 nümunə, neqativ sinifdə isə 1000 nümunə varsa, `scale_pos_weight = 1000/100 = 10` təyin etmək olar. Bu, modelin azlıqda olan sinifə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Misal:\n",
    "\n",
    "```python\n",
    "xgbt = XGBClassifier(max_depth=2,\n",
    "                     learning_rate=0.2,\n",
    "                     objective=\"binary:logistic\",\n",
    "                     n_estimators=10,\n",
    "                     scale_pos_weight=10,  # Siniflər arasındakı balanssızlıq nəzərə alınır\n",
    "                     random_state=123)\n",
    "```\n",
    "\n",
    "Burada `scale_pos_weight=10` təyin edilib. Bu, modelin pozitiv sinifə neqativ sinifdən 10 dəfə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Nəticə\n",
    "`scale_pos_weight` parametri balanssız verilənlər dəstlərində siniflər arasındakı balansı tənzimləyərək, modelin azlıqda olan sinifi düzgün proqnozlaşdırmasını təmin etmək üçün istifadə edilir. Bu parametr xüsusilə tibbi diaqnostika, fırıldaqçılığın aşkarlanması kimi sahələrdə faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e67694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0befd682",
   "metadata": {},
   "source": [
    "## Data Understanding etmek ucun nelere baxim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b969",
   "metadata": {},
   "source": [
    "Bu məlumat dəstini yaxşı anlamaq üçün aşağıdakı addımları ata bilərsiniz:\n",
    "\n",
    "### 1. **Veri Tipini Yoxlayın**\n",
    "   - **Hər bir sütunun** (dəyişənin) tipini yoxlayın (məsələn, integer, float, object). Bu, hər bir dəyişənin necə işlənməli olduğunu anlamağa kömək edəcək. Məsələn, `LoanAmount` və `ApplicantIncome` sayısal dəyərlərdir, amma `Gender`, `Married` və `Loan_Status` kimi dəyişənlər kateqoriyalardır.\n",
    "\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "\n",
    "### 2. **Məlumatların Yoxluğu (Missing Data)**\n",
    "   - Məlumat dəstində boş dəyərlərin olub-olmadığını yoxlayın. Bu, model qurmaq üçün məlumatların keyfiyyətini artırmağa kömək edəcək. Boş dəyərlər varsa, onları doldurma (imputation) və ya silmə metodları tətbiq edə bilərsiniz.\n",
    "\n",
    "   ```python\n",
    "   df.isnull().sum()\n",
    "   ```\n",
    "\n",
    "### 3. **Məlumatların Dağılımı**\n",
    "   - Sayısal dəyişənlərin (məsələn, `ApplicantIncome`, `LoanAmount`) və kateqoriyal dəyişənlərin (`Gender`, `Education`) dağılımını yoxlayın. Bu, məlumatların normal paylanıb-paylanmadığını və ya bəzi dəyişənlərdə outlier-ların olub-olmadığını anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.describe()\n",
    "   df['ApplicantIncome'].hist()\n",
    "   ```\n",
    "\n",
    "### 4. **Kateqoriyal Dəyərlərin Paylanması**\n",
    "   - Kateqoriyal dəyişənlərin (məsələn, `Gender`, `Married`, `Education`) müxtəlif siniflər üzrə paylanmasını yoxlayın. Bu, məlumat dəstində hansı siniflərin dominant olduğunu və modelin necə performans göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df['Gender'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 5. **Hədəf Dəyişən (Target Variable)**\n",
    "   - `Loan_Status` dəyişəninin paylanmasını yoxlayın. Hədəf dəyişənin balanslı və ya balanssız olub-olmadığını bilmək vacibdir. Balanssızlıq varsa, xüsusi metodlar tətbiq etmək lazım ola bilər (məsələn, undersampling, oversampling).\n",
    "\n",
    "   ```python\n",
    "   df['Loan_Status'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 6. **Məlumatların Əlaqəsi (Correlation)**\n",
    "   - Sayısal dəyişənlər arasındakı əlaqəni yoxlayın. Bu, hansı dəyişənlərin bir-biri ilə daha çox əlaqəli olduğunu və hansı dəyişənlərin modelə daha çox təsir göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.corr()\n",
    "   ```\n",
    "\n",
    "### 7. **Pivotal Dəyişənlərin Təsiri**\n",
    "   - Hədəf dəyişən (`Loan_Status`) ilə digər dəyişənlər arasındakı əlaqəni yoxlayın. Məsələn, `ApplicantIncome`, `Credit_History` və `Education` dəyişənlərinin kredit təsdiqinə necə təsir etdiyini araşdırın.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "   ```\n",
    "\n",
    "### 8. **Outlier-ları Yoxlamaq**\n",
    "   - Sayısal dəyişənlərdə outlier-ları yoxlayın və onları necə idarə edəcəyinizi qərar verin. Outlier-lar modeli yanıltmaq riski daşıya bilər.\n",
    "\n",
    "   ```python\n",
    "   df.boxplot(column='LoanAmount')\n",
    "   ```\n",
    "\n",
    "Bu addımları yerinə yetirdikdən sonra məlumat dəstini daha yaxşı anlayacaq və model qurmağa başlamaq üçün yaxşı bir zəmin yaradacaqsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b084870e",
   "metadata": {},
   "source": [
    "## Data Preparation usullari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564113cf",
   "metadata": {},
   "source": [
    "Loan eligibility prediction üçün data preparation mərhələsi çox vacibdir və müvəffəqiyyətli modellər qurmaq üçün düzgün hazırlıq tələb olunur. Aşağıdakı addımlar bu prosesdə sizə kömək edə bilər:\n",
    "\n",
    "### 1. **Veri Yükləmə və İnspeksiya**\n",
    "   - Veriləri yükləyin və ilk baxışda statistik göstəricilərə baxın.\n",
    "   - Datalarda hər hansı bir boş (missing) dəyərin olub olmadığını yoxlayın.\n",
    "   \n",
    "   ```python\n",
    "import pandas as pd\n",
    "\n",
    "# CSV faylı yüklə\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# İlk 5 sətirə baxış\n",
    "print(df.head())\n",
    "\n",
    "# Verinin ümumi təsviri\n",
    "print(df.info())\n",
    "\n",
    "# Statistik məlumat\n",
    "print(df.describe())\n",
    "```\n",
    "\n",
    "### 2. **Boş Dəyərlərin İdarə Edilməsi**\n",
    "   - Boş dəyərləri necə idarə edəcəyinizə qərar verin. Ən çox istifadə olunan metodlar:\n",
    "     - **İlkin Təsnifat**: Boş dəyərləri təyin olunmuş bir dəyər ilə əvəzləmək (məsələn, median, mod, və s.)\n",
    "     - **Sıxlıqla Təyin Edilməsi**: Boş dəyərləri sıxlıqla təyin edilmiş dəyərlərlə əvəzləmək.\n",
    "     \n",
    "```python\n",
    "# Boş dəyərlərin yoxlanması\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Median ilə boş dəyərlərin doldurulması (nümunə: LoanAmount sütunu üçün)\n",
    "df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())\n",
    "\n",
    "# Ən çox istifadə olunan dəyərlə boş dəyərlərin doldurulması (nümunə: Gender sütunu üçün)\n",
    "df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])\n",
    "```\n",
    "\n",
    "### 3. **Kateqoriyalı Dəyərlərin Kodlaşdırılması**\n",
    "   - Kateqoriyalı dəyişənləri (Gender, Married, Education, Self_Employed, Property_Area) rəqəmlə ifadə etməlisiniz.\n",
    "     - **Label Encoding**: Hər bir kateqoriyanı təmsil edən bir rəqəm verilir.\n",
    "     - **One-Hot Encoding**: Hər bir kateqoriya üçün ayrılıqda sütun yaradılır və müvafiq sütunda 1, digərlərində isə 0 olur.\n",
    "        \n",
    "**Label Encoding:**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "df['Married'] = le.fit_transform(df['Married'])\n",
    "df['Education'] = le.fit_transform(df['Education'])\n",
    "df['Self_Employed'] = le.fit_transform(df['Self_Employed'])\n",
    "df['Property_Area'] = le.fit_transform(df['Property_Area'])\n",
    "```\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "```python\n",
    "df = pd.get_dummies(df, columns=['Property_Area', 'Education'], drop_first=True)\n",
    "```        \n",
    "\n",
    "### 4. **Miqdar Dəyərlərin Miqyaslandırılması**\n",
    "   - **Standardization**: Orta və standart sapma əsasında dəyərlərin normallaşdırılması.\n",
    "   - **Normalization**: Dəyərlərin [0, 1] intervalına gətirilməsi.\n",
    "    \n",
    "**Standardization:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['LoanAmount'] = scaler.fit_transform(df[['LoanAmount']])\n",
    "```\n",
    "\n",
    "**Normalization:**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['LoanAmount'] = scaler.fit_transform(df[['LoanAmount']])\n",
    "```    \n",
    "\n",
    "### 5. **Boş Dəyərlərin Aşağı Səviyyədə Dəyərləndirilməsi**\n",
    "   - Verilərin doğruluğunu və əlaqəliliyini artırmaq üçün boş dəyərləri analiz edin.\n",
    "\n",
    "```python\n",
    "# Boş dəyərlərin olub-olmadığını yenidən yoxla\n",
    "print(df.isnull().sum())\n",
    "```    \n",
    "    \n",
    "### 6. **Feature Engineering**\n",
    "   - **Yeni Dəyişənlərin Yaradılması**: Yeni dəyərləri yaratmaq üçün mövcud dəyişənləri birləşdirə bilərsiniz (məsələn, **TotalIncome** = **ApplicantIncome** + **CoapplicantIncome**).\n",
    "   - **Dəyişənlərin Seçilməsi**: Modeliniz üçün ən əhəmiyyətli olan dəyişənləri seçin.\n",
    "    \n",
    "**Yeni Dəyişənlərin Yaradılması:**\n",
    "```python\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "```    \n",
    "\n",
    "**Dəyişənlərin Seçilməsi (Feature Selection):**\n",
    "```python\n",
    "# Ən əhəmiyyətli xüsusiyyətləri müəyyən etmək üçün korelyasiya matrisi\n",
    "corr = df.corr()\n",
    "print(corr['Loan_Status'].sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "### 7. **Outlierların İdarə Edilməsi**\n",
    "   - **Outlier Analizi**: Outlierları aşkar edin və onların təsirini azaltmaq üçün uyğun üsullar tətbiq edin.\n",
    "\n",
    "**Outlier aşkarlanması və onların idarə edilməsi:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# LoanAmount sütunu üçün outlier-ları yoxla\n",
    "Q1 = df['LoanAmount'].quantile(0.25)\n",
    "Q3 = df['LoanAmount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Outlier-ları filtr etmə\n",
    "outliers = (df['LoanAmount'] < (Q1 - 1.5 * IQR)) | (df['LoanAmount'] > (Q3 + 1.5 * IQR))\n",
    "df = df[~outliers]\n",
    "```        \n",
    "             \n",
    "### 8. **Target Variableın İdarə Edilməsi**\n",
    "   - **Label Encoding**: Kreditin təsdiqi dəyişənini (Loan_Status) 0 və 1 ilə təmsil edin.\n",
    "        \n",
    "```python\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "```      \n",
    "        \n",
    "### 9. Data Split\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### 10. Cross-validation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "```\n",
    "\n",
    "### 11. Feature Importance Analizi\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Xüsusiyyətlərin əhəmiyyəti\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(feature_importance)\n",
    "```\n",
    "\n",
    "Bu addımları izləyərək, loan eligibility prediction layihəsində data preparation mərhələsini tamamlaya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab60505b",
   "metadata": {},
   "source": [
    "## Men burda bezi data preparation isleri gormisem.  ne yaddan cixib elemimisem sen onlari izah ele ve kodu yaz. Yani sonra men model quracam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d7cd",
   "metadata": {},
   "source": [
    "Mən notebook-dan aşağıdakı data preparation addımlarını aşkar etdim. Gördüyünüz işlər bunlardır:\n",
    "\n",
    "### 1. **Verilənlərin Yüklənməsi**\n",
    "   İlk addım olaraq, verilənlər `loan_prediction.csv` faylından yüklənib və `df` DataFrame-i yaradılıb.\n",
    "\n",
    "### 2. **Sütun Adlarının və Məlumatın Araşdırılması**\n",
    "   `df.columns` və `df.info()` metodları istifadə edilərək verilənlərin sütun adları və strukturu araşdırılıb.\n",
    "\n",
    "### 3. **Verilənlərin İzlənməsi**\n",
    "   - **Dəyərlərin sayılması**: `Dependents` və `Loan_Status` sütunlarında dəyərlərin sayı yoxlanılıb (`value_counts`).\n",
    "   - **Balansın yoxlanılması**: `Loan_Status` sütununda balansın olub-olmadığı yoxlanılıb. Görünür, 192/612 və 422/612 hesabatları göstərir ki, balans problemi yoxdur.\n",
    "\n",
    "### 4. **Boş Dəyərlərin Yoxlanılması və Doldurulması**\n",
    "   - **Boş Dəyərlərin Araşdırılması**: `df.isnull().sum()` metodu ilə dataset-də boş dəyərlər yoxlanılıb.\n",
    "   - **Boş Dəyərlərin Doldurulması**: Aşağıdakı sütunlarda boş dəyərlər doldurulub:\n",
    "     - `Gender`: Mod (ən çox təkrarlanan) dəyər ilə doldurulub.\n",
    "     - `Married`, `Dependents`, `Self_Employed`, `Credit_History`: Bu sütunlar da mod dəyəri ilə doldurulub.\n",
    "\n",
    "### 5. **Hələ Yadınızdan Çıxmış Ola Bilən Data Preparation Addımları**\n",
    "   - **Outlier Detection**: Məsələn, `ApplicantIncome` və `LoanAmount` üçün outlier-lərin yoxlanılması və ya uyğunlaşdırılması (log transformasiya və s.).\n",
    "   - **Sütunların Normalizasiyası/Standartizasiyası**: `ApplicantIncome`, `LoanAmount` kimi numeric dəyişənlər üçün normalizasiya və ya standartizasiya aparıla bilər.\n",
    "   - **Categorical Encoding**: Kategorik dəyişənlər (`Gender`, `Married`, `Education`, `Property_Area`, və s.) üçün one-hot encoding və ya label encoding tətbiq oluna bilər.\n",
    "   - **Feature Engineering**: Yeni dəyişənlərin yaradılması, məsələn, `TotalIncome` (müraciət edən və birgə müraciət edənin gəliri toplamı).\n",
    "\n",
    "Bu addımları tətbiq etmək üçün sizə uyğun kodları təqdim edə bilərəm. Hansı xüsusi addımlar haqqında məlumat əldə etmək istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133173aa",
   "metadata": {},
   "source": [
    "Aşağıda, qeyd etdiyiniz dataya əlavə olaraq həyata keçirə biləcəyiniz data preparation addımlarını təqdim edirəm:\n",
    "\n",
    "### 1. **Outlier-ların Yoxlanılması və Müxtəlifləşdirilməsi**\n",
    "Numeric dəyişənlər (`ApplicantIncome`, `LoanAmount`) üçün outlier-ların olub-olmadığını yoxlayıb, ehtiyac varsa log transformasiya tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ApplicantIncome üçün outlier-ların yoxlanılması\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['ApplicantIncome'])\n",
    "plt.title('ApplicantIncome Outliers')\n",
    "\n",
    "# Log transformasiya ilə outlier-ların uyğunlaşdırılması\n",
    "df['Log_ApplicantIncome'] = np.log1p(df['ApplicantIncome'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_ApplicantIncome'])\n",
    "plt.title('Log Transformed ApplicantIncome')\n",
    "plt.show()\n",
    "\n",
    "# LoanAmount üçün eyni proseduru həyata keçirə bilərsiniz\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)  # Boş dəyərləri median ilə doldurmaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['LoanAmount'])\n",
    "plt.title('LoanAmount Outliers')\n",
    "\n",
    "df['Log_LoanAmount'] = np.log1p(df['LoanAmount'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_LoanAmount'])\n",
    "plt.title('Log Transformed LoanAmount')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. **Categorical Encoding (One-Hot Encoding)**\n",
    "Kategorik dəyişənlər üçün one-hot encoding tətbiq edərək, onları model üçün istifadə edə biləcəyiniz numerik formaya çevirmək.\n",
    "\n",
    "```python\n",
    "# One-hot encoding tətbiq etmək\n",
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents'], drop_first=True)\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 3. **Yeni Dəyişənlər Yaratmaq (Feature Engineering)**\n",
    "Müraciət edən şəxsin və birgə müraciət edən şəxsin gəlirini toplam olaraq yeni bir dəyişən yaratmaq.\n",
    "\n",
    "```python\n",
    "# TotalIncome adlı yeni bir dəyişən yaratmaq\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "\n",
    "# TotalIncome üçün log transformasiya tətbiq etmək\n",
    "df['Log_TotalIncome'] = np.log1p(df['TotalIncome'])\n",
    "\n",
    "# Vizual olaraq log transformasiyanın təsirini yoxlamaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['TotalIncome'])\n",
    "plt.title('TotalIncome Outliers')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_TotalIncome'])\n",
    "plt.title('Log Transformed TotalIncome')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4. **Numeric Dəyərlərin Normalizasiyası və ya Standartizasiyası**\n",
    "Numeric dəyişənlər üçün normalizasiya (Min-Max scaling) və ya standartizasiya (Z-score normalization) tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Numeric dəyişənləri müəyyənləşdirin\n",
    "numeric_features = ['Log_ApplicantIncome', 'Log_LoanAmount', 'Log_TotalIncome']\n",
    "\n",
    "# Standartizasiya tətbiq etmək\n",
    "scaler = StandardScaler()\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Bu addımların hamısı datanızı model üçün hazır etmək məqsədi daşıyır. Bu addımlardan sonra model qurma mərhələsinə keçə bilərsiniz. Hansısa spesifik addım haqqında daha çox məlumat və ya əlavə izah istəsəniz, mənə bildirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa7140f",
   "metadata": {},
   "source": [
    "## Mene paired sample t test et bu iki sutunu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eab99b",
   "metadata": {},
   "source": [
    "Paired sample t-test, eyni subyektlərin iki müstəqil dəyişən üzərində ölçülən nəticələrinin statistik olaraq əhəmiyyətli fərqini yoxlamaq üçün istifadə olunur. Lakin sizin nümunədə `Loan_Status` (binary) və `ApplicantIncome` (numeric continuous) dəyişənləri arasında birbaşa `paired t-test` tətbiq etmək məntiqli olmayacaq, çünki `Loan_Status` müstəqil dəyişən, `ApplicantIncome` isə davamlı dəyişəndir. \n",
    "\n",
    "Bununla belə, eyni subyektlərdə iki ölçüm arasındakı fərqi təhlil etmək üçün `paired t-test` istifadə oluna bilər. Məsələn, iki müxtəlif zaman nöqtəsində eyni subyektlərdəki `ApplicantIncome` dəyişəni ölçülə bilər. Bu halda, `paired t-test` aşağıdakı şəkildə tətbiq oluna bilər:\n",
    "\n",
    "### Python Kod Nümunəsi\n",
    "\n",
    "1. **Bir Dataset Yaradın**: İki ölçüm arasındakı fərqi təsvir etmək üçün iki fərqli gəlir sütunu yaradın.\n",
    "2. **Paired Sample t-Test Tətbiq Edin**: `scipy.stats` kitabxanasını istifadə edərək.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset yaratmaq üçün funksiyalar\n",
    "def generate_data(num_samples):\n",
    "    np.random.seed(42)\n",
    "    # Təsadüfi numeric continuous dəyərlər\n",
    "    applicant_income_1 = np.random.rand(num_samples) * 10000  # 0-10000 arasında gəlir\n",
    "    applicant_income_2 = applicant_income_1 + np.random.randn(num_samples) * 2000  # ikinci ölçüm, biraz fərqli\n",
    "    \n",
    "    # DataFrame yaratmaq\n",
    "    df = pd.DataFrame({\n",
    "        'ApplicantIncome_1': applicant_income_1,\n",
    "        'ApplicantIncome_2': applicant_income_2\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Data yaratmaq\n",
    "df = generate_data(1000)\n",
    "\n",
    "# Paired sample t-test tətbiq etmək\n",
    "t_statistic, p_value = ttest_rel(df['ApplicantIncome_1'], df['ApplicantIncome_2'])\n",
    "\n",
    "print(f'T-statistic: {t_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Korrelyasiya matrisini hesablamaq\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Korrelyasiya matrisini vizualizasiya etmək\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Açıklama\n",
    "\n",
    "1. **Data Yaratmaq**: `generate_data` funksiyası iki fərqli `ApplicantIncome` sütunu ilə bir dataset yaradır.\n",
    "2. **Paired Sample t-Test**: `ttest_rel` funksiyasını istifadə edərək iki müstəqil gəlir sütunu arasındakı fərqi test edir.\n",
    "3. **Korrelyasiya Matrisini Hesablamaq**: Yaradılan data üzərində korrelyasiya matrisini və heatmap vizualizasiyasını təqdim edir.\n",
    "\n",
    "Bu kod nümunəsi `ApplicantIncome` dəyişəninin iki fərqli ölçüm arasında əhəmiyyətli bir fərqin olub olmadığını yoxlamaq üçün `paired sample t-test` tətbiq edir. Əgər siz `Loan_Status` ilə `ApplicantIncome` arasında fərq yoxlamaq istəyirsinizsə, fərqli statistik testlər (məsələn, t-test, ANOVA) və ya metodlar istifadə etməlisiniz, çünki bu iki dəyişən fərqli növlərdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9586093",
   "metadata": {},
   "source": [
    "## plt.hist(np.log(df['CoapplicantIncome'] + 1))\n",
    "* niye +1 edirik?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d96f9",
   "metadata": {},
   "source": [
    "`np.log(df['CoapplicantIncome'] + 1)` əməliyyatında `+1` etməyin səbəbi, logaritma funksiyasının tərifinə görə sıfır və mənfi dəyərlərin logaritmı tərif olunmur. Əgər `CoapplicantIncome` sütununda sıfır dəyəri varsa, `np.log(0)` undefined (müəyyən edilməmiş) olacaq və bu da səhvlərə səbəb ola bilər.\n",
    "\n",
    "`+1` əlavə etməklə, bütün dəyərlər ən azı 1 olacaq və logaritmik transformasiyanın tətbiqi mümkün olacaq. Bu həmçinin dəyərləri bir qədər dəyişir, lakin ümumi tendensiyanı və ya datanın strukturu əhəmiyyətli dərəcədə dəyişmir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3901c039",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansina random_state vermek lazimdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bcab5",
   "metadata": {},
   "source": [
    "Boosting modellərində `random_state` parametri modelin təkrarlanabilməsini təmin etmək üçün istifadə olunur. Bu, təlim zamanı təsadüfi elementlərin (məsələn, təlim məlumatlarının təsadüfi seçilməsi, təsadüfi başlanğıc ağırlıqları və s.) nəzarət edilməsi üçün bir toxum dəyəri (seed value) təmin edir.\n",
    "\n",
    "### Hər hansı bir `random_state` dəyəri təyin edərkən:\n",
    "\n",
    "- **Sabit bir dəyər təyin etmək** (məsələn, `random_state=42`): Təlim və test proseslərinin hər dəfə eyni nəticəni verməsini təmin edər. Bu, modeli təkrarlamaq və nəticələri yoxlamaq üçün faydalıdır.\n",
    "  \n",
    "- **`random_state`-i `None` olaraq buraxmaq**: Modelin hər dəfə fərqli bir təsadüfi dəyər seçməsinə səbəb olar. Bu da modelin hər dəfə fərqli nəticələr verməsi ilə nəticələnir.\n",
    "\n",
    "### Fərqli Boosting Modellərində `random_state`:\n",
    "\n",
    "1. **XGBoost (`xgboost.XGBClassifier` və ya `xgboost.XGBRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Sabit bir dəyər təyin edərək (məsələn, `random_state=42`), modeli təkrarlamaq mümkündür.\n",
    "\n",
    "2. **LightGBM (`lightgbm.LGBMClassifier` və ya `lightgbm.LGBMRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Burada da sabit bir dəyər təyin etməklə modeli təkrarlamaq mümkündür.\n",
    "\n",
    "3. **CatBoost (`catboost.CatBoostClassifier` və ya `catboost.CatBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Eyni təkrarlanabilirlik üçün sabit bir dəyər istifadə edilə bilər.\n",
    "\n",
    "4. **AdaBoost (`sklearn.ensemble.AdaBoostClassifier` və ya `sklearn.ensemble.AdaBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Təkrarlanabilirlik üçün sabit bir dəyər verilməsi məsləhətlidir.\n",
    "\n",
    "### Hansı `random_state` təyin edilməlidir?\n",
    "\n",
    "- **Sabit dəyər olaraq `42`**: Bu, elm sahəsində çox yaygın olaraq istifadə olunur və təkrarlanabilirlik təmin etmək üçün yaxşı bir seçimdir.\n",
    "- **Başqa bir sabit dəyər**: Əgər xüsusi olaraq fərqli bir toxum dəyəri istəyirsinizsə, istənilən ədəd seçilə bilər.\n",
    "- **`None` olaraq buraxmaq**: Hər dəfə təsadüfi dəyərlər istəyirsinizsə, `random_state=None` olaraq buraxıla bilər.\n",
    "\n",
    "Nəticədə, hansı `random_state` istifadə edəcəyiniz sizin proyektinizin tələblərindən asılıdır. Əgər nəticələrin hər dəfə təkrarlanmasını istəyirsinizsə, sabit bir `random_state` dəyəri təyin etmək yaxşı bir təcrübədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f53ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad97c34",
   "metadata": {},
   "source": [
    "## [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "\n",
    "## Nece edim bu warning olmasin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee933e3",
   "metadata": {},
   "source": [
    "Bu xəbərdarlıqlar, `LightGBM` modelini təlim etdirərkən yaranır və bu xəbərdarlıqların əsas səbəbi budur ki, model təlim zamanı bəzi node-lar üçün (qovşaqlar üçün) mənfi və ya sıfır dəyərində \"gain\" əldə edir. Bu halda, əlavə bir split (qovşaqda yeni bir bölmə) yaratmaq mənasız olur, çünki bu heç bir yaxşılaşma gətirməyəcək.\n",
    "\n",
    "### Bu xəbərdarlıqların səbəbləri:\n",
    "\n",
    "1. **Məlumatların Paylanması**: Məlumatların dəyər spektri çox dar ola bilər və ya bəzi dəyərlər çox nadir ola bilər. Bu halda, hər hansı bir split üçün müsbət \"gain\" əldə etmək çətin ola bilər.\n",
    "\n",
    "2. **Parametr Dəyərləri**: Modelin parametr dəyərləri (məsələn, `min_child_samples`, `min_split_gain`, `max_depth`) çox sərt olaraq təyin edilmiş ola bilər, bu da modelin çox sayda split yaratmasına maneə ola bilər.\n",
    "\n",
    "3. **Overfitting**: Model təlim zamanı çox sayda split etməyə çalışarkən overfitting-i önləmək üçün bu xəbərdarlıqları verir.\n",
    "\n",
    "### Çözüm Yolları:\n",
    "\n",
    "1. **Parametrləri Yenidən Dəyərləndirin**:\n",
    "   - `min_child_samples` dəyərini azaldaraq daha çox split imkanı verə bilərsiniz.\n",
    "   - `min_split_gain` dəyərini azaldaraq daha kiçik split-ləri icazə verə bilərsiniz.\n",
    "   - `max_depth` dəyərini artırmaq da bəzi hallarda kömək edə bilər.\n",
    "\n",
    "2. **Məlumatları Təkrardan Analiz Edin**:\n",
    "   - Məlumatlarınızı yoxlayın və fərqli qruplar arasında balansı təmin etməyə çalışın.\n",
    "   - Ehtiyac varsa, məlumatları transformasiya edərək (məsələn, log transformasiyası) modelin daha yaxşı öyrənməsi üçün şərait yaradın.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Məlumat dəstinizə əlavə xüsusiyyətlər əlavə etməyi və ya bəzi xüsusiyyətləri yenidən yaratmağı nəzərdən keçirə bilərsiniz. Bu, modelin daha yaxşı öyrənməsinə kömək edə bilər.\n",
    "\n",
    "Bu xəbərdarlıqlar modelin nəticələrinə birbaşa mənfi təsir göstərməyə bilər, ancaq məlumatların və parametrlərin diqqətlə incələnməsi modelin performansını artırmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082dd5a4",
   "metadata": {},
   "source": [
    "LightGBM modelini qurarkən bu xəbərdarlıqların qarşısını almaq üçün bəzi parametrləri aşağıdakı kimi tənzimləyə bilərsiniz:\n",
    "\n",
    "1. **`min_split_gain`** dəyərini azaldaraq daha kiçik split-ləri icazə vermək.\n",
    "2. **`min_child_samples`** dəyərini artırmaq, beləliklə model yalnız müəyyən sayda nümunələr üçün split edər.\n",
    "3. **`max_depth`** dəyərini artırmaq və ya başqa dəyərlərini tənzimləməklə modelin daha yaxşı performans göstərməsini təmin etmək.\n",
    "\n",
    "Aşağıda bir nümunə LightGBM kodu verilmişdir:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',  # Klassifikasiya üçün\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,  # Split üçün minimum \"gain\"\n",
    "    min_child_samples=20,  # Split üçün minimum nümunə sayı\n",
    "    max_depth=7,  # Maksimum dərinlik\n",
    "    random_state=42  # Reproducibility üçün\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### İpuçları:\n",
    "- **`min_split_gain`**: Bu dəyəri artırmaq splitlərin daha da mənalı olmasını təmin edəcək, beləliklə bu xəbərdarlıqların sayı azalacaq.\n",
    "- **`min_child_samples`**: Bu dəyəri artırmaq, çox kiçik nümunələr üçün split yaratmamaq üçün istifadə edilə bilər.\n",
    "- **`max_depth`**: Dərinliyi artırmaq, modelin daha çox split etməsinə icazə verəcək, ancaq overfitting riskini də nəzərə almaq lazımdır.\n",
    "\n",
    "Bu parametrlər xəbərdarlıqların qarşısını ala bilər və ya sayını azalda bilər. Ayrıca, bu parametrləri layihənizin spesifik tələblərinə uyğun tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13485373",
   "metadata": {},
   "source": [
    "## yene warning gelir...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492efc3",
   "metadata": {},
   "source": [
    "LightGBM-də xəbərdarlıqları azaltmaq üçün `min_split_gain`, `min_child_samples` və `max_depth` kimi parametrləri dəyişdirmək kifayət etmədikdə, digər variantları da yoxlamaq olar. Xəbərdarlıqları tamamilə aradan qaldırmaq üçün əlavə parametrlərdən istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`verbosity` və ya `verbose` Parametrindən İstifadə Edin:**\n",
    "LightGBM modellərində `verbosity` (və ya `verbose`) parametrini istifadə edərək xəbərdarlıqların səviyyəsini tənzimləyə bilərsiniz. Xəbərdarlıqların qarşısını almaq üçün `verbosity` dəyərini `-1` təyin edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1  # Warning-ları tamamilə söndürmək\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### 2. **`early_stopping_rounds` İstifadə Edərək Overfitting-i Azaltmaq:**\n",
    "`early_stopping_rounds` parametrini istifadə edərək, təlim zamanı modelin lazımsız splitlər etməsinin qarşısını ala bilərsiniz. Bu, xəbərdarlıqların azalmasına kömək edə bilər.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "```\n",
    "\n",
    "### 3. **`min_child_weight` Parametrini Dəyişdirin:**\n",
    "`min_child_weight` parametrini artırmaqla xəbərdarlıqların qarşısını almaq mümkündür. Bu, splitlərin daha da məntiqli olmasını təmin edəcək.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=1.0,  # Dəyəri artırmaqla xəbərdarlıqların qarşısını almaq\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri birləşdirərək və tənzimləyərək xəbərdarlıqların sayını azalda bilərsiniz. Ancaq `verbosity=-1` parametri bu xəbərdarlıqları tamamilə gizlədəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc75a",
   "metadata": {},
   "source": [
    "## SVC \n",
    "* kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’,  \n",
    "* C : float,  default=1.0,  \n",
    "* degree: int, default=3, \n",
    "* gamma: {‘scale’, ‘auto’} or float, default=’scale’  \n",
    "### Bu parameterleri haqqinda genis meulumat yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd3460",
   "metadata": {},
   "source": [
    "SVC (Support Vector Classifier) modelində **kernel** funksiyası, verilənlər arasındakı qeyri-xətti əlaqələri müəyyən etmək üçün istifadə olunur. Kernel funksiyası, verilənlərin daha yüksək ölçülü bir fəzaya proyeksiya edilməsini təmin edir, beləliklə verilənlər xətti olaraq ayrıla bilməyən hallarda belə effektiv bir ayrıcı xətt tapıla bilir.\n",
    "\n",
    "### Kernel Növləri\n",
    "\n",
    "#### 1. **Linear Kernel (‘linear’)**\n",
    "- **Təsvir:** Verilənlər xətti olaraq ayrılabilirsə, linear kernel istifadə olunur. Xətti kernel SVC modelində xüsusiyyətlərin (features) xətti birləşməsini hesablayır.\n",
    "- **Tətbiq Sahəsi:** Sadə və xətti olan verilənlər üçün effektivdir. Məsələn, eyni tipli verilənlər iki sinif arasında çoxluqda ayrılabilirsə, linear kernel uyğundur.\n",
    "- **Riyazi İzah:** Xətti kernel funksiyası \\( K(x_i, x_j) = x_i^T x_j \\) olaraq göstərilir. Burada \\( x_i \\) və \\( x_j \\) verilən nöqtələrdir.\n",
    "\n",
    "#### 2. **Polynomial Kernel (‘poly’)**\n",
    "- **Təsvir:** Polinomial kernel, verilənlər arasında qeyri-xətti əlaqələri təyin etmək üçün istifadə olunur. Bu kernel funksiyası, xüsusiyyətlərin çoxsahəli kombinasiya edilərək daha kompleks bir model yaratmağa imkan verir.\n",
    "- **Tətbiq Sahəsi:** Verilənlərdə dərəcəli bir münasibət varsa (məsələn, bir sinifin xüsusiyyətləri digərindən polinomial olaraq asılıdırsa), bu kernel effektivdir.\n",
    "- **Riyazi İzah:** Polinomial kernel funksiyası \\( K(x_i, x_j) = (x_i^T x_j + c)^d \\) olaraq göstərilir. Burada \\( d \\) polinomial dərəcədir, \\( c \\) isə sabitdir.\n",
    "\n",
    "#### 3. **RBF Kernel (Radial Basis Function, ‘rbf’)**\n",
    "- **Təsvir:** RBF kernel funksiyası, xüsusən verilənlər arasında kompleks və qeyri-xətti əlaqələr varsa, çox güclü bir seçimdir. Bu kernel funksiyası, hər bir verilən nöqtənin məsafəsini nəzərə alaraq, daha yüksək ölçülü fəzada onların təsir sahəsini genişləndirir.\n",
    "- **Tətbiq Sahəsi:** Çoxlu xüsusiyyətləri olan və onların arasında əlaqənin mürəkkəb olduğu verilənlərdə RBF kernel çox istifadə edilir.\n",
    "- **Riyazi İzah:** RBF kernel funksiyası \\( K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\) olaraq göstərilir. Burada \\( \\gamma \\) kernelin təsir sahəsini təyin edən parametrdir.\n",
    "\n",
    "#### 4. **Sigmoid Kernel (‘sigmoid’)**\n",
    "- **Təsvir:** Sigmoid kernel, sinir şəbəkələrindəki aktivasiya funksiyasına bənzər bir funksiyadır və iki verilən nöqtə arasındakı məsafəni qeyri-xətti olaraq modelləşdirir.\n",
    "- **Tətbiq Sahəsi:** Sigmoid kernel, xüsusən logistik əlaqələr olan verilənlərdə effektivdir. Bu kernel logistik reqlamentasiya və sinir şəbəkələri ilə oxşarlıq göstərir.\n",
    "- **Riyazi İzah:** Sigmoid kernel funksiyası \\( K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c) \\) olaraq göstərilir. Burada \\( \\alpha \\) və \\( c \\) tənzimləmə parametrləridir.\n",
    "\n",
    "#### 5. **Precomputed Kernel (‘precomputed’)**\n",
    "- **Təsvir:** Bu seçimdə kernel funksiyası əvvəlcədən hesablanır və SVC modelinə ötürülür. Bu, xüsusən xüsusi kernel funksiyaları istifadə edildikdə və ya verilənlərin çox olduğu hallarda istifadə olunur.\n",
    "- **Tətbiq Sahəsi:** Kernel matrisini əvvəldən hesablamağa ehtiyac olduğu hallarda və ya xüsusi kernel funksiyaları tətbiq edilən vəziyyətlərdə istifadə edilir.\n",
    "\n",
    "### Kernel Funksiyalarının Seçimi\n",
    "\n",
    "- **Linear kernel** verilənlərdə sadə və xətti bir əlaqə varsa uyğundur.\n",
    "- **Polynomial kernel** qeyri-xətti, lakin mürəkkəb olmayan münasibətləri modelləşdirmək üçün istifadə olunur.\n",
    "- **RBF kernel** daha mürəkkəb və qeyri-xətti verilənlərdə optimal performans göstərir.\n",
    "- **Sigmoid kernel** logistik münasibətləri modelləşdirmək üçün yaxşı seçimdir.\n",
    "- **Precomputed kernel** xüsusi kernel funksiyaları istifadə edilərkən və ya kernel matrixini əvvəlcədən hesablamaq istədikdə istifadə olunur.\n",
    "\n",
    "### Qrafik Təsviri\n",
    "Kernel funksiyalarının hər birinin təsirini göstərən qrafiklərlə izah edə bilərəm. Məsələn, verilənlərə tətbiq edilən linear, polynomial və RBF kernel funksiyalarının qərar sərhədlərini necə dəyişdiyini göstərə bilərəm. Bu, müxtəlif kernel növlərinin verilənlər üzərində necə işlədiyini və fərqli təsir göstərdiyini daha aydın şəkildə başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb9972",
   "metadata": {},
   "source": [
    "## C parametri\n",
    "C parametri, Support Vector Classifier (SVC) modelində cəza tənzimləyicisi (regularization parameter) olaraq istifadə olunur. Bu parametr modelin margin (ayırıcı xəttin yaxınlığında olan nümunələr) genişliyi ilə səhvlər arasında bir balans yaradır. C parametri aşağıdakı iki əsas amilə təsir edir:\n",
    "\n",
    "### 1. **Margin Genişliyi və Klassifikasiya Səhvləri**\n",
    "C parametri marginin genişliyini və modelin səhvlərə olan dözümlülüyünü tənzimləyir:\n",
    "\n",
    "- **Kiçik C Dəyəri (C < 1):** \n",
    "  - Model daha geniş margin yaratmağa çalışır. Bu, daha çox səhv qəbul edilməsinə səbəb olur, lakin model daha ümumiləşdirilmiş (generalized) olur. Yəni, model overfitting (həddindən artıq uyğunlaşma) riskini azaldır.\n",
    "  - Məsələn, qərar sərhədi çox yaxın olan iki fərqli sinif varsa, kiçik C dəyəri ilə model bu sərhədin hər iki tərəfindəki bəzi səhvləri göz ardı edərək daha hamar bir ayrıcı xətt çəkər.\n",
    "\n",
    "- **Böyük C Dəyəri (C > 1):**\n",
    "  - Model səhvləri minimuma endirmək üçün daha dar margin yaratmağa çalışır. Bu, modelin training setinə çox uyğunlaşmasına və overfitting etməsinə səbəb ola bilər.\n",
    "  - Böyük C dəyərində model hər bir səhvi ciddi şəkildə cəzalandırır və səhvləri aradan qaldırmaq üçün qərar sərhədini həmin səhvlərə uyğunlaşdırır.\n",
    "\n",
    "### 2. **Overfitting və Underfitting**\n",
    "- **Kiçik C Dəyəri:** Underfitting riski artır, yəni model verilənlərə kifayət qədər uyğunlaşmır və bəzi məlumat nümunələrini düzgün klassifikasiya edə bilmir.\n",
    "- **Böyük C Dəyəri:** Overfitting riski artır, yəni model verilənlərə həddindən artıq uyğunlaşır və yeni verilənlərə ümumiləşdirməkdə çətinlik çəkir.\n",
    "\n",
    "### Qrafik İzahı\n",
    "Qrafikdə C parametri ilə margin genişliyi arasındakı əlaqəni belə göstərə bilərik:\n",
    "\n",
    "- **Kiçik C dəyərində** margin daha geniş olacaq, lakin bəzi nümunələr marginin içində qalacaq (yəni səhvlər göz ardı ediləcək).\n",
    "- **Böyük C dəyərində** margin daha dar olacaq, nümunələr marginə çox yaxın olacaq və model hər bir nümunəni düzgün klassifikasiya etməyə çalışacaq.\n",
    "\n",
    "Bu izahı qrafiklə də göstərməyimi istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d7378",
   "metadata": {},
   "source": [
    "## Margin genisliyi\n",
    "**Margin**, Support Vector Classifier (SVC) modelində verilənlər sinifləri arasındakı ən yaxın nöqtələrə qədər olan məsafədir. Daha dəqiq desək, margin iki sinif arasında çəkilmiş ayrıcı xətt (decision boundary) ilə həmin siniflərə aid olan ən yaxın nümunələr (support vectors) arasındakı məsafəni ifadə edir. SVC modelində əsas məqsəd bu margin-i maksimuma çatdırmaqdır, çünki daha böyük bir margin ümumiləşdirilmənin daha yaxşı olmasına və modelin daha yaxşı performans göstərməsinə səbəb ola bilər.\n",
    "\n",
    "### Margin-in Əsas Məqamları:\n",
    "\n",
    "1. **Maksimum Margin:** \n",
    "   - SVC modelinin əsas məqsədi verilən iki sinif arasındakı margin-i maksimuma çatdırmaqdır. Yəni, ayrıcı xətt bu sinifləri mümkün qədər uzaq tutmalıdır. Bu, modelin daha çox ümumiləşdirmə qabiliyyətinə sahib olmasına kömək edir.\n",
    "   - Məsələn, aşağıdakı şəkildə iki sinif arasındakı ayrıcı xətt ən geniş margin-ə malikdir və bu xətt margin-i maksimum edən ayrıcı xətdir.\n",
    "\n",
    "2. **Support Vectors:** \n",
    "   - Margin-i təyin edən verilən nöqtələrə **support vectors** deyilir. Bu nöqtələr margin-in sərhədlərini müəyyənləşdirir. SVC modeli bu nöqtələr üzərində işləyir və digər nöqtələr margin-in genişliyini təsir etmir.\n",
    "   - Əgər support vectors modelləşdirilmə zamanı dəyişərsə, margin və ayrıcı xətt də dəyişə bilər.\n",
    "\n",
    "3. **Margin-in Genişliyi və Performans:**\n",
    "   - Daha geniş bir margin, verilənləri daha yaxşı ayırır və bu, overfitting riskini azaldır, çünki model daha ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "   - Əgər margin çox dardırsa, model verilənlərə çox yaxınlaşır və overfitting riski artır, yəni model training setində yaxşı performans göstərsə də, yeni verilənlərdə zəif performans göstərə bilər.\n",
    "\n",
    "4. **Margin və `C` Parametri:**\n",
    "   - `C` parametri margin-in ölçüsünü idarə edir. Kiçik `C` dəyəri daha geniş margin-i təşviq edir və daha az səhv nümunələrə icazə verir. Yüksək `C` dəyəri isə margin-i daraldır və modelin verilənlərə daha çox uyğunlaşmasına səbəb olur, lakin overfitting riski artır.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "Bir qrafikdə margin-i təsvir etmək olar:\n",
    "- İki sinif nöqtələrinin olduğu bir qrafikdə, bir ayrıcı xətt (decision boundary) çəkilir və bu xəttin hər iki tərəfində margin göstərilir.\n",
    "- Ayrıcı xəttə ən yaxın olan nöqtələr support vectors olaraq göstərilir.\n",
    "- Margin-in ölçüsü bu nöqtələr ilə ayrıcı xətt arasındakı məsafə ilə təyin olunur.\n",
    "\n",
    "İstəyirsinizsə, margin və support vectors-i göstərən belə bir qrafik hazırlaya bilərəm. Bu, margin-in necə işlədiyini və onun model performansına necə təsir etdiyini daha aydın izah edəcək."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad67c5",
   "metadata": {},
   "source": [
    "## degree parametri\n",
    "`degree` parametri, Support Vector Classifier (SVC) modelində yalnız **polinomial kernel** (`'poly'`) üçün istifadə edilir və polinomial kernelin dərəcəsini (degree) təyin edir. Bu parametr verilənlərin xüsusiyyətlərini daha yüksək dərəcədə olan bir fəzaya proyeksiya etməyə imkan verir, beləliklə verilənlərdəki qeyri-xətti əlaqələri daha yaxşı modelləşdirmək mümkündür.\n",
    "\n",
    "### `degree` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Polinomial Kernelin Kompleksliyi**\n",
    "- **Kiçik dərəcə (d = 2, 3):**\n",
    "  - Nisbətən sadə, lakin qeyri-xətti əlaqələri modelləşdirmək üçün uyğundur.\n",
    "  - Məsələn, iki sinif arasında sadə, lakin qeyri-xətti bir sərhəd varsa, dərəcə 2 və ya 3 istifadə edilə bilər. Bu vəziyyətdə polinomial kernel ikinci və ya üçüncü dərəcəli polinomlara əsaslanacaq.\n",
    "\n",
    "- **Böyük dərəcə (d > 3):**\n",
    "  - Daha kompleks və mürəkkəb əlaqələri modelləşdirmək üçün istifadə olunur.\n",
    "  - Dərəcənin artırılması, kernel funksiyasının daha yüksək dərəcəli polinomlara əsaslanmasını təmin edir və beləliklə, çoxsaylı xüsusiyyətlər arasındakı mürəkkəb əlaqələri modelləşdirir.\n",
    "\n",
    "#### 2. **Overfitting Riski**\n",
    "- **Yüksək `degree` (dərəcə) dəyəri:**\n",
    "  - Overfitting riski artır, çünki model training datasetindəki hər bir nümunəni düzgün uyğunlaşdırmağa çalışır. Bu, modelin training setinə çox yaxşı uyğunlaşması, lakin yeni verilənlərdə ümumiləşdirmədə çətinlik çəkməsi deməkdir.\n",
    "  - Məsələn, çox yüksək dərəcəli polinomial kernel hər bir nümunə üçün fərdi qərar sərhədləri çəkə bilər, lakin bu, modelin daha sonra səhvlərlə üzləşməsinə səbəb ola bilər.\n",
    "\n",
    "- **Aşağı `degree` (dərəcə) dəyəri:**\n",
    "  - Underfitting riski artır, çünki model verilənlərdəki daha mürəkkəb əlaqələri düzgün modelləşdirə bilmir. Bu halda, model siniflər arasındakı daha kompleks münasibətləri nəzərə almadan sadə bir sərhəd çəkə bilər.\n",
    "\n",
    "#### 3. **Dərəcənin Seçilməsi**\n",
    "- `degree` parametrini seçərkən verilənlərin xüsusiyyətlərinə və onların arasındakı əlaqələrə diqqət yetirmək vacibdir.\n",
    "- Nəzərə alınmalıdır ki, dərəcənin artırılması modelin kompleksliyini və hesablama tələblərini artıracaq.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Aşağı dərəcə** (d = 2): Polinomial kernel, iki sinif arasında nisbətən sadə və əyri bir sərhəd çəkəcək.\n",
    "- **Orta dərəcə** (d = 3): Kernel funksiyası daha mürəkkəb, lakin hələ də idarə edilə bilən bir sərhəd çəkəcək.\n",
    "- **Yüksək dərəcə** (d > 3): Kernel funksiyası çox mürəkkəb bir sərhəd çəkəcək, bu da overfittingə səbəb ola bilər.\n",
    "\n",
    "Bu təsiri qrafiklərlə göstərmək üçün, müxtəlif dərəcələr üçün polinomial kernelin necə işlədiyini qrafiklərlə göstərə bilərəm. Bu cür qrafiklər dərəcə parametri ilə decision boundary-nin necə dəyişdiyini daha yaxşı başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9557d7f",
   "metadata": {},
   "source": [
    "## gamma parametri\n",
    "`gamma` parametri, Support Vector Classifier (SVC) modelində **RBF (Radial Basis Function)**, **polinomial**, və **sigmoid** kernel funksiyaları üçün istifadə edilir. Bu parametr, verilənlər arasında məsafənin təsirini tənzimləyir və modelin qərar sərhədlərinin nə qədər yaxın və ya geniş olacağını müəyyən edir.\n",
    "\n",
    "### `gamma` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Verilənlər Arasındakı Məsafə**\n",
    "- **Yüksək `gamma` dəyəri:**\n",
    "  - Verilənlərdəki hər bir nöqtənin təsir sahəsini daraldır, yəni hər bir nöqtənin təsiri çox yaxın məsafədə olur. Bu, modelin verilənlərə çox yaxşı uyğunlaşmasına və beləliklə, overfitting etməsinə səbəb ola bilər.\n",
    "  - Bu halda, model hər bir nümunəni çox yaxından izləyir və qərar sərhədləri çox qısa məsafələrdə dəyişir.\n",
    "  \n",
    "- **Aşağı `gamma` dəyəri:**\n",
    "  - Verilənlərdəki nöqtələrin təsir sahəsi genişlənir, yəni hər bir nöqtənin təsiri daha uzaq məsafədə hiss olunur. Bu, modelin daha hamar və ümumiləşdirilmiş bir sərhəd yaratmasına imkan verir.\n",
    "  - Bu vəziyyətdə, model fərdi nümunələrə çox yaxınlaşmır və daha geniş, ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "\n",
    "#### 2. **Overfitting və Underfitting**\n",
    "- **Yüksək `gamma`:** \n",
    "  - Model verilənlərə çox uyğunlaşır (overfitting), bu da training setində yüksək dəqiqlik əldə edə bilər, lakin yeni verilənlərə qarşı zəif performans göstərə bilər.\n",
    "  - Qərar sərhədi çox qarmaqarışıq və mürəkkəb olur, belə ki, hətta kiçik fərqlər də sərhədi dəyişə bilər.\n",
    "  \n",
    "- **Aşağı `gamma`:** \n",
    "  - Model verilənlərə kifayət qədər uyğunlaşmır (underfitting), bu da training setində aşağı dəqiqlik, lakin yeni verilənlərə qarşı daha yaxşı ümumiləşdirmə ilə nəticələnə bilər.\n",
    "  - Qərar sərhədi daha sadə və hamar olur.\n",
    "\n",
    "### `gamma` Parametrinin Dəyərləri\n",
    "\n",
    "#### 1. **‘scale’ (default dəyəri):**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına və məlumatların yayılmasına əsasən avtomatik olaraq təyin olunur.\n",
    "  - Riyazi olaraq, `scale` dəyəri $  \\frac{1}{n \\times \\text{X.var}}  $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayı, `X.var` isə məlumatın dispersiyasını (variance) ifadə edir.\n",
    "  - Bu, orta bir gamma dəyəri verir və modelin çox uyğunlaşmasının (overfitting) qarşısını almaq üçün yaxşı balanslaşdırılmışdır.\n",
    "\n",
    "#### 2. **‘auto’:**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına (n_features) əsasən təyin olunur.\n",
    "  - Riyazi olaraq, `auto` dəyəri $ \\frac{1}{n} $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayıdır.\n",
    "  - Bu, daha az xüsusiyyətli verilənlər üçün uyğundur və daha geniş təsir sahəsi verir. Lakin böyük datasetlərdə underfitting riski artır.\n",
    "\n",
    "#### 3. **Float dəyəri:**\n",
    "  - `gamma` dəyərini özünüz təyin edə bilərsiniz. Məsələn, `gamma=0.1` və ya `gamma=1.0` kimi.\n",
    "  - Bu, verilənlərə xüsusi uyğunlaşdırılmış gamma dəyəri ilə daha dəqiq idarə etmə imkanı verir.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Yüksək `gamma` dəyəri:** Qərar sərhədi çox mürəkkəb və dar olacaq, hər bir nümunəyə uyğunlaşaraq sərhədin tez-tez dəyişməsinə səbəb olacaq.\n",
    "- **Aşağı `gamma` dəyəri:** Qərar sərhədi daha geniş və hamar olacaq, fərdi nümunələrin təsirindən daha az asılı olacaq.\n",
    "\n",
    "Bu təsirləri qrafiklərlə də göstərə bilərəm. `gamma` parametrinin fərqli dəyərləri üçün decision boundary-lərin necə dəyişdiyini qrafiklərlə təsvir edə bilərəm. Bu, `gamma` parametrinin modelin performansına necə təsir etdiyini daha aydın başa düşməyinizə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1eae8ac",
   "metadata": {},
   "source": [
    "## Cross validation-a X, Y verilmelidir, yoxsa X_train, Y_train ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe845f",
   "metadata": {},
   "source": [
    "Cross-validation (çarpaz doğrulama) prosesində modelin performansını qiymətləndirmək üçün **orijinal verilənlər dəsti** (`X` və `Y`) istifadə olunur. Bu prosesdə, orijinal verilənlər dəsti bir neçə alt dəstəyə bölünür, daha sonra bu alt dəstələrdən biri test dəsti kimi istifadə edilir, qalanları isə train (öyrədici) dəsti kimi istifadə olunur.\n",
    "\n",
    "### Ətraflı İzahetmə\n",
    "\n",
    "- **X və Y:** Bu, sizin bütün verilənlər dəstinizdir. `X` bağımsız dəyişənləri (features) və `Y` isə bağımlı dəyişəni (target, label) təmsil edir.\n",
    "\n",
    "- **Cross-Validation Prosesində:**\n",
    "  - Verilənlər `k` alt dəstəyə bölünür (`k`-fold cross-validation).\n",
    "  - Hər dövrədə (`fold`) bir alt dəstə test dəsti kimi istifadə olunur, qalan `k-1` alt dəstə isə train dəsti kimi istifadə olunur.\n",
    "  - Bu dövrə `k` dəfə təkrarlanır və hər dəfə fərqli bir alt dəstə test dəsti kimi istifadə edilir.\n",
    "  - Bu prosesin nəticəsində modelin performansı hər bir fold üçün ölçülür və son nəticə olaraq bu performans ölçülərinin ortalaması götürülür.\n",
    "\n",
    "**Nəticə:**\n",
    "Cross-validation tətbiq edərkən, bütün verilənlər dəstinizi (`X` və `Y`) funksiyaya ötürməlisiniz. Model bu verilənlər dəstini daxildə train və test dəstlərinə böləcək və cross-validation prosesini icra edəcək. Beləliklə, siz train/test bölünməsini özünüz etmirsiniz; bunu cross-validation avtomatik olaraq edir.\n",
    "\n",
    "### İstifadə Nümunəsi:\n",
    "\n",
    "Python-da `sklearn` kitabxanasından `cross_val_score` funksiyasını istifadə edərkən, verilənlər dəstini aşağıdakı kimi daxil edirsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Bütün verilənlər dəstinizi istifadə edin\n",
    "model = SVC(kernel='linear')\n",
    "scores = cross_val_score(model, X, Y, cv=5)\n",
    "```\n",
    "\n",
    "Burada `X` və `Y` tam verilənlər dəstidir və `cv=5` isə 5-fold cross-validation həyata keçirdiyinizi göstərir. Bu funksiyada `cross_val_score` verilənlər dəstini avtomatik olaraq train və test dəstlərinə böləcək və cross-validation-u icra edəcək.\n",
    "\n",
    "**Diqqət:** Əgər `X_train` və `Y_train` artıq bölünmüş verilənlər dəstinə sahibsinizsə və cross-validation tətbiq etmək istəyirsinizsə, o zaman həmin verilənlər dəstini də `X` və `Y` kimi istifadə edə bilərsiniz, lakin bu halda yalnız həmin bölünmüş train dəstiniz üzərində cross-validation icra ediləcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64e706b",
   "metadata": {},
   "source": [
    "## Code izah ele?\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88052caa",
   "metadata": {},
   "source": [
    "Bu kod parçası bir korelyasiya heatmap-i (istilik xəritəsi) yaratmaq üçün istifadə olunur. İndi hər sətirə nəzər salaq:\n",
    "\n",
    "### 1. Şəkil ölçüsünü təyin edir:\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "```\n",
    "- `figsize` parametri vasitəsilə çıxan şəkilin ölçüsünü (en və hündürlük) təyin edir. Burada 16x12 ölçülərində bir şəkil yaradır.\n",
    "\n",
    "### 2. Üst üçbucağı gizlətmək üçün maska yaradılır:\n",
    "```python\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "```\n",
    "- Bu sətir korelyasiya matrisinin üst üçbucaqlı hissəsini gizlətmək üçün bir maska yaradır. \n",
    "- `np.ones_like(df.corr())` matrisin bütün elementlərini 1 ilə doldurur.\n",
    "- `np.triu` funksiyası isə bu 1-lik matrisin yuxarı üçbucağını saxlayır (alt üçbucağını 0 edir).\n",
    "- Bu maska, korelyasiya matrisinin yarısını (üst üçbucağı) gizlədərək yalnız alt yarısını görməyə imkan verir.\n",
    "\n",
    "### 3. Korelyasiya heatmap-i yaradılır:\n",
    "```python\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "```\n",
    "- `df.corr()` DataFrame-in sütunları arasındakı korelyasiya matrisini qaytarır.\n",
    "- `round(df.corr(),1)` korelyasiya dəyərlərini 1 ondalık rəqəmlə yuvarlayır.\n",
    "- `mask=mask` parametri ilə yaratdığınız maska tətbiq edilir, beləliklə üst üçbucaqdakı dəyərlər gizlədilir.\n",
    "- `vmin` və `vmax` parametrləri heatmap-də minimum və maksimum dəyərləri təyin edir (burada -1 və 1 arasındadır).\n",
    "- `annot=True` parametri hüceyrələrin içində dəyərlərin göstərilməsini təmin edir.\n",
    "- `cmap='BrBG'` rəng xəritəsini təyin edir. Burada \"BrBG\" palitrası istifadə olunur.\n",
    "\n",
    "### 4. Heatmap başlığı və formatlama:\n",
    "```python\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```\n",
    "- `set_title` funksiyası heatmap-in başlığını \"Correlation Heatmap\" olaraq təyin edir.\n",
    "- `fontdict={'fontsize':18}` başlığın şrift ölçüsünü 18 olaraq təyin edir.\n",
    "- `pad=16` başlıq ilə heatmap arasında boşluğun ölçüsünü təyin edir.\n",
    "\n",
    "Bu kod ilə, DataFrame-in sütunları arasındakı korelyasiyaları asanlıqla görsəl formada təqdim edə bilərsiniz. Korelyasiya heatmap-ləri, xüsusən dəyişənlər arasındakı əlaqələri təhlil etmək üçün faydalıdır. Yuxarı üçbucağı maskalamaq isə məlumatın təkrarlanmasını və vizual qarışıqlığı azaldır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b884cb07",
   "metadata": {},
   "source": [
    "## iki feature arasinda corr 1 dir. bu ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48708488",
   "metadata": {},
   "source": [
    "İki dəyişən (feature) arasındakı korelyasiyanın (corr) 1 olması onların tam müsbət əlaqədə olduğunu göstərir. Bu vəziyyətin mənası aşağıdakılardır:\n",
    "\n",
    "1. **Tam Müsbət Xətti Əlaqə:** Korelyasiya əmsalı 1 olan iki dəyişən bir-birinə tam xətti şəkildə bağlıdır. Yəni, bir dəyişən artdıqca, digəri də eyni nisbətdə artır. Məsələn, bir dəyişən 10% artarsa, digəri də mütləq şəkildə 10% artacaq.\n",
    "\n",
    "2. **Biri Digərini Tamamilə Təyin Edir:** Əgər iki dəyişən arasında korelyasiya 1-dirsə, bu o deməkdir ki, bir dəyişənin dəyərləri digər dəyişənin dəyərlərindən tamamilə asılıdır. Bir dəyişən digərini eyni şəkildə təkrarlayır və ya onun sadə bir xətti funksiyasıdır.\n",
    "\n",
    "3. **Multikollinearlıq Problemi:** Əgər bir modeldə iki dəyişən arasında korelyasiya 1-dirsə, bu multikollinearlıq problemi yarada bilər. Multikollinearlıq modelin performansını və etibarlılığını mənfi təsir edə bilər, çünki dəyişənlərdən biri digərini tam əvəz edir, və nəticədə modeldə lazımsız təkrarlanma yaranır.\n",
    "\n",
    "### Nəticə:\n",
    "Korelyasiyanın 1 olması iki dəyişənin eyni məlumatı daşıdığı mənasına gəlir. Birini modeldə saxlamaq kifayətdir; ikincisi isə redundant ola bilər və modelin performansını yaxşılaşdırmaq üçün çıxarılmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e951b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4c22e4",
   "metadata": {},
   "source": [
    "## Bes categoryc deyerler arasinda Multikollinearlıq nece teyin etmek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4935c",
   "metadata": {},
   "source": [
    "Kategorik (category) dəyişənlər üçün tam uyğunluq və ya çox oxşarlığı müəyyən etmək numeric (rəqəmsal) dəyişənlərdəki kimi sadə deyil, lakin müəyyən üsullar və yanaşmalar mövcuddur:\n",
    "\n",
    "### 1. **Crosstab (Çapraz cədvəl) və ya Contingency Table:**\n",
    "   - İki və ya daha çox kategorik dəyişəni analiz etmək üçün `pd.crosstab()` funksiyasını istifadə edə bilərsiniz. Bu, dəyişənlər arasındakı əlaqələri görmək üçün faydalıdır.\n",
    "   - Əgər iki kategorik dəyişənin crosstab cədvəli eynidirsə, bu onların bir-birinə çox oxşar olduğunu göstərir.\n",
    "   - Məsələn, eyni dəyərləri ehtiva edən iki dəyişənin bütün crosstab dəyərləri (yəni sıxlıqları) eyni olacaq.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   ```\n",
    "\n",
    "### 2. **Khi-kvadrat Testi (Chi-Square Test):**\n",
    "   - Khi-kvadrat testi iki kategorik dəyişənin bir-birindən asılı olub-olmadığını yoxlamaq üçün istifadə olunur. Əgər iki dəyişənin arasında çox güclü bir əlaqə varsa (yüksək Chi-square dəyəri və çox aşağı p-value), bu onların bir-birinə çox oxşar olduğuna işarə edə bilər.\n",
    "   \n",
    "   ```python\n",
    "   from scipy.stats import chi2_contingency\n",
    "   contingency_table = pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "   ```\n",
    "\n",
    "### 3. **Mutual Information (Özəl İnformasiya):**\n",
    "   - Mutual Information metodu kategorik dəyişənlər arasında nə qədər informasiya paylaşdığını ölçür. Yüksək mutual information dəyəri onların güclü bir əlaqəyə malik olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.feature_selection import mutual_info_classif\n",
    "   mi = mutual_info_classif(df[['category_feature1']], df['category_feature2'])\n",
    "   print(mi)\n",
    "   ```\n",
    "\n",
    "### 4. **Label Encoding və Korelyasiya:**\n",
    "   - Kategorik dəyişənləri öncə `Label Encoding` və ya `One-Hot Encoding` ilə rəqəmsal formaya çevirərək, daha sonra onların arasında korelyasiya təhlili apara bilərsiniz. Bu, xüsusilə ordinal (sıralı) kategorik dəyişənlər üçün daha mənalı ola bilər.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "   le = LabelEncoder()\n",
    "   df['encoded_feature1'] = le.fit_transform(df['category_feature1'])\n",
    "   df['encoded_feature2'] = le.fit_transform(df['category_feature2'])\n",
    "   \n",
    "   correlation = df[['encoded_feature1', 'encoded_feature2']].corr()\n",
    "   ```\n",
    "\n",
    "### 5. **Jaccard Similarity:**\n",
    "   - İki kategorik dəyişənin kəsişməsini və birləşməsini müqayisə etmək üçün Jaccard bənzərlik metrikasını istifadə edə bilərsiniz. Əgər Jaccard bənzərliyi 1-dirsə, bu, iki dəyişənin tam eyni olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.metrics import jaccard_score\n",
    "   similarity = jaccard_score(df['category_feature1'], df['category_feature2'], average='macro')\n",
    "   ```\n",
    "\n",
    "### 6. **Mode və ya Value Counts:**\n",
    "   - Hər bir kategorik dəyişənin `mode` (ən çox təkrarlanan dəyər) və ya `value_counts` statistikalarını müqayisə edərək oxşarlığı təhlil edə bilərsiniz. Əgər iki dəyişənin dəyərləri və onların sıxlıqları çox bənzərdirsə, bu oxşarlıq göstəricisi ola bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Numeric dəyişənlərdəki kimi, kategorik dəyişənlər arasında da tam uyğunluq və güclü oxşarlığı müəyyən etmək üçün müxtəlif statistik metodlardan istifadə edilə bilər. Bu yanaşmalarla kategorik dəyişənlərin redundant (təkrarlanan) olub-olmadığını müəyyənləşdirə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217e6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b96ad99d",
   "metadata": {},
   "source": [
    "## VIf score hem numeric,  hem categoryc ucun isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ffe37",
   "metadata": {},
   "source": [
    "VIF (Variance Inflation Factor) skoru yalnız numeric (rəqəmsal) dəyişənlər üçün tətbiq olunur. Bu metod multikollinearlığı, yəni bir rəqəmsal dəyişənin digər rəqəmsal dəyişənlərlə nə qədər əlaqəli olduğunu qiymətləndirir. VIF, rəqəmsal dəyişənlər arasında xətti asılılıqların olub-olmamasını təhlil etmək üçün istifadə olunur.\n",
    "\n",
    "### Niyə VIF yalnız numeric dəyişənlər üçün işləyir?\n",
    "\n",
    "1. **Linear Regression Əsaslı:** \n",
    "   - VIF, bir dəyişəni digər rəqəmsal dəyişənlərə qarşı bir lineer regresiya modeli ilə proqnozlaşdırmaqla hesablanır. Kategorik dəyişənlər birbaşa linear regressiyada mənalı bir şəkildə istifadə edilə bilməz, çünki onlar saylı deyil.\n",
    "\n",
    "2. **Multikollinearlıq:**\n",
    "   - Multikollinearlıq problemi xətti modellərdə yaranır, çünki bu modellər dəyişənlər arasında xətti əlaqələrdən asılıdır. Kategorik dəyişənlər isə qeyri-xətti xüsusiyyətlərə malikdir, buna görə də multikollinearlıq VIF ilə qiymətləndirilmir.\n",
    "\n",
    "### Kategorik dəyişənlər üçün nə istifadə etmək olar?\n",
    "\n",
    "Kategorik dəyişənlər üçün VIF-ə bənzər bir üsul yoxdur, lakin onların redundant olub olmadığını müəyyənləşdirmək üçün aşağıdakı metodlar istifadə edilə bilər:\n",
    "\n",
    "1. **Chi-Square Testi:**\n",
    "   - Kategorik dəyişənlər arasında asılılıq dərəcəsini ölçmək üçün Chi-Square testi istifadə edilə bilər. Bu, kategorik dəyişənlərin bir-birinə nə qədər bağlı olduğunu göstərir.\n",
    "\n",
    "2. **Mutual Information:**\n",
    "   - Mutual Information kategorik dəyişənlər arasında nə qədər məlumatın paylaşıldığını ölçür. Yüksək mutual information dəyəri onların bir-birinə çox oxşar və ya əlaqəli olduğunu göstərir.\n",
    "\n",
    "3. **Label Encoding ilə Korelyasiya:**\n",
    "   - Kategorik dəyişənlər Label Encoding və ya One-Hot Encoding ilə numeric formaya çevrilə bilər, daha sonra bu dəyişənlər arasında korelyasiya hesablanıb redundant olanlar müəyyən edilə bilər.\n",
    "\n",
    "### Nəticə:\n",
    "VIF yalnız numeric dəyişənlər üçün istifadə edilir. Kategorik dəyişənlər üçün isə digər statistik metodlardan istifadə edərək, redundantlıq və oxşarlığı qiymətləndirmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad8228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdbb49d9",
   "metadata": {},
   "source": [
    "* from sklearn.model_selection import train_test_split\n",
    "* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "## stratify=y  ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67be01f",
   "metadata": {},
   "source": [
    "`stratify=y` parametri, `train_test_split` funksiyasında verilən məlumatların sınıflar üzrə balanslı şəkildə bölünməsini təmin edir. Yəni, orijinal `y` target (hədəf) dəyişənindəki sınıfların nisbəti necədirsə, həm təlim, həm də test dəstində də həmin sınıf nisbətləri qorunacaq.\n",
    "\n",
    "### Misal:\n",
    "Tutaq ki, sizin `y` hədəf dəyişəninizdə iki sınıf (məsələn, 0 və 1) var və bunların nisbəti belədir:\n",
    "- `0` sınıfı: 80%\n",
    "- `1` sınıfı: 20%\n",
    "\n",
    "Əgər siz `stratify=y` parametri ilə bölmə aparsanız, həm təlim dəstində, həm də test dəstində bu sınıfların nisbəti eyni qalacaq (80% və 20%). Bu, modelin hər iki dəstdə məlumatların təmsilçiliyini qoruması üçün faydalıdır.\n",
    "\n",
    "Əgər `stratify=y` olmasa, bölünmə təsadüfi olar və sınıfların nisbəti təlim və test dəstlərində qeyri-bərabər ola bilər, bu da modelin performansını mənfi təsir edə bilər, xüsusilə də sınıflar arasında balanssızlıq varsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082402e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513f1e35",
   "metadata": {},
   "source": [
    "## XGBoost esas parameterleri ve o parameterlere hyperparameter tuning ucun hansi deyerler vermek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe8fdd",
   "metadata": {},
   "source": [
    "XGBoost modellərində əsas parametrlər və onların hyperparameter tuning (hiperparametr tənzimlənməsi) üçün dəyərləri aşağıda göstərilmişdir. Bu dəyərlər `GridSearchCV` və ya `BayesSearchCV` kimi alətlərlə tuning üçün istifadə oluna bilər.\n",
    "\n",
    "### Əsas XGBoost Parametrləri\n",
    "\n",
    "1. **`n_estimators`**:\n",
    "   - Açıklama: Modelin istifadə etdiyi ağacların sayı.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `100, 200, 500, 1000`\n",
    "\n",
    "2. **`learning_rate`** (və ya `eta`):\n",
    "   - Açıklama: Modelin öyrənmə sürəti. Kiçik dəyərlər öyrənməni yavaşladır, ancaq nəticəni yaxşılaşdırır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.01, 0.05, 0.1, 0.2`\n",
    "\n",
    "3. **`max_depth`**:\n",
    "   - Açıklama: Hər bir qərar ağacının maksimum dərinliyi. Daha böyük dərinliklər modelin mürəkkəbliyini artırır, lakin overfitting-ə səbəb ola bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `3, 5, 7, 10`\n",
    "\n",
    "4. **`min_child_weight`**:\n",
    "   - Açıklama: Bir yarpaq qovşağının ən az nə qədər müşahidəyə sahib olmalı olduğunu təyin edir. Böyük dəyərlər underfitting yarada bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `1, 3, 5, 7`\n",
    "\n",
    "5. **`subsample`**:\n",
    "   - Açıklama: Təlim üçün istifadə edilən məlumatların nisbəti. Overfitting-i azaltmaq üçün faydalıdır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "6. **`colsample_bytree`**:\n",
    "   - Açıklama: Hər bir ağac üçün təsadüfi seçilən xüsusiyyətlərin nisbəti.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "7. **`gamma`**:\n",
    "   - Açıklama: Ağacın budaqlanmasına qərar verən minimal itki azalma dəyəri. Dəyər nə qədər yüksəkdirsə, budaqlanma daha sərt olur.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.1, 0.2, 0.5`\n",
    "\n",
    "8. **`reg_alpha`** (L1 tənzimlənməsi):\n",
    "   - Açıklama: L1 regularization-u. Daha yüksək dəyərlər sadə modellərə gətirib çıxara bilər və overfitting-i azalda bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "9. **`reg_lambda`** (L2 tənzimlənməsi):\n",
    "   - Açıklama: L2 regularization-u. Overfitting-in qarşısını almağa kömək edir.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "### Hyperparameter Tuning Misalı (`GridSearchCV`):\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "Bu tuning prosesini modelinizə uyğun olaraq dəyişə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4458f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33468f9c",
   "metadata": {},
   "source": [
    "## GridSearchCV, RandomizedSearchCV və BayesSearchCV Parameters cedveli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f25710",
   "metadata": {},
   "source": [
    "Əlbəttə! Aşağıda `GridSearchCV`, `RandomizedSearchCV` və `BayesSearchCV` metodlarının əsas parametrlərini Jupyter Notebook üçün uyğun şəkildə cədvəl şəklində təqdim edirəm.\n",
    "\n",
    "| **Parameter**               | **GridSearchCV (scikit-learn)**                                   | **RandomizedSearchCV (scikit-learn)**                     | **BayesSearchCV (scikit-optimize)**                        |\n",
    "|-----------------------------|-------------------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **estimator**               | Optimallaşdırılacaq maşın öyrənmə modeli                          | Optimallaşdırılacaq maşın öyrənmə modeli                    | Optimallaşdırılacaq maşın öyrənmə modeli                     |\n",
    "| **param_grid / param_distributions / search_spaces** | `param_grid`: dict – Parametrlərin tam qrafiki                | `param_distributions`: dict və ya list – Parametrlərin paylanması | `search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər |\n",
    "| **n_iter**                  | Tətbiq olunmur                                                    | `n_iter`: int – Nümunə parametrlərin sayı                  | `n_iter`: int – Optimallaşdırma üçün iterasiya sayı         |\n",
    "| **scoring**                 | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası |\n",
    "| **cv**                      | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası |\n",
    "| **n_jobs**                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı            | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı             |\n",
    "| **refit**                   | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma      | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma |\n",
    "| **verbose**                 | `verbose`: int – Verbosity səviyyəsi                              | `verbose`: int – Verbosity səviyyəsi                      | `verbose`: int – Verbosity səviyyəsi                        |\n",
    "| **pre_dispatch**            | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı  | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı | Tətbiq olunmur, implementasiyaya bağlı                      |\n",
    "| **random_state**            | Tətbiq olunmur                                                    | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması |\n",
    "| **error_score**             | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | Implementasiyaya bağlı ola bilər                              |\n",
    "| **return_train_score**      | `return_train_score`: bool – Təlim skorlarını daxil etmək         | `return_train_score`: bool – Təlim skorlarını daxil etmək   | `return_train_score`: bool – Təlim skorlarını daxil etmək     |\n",
    "| **optimizer**               | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `optimizer`: str – Optimallaşdırma alqoritmi, məsələn \"auto\", \"GP\" və s. |\n",
    "| **n_points**                | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı |\n",
    "\n",
    "### Qısa İzahat\n",
    "\n",
    "- **GridSearchCV**:\n",
    "  - Bütün mümkün parametrlərin kombinasyonlarını sınayır.\n",
    "  - `param_grid` vasitəsilə tam qrafik təyin olunur.\n",
    "  - Daha çox hesablama vaxtı tələb edir, xüsusilə parametrlərin sayı çox olduqda.\n",
    "\n",
    "- **RandomizedSearchCV**:\n",
    "  - Parametrlərin müəyyən paylanmalardan təsadüfi seçilmiş kombinasiyalarını sınayır.\n",
    "  - `n_iter` ilə sınanacaq kombinasiyaların sayını təyin edə bilərsiniz.\n",
    "  - Hesablama baxımından daha səmərəlidir, xüsusilə geniş parametrlər sahəsində.\n",
    "\n",
    "- **BayesSearchCV**:\n",
    "  - Bayes optimallaşdırması istifadə edərək daha ağıllı axtarış aparır.\n",
    "  - `search_spaces` ilə axtarış sahəsi təyin olunur.\n",
    "  - Daha az iterasiya ilə optimal nəticələr əldə etməyə çalışır.\n",
    "\n",
    "### Jupyter Notebook-da Görüntüləmək\n",
    "\n",
    "Bu cədvəli Jupyter Notebook-da gözəl şəkildə görüntüləmək üçün Markdown hüceyrəsində yuxarıdakı cədvəl formatını istifadə edə bilərsiniz. Alternativ olaraq, `pandas` kitabxanasından istifadə edərək DataFrame şəklində də təqdim edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Parameter\": [\n",
    "        \"estimator\",\n",
    "        \"param_grid / param_distributions / search_spaces\",\n",
    "        \"n_iter\",\n",
    "        \"scoring\",\n",
    "        \"cv\",\n",
    "        \"n_jobs\",\n",
    "        \"refit\",\n",
    "        \"verbose\",\n",
    "        \"pre_dispatch\",\n",
    "        \"random_state\",\n",
    "        \"error_score\",\n",
    "        \"return_train_score\",\n",
    "        \"optimizer\",\n",
    "        \"n_points\"\n",
    "    ],\n",
    "    \"GridSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_grid`: dict – Parametrlərin tam qrafiki\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"RandomizedSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_distributions`: dict və ya list – Parametrlərin paylanması\",\n",
    "        \"`n_iter`: int – Nümunə parametrlərin sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"BayesSearchCV (scikit-optimize)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər\",\n",
    "        \"`n_iter`: int – Optimallaşdırma üçün iterasiya sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"Tətbiq olunmur, implementasiyaya bağlı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"Implementasiyaya bağlı ola bilər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"`optimizer`: str – Optimallaşdırma alqoritmi, məsələn 'auto', 'GP' və s.\",\n",
    "        \"`n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "```\n",
    "\n",
    "Bu kodu işlədərək daha interaktiv və oxunaqlı bir cədvəl əldə edə bilərsiniz.\n",
    "\n",
    "Ümid edirəm ki, bu məlumatlar sizin üçün faydalı oldu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "034bc5a1",
   "metadata": {},
   "source": [
    "## imbalance problemi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa70613",
   "metadata": {},
   "source": [
    "Imbalance problemi, xüsusilə təsnifat (classification) problemlərində rast gəlinən vəziyyətdir. Bu zaman siniflər arasında böyük fərqlilik olur, yəni bir sinif çox az, digəri isə çox sayda nümunə ehtiva edir. Məsələn, xəstəlik diaqnozu ilə bağlı bir datasetdə xəstəlik olan insanların sayı çox az, sağlam olan insanların sayı isə çox ola bilər.\n",
    "\n",
    "Imbalance problemi modelin performansını mənfi təsir edir, çünki model çox vaxt dominant sinifin nümunələrini düzgün təsnif etməyə meyilli olur. Bunun nəticəsində, model azlıqda olan sinifin nümunələrini düzgün öyrənə bilmir və bu da overall performansı zəiflədir.\n",
    "\n",
    "Bu problemi həll etmək üçün bir neçə metod istifadə olunur:\n",
    "\n",
    "### 1. **Resampling (Nümunələrin Dəyişdirilməsi):**\n",
    "   - **Oversampling (Çoxaldma):** Azlıqda olan sinifin nümunələrini çoxaltmaqla balans yaratmaq olar. `SMOTE (Synthetic Minority Over-sampling Technique)` kimi metodlar istifadə olunur.\n",
    "   - **Undersampling (Azaldılma):** Çoxluqda olan sinifin nümunələrini azaldaraq balans təmin etmək olar. Lakin bu üsul datanın bir hissəsini itirmək riskini yaradır.\n",
    "\n",
    "### 2. **Class Weights (Sinif Çəki Dəyərləri):**\n",
    "   Təsnifat modellərində (məsələn, `RandomForest`, `SVM`, `XGBoost`) sinif çəki dəyərlərini tənzimləməklə azlıqda olan sinifin daha vacib olduğunu göstərmək olar. Bununla model azlıq sinifə daha çox diqqət yetirir.\n",
    "\n",
    "### 3. **Anomaly Detection (Anomaliya Aşkarlanması):**\n",
    "   Əgər azlıq sinifi anomaliyaya bənzəyirsə, anomaly detection metodları ilə bu sinifi tapmaq mümkün ola bilər.\n",
    "\n",
    "### 4. **Evaluation Metrics (Qiymətləndirmə Metodları):**\n",
    "   Klassik ölçüm metrikləri (məsələn, accuracy) imbalance datada yanıltıcı ola bilər. Bunun yerinə `precision`, `recall`, `F1-score`, `ROC-AUC` kimi metriklərdən istifadə etmək daha uyğundur.\n",
    "\n",
    "Imbalance problemi ilə qarşılaşanda bu metodlardan birini və ya bir neçəsini sınamaq balanslı və daha performanslı bir model yaratmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d354699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786294d8",
   "metadata": {},
   "source": [
    "## Nece faiz nisbetden sonra imblance sayilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2bbeb",
   "metadata": {},
   "source": [
    "Imbalance problemi adətən siniflər arasında əhəmiyyətli fərqlər olduqda meydana çıxır. Ümumiyyətlə, siniflər arasında 1:1.5 və ya 1:2 nisbəti qədər fərq olduqda balanslı (balanced) hesab edilir. Əgər bir sinif digərindən 1:4 və ya daha çox dəfə böyükdürsə, bu artıq imbalance problemi kimi qiymətləndirilə bilər.\n",
    "\n",
    "Daha dəqiq desək, bəzi tədqiqatçılar siniflər arasında 70% və ya daha çox fərqin (yəni bir sinifin nümunələrinin 70%-dən çoxunu təşkil etməsi) imbalance problemi yaratdığını deyirlər. Məsələn, bir datasetdə bir sinif 90% və digər sinif 10% təşkil edirsə, bu ciddi bir imbalance olaraq qəbul edilir.\n",
    "\n",
    "Əsas qayda odur ki, əgər azlıqda olan sinifin modellə düzgün öyrənilməsi çətinləşirsə və model çoxluqda olan sinifə üstünlük verirsə, bu zaman imbalance problemi var deməkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372da750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d5322b",
   "metadata": {},
   "source": [
    "## Butun feature sutunlari etsem ne bas verecek ki?  1 ve 0 olanlar deyise biler. yoxsa oldugu kimi saxlayacaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e99c66",
   "metadata": {},
   "source": [
    "Əgər datada categorical dəyişənləri artıq `1` və `0` olaraq çevirmisinizsə, həmin dəyişənlər artıq binary formatdadır və onları yenidən scale etmək lazım deyil. Bu tip binary dəyişənlər onsuz da məhdud aralıqda olur (0 və 1), ona görə də scale etməyə ehtiyac yoxdur.\n",
    "\n",
    "Numeric dəyişənləri isə scale etməniz məqsədəuyğundur, çünki onlar fərqli böyüklükdə ola bilər və bəzi modellər (məsələn, SVM, KNN və ya linear regression modelləri) fərqli miqyaslarda olan dəyişənlərdən mənfi təsirlənə bilər.\n",
    "\n",
    "Yəni, ümumi tövsiyə belədir:\n",
    "- Numeric dəyişənləri scale edin.\n",
    "- Binary (0 və 1 olan categorical) dəyişənləri scale etməyin.\n",
    "\n",
    "Bu addımları tətbiq etməklə datanızı daha optimal şəkildə modelə təqdim edə bilərsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357584b",
   "metadata": {},
   "source": [
    "Əgər bütün feature sütunlarını (həm numeric, həm də binary olanları) scale etsəniz, 0 və 1 olan binary sütunlar da dəyişəcək. Məsələn, `StandardScaler` və ya `MinMaxScaler` tətbiq edildikdə bu binary sütunlar 0 və 1 arasında başqa qiymətlər alacaq, çünki scale prosesi bütün dəyişənləri yeni miqyasda sıxışdırmağa çalışır.\n",
    "\n",
    "Məsələn, əgər `MinMaxScaler` tətbiq edirsinizsə:\n",
    "- `0` olan dəyərlər dəyişməz qalacaq.\n",
    "- `1` olan dəyərlər isə `min` və `max` aralığında yeni dəyər ala bilər (məsələn, 0.5 və ya başqa bir qiymət).\n",
    "\n",
    "Bu, modelin davranışını mənfi təsir edə bilər, çünki bəzi modellər binary dəyişənləri orijinal 0 və 1 aralığında gözləyir. Ona görə də binary sütunları scale etməmək daha məqsədəuyğundur.\n",
    "\n",
    "Əgər ümumi datanı scale etmək istəyirsinizsə, binary sütunları istisna etmək üçün kodunuzda bu dəyişənləri seçmədən yalnız numeric sütunlara scale tətbiq etməniz daha doğru olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34302a69",
   "metadata": {},
   "source": [
    "## Bes nece edim axi. nece kod yazim ki binary olanalra toxunmasin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0765452",
   "metadata": {},
   "source": [
    "Binary sütunlara toxunmadan yalnız numeric sütunları scale etmək üçün əvvəlcə numeric və binary sütunları ayırmalısınız. Python və `pandas` istifadə edərək bunu etmək üçün aşağıdakı addımları izləyə bilərsiniz:\n",
    "\n",
    "1. **Binary və numeric sütunları ayırın.**\n",
    "2. **Yalnız numeric sütunlara `scaler` tətbiq edin.**\n",
    "3. **Yenidən datafraimi birləşdirin.**\n",
    "\n",
    "Məsələn, aşağıdakı kimi bir kod yaza bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Məsələn, data frame-iniz olsun\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# Target və features sütunlarını müəyyən edin (məsələn, 'target' adlı bir sütun)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split edin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numeric sütunları seçin\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Binary sütunları müəyyən edin\n",
    "binary_cols = [col for col in numeric_cols if X_train[col].nunique() == 2 and sorted(X_train[col].unique()) == [0, 1]]\n",
    "\n",
    "# Binary olmayan numeric sütunları tapın\n",
    "numeric_cols_to_scale = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Scaler tətbiq edin yalnız numeric sütunlara\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Yalnız train datasında fit edin, amma həm train, həm də test datasında transform edin\n",
    "X_train[numeric_cols_to_scale] = scaler.fit_transform(X_train[numeric_cols_to_scale])\n",
    "X_test[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
    "\n",
    "# Artıq yalnız numeric sütunlar scale olundu, binary sütunlara toxunulmadı\n",
    "```\n",
    "\n",
    "### Açıqlama:\n",
    "- `select_dtypes(include=['float64', 'int64'])`: Numeric sütunları seçir.\n",
    "- `df[col].nunique() == 2`: Sütunda yalnız 2 unikal dəyər (0 və 1) olduğunu yoxlayır.\n",
    "- `sorted(df[col].unique()) == [0, 1]`: Sütundakı dəyərlərin həqiqətən 0 və 1 olmasını yoxlayır.\n",
    "- Son olaraq, yalnız numeric sütunlar scale olunur, binary sütunlara toxunulmur.\n",
    "\n",
    "Beləliklə, yalnız numeric sütunları scale edə bilərsiniz, binary sütunlar isə dəyişməz qalacaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eccd2960",
   "metadata": {},
   "source": [
    "## Outlier sixmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f32fe",
   "metadata": {},
   "source": [
    "Outlier-ləri sıxmaq üçün bir neçə fərqli metod var. Bu metodlar datasetin növündən, təbiətindən və məqsədlərinizdən asılı olaraq dəyişir. Aşağıda outlier-ləri sıxmaq üçün ən çox istifadə olunan üsullar və onların Python-da tətbiqi nümunələri ilə tanış ola bilərsiniz.\n",
    "\n",
    "### 1. **Z-Score Metodu ilə Sıxmaq**\n",
    "Z-score, hər bir dəyərin ortalamadan neçə standart sapma uzaqda olduğunu göstərir. Əgər bir dəyər müəyyən bir həddən böyükdürsə (məsələn, 3 standart sapmadan uzaqdırsa), onu həmin həddə sıxmaq olar.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "df\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score hesablayın (sayısal sütunlar üçün)\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# Z-scores hesablayın\n",
    "z_scores = zscore(df_)\n",
    "\n",
    "# Threshold təyin edin\n",
    "threshold = 3\n",
    "\n",
    "# Z-scores dəyəri 3-dən böyük olanları 3-ə sıxın, -3-dən kiçik olanları isə -3-ə sıxın\n",
    "z_scores_compressed = np.where(z_scores > threshold, threshold, \n",
    "                               np.where(z_scores < -threshold, -threshold, z_scores))\n",
    "\n",
    "# Z-scores əsasında sıxılmış dəyərləri orijinal df_-dəki dəyərlərlə əvəz edin\n",
    "df_compressed = df_.copy()\n",
    "\n",
    "for i, col in enumerate(df_.columns):\n",
    "    # Hər sütun üçün sıxılmış Z-scores dəyərlərini yenidən orijinal miqyasına çevirin\n",
    "    df_compressed[col] = (z_scores_compressed[:, i] * df_[col].std()) + df_[col].mean()\n",
    "\n",
    "df_compressed\n",
    "```\n",
    "\n",
    "### 2. **IQR (Interquartile Range) Metodu ilə Sıxmaq**\n",
    "IQR metodu, dəyərləri 1-ci və 3-cü kvartillər (Q1 və Q3) əsasında müəyyənləşdirir. Outlier-lər Q1 - 1.5 * IQR və Q3 + 1.5 * IQR intervalından kənarda olan dəyərlərdir. Bu hədləri aşan dəyərləri həmin həddə sıxışdıra bilərsiniz.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CSV faylını oxuyun\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "\n",
    "# Sayısal sütunları seçin və lazımsız sütunları silin\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "\n",
    "# Nəmissing dəyərləri sütun ortalamaları ilə doldurun\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# IQR istifadə edərək outlier-ləri sıxın\n",
    "df_clipped = df_.copy()\n",
    "\n",
    "for col in df_clipped.columns:\n",
    "    Q1 = df_clipped[col].quantile(0.25)\n",
    "    Q3 = df_clipped[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Outlier sərhədlərini müəyyən edin\n",
    "    lower_fence = Q1 - 1.5 * IQR\n",
    "    upper_fence = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Outlier-ləri sıxın\n",
    "    df_clipped[col] = df_clipped[col].clip(lower=lower_fence, upper=upper_fence)\n",
    "\n",
    "df_clipped\n",
    "```\n",
    "\n",
    "### 3. **Winsorization (Sərhəd Dəyərlərə Sıxmaq)**\n",
    "Winsorization metodu, outlier-ləri müəyyən bir həddə sıxışdırır, yəni outlier-lər alt və üst limitlərdən kənara çıxa bilmir.\n",
    "\n",
    "```python\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize tətbiq edin (1% alt və 1% üst həddə sıxışdırma)\n",
    "df_winsorized = winsorize(df, limits=[0.01, 0.01])\n",
    "\n",
    "# Bu dəyərləri müəyyən bir həddə sıxışdırmış olursunuz\n",
    "```\n",
    "\n",
    "### 4. **Percentile Metodu ilə Sıxmaq**\n",
    "Bəzi hallarda, outlier-ləri percentil əsaslı metodlarla müəyyən etmək faydalı ola bilər. 1%-dən aşağı və ya 99%-dən yuxarı olan dəyərləri sıxmaq üçün aşağıdakı koddan istifadə edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "# Alt və üst hədləri təyin edin\n",
    "lower_percentile = df.quantile(0.01)\n",
    "upper_percentile = df.quantile(0.99)\n",
    "\n",
    "# Outlier-ləri sıxın\n",
    "df_clipped = np.where(df < lower_percentile, lower_percentile, df)\n",
    "df_clipped = np.where(df > upper_percentile, upper_percentile, df)\n",
    "```\n",
    "\n",
    "### 5. **Log Transformasiya**\n",
    "Əgər outlier-lər data massivində çox geniş fərqlənirsə, log transformasiya dəyərləri daha kiçik miqyasda sıxmağa kömək edə bilər.\n",
    "\n",
    "```python\n",
    "df_log_transformed = np.log1p(df)  # 1 əlavə edərək log transformasiya edin\n",
    "```\n",
    "\n",
    "### 6. **Clipping (Sərhədli Dəyərlərlə Əvəz Etmək)**\n",
    "Bir çox hallarda dəyərləri müəyyən bir hədd ilə sıxmaq üçün birbaşa `numpy`-ın `clip()` funksiyasından istifadə edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "df_clipped = np.clip(df, lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "### Hansı Metodu Seçməli?\n",
    "- **Z-score**: Daha çox normal paylanma üçün uyğundur.\n",
    "- **IQR**: Paylanma asimmetrikdirsə və outlier-lər çox genişdirsə istifadə edilir.\n",
    "- **Winsorization və Percentile**: Daha stabildir və sərhəd dəyərləri təyin etmək asandır.\n",
    "- **Log Transformasiya**: Exponential (eksponential) və ya geniş miqyaslı paylanmalar üçün uyğundur.\n",
    "\n",
    "Hansı metoddan istifadə edəcəyiniz datasetin təbiətindən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf446d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2677df6d",
   "metadata": {},
   "source": [
    "## Missing deyerleri doldurmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1816a7",
   "metadata": {},
   "source": [
    "Missing dəyərləri doldurmaq (imputation) analitik və maşın öyrənməsi işlərində tez-tez qarşılaşılan problemlərdən biridir. Aşağıda bir neçə effektiv yanaşma və onların Python vasitəsilə həyata keçirilməsi yollarını görə bilərsiniz.\n",
    "\n",
    "### 1. **Ortalama (Mean) ilə Doldurma**\n",
    "Bu üsul sadədir və əsasən **numerik** verilənlər üçün tətbiq olunur. Missing dəyərləri həmin sütunun ortalama (mean) dəyəri ilə əvəz edirsiniz.\n",
    "\n",
    "```python\n",
    "# Numerik sütundakı missing dəyərləri ortalama ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mean())\n",
    "```\n",
    "\n",
    "### 2. **Median (Orta Əsaslı) Doldurma**\n",
    "Median üsulu, xüsusilə outlier-lərin çox olduğu datasetlərdə daha sabit nəticələr verir. Bu üsul da əsasən **numerik** verilənlər üçün istifadə olunur.\n",
    "\n",
    "```python\n",
    "# Missing dəyərləri median ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].median())\n",
    "```\n",
    "\n",
    "### 3. **Mod (Modus) ilə Doldurma**\n",
    "**Kategorik** (category) verilənlər üçün daha çox istifadə edilən üsuldur. Mod üsulu, sütundakı ən çox təkrarlanan dəyəri missing dəyərlərə əlavə edir.\n",
    "\n",
    "```python\n",
    "# Kategorik sütunu mod ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mode()[0])\n",
    "```\n",
    "\n",
    "### 4. **Sabit Dəyər ilə Doldurma**\n",
    "Bəzi hallarda missing dəyərləri sabit bir dəyərlə əvəz etmək lazım ola bilər (məsələn, 0 və ya müəyyən kateqoriya).\n",
    "\n",
    "```python\n",
    "# Sabit dəyərlə doldurmaq (məsələn, 0 ilə)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(0)\n",
    "\n",
    "# Kategorik verilənlər üçün sabit kateqoriya təyin etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna('Unknown')\n",
    "```\n",
    "\n",
    "### 5. **Linear İnteqrasiya ilə Doldurma (Interpolation)**\n",
    "Linear inteqrasiya metodu zaman seriyası məlumatları və ya ardıcıl numerik məlumatlar üçün uyğundur. Məlumatın əvvəlki və sonrakı dəyərləri əsasında missing dəyərləri interpolasiya edərək hesablayır.\n",
    "\n",
    "```python\n",
    "# Linear interpolation tətbiq etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].interpolate()\n",
    "```\n",
    "\n",
    "### 6. **K-NN (K-Nearest Neighbors) İmputation**\n",
    "K-NN imputation hər bir missing dəyəri ən yaxın qonşu dəyərlər əsasında tamamlayır. Bu üsul həm numerik, həm də kategorik dəyərlər üçün istifadə oluna bilər.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)  # 5 ən yaxın qonşu ilə doldurmaq\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 7. **MICE (Multiple Imputation by Chained Equations)**\n",
    "MICE çoxlu itkin dəyərlərin olduğu datasetlər üçün istifadə edilən bir üsuldur. Bu metod hər bir missing dəyəri digər dəyişənlərdən istifadə edərək təkrarlanmış şəkildə təxmin edir.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# MICE ilə doldurma\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 8. **Fərqli Sütunlar Əsasında Doldurma**\n",
    "Bəzi hallarda missing dəyərləri doldurmaq üçün başqa bir sütunun məlumatından istifadə etmək lazım ola bilər.\n",
    "\n",
    "```python\n",
    "# Əgər 'sütun_adi1' missingdirsə, onu 'sütun_adi2' ilə doldur\n",
    "df['sütun_adi1'] = df['sütun_adi1'].fillna(df['sütun_adi2'])\n",
    "```\n",
    "\n",
    "### 9. **Foward Fill və Backward Fill (Zaman Seriyasında)**\n",
    "Zaman seriyasında əvvəlki və ya sonrakı dəyərləri missing dəyərlə əvəz etməyə imkan verir.\n",
    "\n",
    "```python\n",
    "# Əvvəlki dəyərlə doldurmaq (forward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='ffill')\n",
    "\n",
    "# Sonrakı dəyərlə doldurmaq (backward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='bfill')\n",
    "```\n",
    "\n",
    "### 10. **Model Əsaslı Doldurma**\n",
    "Daha kompleks yanaşma kimi, missing dəyərləri təxmin etmək üçün maşın öyrənməsi modellərindən istifadə edə bilərsiniz. Məsələn, regression və ya classification modelləri ilə missing dəyərləri təxmin etmək mümkündür.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Missing dəyərləri təxmin etmək üçün model istifadə edin\n",
    "model = LinearRegression()\n",
    "X_train = df.dropna(subset=['sütun_adi'])  # Missing olmayan dəyərlərlə modeli öyrədin\n",
    "y_train = X_train.pop('sütun_adi')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Missing dəyərləri təxmin edin\n",
    "X_missing = df[df['sütun_adi'].isnull()].drop('sütun_adi', axis=1)\n",
    "df.loc[df['sütun_adi'].isnull(), 'sütun_adi'] = model.predict(X_missing)\n",
    "```\n",
    "\n",
    "### 11. **Linear Regression / Random Forest Imputation**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Random Forest ilə itkin dəyərlərin doldurulması\n",
    "rf = RandomForestRegressor()\n",
    "df_filled = SimpleImputer(strategy=\"mean\").fit_transform(df)\n",
    "```\n",
    "\n",
    "\n",
    "### Hansı Metodun Seçilməsi:\n",
    "- **Ortalama və Median**: Numerik sütunlar üçün tez-tez istifadə olunur.\n",
    "- **Modus və Sabit Dəyər**: Kategorik sütunlar üçün uyğundur.\n",
    "- **K-NN və MICE**: Çox dəyişkənli imputation tələb edən datasetlərdə daha dəqiqdir.\n",
    "- **Interpolation**: Zaman seriyaları üçün ideal metodlardan biridir.\n",
    "\n",
    "### Ümumi Yekun:\n",
    "- **Sadə datasetlər üçün**: **Mean/Median/Mode imputation** üsulları kifayət qədər sadə və tezdir.\n",
    "- **Kateqorial dəyişənlər üçün**: **Mode imputation** və ya **KNN imputation** uyğun gəlir.\n",
    "- **Daha mürəkkəb modellər üçün**: **MICE**, **KNN**, və ya **Random Forest imputation** üsulları çox ölçülü məlumatlar üçün daha dəqiq nəticələr verir, çünki dəyişənlər arasındakı əlaqələri nəzərə alır.\n",
    "\n",
    "### Ən yaxşı üsulun seçilməsi üçün ipuçları:\n",
    "- **Məlumatın ölçüsü**: Kiçik datasetlər üçün MICE və ya KNN daha yaxşı nəticə verə bilər.\n",
    "- **İtkin dəyərlərin faizi**: Əgər çox sayda itkin dəyər varsa, mürəkkəb modellər (MICE, KNN, Random Forest) daha dəqiq nəticə verəcək.\n",
    "- **Dəyişənlər arasındakı əlaqə**: Əgər dəyişənlər arasında güclü əlaqə varsa, bu əlaqələri nəzərə alan metodlar (MICE, Regression, Random Forest) daha yaxşı işləyir.\n",
    "\n",
    "Hansı metodu seçməyiniz datasetin növündən və analiz məqsədinizdən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c71e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09a576b",
   "metadata": {},
   "source": [
    "## SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea309cc",
   "metadata": {},
   "source": [
    "**SimpleImputer** sklearn kitabxanasında mövcud olan sadə və effektiv bir imputation vasitəsidir. O, datasetdəki itkin (null) dəyərləri müəyyən bir strategiya əsasında doldurmaq üçün istifadə olunur. **SimpleImputer** müxtəlif dəyərləri, məsələn, **orta (mean)**, **median**, **ən çox təkrarlanan dəyər (mode)** və ya xüsusi bir dəyər ilə itkin dəyərləri əvəz etməyə imkan verir.\n",
    "\n",
    "### SimpleImputer-in istifadəsi və xüsusiyyətləri:\n",
    "\n",
    "- **Strategiyalar**:\n",
    "  - **mean**: İtkin dəyərləri sütundakı orta dəyərlə doldurur. Davamlı dəyişənlər üçün istifadə olunur.\n",
    "  - **median**: İtkin dəyərləri median dəyəri ilə doldurur. Xüsusilə asimmetrik paylanmalar üçün faydalıdır.\n",
    "  - **most_frequent**: İtkin dəyərləri ən çox təkrarlanan dəyərlə əvəz edir. Bu üsul həm kateqorial, həm də davamlı dəyişənlər üçün istifadə oluna bilər.\n",
    "  - **constant**: İtkin dəyərləri müəyyən edilən sabit bir dəyər ilə doldurur. Məsələn, `0` və ya `missing` kimi xüsusi bir dəyər istifadə edilə bilər.\n",
    "\n",
    "### SimpleImputer-in tətbiqi:\n",
    "\n",
    "Burada **SimpleImputer**-in müxtəlif strategiyalarla necə istifadə ediləcəyini göstərən bir nümunə var:\n",
    "\n",
    "#### 1. **Orta dəyərlə (mean) doldurmaq:**\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Datasetin yaradılması\n",
    "data = {'age': [25, 30, None, 35, 40, None, 50],\n",
    "        'salary': [50000, 60000, 55000, None, 70000, 80000, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# SimpleImputer ilə orta dəyərlə (mean) doldurmaq\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 2. **Median ilə doldurmaq:**\n",
    "\n",
    "```python\n",
    "# Median strategiyası\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df['salary'] = imputer_median.fit_transform(df[['salary']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 3. **Ən çox təkrarlanan dəyərlə (most_frequent) doldurmaq:**\n",
    "\n",
    "```python\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df['age'] = imputer_mode.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 4. **Sabit bir dəyərlə doldurmaq:**\n",
    "\n",
    "```python\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df['age'] = imputer_constant.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Üstünlükləri:\n",
    "- **Sadə və sürətli**: Datasetdə çox sayda itkin dəyər varsa belə, sadə üsullarla onları doldurmaq imkanı verir.\n",
    "- **Çox yönlülük**: Həm davamlı (numeric) dəyişənlər, həm də kateqorial (categorical) dəyişənlər üçün istifadə edilə bilər.\n",
    "- **Asan tətbiq oluna bilir**: sklearn kitabxanası ilə birgə istifadə olunur və model qurma prosesinə asanlıqla daxil edilir.\n",
    "\n",
    "### Mənfi cəhətləri:\n",
    "- **Sadə yanaşma**: **SimpleImputer** dəyişənlər arasındakı əlaqəni nəzərə almır. Bu, mürəkkəb datasetlər üçün dəyərlərin doldurulmasında kifayət qədər dəqiq olmaya bilər.\n",
    "- **Dəyər dispersiyasını azaldır**: Orta və ya median ilə doldurulduqda, məlumatın dispersiyası (müxtəlifliyi) azalır, bu da model performansına təsir göstərə bilər.\n",
    "\n",
    "SimpleImputer sadə datasetlərdə və ya ilkin təmizləmə mərhələsində effektivdir, lakin daha mürəkkəb datasetlərdə MICE, KNN və ya Random Forest kimi daha inkişaf etmiş metodlarla müqayisədə məhdudiyyətləri var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f97376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d81d7a0",
   "metadata": {},
   "source": [
    "## Fillna ve SimpleImputer ferqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fab80",
   "metadata": {},
   "source": [
    "Bəli, hər iki yanaşma mahiyyətcə oxşar iş görür — **itkin dəyərləri (NaN)** sütundakı **orta dəyərlə (mean)** doldurur, lakin işləmə prinsiplərində bəzi fərqlər var. Gəlin fərqləri detallı şəkildə izah edim:\n",
    "\n",
    "### 1. **`df['age'].fillna(df['age'].mean())` ilə doldurmaq**:\n",
    "\n",
    "Bu üsul **Pandas**-ın daxili metodunu istifadə edir və yalnız sadə bir əməliyyatdır. Bu metod itkin dəyərləri **kolonun ortalama dəyəri ilə** doldurur.\n",
    "\n",
    "```python\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "```\n",
    "\n",
    "- **Pros**:\n",
    "  - Çox sadədir və sürətlidir.\n",
    "  - Kiçik datasetlər və sadə hallarda rahatdır.\n",
    "  \n",
    "- **Cons**:\n",
    "  - Yalnız **statistik metodlar** ilə işləyir (mean, median və s.).\n",
    "  - Yalnız bir sütunu nəzərə alır və heç bir əlavə analiz və ya dəyişənlər arasındakı əlaqəni istifadə etmir.\n",
    "\n",
    "### 2. **`SimpleImputer(strategy='mean')` ilə doldurmaq**:\n",
    "\n",
    "`SimpleImputer` isə **scikit-learn** kitabxanasından olan bir metoddur və itkin dəyərlərin doldurulması üçün daha geniş imkanlar yaradır. Burada `SimpleImputer` həmçinin daha mürəkkəb üsullarla, məsələn, **median**, **mode** və ya **sabit bir dəyər** ilə itkin dəyərləri doldura bilər.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "```\n",
    "\n",
    "- **Pros**:\n",
    "  - Daha çevikdir və müxtəlif strategiyaları (mean, median, mode və s.) dəstəkləyir.\n",
    "  - Digər maşın öyrənməsi (ML) metodları ilə birgə rahat istifadə oluna bilir.\n",
    "  - **Multiple column imputation** dəstəklənir — yəni bir neçə sütunu eyni anda doldura bilərsiniz.\n",
    "\n",
    "- **Cons**:\n",
    "  - Pandas metoduna nisbətən bir qədər daha mürəkkəbdir.\n",
    "  - Pandas fillna metoduna nisbətən daha çox hesablama vaxtı tələb edə bilər (böyük datasetlərdə).\n",
    "\n",
    "### Fərqli cəhətlər:\n",
    "\n",
    "1. **Pandas `fillna()`** yalnız bir sütunun ortalamasını istifadə edir və dəyişənlər arasındakı heç bir əlaqəni nəzərə almır. O, daha sadə yanaşmadır və yalnız bir sütun üzərində işləyir.\n",
    "\n",
    "2. **SimpleImputer** isə xüsusilə bir neçə sütun üçün işləyə bilər və maşın öyrənməsi prosesində daha sistematik bir yanaşma tələb edir. Eyni zamanda `SimpleImputer` ilə birdən çox sütunda imputation tətbiq edərək işləyə bilərsiniz.\n",
    "\n",
    "#### Əlavə Misal (Bir neçə sütunu doldurmaq):\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])\n",
    "```\n",
    "Bu, həm `age`, həm də `salary` sütunlarındakı NaN dəyərlərini **orta** ilə dolduracaq.\n",
    "\n",
    "### Yekun:\n",
    "- **Kiçik və sadə hallarda**: Pandas-ın `fillna()` metodu daha sadədir və bir qədər daha sürətli işləyir.\n",
    "- **Böyük və mürəkkəb hallarda**: `SimpleImputer` çox yönlü və daha uyğun metoddur, xüsusilə maşın öyrənməsi modellərində və birdən çox sütunlu datasetlərdə imputation etmək lazım olduqda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc367e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d9866c",
   "metadata": {},
   "source": [
    "## XGBOOST esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e625c",
   "metadata": {},
   "source": [
    "XGBoost-un əsas parametrləri və onların təklif olunan dəyərləri aşağıdakılardır:\n",
    "\n",
    "### 1. `booster`\n",
    "- **Izah**: Modelin öyrənmə üsulunu təyin edir. \n",
    "- **Dəyərlər**:\n",
    "  - `'gbtree'`: Ən çox istifadə edilən ağac əsaslı modeldir. Çoxsaylı qərar ağacları yaradır.\n",
    "  - `'gblinear'`: Xətti modeldir. Xətti birbaşa nəticələr təxmin edir.\n",
    "  - `'dart'`: Dropouts ilə ağac modelləri, daha çox tənzimləmə təmin edir.\n",
    "\n",
    "### 2. `learning_rate` (və ya `eta`)\n",
    "- **Izah**: Hər iterasiyada öyrənmə sürətini müəyyən edir. Kiçik dəyərlər daha yavaş öyrənmə sürəti təmin edir və daha çox iterasiya tələb edə bilər.\n",
    "- **Dəyərlər**: 0.01 ilə 0.3 arasında, məsələn, 0.01, 0.1, 0.2.\n",
    "\n",
    "### 3. `n_estimators`\n",
    "- **Izah**: Təlimdə istifadə olunan ağacların sayını göstərir.\n",
    "- **Dəyərlər**: 50 ilə 1000 arasında, məsələn, 100, 200, 500.\n",
    "\n",
    "### 4. `max_depth`\n",
    "- **Izah**: Hər bir qərar ağacının maksimum dərinliyini müəyyən edir. Daha böyük dəyərlər daha kompleks ağaclar yaradır.\n",
    "- **Dəyərlər**: 3 ilə 15 arasında, məsələn, 3, 6, 10.\n",
    "\n",
    "### 5. `min_child_weight`\n",
    "- **Izah**: Bir düyündəki minimum nümunə çəkisini təyin edir. Bu, bir düyünün bölünməsi üçün tələb olunan minimal nümunə çəkisini göstərir.\n",
    "- **Dəyərlər**: 1 ilə 10 arasında, məsələn, 1, 3, 5.\n",
    "\n",
    "### 6. `subsample`\n",
    "- **Izah**: Hər iterasiyada istifadə olunan nümunələrin hissəsini müəyyən edir. Kiçik dəyərlər daha çox tənzimləmə təmin edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 7. `colsample_bytree`\n",
    "- **Izah**: Hər ağac üçün istifadə olunan xüsusiyyətlərin hissəsini müəyyən edir. Bu parametr xüsusiyyətlərin təsadüfi alt qruplarını istifadə edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 8. `lambda` (və ya `reg_lambda`)\n",
    "- **Izah**: L2 (Ridge) cərimələmə parametridir. Modelin sadələşdirilməsinə kömək edir və aşırı uyğunlaşmanı azaldır.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 9. `alpha` (və ya `reg_alpha`)\n",
    "- **Izah**: L1 (Lasso) cərimələmə parametridir. Bu, bəzi xüsusiyyətlərin çəkilərini sıfıra endirərək modelin sadələşdirilməsinə kömək edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 10. `gamma`\n",
    "- **Izah**: Bölünmə üçün tələb olunan minimal qazanma dəyərini təyin edir. Daha yüksək dəyərlər ağacların bölünməsini daha sərt edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 5.\n",
    "\n",
    "### 11. `scale_pos_weight`\n",
    "- **Izah**: Sinif balansı məsələlərində istifadə olunur, çünki modelin siniflərin tarazlığına uyğunlaşdırılmasına kömək edir.\n",
    "- **Dəyərlər**: 1 ilə (nöqtələr arasındakı fərqə əsasən dəyişir), məsələn, 1, 10, 20.\n",
    "\n",
    "### 12. `objective`\n",
    "- **Izah**: Məqsəd funksiyasını müəyyən edir. Problemin növünə görə dəyişir, məsələn, `reg:squarederror` (regressiya üçün), `binary:logistic` (ikili təsnifat üçün).\n",
    "- **Dəyərlər**:\n",
    "  - `'reg:squarederror'`: Regressiya üçün.\n",
    "  - `'binary:logistic'`: İkili təsnifat üçün.\n",
    "  - `'multi:softmax'`: Çoxsinifli təsnifat üçün (proqnozlar sinif etiketləri şəklində verilir).\n",
    "\n",
    "### 13. `eval_metric`\n",
    "- **Izah**: Modelin qiymətləndirilməsi üçün istifadə olunan ölçü vahidini müəyyən edir.\n",
    "- **Dəyərlər**:\n",
    "  - `'rmse'`: Kök orta kvadrat səhv (regressiya üçün).\n",
    "  - `'logloss'`: Loqaritmik itki (ikili təsnifat üçün).\n",
    "  - `'mlogloss'`: Çoxsinifli loqaritmik itki (çoxsinifli təsnifat üçün).\n",
    "\n",
    "Parametrləri tənzimləyərkən, ən yaxşı nəticələri əldə etmək üçün çapraz təsdiqləmə (cross-validation) istifadə edərək, hər parametrin müxtəlif dəyərlərini yoxlamaq tövsiyə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664514d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06f0c06",
   "metadata": {},
   "source": [
    "## GridSearchCV esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eea088",
   "metadata": {},
   "source": [
    "`GridSearchCV`, verilən parametrlər üçün bütün mümkün kombinasiyaları sınayaraq ən yaxşı nəticəni tapmaq üçün istifadə olunur. Bu alqoritm hər bir parametri müxtəlif dəyərlərlə sınayır və bu, daha dəqiq bir optimallaşdırma aparmağa imkan verir. Lakin, çox sayda parametr və dəyərlər olduqda bu proses çox vaxt ala bilər.\n",
    "\n",
    "### Əsas Parametrlər və Onların İzahı\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **Açıqlama**: Parametrlərini optimallaşdırmaq istədiyiniz modeldir. Bu model, `fit` və `predict` metodlarına sahib olmalıdır.\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()`, `LogisticRegression()`, və s.\n",
    "\n",
    "2. **`param_grid`**:\n",
    "   - **Açıqlama**: Modelin optimallaşdırılacaq hiperparametrlərinin dəyərlərini təyin edən grid (tor) strukturdur. Bütün parametrlərin dəyərlərini sınamaq üçün istifadə olunur.\n",
    "   - **Dəyərlər**: Parametrlərin mümkün dəyərlərini ehtiva edən `dict` formatında olmalıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     param_grid = {\n",
    "         'n_estimators': [50, 100, 200],\n",
    "         'learning_rate': [0.01, 0.1, 0.2],\n",
    "         'max_depth': [3, 5, 7],\n",
    "         'subsample': [0.6, 0.8, 1.0],\n",
    "         'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün istifadə olunan qiymətləndirmə kriteriyasıdır.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`\n",
    "\n",
    "4. **`cv`** (Cross-validation):\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə prosesində məlumatları neçə qrupa bölmək lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri (məsələn, 3, 5, 10).\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "5. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir. Bütün nüvələri istifadə etməklə paralelləşdirilmiş hesablama mümkündür.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "6. **`verbose`**:\n",
    "   - **Açıqlama**: Axtarış prosesi zamanı nə qədər detallı geri bildirim göstəriləcəyini təyin edir.\n",
    "   - **Dəyərlər**: 0 (səssiz rejim), 1 (minimum məlumat), 2 və daha yüksək (daha çox məlumat).\n",
    "   - **Nümunə**: `verbose=2`\n",
    "\n",
    "7. **`refit`**:\n",
    "   - **Açıqlama**: Ən yaxşı tapılan parametrlərlə modelin yenidən uyğunlaşdırılıb-uyğunlaşdırılmayacağını təyin edir.\n",
    "   - **Dəyərlər**: `True` (model refit ediləcək), `False` (model refit edilməyəcək).\n",
    "   - **Nümunə**: `refit=True`\n",
    "\n",
    "8. **`pre_dispatch`**:\n",
    "   - **Açıqlama**: Paralelləşdirilmiş hesablama zamanı nə qədər iş tapşırığının əvvəlcədən göndərilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri və ya `'2*n_jobs'` kimi nisbət.\n",
    "   - **Nümunə**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "9. **`error_score`**:\n",
    "   - **Açıqlama**: Modelin bir parametri ilə səhv baş verərsə, o vəziyyətdə neçə xal verilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `float` və ya `np.nan` dəyəri (nəticə olmaması).\n",
    "   - **Nümunə**: `error_score=np.nan`\n",
    "\n",
    "### GridSearchCV ilə XGBClassifier tətbiqi\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr grid-i\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# XGBClassifier modeli\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# GridSearchCV obyektinin yaradılması\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Modelin uyğunlaşdırılması (fit)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və nəticəni çap etmək\n",
    "print(f\"Ən yaxşı parametrlər: {grid_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {grid_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### Parametrlərin Dəyərlər Aralıqları\n",
    "\n",
    "1. **`n_estimators`** (Ağac sayı):\n",
    "   - Aralıq: [50, 100, 200, 500]\n",
    "   \n",
    "2. **`max_depth`** (Ağacların dərinliyi):\n",
    "   - Aralıq: [3, 5, 7, 10]\n",
    "   \n",
    "3. **`learning_rate`** (Öyrənmə sürəti):\n",
    "   - Aralıq: [0.01, 0.05, 0.1, 0.2]\n",
    "   \n",
    "4. **`subsample`** (Nümunə payı):\n",
    "   - Aralıq: [0.5, 0.7, 1.0]\n",
    "   \n",
    "5. **`colsample_bytree`** (Xüsusiyyət payı):\n",
    "   - Aralıq: [0.6, 0.8, 1.0]\n",
    "\n",
    "6. **`gamma`** (Bölünmə üçün minimum itki azaldılması):\n",
    "   - Aralıq: [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "Bu aralıqlar verilən problemə görə genişlədilə və ya daraldıla bilər. `GridSearchCV` bütün bu aralıqlardakı mümkün kombinasiyaları sınayaraq ən optimal parametrləri təyin edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889bfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bace169f",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1859a5",
   "metadata": {},
   "source": [
    "`RandomizedSearchCV` istifadə edərkən, müxtəlif parametrlər var və hər birinin müəyyən funksiyası var. Hər parametrin də nə dəyər ala biləcəyini və bu dəyərlərin aralıqlarını başa düşmək vacibdir. Gəlin, əsas parametrləri və onların izahını nəzərdən keçirək.\n",
    "\n",
    "### `RandomizedSearchCV` əsas parametrləri və izahı:\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **İzah**: Axtarılacaq model. Məsələn, `XGBClassifier`, `SVC`, `RandomForestClassifier`, və s.\n",
    "   - **Dəyər aralığı**: Bir machine learning modeli (`estimator`) obyektini alır.\n",
    "\n",
    "2. **`param_distributions`**:\n",
    "   - **İzah**: Hiperparametrlərin təsadüfi axtarışı üçün parametr dəyərlərinin aralıqları.\n",
    "   - **Dəyər aralığı**: Hiperparametrlərin mümkün dəyərlərinin siyahısını və ya numpy array-i kimi obyektlər alır.\n",
    "   - **Misal**:\n",
    "     ```python\n",
    "     param_dist = {\n",
    "         'n_estimators': np.arange(50, 400, 50),\n",
    "         'max_depth': [3, 4, 5, 6],\n",
    "         'learning_rate': np.linspace(0.01, 0.3, 10)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`n_iter`**:\n",
    "   - **İzah**: Axtarış zamanı təsadüfi olaraq neçə kombinasiya yoxlanılacaq.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (adətən 10-100 arası). Axtarışın sürəti və keyfiyyəti arasında balans yaradılır.\n",
    "   - **Misal**: `n_iter=50`\n",
    "\n",
    "4. **`cv`**:\n",
    "   - **İzah**: Çarpaz təsdiqləmə qatlarının sayı (neçə fold ilə model yoxlanılacaq).\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (məsələn, `cv=3`, `cv=5`). Daha çox qat dəqiq nəticələr verə bilər, lakin təlim müddətini uzadır.\n",
    "   - **Misal**: `cv=5` (beş qatlı çarpaz təsdiqləmə)\n",
    "\n",
    "5. **`scoring`**:\n",
    "   - **İzah**: Modelin performansını qiymətləndirmək üçün ölçü.\n",
    "   - **Dəyər aralığı**: `accuracy`, `f1`, `roc_auc`, `precision`, `recall`, və s.\n",
    "   - **Misal**: `scoring='accuracy'`, `scoring='roc_auc'`\n",
    "\n",
    "6. **`random_state`**:\n",
    "   - **İzah**: Təkrarlanabilirlik üçün təsadüfi toxum. Nəticələrin təkrarlanabilməsi üçün istifadə edilir.\n",
    "   - **Dəyər aralığı**: İstənilən tam ədəd (məsələn, `random_state=42`).\n",
    "   - **Misal**: `random_state=42`\n",
    "\n",
    "7. **`verbose`**:\n",
    "   - **İzah**: Təlim prosesi haqqında ekrana nə qədər məlumat çıxarılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: `0`, `1`, `2`, və ya daha yüksək (daha yüksək dəyərlər daha çox məlumat göstərir).\n",
    "   - **Misal**: `verbose=1` (orta dərəcədə məlumat)\n",
    "\n",
    "8. **`n_jobs`**:\n",
    "   - **İzah**: Paralel işləmə üçün istifadə ediləcək CPU nüvələrinin sayı.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd, `-1` bütün nüvələri istifadə etmək deməkdir.\n",
    "   - **Misal**: `n_jobs=-1` (bütün mövcud CPU nüvələrini istifadə etmək üçün)\n",
    "\n",
    "9. **`pre_dispatch`**:\n",
    "   - **İzah**: Paralel axtarış üçün neçə iş prosesinin əvvəlcədən yaradılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: Tam ədəd və ya `all` kimi dəyərlər ala bilər.\n",
    "   - **Misal**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "### XGBClassifier ilə `RandomizedSearchCV` misalı:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr aralığı\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 400, 50),       # Ağac sayı\n",
    "    'max_depth': np.arange(3, 10),                # Ağacların dərinliyi\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Öyrənmə sürəti\n",
    "    'subsample': np.linspace(0.5, 1.0, 5),        # Nümunə payı\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 5), # Xüsusiyyət payı\n",
    "    'gamma': np.linspace(0, 0.5, 5),              # Bölünmə üçün minimum itki azaldılması\n",
    "    'reg_alpha': np.linspace(0, 1, 5),            # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': np.linspace(1, 2, 5),           # L2 cəriməsi (Ridge)\n",
    "    'booster': ['gbtree', 'dart']                 # Booster növü\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV obyektinin yaradılması\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50,           # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=5,                 # Çarpaz təsdiqləmə qatları\n",
    "    verbose=1,            # Geri bildirim səviyyəsi\n",
    "    random_state=42,      # Təsadüfi toxum\n",
    "    n_jobs=-1             # Bütün CPU nüvələrini istifadə et\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {random_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {random_search.best_score_}\")\n",
    "```\n",
    "\n",
    "Bu kod `RandomizedSearchCV` istifadə edərək `XGBClassifier` modelinin hiperparametrlərini optimallaşdırır və ən yaxşı parametrləri çıxarır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9486f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3063f1",
   "metadata": {},
   "source": [
    "## BayesSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07a07e",
   "metadata": {},
   "source": [
    "`BayesSearchCV` modeli, hiperparametr optimallaşdırması üçün Bayesian Optimization metodundan istifadə edən bir alqoritmdir. Bu metod, hiperparametr arama prosesini daha ağıllı və sürətli edir, çünki hər bir axtarış addımı əvvəlki nəticələrə əsaslanaraq həyata keçirilir.\n",
    "\n",
    "### 1. **`estimator`**:\n",
    "   - **Açıqlama**: Optimallaşdırmaq istədiyiniz model obyektidir (məsələn, `XGBClassifier`).\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()` və s.\n",
    "\n",
    "### 2. **`search_spaces`**:\n",
    "   - **Açıqlama**: Bu parametr, optimallaşdırılacaq hiperparametrləri və onların dəyərlər aralığını təyin edir. `BayesSearchCV` bu aralığa əsaslanaraq axtarış həyata keçirir.\n",
    "   - **Dəyərlər**: Müxtəlif parametrlərin dəyərlər aralığıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     search_spaces = {\n",
    "         'n_estimators': (50, 400),\n",
    "         'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "         'max_depth': (3, 10),\n",
    "         'subsample': (0.5, 1.0),\n",
    "         'colsample_bytree': (0.5, 1.0),\n",
    "         'gamma': (0, 0.5),\n",
    "         'reg_alpha': (0, 1),\n",
    "         'reg_lambda': (1, 2)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### 3. **`n_iter`**:\n",
    "   - **Açıqlama**: Axtarış zamanı edilməsi planlaşdırılan iterasiya sayı. Bu dəyər Bayesian Optimization ilə təyin olunan fərqli hiperparametr dəstlərinin sayını bildirir.\n",
    "   - **Dəyərlər**: Müxtəlif iterasiya sayı.\n",
    "   - **Nümunə**: `n_iter=30` (30 iterasiya).\n",
    "\n",
    "### 4. **`cv`**:\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə (Cross-validation) qatlarının sayını təyin edir. Məlumatları neçə qrupa böləcəyini göstərir.\n",
    "   - **Dəyərlər**: 3, 5 və ya 10.\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "### 5. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün hansı metrikdən istifadə olunacağını təyin edir.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`.\n",
    "\n",
    "### 6. **`n_points`**:\n",
    "   - **Açıqlama**: Hər iterasiyada eyni anda neçə fərqli parametri sınamaq lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `n_points=1` (hər iterasiyada bir parametr dəsti sınanır), `n_points=2` (hər iterasiyada iki parametr dəsti sınanır).\n",
    "   - **Nümunə**: `n_points=3` (hər iterasiyada 3 fərqli parametr dəsti sınanır).\n",
    "\n",
    "### 7. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "### 8. **`random_state`**:\n",
    "   - **Açıqlama**: Nəticələrin təkrar istehsal edilə bilməsi üçün təsadüfi toxum dəyəri təyin edir.\n",
    "   - **Dəyərlər**: İstənilən integer dəyəri.\n",
    "   - **Nümunə**: `random_state=42`.\n",
    "\n",
    "### 9. **`optimizer_kwargs`**:\n",
    "   - **Açıqlama**: Bayesian Optimization alqoritmi ilə bağlı əlavə seçimləri idarə edir (məsələn, akvizisiya funksiyası).\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     optimizer_kwargs = {\n",
    "         'base_estimator': 'GP',  # Gaussian Process (GP) ilə optimizasiya edəcək\n",
    "         'acq_func': 'EI'         # Expected Improvement (EI) akvizisiya funksiyası\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### XGBClassifier ilə `BayesSearchCV` tətbiqi nümunəsi:\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Hiperparametr aralığı\n",
    "search_spaces = {\n",
    "    'n_estimators': (50, 400),                # Ağac sayı\n",
    "    'learning_rate': (0.01, 0.3, 'log-uniform'), # Öyrənmə sürəti\n",
    "    'max_depth': (3, 10),                     # Maksimum dərinlik\n",
    "    'subsample': (0.5, 1.0),                  # Nümunə payı\n",
    "    'colsample_bytree': (0.5, 1.0),           # Xüsusiyyət payı\n",
    "    'gamma': (0, 0.5),                        # Minimum itki azaldılması\n",
    "    'reg_alpha': (0, 1),                      # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': (1, 2)                      # L2 cəriməsi (Ridge)\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# BayesSearchCV obyektinin yaradılması\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb, \n",
    "    search_spaces=search_spaces, \n",
    "    n_iter=30,            # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=3,                 # Çarpaz təsdiqləmə qatları\n",
    "    n_jobs=-1,            # Bütün CPU nüvələrini istifadə et\n",
    "    random_state=42       # Təsadüfi toxum\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {bayes_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {bayes_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### BayesSearchCV-nin əsas üstünlükləri:\n",
    "- **Təsadüfi olmaması**: Bayesian Optimization əvvəlki nəticələrə əsaslanaraq növbəti parametri ağıllı şəkildə seçir.\n",
    "- **Daha az iterasiya**: RandomizedSearchCV ilə müqayisədə daha az iterasiya ilə daha optimal nəticələr əldə etmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df419cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40fef1d9",
   "metadata": {},
   "source": [
    "## GridSearchCV, RandomizedSearchCV, BayesSearchCV ferqler cedveli?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8222994",
   "metadata": {},
   "source": [
    "Aşağıda metodları yuxarıda göstərən cədvəl formasını təqdim edirəm:\n",
    "\n",
    "| **Metod**             | **GridSearchCV**                                                            | **RandomizedSearchCV**                                                   | **BayesSearchCV**                                                        |\n",
    "|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **İş Prinsipi**        | Bütün parametrlərin mümkün kombinasiyalarını yoxlayır.                       | Parametrlərin mümkün dəyərlərini təsadüfi seçir və müəyyən sayda kombinasiyanı yoxlayır. | Keçmiş nəticələrə əsasən növbəti sınanacaq parametrləri daha ağıllı seçir. |\n",
    "| **Üstünlüklər**        | Tam dəqiqliklə optimal parametrləri tapır.                                   | Daha sürətli və resurs qənaətcilidir.                                     | Daha ağıllı seçimlər edir, daha səmərəli və tez converge olur.             |\n",
    "| **Çatışmazlıqlar**     | Hesablama baxımından çox vaxt və resurs tələb edir.                          | Tam optimal parametrləri tapmaya bilər.                                   | Implementasiyası daha mürəkkəb ola bilər.                                  |\n",
    "| **Nə vaxt istifadə olunur** | Kiçik parametrlər sahəsində tam optimal parametrləri tapmaq istəyəndə.       | Böyük datasetlər və çoxlu sayda parametrlərlə işləyərkən.                 | Daha böyük parametrlər sahəsi üçün və daha səmərəli hyperparameter tuning üçün. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3679672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267a3c5b",
   "metadata": {},
   "source": [
    "## K-means ve AgglomerativeClustering ferqi cedvel seklinde cap ele?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68addf",
   "metadata": {},
   "source": [
    "Aşağıda K-Means və Agglomerative Clustering modellərinin fərqlərini cədvəl şəklində göstərə bilərik:\n",
    "\n",
    "| **Xüsusiyyət**                    | **K-Means**                                 | **Agglomerative Clustering**                        |\n",
    "|-----------------------------------|---------------------------------------------|----------------------------------------------------|\n",
    "| **Metod**                         | Mərkəzə əsaslanan klasterləşdirmə            | Hierarchical (Aqqlomerativ) klasterləşdirmə        |\n",
    "| **İşləmə Prosesi**                | Klaster mərkəzləri təyin edilir və təkrarən yenilənir. | Aşağıdan yuxarıya doğru qruplar birləşdirilir.      |\n",
    "| **Klaster Sayı**                  | Əvvəldən müəyyən edilməlidir (`n_clusters`).  | Əvvəldən müəyyən edilməlidir (`n_clusters`).        |\n",
    "| **Hesablama Mürəkkəbliyi**        | Nisbətən sürətli, \\(O(n \\cdot k \\cdot d)\\)   | Nisbətən yavaş, \\(O(n^3)\\)                          |\n",
    "| **Klaster Forması**               | Sferik (klasterlər dairəvi olur)             | Daha çox forma elastikliyi, qeyri-sferik klasterlər |\n",
    "| **Məlumat Strukturu**             | Yalnız nöqtələr və klaster mərkəzləri        | Dendrogram (ağac strukturu)                        |\n",
    "| **Məsafə Ölcüsü**                 | Adətən Euclidean məsafəsi                    | Müxtəlif məsafə metrikləri (Euclidean, Manhattan)  |\n",
    "| **Klaster Mərkəzi**               | Hər klaster üçün bir mərkəz nöqtəsi vardır   | Klasterlərin mərkəzi anlayışı yoxdur               |\n",
    "| **Böyük Məlumatlar**              | Daha çox böyük məlumat dəstləri üçün uyğundur | Kiçik məlumat dəstləri üçün daha uyğundur          |\n",
    "| **Təsirli Klasterlərin Sayı**     | Sabit klaster sayı verir                     | Hierarchical model olaraq müxtəlif səviyyələrdə klasterlər yarada bilir |\n",
    "| **İşləmə Sürəti**                 | Daha sürətli                                 | Daha yavaş                                          |\n",
    "| **Təsadüfi Başlanğıc Problemi**    | Bəli, təsadüfi başlanğıc nöqtələri istifadə edir, nəticələr dəyişə bilər | Xeyr, başlanğıc nöqtələri sabitdir                |\n",
    "\n",
    "Bu cədvəl K-Means və Agglomerative Clustering modelləri arasında əsas fərqləri əks etdirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11275027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51b820f",
   "metadata": {},
   "source": [
    "## K-means ve AgglomerativeClustering isleme prinsipleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a298c02",
   "metadata": {},
   "source": [
    "Aşağıda K-Means və Agglomerative Clustering modellərinin işləmə prinsiplərini cədvəl şəklində verək:\n",
    "\n",
    "| **Xüsusiyyət**                  | **K-Means İşləmə Prinsipi**                                                                 | **Agglomerative Clustering İşləmə Prinsipi**                                                        |\n",
    "|---------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Başlanğıc**                   | Klaster mərkəzləri (centroids) təsadüfi olaraq seçilir.                                      | Hər bir nöqtə əvvəlcə öz-özünə klasterdir.                                                           |\n",
    "| **Addım 1: Klasterləşdirmə**     | Hər bir nöqtə ona ən yaxın mərkəzə (centroid) təyin edilir.                                 | Ən yaxın nöqtələr (və ya klasterlər) cüt-cüt birləşdirilir.                                          |\n",
    "| **Addım 2: Mərkəz Yenilənməsi** | Hər bir klasterin mərkəzi (centroid) həmin klasterdəki nöqtələrin ortası olaraq yenilənir.  | Qonşu klasterlər birləşdirildikcə klaster mərkəzləri təyin olunmur, klasterlər ardıcıl birləşdirilir. |\n",
    "| **Təkrarlama**                  | Nöqtələr yenidən ən yaxın mərkəzə təyin edilir, bu proses mərkəzlər sabit qalana qədər təkrarlanır. | Bütün nöqtələr birləşdirilənə qədər bu birləşmə prosesi davam edir.                                   |\n",
    "| **Nəticə**                      | Sabit klaster mərkəzləri və ona uyğun nöqtələr ilə bitir.                                    | Klasterlər aralarındakı məsafələrə əsasən hierarchical (ağacvari) formada təşkil olunur.               |\n",
    "| **Klaster Sayı**                | İstifadəçi əvvəlcədən klaster sayını müəyyən etməlidir.                                      | İstifadəçi əvvəlcədən klaster sayını müəyyən etməli, ya da dendrogram vasitəsilə qərar verə bilər.     |\n",
    "| **Məsafə Ölçüsü**               | Nöqtələr mərkəzlərdən Euclidean məsafəsinə görə təyin edilir.                                | Nöqtələr arası Euclidean, Manhattan və ya digər məsafə metriklərinə görə ən yaxın klaster tapılır.     |\n",
    "| **Təkrarlanma Sayı**            | Mərkəzlər sabitləşənə qədər təkrarlanır.                                                    | Bütün nöqtələr tək bir klasterə birləşdirilənə qədər təkrarlanır.                                     |\n",
    "\n",
    "Bu cədvəl, hər iki modelin işləmə prinsiplərini sadə və qarşılaşdırmalı şəkildə izah edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc42a2d0",
   "metadata": {},
   "source": [
    "## Autocorrelation (Avtokorrelyasiya) ve  Multicollinearity (Çoxxəttlik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb1d7c",
   "metadata": {},
   "source": [
    "**Autocorrelation** və **Multicollinearity** statistik modellərdə fərqli problemlər və anlayışlar olaraq çıxış edir. Onları ayrı-ayrılıqda izah edim:\n",
    "\n",
    "### 1. **Autocorrelation (Avtokorrelyasiya):**\n",
    "Avtokorrelyasiya, bir zaman seriyasında müşahidələrin öz əvvəlki dəyərləri ilə bağlı olduğu vəziyyətdir. Yəni, bir dəyişənin cari dəyəri əvvəlki zaman nöqtələrindəki dəyərləri ilə korrelyasiyadadır. Avtokorrelyasiya zamanı ardıcıl müşahidələr bir-birindən asılı ola bilər.\n",
    "\n",
    "- **Məqsəd:** Avtokorrelyasiyanı müəyyən etmək üçün əsasən zaman seriyası analizində istifadə edilir.\n",
    "- **Nümunə:** Hava istiliyi seriyasında bugünkü istilik dəyərinin dünənki və ondan əvvəlki günlərlə bağlı olması.\n",
    "- **Problem:** Avtokorrelyasiya olduqda statistik modellər (məsələn, regresiya modelləri) düzgün nəticələr verməyə bilər, çünki müşahidələr arasında müstəqillik pozulur.\n",
    "\n",
    "#### Avtokorrelyasiyanı aşkar etmək üçün metodlar:\n",
    "- **Durbin-Watson testi**: Bu test avtokorrelyasiyanın olub-olmamasını yoxlayır. Dəyərlər 0-dan 4-ə qədər olur, 2 isə heç bir avtokorrelyasiyanın olmadığını göstərir.\n",
    "\n",
    "### 2. **Multicollinearity (Çoxxəttlik):**\n",
    "Çoxxəttlik, bir neçə müstəqil dəyişənin bir-biri ilə güclü korrelyasiya göstərdiyi vəziyyətdir. Bu, əsasən çoxsaylı xətti regresiya modellərində ortaya çıxır və modelin dəqiq interpretasiyasını çətinləşdirir.\n",
    "\n",
    "- **Məqsəd:** Multicollinearity-dən qaçınmaq, modelin sadəliyini və proqnoz gücünü qorumaq üçün vacibdir.\n",
    "- **Nümunə:** \"Ev sahəsinin ölçüsü\" və \"Otaq sayı\" kimi dəyişənlər bir-birinə güclü korrelyasiya göstərə bilər.\n",
    "- **Problem:** Çoxxəttlik olduqda regresiya əmsalları qeyri-sabit ola bilər, yəni kiçik dəyişikliklər modeldəki əmsalları kəskin dəyişdirə bilər.\n",
    "\n",
    "#### Multicollinearity aşkar etmək üçün metodlar:\n",
    "- **Variance Inflation Factor (VIF)**: VIF dəyərləri dəyişənlərin çoxxəttlik probleminin nə qədər ciddi olduğunu göstərir. VIF dəyəri 5-dən böyük olduqda, çoxxəttlik ciddi bir problem sayılır.\n",
    "\n",
    "Bu anlayışlar fərqli olsa da, hər ikisi statistik modellərin düzgün işləməsi üçün nəzərə alınmalı olan amillərdir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3be0e",
   "metadata": {},
   "source": [
    "## Avtokorrelyasiya və çoxxəttlik üçün real misallar aşağıdakı kimidir:\n",
    "\n",
    "### 1. **Autocorrelation (Avtokorrelyasiya):**\n",
    "**Misal:** Hava istiliyi proqnozları.\n",
    "\n",
    "- **Real vəziyyət:** Bir şəhərin gündəlik hava istiliklərini müşahidə edirik. Deyək ki, dünən 30°C idi, bu gün isə 31°C. Sabahın proqnozu da dünən və bu günkü temperaturla bağlıdır. Beləliklə, bugünkü istilik sabahın istiliyini müəyyən dərəcədə təsir edə bilər. Bu ardıcıl müşahidələr arasındakı asılılıq avtokorrelyasiyadır.\n",
    "  \n",
    "- **Problem:** Hava proqnozu kimi zaman seriyasında ardıcıl dəyərlər bir-birinə çox bağlıdırsa, avtokorrelyasiya problemi yaranır və bunu nəzərə almayan modellər səhv nəticələr verə bilər.\n",
    "\n",
    "**Praktik təsir:** Bir iqtisadiyyatda aylıq satış rəqəmlərinin əvvəlki aylara təsir etməsi də avtokorrelyasiya ilə əlaqəli ola bilər. Məsələn, şirkət dekabr ayında çox satış edirsə, yanvar ayında həmin trend davam edə bilər.\n",
    "\n",
    "### 2. **Multicollinearity (Çoxxəttlik):**\n",
    "**Misal:** Ev qiymətləri ilə bağlı analiz.\n",
    "\n",
    "- **Real vəziyyət:** Bir şəhərdə ev qiymətləri modelini qurmaq istəyirsiniz. Modelinizdə dəyişənlər kimi evin sahəsi, otaq sayı, və hamam sayı kimi məlumatlar var. Evin sahəsi ilə otaq sayı arasında güclü bir əlaqə var, çünki daha böyük sahəsi olan evlər daha çox otağa malikdir.\n",
    "\n",
    "- **Problem:** Bu vəziyyətdə müstəqil dəyişənlər bir-biri ilə çox yüksək korrelyasiya göstərir. Bu, çoxxəttlik problemidir. Modelinizdə həm evin sahəsi, həm də otaq sayı varsa, bu iki dəyişən bir-birini təkrarlaya bilər və nəticədə modeldəki regresiya əmsalları qeyri-sabit olur.\n",
    "\n",
    "**Praktik təsir:** İqtisadi modeldə həm ümumi gəliri, həm də şəxsi gəliri müstəqil dəyişən kimi istifadə etmək çoxxəttliyə səbəb ola bilər, çünki şəxsi gəlir ümumi gəlirin bir hissəsidir. Bu da modelin proqnoz gücünü zəiflədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a162c2",
   "metadata": {},
   "source": [
    "## LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0eae8e",
   "metadata": {},
   "source": [
    "Bu kod, DataFrame-də obyekt (`object`) tipində olan sütunları `LabelEncoder` istifadə edərək ədədi dəyərlərə çevirir və hər sütun üçün istifadə olunan `LabelEncoder` obyektini saxlayır. Məsələn:\n",
    "\n",
    "```python\n",
    "label_encoders = {}  # Hər sütun üçün LabelEncoder obyektlərini saxlamaq üçün boş lüğət yaradılır.\n",
    "\n",
    "# Obyekt tipli sütunlar arasında dövr edilir.\n",
    "for column in df_final_2.select_dtypes(include=['object']).columns:  \n",
    "    le = LabelEncoder()  # Hər sütun üçün yeni LabelEncoder obyekti yaradılır.\n",
    "    # Həmin sütun ədədi dəyərlərə çevrilir və DataFrame-dəki həmin sütunu əvəz edir.\n",
    "    df_final_2[column] = le.fit_transform(df_final_2[column])  \n",
    "    label_encoders[column] = le  # LabelEncoder obyekti saxlanılır.\n",
    "```\n",
    "\n",
    "1. İlk olaraq, boş bir `label_encoders` lüğəti yaradılır. Bu lüğətdə sütun adlarına uyğun olaraq `LabelEncoder` obyektləri saxlanacaq. Bu, sonradan lazım olarsa, kodlaşdırılmış dəyərləri orijinal vəziyyətinə geri çevirməyə imkan verir.\n",
    "\n",
    "2. `for column in df_final_2.select_dtypes(include=['object']).columns` döngəsi vasitəsilə DataFrame-dəki obyekt tipində olan bütün sütunlar tapılır və hər biri döngədə işlədirilir.\n",
    "\n",
    "3. Hər sütun üçün yeni bir `LabelEncoder` obyekti yaradılır (`le = LabelEncoder()`), daha sonra həmin sütunun dəyərləri `le.fit_transform(df_final_2[column])` vasitəsilə ədədi kodlara çevrilir. Bu çevrilmiş sütun DataFrame-dəki orijinal sütunu əvəz edir.\n",
    "\n",
    "4. Hər sütun üçün yaradılmış `LabelEncoder` obyekti `label_encoders[column] = le` ilə lüğətdə saxlanır ki, sonradan sütunların orijinal dəyərlərinə qayıda bilmək mümkün olsun.\n",
    "\n",
    "Bu şəkildə, mətn və ya obyekt tipli dəyərlər ədədi dəyərlərə çevrilir və bu sütunların `LabelEncoder` obyektləri də saxlanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124461e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6454c2e8",
   "metadata": {},
   "source": [
    "## LabelEncoder ilə çevrilmiş dəyərləri orijinal vəziyyətinə geri çevirmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf1f4a",
   "metadata": {},
   "source": [
    "`LabelEncoder` ilə çevrilmiş dəyərləri orijinal vəziyyətinə geri çevirmək üçün saxlanmış `LabelEncoder` obyektlərindən istifadə edərək kodlaşdırılmış dəyərləri geri qaytarmaq mümkündür. Məsələn, əgər sütunu əvvəlcə `LabelEncoder` ilə çevirmisinizsə, həmin sütunu geri çevirmək üçün `inverse_transform()` metodunu istifadə edirsiniz.\n",
    "\n",
    "Kodun necə işlədiyini göstərim:\n",
    "\n",
    "```python\n",
    "# Sütunu geri çevirmək üçün əvvəl saxlanmış `LabelEncoder` obyektini istifadə edirik.\n",
    "for column, le in label_encoders.items():\n",
    "    df_final_2[column] = le.inverse_transform(df_final_2[column])\n",
    "```\n",
    "\n",
    "İzah:\n",
    "\n",
    "1. `label_encoders.items()` vasitəsilə saxladığınız bütün sütunlar və onların müvafiq `LabelEncoder` obyektləri arasında dövr edilir.\n",
    "2. `df_final_2[column] = le.inverse_transform(df_final_2[column])` xətti ilə əvvəl kodlaşdırılmış dəyərləri yenidən orijinal vəziyyətinə çevirirsiniz. Bu metod sütunda olan ədədi dəyərləri orijinal mətn və ya obyekt dəyərlərinə geri qaytarır.\n",
    "\n",
    "Beləliklə, DataFrame-dəki bütün çevrilmiş sütunlar orijinal mətn və obyekt dəyərlərinə qaytarılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fab710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55de2835",
   "metadata": {},
   "source": [
    "## df datam var VIF hesablama codu yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b1571",
   "metadata": {},
   "source": [
    "Əgər `df` DataFrame-inizdə multicollinearity-ni qiymətləndirmək üçün VIF (Variance Inflation Factor) hesablamaq istəyirsinizsə, aşağıdakı kodu istifadə edə bilərsiniz. Bu kod müstəqil dəyişənlər üçün VIF dəyərlərini hesablayacaq və nəticələri çap edəcəkdir:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# DataFrame-dən müstəqil dəyişənləri seçin (əgər target sütunu varsa onu çıxarın)\n",
    "# Burada target sütunu olmadığı təqdirdə bütün sütunları istifadə edirik\n",
    "X_VIF = df.copy()\n",
    "\n",
    "# Sabit sütununu əlavə edin\n",
    "X_VIF = add_constant(X_VIF)\n",
    "\n",
    "# VIF dəyərlərini hesablayın\n",
    "vif = pd.DataFrame()\n",
    "vif['Feature'] = X_VIF.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_VIF.values, i) for i in range(X_VIF.shape[1])]\n",
    "\n",
    "print(vif)\n",
    "```\n",
    "\n",
    "### İzahat\n",
    "\n",
    "1. **DataFrame-i Nümunə**:\n",
    "   - Bu kod, `df` adlı DataFrame-dəki bütün müstəqil dəyişənlər üzərində VIF hesablaması aparır. Əgər sizin `df` DataFrame-də target sütunu varsa, onu çıxarmaq lazımdır. Məsələn, target sütunu sonuncu sütun olarsa, `df.iloc[:, :-1]` istifadə edə bilərsiniz.\n",
    "\n",
    "2. **Sabit Sütununu Əlavə Etmək**:\n",
    "   - `add_constant` funksiyası ilə sabit (intercept) sütunu əlavə edilir. Bu, VIF hesablama prosesində istifadə olunur.\n",
    "\n",
    "3. **VIF Hesablama**:\n",
    "   - `variance_inflation_factor` funksiyasından istifadə edərək VIF dəyərləri hesablanır. Hər bir sütun üçün VIF dəyərləri hesablamaq üçün `X_VIF.values` və sütun indeksləri istifadə olunur.\n",
    "\n",
    "4. **Nəticələrin Çap Edilməsi**:\n",
    "   - VIF dəyərləri və müvafiq sütun adları `vif` adlı DataFrame-də toplanır və çap olunur.\n",
    "\n",
    "Bu kod müstəqil dəyişənlərin multicollinearity səviyyəsini qiymətləndirməkdə kömək edəcəkdir. VIF dəyərləri 10-dan böyük olan dəyişənlər multicollinearity problemi göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da0c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9062bb4",
   "metadata": {},
   "source": [
    "## iki sutunun deyerleri hamisi eyni olub olmaidigini nece yxolayim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9ee82",
   "metadata": {},
   "source": [
    "İki sütunun dəyərlərinin tamamilə eyni olub-olmadığını yoxlamaq üçün aşağıdakı yolları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`equals()` metodu:**\n",
    "Bu üsul tam uyğunluğu yoxlayır və `True` və ya `False` qaytarır.\n",
    "\n",
    "```python\n",
    "df['sutun1'].equals(df['sutun2'])\n",
    "```\n",
    "\n",
    "### 2. **`==` operatoru ilə:** \n",
    "Bu üsul iki sütunun elementlərini müqayisə edir və nəticəni boolean bir sütun kimi qaytarır. Hamısının eyni olub-olmadığını yoxlamaq üçün `all()` funksiyasını istifadə edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "(df['sutun1'] == df['sutun2']).all()\n",
    "```\n",
    "\n",
    "### 3. **`np.array_equal()` funksiyası ilə:** \n",
    "Bu da iki sütunu müqayisə edir və tam bərabərlik üçün `True` və ya `False` qaytarır.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "np.array_equal(df['sutun1'], df['sutun2'])\n",
    "```\n",
    "\n",
    "Bu üsulların hər biri iki sütunun eyni olub-olmadığını yoxlamaq üçün faydalıdır. Əgər hansısa bir fərq varsa, `False` qaytaracaqdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54cce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd31f0a1",
   "metadata": {},
   "source": [
    "## Klaster analizində modellərin düzgünlüyünü qiymətləndirmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84a4df",
   "metadata": {},
   "source": [
    "Klaster analizində modellərin düzgünlüyünü qiymətləndirmək üçün bir neçə daha yaxşı metod mövcuddur. Aşağıdakı metodlardan istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Adjusted Rand Index (ARI)**:\n",
    "   - ARI nümunələrin həqiqi etiketlərə uyğun olub-olmamasını yoxlayan bir metrikdir. Bu metod, təsadüfi uyğunluqları nəzərə alaraq klasterlərin keyfiyyətini ölçür. ARI dəyəri 0-dan 1-ə qədərdir. 1 ən yaxşı uyğunluğu, 0 isə təsadüfi uyğunluğu göstərir.\n",
    "   - **ARI üstünlüyü**: Klasterlərin tərkibini və düzgün bölüşdürməni təsadüfi uyğunluqları da nəzərə alaraq dəqiqliklə ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "   ari_kmeans = adjusted_rand_score(df_final['species'], df_final['k_means_cluster'])\n",
    "   ari_ac = adjusted_rand_score(df_final['species'], df_final['ac_cluster'])\n",
    "   ari_mini = adjusted_rand_score(df_final['species'], df_final['mini_cluster'])\n",
    "\n",
    "   print(\"ARI K-Means:\", ari_kmeans)\n",
    "   print(\"ARI Agglomerative Clustering:\", ari_ac)\n",
    "   print(\"ARI MiniBatchKMeans:\", ari_mini)\n",
    "   ```\n",
    "\n",
    "### 2. **Normalized Mutual Information (NMI)**:\n",
    "   - NMI klasterlər arasındakı qarşılıqlı əlaqəni və informasiyanın nə qədər itirildiyini ölçür. Dəyəri 0 ilə 1 arasında dəyişir; 1 dəyəri klasterlərin tam uyğun olduğunu göstərir.\n",
    "   - **NMI üstünlüyü**: Təkcə düzgün yerləşdirməni deyil, həm də klasterlərdə olan məlumatların keyfiyyətini ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "   nmi_kmeans = normalized_mutual_info_score(df_final['species'], df_final['k_means_cluster'])\n",
    "   nmi_ac = normalized_mutual_info_score(df_final['species'], df_final['ac_cluster'])\n",
    "   nmi_mini = normalized_mutual_info_score(df_final['species'], df_final['mini_cluster'])\n",
    "\n",
    "   print(\"NMI K-Means:\", nmi_kmeans)\n",
    "   print(\"NMI Agglomerative Clustering:\", nmi_ac)\n",
    "   print(\"NMI MiniBatchKMeans:\", nmi_mini)\n",
    "   ```\n",
    "\n",
    "### 3. **Silhouette Score**:\n",
    "   - Silhouette skoru klasterlərin içindəki nümunələrin nə qədər yaxın olduğunu və digər klasterlərdən nə qədər uzaq olduğunu ölçür. Bu metrik klasterlərin təbii olub-olmadığını qiymətləndirir.\n",
    "   - **Silhouette üstünlüyü**: Klasterlər arası fərqliliyi də ölçür və hər klasterin uyğun təyin olunduğunu göstərir.\n",
    "   - Ancaq burada həqiqi etiketlərə ehtiyac yoxdur, yalnız klasterləşdirmənin uyğunluğunu ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import silhouette_score\n",
    "\n",
    "   silhouette_kmeans = silhouette_score(df_final.drop(columns=['species']), df_final['k_means_cluster'])\n",
    "   silhouette_ac = silhouette_score(df_final.drop(columns=['species']), df_final['ac_cluster'])\n",
    "   silhouette_mini = silhouette_score(df_final.drop(columns=['species']), df_final['mini_cluster'])\n",
    "\n",
    "   print(\"Silhouette K-Means:\", silhouette_kmeans)\n",
    "   print(\"Silhouette Agglomerative Clustering:\", silhouette_ac)\n",
    "   print(\"Silhouette MiniBatchKMeans:\", silhouette_mini)\n",
    "   ```\n",
    "\n",
    "### 4. **Confusion Matrix (Karmaşıqlıq Matrisi)**:\n",
    "   - Klasterləşdirmənin performansını vizuallaşdırmaq üçün **confusion matrix** (çapraz matrisi) yarada bilərsiniz. Bu metod, hər klasterlə real etiketlər arasındakı uyğunluğu göstərir.\n",
    "   - **Üstünlüyü**: Hər bir klasterin nə qədər düzgün təyin olunduğunu dəqiq göstərə bilər.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import confusion_matrix\n",
    "   import seaborn as sns\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   cm_kmeans = confusion_matrix(df_final['species'], df_final['k_means_cluster'])\n",
    "   sns.heatmap(cm_kmeans, annot=True, fmt=\"d\")\n",
    "   plt.title('Confusion Matrix for K-Means')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "### Hansı daha yaxşıdır?\n",
    "\n",
    "- **ARI və NMI** real etiketlərlə klasterləşdirmənin nə qədər uyğun olduğunu daha dəqiq göstərir və təsadüfi uyğunluqları da nəzərə alır. Bu səbəbdən, ARI və NMI daha **dəqiq metriklər**dir.\n",
    "- **Silhouette Score** klasterlərin öz içindəki uyğunluğu göstərsə də, real etiketləri nəzərə almır, ancaq klasterlərin nə qədər təbii formalaşdığını göstərir.\n",
    "- **Confusion Matrix** isə klasterləşdirmə nəticələrini vizual olaraq başa düşmək üçün çox faydalıdır.\n",
    "\n",
    "### Tövsiyə:\n",
    "- Daha yaxşı bir ölçü əldə etmək üçün **Adjusted Rand Index (ARI)** və **Normalized Mutual Information (NMI)** istifadə edin.\n",
    "- Klasterlərin uyğunluğunu vizuallaşdırmaq üçün isə **Confusion Matrix** kömək edər.\n",
    "\n",
    "Bu metodlar nəticələr arasındakı uyğunsuzluqları daha dəqiq müəyyən etməyə kömək edəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8318c4e3",
   "metadata": {},
   "source": [
    "## Explained_Variance ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e3725",
   "metadata": {},
   "source": [
    "`explained_variance` (izah edilən dispersiya), xüsusən **PCA** (Principal Component Analysis) və digər statistik modellərdə istifadə edilən bir termindir və bir modelin məlumat dəstindəki variasiyanın nə qədərini izah etdiyini göstərir. \n",
    "\n",
    "### PCA kontekstində `explained_variance` nədir?\n",
    "\n",
    "PCA-da, məlumatların əsas komponentlərə parçalanması ilə hər bir komponent müəyyən miqdarda dispersiyanı izah edir. Bu komponentlər, məlumat dəstindəki əsas məlumat strukturlarını tutmağa çalışır. \n",
    "\n",
    "- **İzah edilən dispersiya** hər bir əsas komponentin məlumatlardakı ümumi variasiyanın nə qədərini izah etdiyini ölçür.\n",
    "- Daha çox dispersiya izah edən bir komponent, daha çox məlumat tutmuş olur.\n",
    "\n",
    "### Məsələn:\n",
    "Bir datasetdə PCA tətbiq etdiyiniz zaman, ilk əsas komponent məlumat dəstinin ən böyük dispersiyasını izah edir. İkinci komponent isə, qalan dispersiyanın maksimum hissəsini izah edir və s.\n",
    "\n",
    "### Formel tərif:\n",
    "Əgər bir məlumat dəstindəki ümumi dispersiya (total variance) 100% olsa, əsas komponentlərin hər biri bu dispersiyadan bir hissəni izah edir. Məsələn:\n",
    "- İlk komponent: 60% izah edir,\n",
    "- İkinci komponent: 25% izah edir,\n",
    "- Üçüncü komponent: 10% izah edir.\n",
    "\n",
    "Bu, məlumatın böyük bir hissəsinin ilk bir neçə komponentdə cəmləndiyini göstərir və bu da yüksək ölçülü datanı daha az ölçülü bir formaya salarkən məlumatın çoxunu itirməməyə kömək edir.\n",
    "\n",
    "### Python-da PCA ilə `explained_variance_ratio_` nümunəsi:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# NumPy ilə təsadüfi bir data yaradırıq\n",
    "X = np.random.rand(100, 5)\n",
    "\n",
    "# PCA modelini qururuq və tətbiq edirik\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# İzah edilən dispersiyanı çap edirik\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "Burada `explained_variance_ratio_` hər bir əsas komponentin izah etdiyi dispersiyanın nisbətini göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cb84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e3d497",
   "metadata": {},
   "source": [
    "## LabelEncoder ve get_dummies ferqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f242d00",
   "metadata": {},
   "source": [
    "Burada `LabelEncoder` istifadə edərək kateqorik dəyişənləri ədədi dəyərlərə çevirmişik, lakin bu həmişə ən yaxşı yanaşma olmaya bilər. **Niyə `get_dummies` istifadə etmədik** sualına cavab olaraq, bu iki metodun fərqli məqsədləri olduğunu başa düşmək vacibdir:\n",
    "\n",
    "### 1. **`LabelEncoder`** nədir?\n",
    "`LabelEncoder`, kateqorik dəyişənləri tək bir sütun şəklində ədədi dəyərlərə çevirir. Hər bir kateqorik dəyərə müəyyən bir tam ədəd təyin edir:\n",
    "- Məsələn, cinsiyyəti (`Sex`) təyin edərkən, `Male` = 1, `Female` = 0 kimi kodlaşdırılır.\n",
    "\n",
    "Bu, sıravi (ordinal) və ya sadəcə iki dəyəri olan (binary) dəyişənlər üçün uyğundur, çünki bu cür dəyişənlərdə dəyərlər arasındakı fərq sıralı mənaya malik ola bilər.\n",
    "\n",
    "Ancaq çoxlu kateqorik dəyəri olan dəyişənlər üçün (məsələn, `Embarked` — C, Q, S) bu metod həmişə uyğun olmur. Çünki `LabelEncoder` ilə təyin olunan dəyərlər arasında sıralı münasibət yarana bilər, halbuki bu dəyərlərin bir-birinə qarşı xüsusi bir sıralaması yoxdur (C = 0, Q = 1, S = 2). Bu, modelin performansına mənfi təsir göstərə bilər.\n",
    "\n",
    "### 2. **`get_dummies`** nədir?\n",
    "`get_dummies`, kateqorik dəyişənləri **one-hot encoding** ilə çoxlu ədədi sütuna çevirir. Hər bir kateqorik dəyər üçün ayrıca sütun yaradır və həmin dəyərin olduğu yerlərə 1, olmadığı yerlərə isə 0 təyin edir:\n",
    "- Məsələn, `Embarked` üçün üç sütun yaranır: `Embarked_C`, `Embarked_Q`, `Embarked_S`, və hər birində yalnız bir 1, digərlərində isə 0 olur.\n",
    "\n",
    "Bu metod daha uyğundur, çünki dəyərlər arasında süni sıralama yaranmır.\n",
    "\n",
    "### Bu nümunədə niyə `LabelEncoder` istifadə etdik?\n",
    "Bu sualın cavabı daha çox probleminizə və istifadə etdiyiniz modelə bağlıdır:\n",
    "- **Sadə model**: Əgər sadə bir model qurursunuzsa və `LabelEncoder` ilə kodlaşdırma nəticəsində süni sıralama problemi yaratmırsa, bu üsul sadəlik baxımından uyğundur.\n",
    "- **Daha mürəkkəb modellər** və ya çox dəyərli kateqorik dəyişənlər üçün isə **`get_dummies`** daha yaxşı nəticələr verəcək, çünki o, hər bir dəyərə uyğun ayrı sütunlar yaradır və aralarında sıralı münasibət yaratmır.\n",
    "\n",
    "Əgər `get_dummies` istifadə etmək istəsəniz, bunu aşağıdakı kimi edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "# 'Sex' və 'Embarked' dəyişənləri üçün get_dummies istifadə edirik\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Nəticədə 'Sex_male' və 'Embarked_S', 'Embarked_Q' sütunları yaranacaq\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Burada `drop_first=True` ilə birinci kateqoriyanı atırıq, çünki iki dəyərdən birini təyin etmək kifayət edir (dummy trap-dən qaçınmaq üçün)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc8495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a165b6c",
   "metadata": {},
   "source": [
    "## import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b7482",
   "metadata": {},
   "source": [
    "`plotly.express` interaktiv vizuallaşdırma üçün istifadə edilən bir Python kitabxanasıdır. Bu kitabxana, sadə və səmərəli şəkildə interaktiv qrafiklər yaratmağa imkan verir. Əsas xüsusiyyətləri aşağıdakılardır:\n",
    "\n",
    "### Xüsusiyyətlər:\n",
    "1. **İnteraktiv qrafiklər:** Qrafiklər zoom etmək, üzərində məlumatları göstərmək və filtr tətbiq etmək kimi funksionallıqlara malikdir.\n",
    "2. **Sadə sintaksis:** `plotly.express` ilə qrafiklər yaratmaq asandır. Çox az kodla mürəkkəb qrafiklər yarada bilərsiniz.\n",
    "3. **Çoxsaylı qrafik tipləri:** Scatter plots, line charts, bar charts, histograms, pie charts, box plots və s. kimi müxtəlif növ qrafiklər yarada bilərsiniz.\n",
    "4. **Dataların asan inteqrasiyası:** Pandas DataFrame-ləri və ya digər data strukturlarını birbaşa qrafiklərə daxil edə bilərsiniz.\n",
    "5. **İnteraktiv interfeys:** Qrafiklər veb-səhifələrdə, Jupyter notebook-larda və Dash tətbiqlərində interaktiv olaraq göstərilə bilər.\n",
    "\n",
    "### Məsələn, scatter plot yaratmaq üçün sadə nümunə:\n",
    "```python\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Məlumat yaradılması\n",
    "df = pd.DataFrame({\n",
    "    'X': [1, 2, 3, 4, 5],\n",
    "    'Y': [10, 11, 12, 13, 14]\n",
    "})\n",
    "\n",
    "# Scatter plot\n",
    "fig = px.scatter(df, x='X', y='Y', title='Simple Scatter Plot')\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Bu nümunədə, `plotly.express` Pandas DataFrame-dəki verilənləri vizuallaşdırır və nəticə interaktiv bir scatter plot olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e98f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c723bff",
   "metadata": {},
   "source": [
    "## CRISP DM (Məlumat mədənçiliyi üçün sahələrarası standart proses) nedir?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439b83c",
   "metadata": {},
   "source": [
    "CRISP-DM (Cross-Industry Standard Process for Data Mining) data mining və data analitika layihələri üçün geniş istifadə olunan bir metodologiyadır. Bu proses, hər sahədə data analitik layihələrə uyğun və sistematik bir çərçivə təqdim edir. CRISP-DM aşağıdakı altı əsas mərhələdən ibarətdir:\n",
    "\n",
    "1. **Business Understanding (Biznes Anlayışı):** Layihənin məqsədlərini və biznes ehtiyaclarını başa düşmək. Hansı suallara cavab tapılmalı olduğu və data mining tətbiqinin biznesə necə dəyər qatacağı təyin edilir.\n",
    "\n",
    "2. **Data Understanding (Data Anlayışı):** Lazımi məlumatları toplamaq və ilkin olaraq analiz etmək. Bu mərhələdə datanın keyfiyyəti, strukturlaşdırılması və mövcud məlumatların uyğunluğu araşdırılır.\n",
    "\n",
    "3. **Data Preparation (Data Hazırlığı):** Model qurmaq üçün datanın təmizlənməsi və transformasiya edilməsi. Bu mərhələdə datanın düzgün formatda olması üçün dəyişikliklər aparılır, boş dəyərlər idarə edilir və lazım gəldikdə yeni dəyişənlər yaradılır.\n",
    "\n",
    "4. **Modeling (Model Qurulması):** Seçilmiş machine learning və ya data mining modelləri ilə analitik modellərin qurulması. Həmçinin, modellərin performansını qiymətləndirmək üçün fərqli parametrlər test olunur.\n",
    "\n",
    "5. **Evaluation (Dəyərləndirmə):** Qurulmuş modelin nəticələrinin biznes ehtiyaclarına cavab verib-vermədiyini yoxlamaq. Bu mərhələdə modelin düzgün işlədiyinə əmin olmaq üçün nəticələr təhlil edilir.\n",
    "\n",
    "6. **Deployment (İstifadəyə Verilməsi):** Modeldən əldə olunan nəticələrin real dünya problemlərini həll etmək üçün tətbiqi. Bu mərhələdə modelin inteqrasiyası, nəticələrin biznesə geri ötürülməsi və davamlı izlənməsi həyata keçirilir.\n",
    "\n",
    "CRISP-DM data mining və machine learning layihələrində ən çox qəbul edilmiş metodologiyalardan biri hesab edilir, çünki çevik, sektorlararası və strukturlu yanaşma təqdim edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c04f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59421d85",
   "metadata": {},
   "source": [
    "## API nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52785dff",
   "metadata": {},
   "source": [
    "API (Application Programming Interface) — tətbiqlərin bir-biri ilə ünsiyyət qurmasını təmin edən interfeysdir. Bu, proqramların və sistemlərin bir-birinə sorğu göndərməsi və məlumat alması üçün müəyyən edilmiş qaydalar və protokollar toplusudur.\n",
    "\n",
    "API-lər müxtəlif növ ola bilər:\n",
    "\n",
    "1. **Web API-lər** – Veb əsaslı sistemlərdə istifadə olunur və HTTP protokolu ilə işləyir. Məsələn, bir vebsaytın və ya mobil tətbiqin serverdən məlumatları API vasitəsilə sorğu edib alması.\n",
    "   \n",
    "2. **Operativ Sistem API-ləri** – Tətbiqlərin əməliyyat sisteminə sorğu göndərməsi və onun funksiyalarına (məsələn, fayl sisteminə giriş) çıxış əldə etməsi.\n",
    "\n",
    "3. **Kitabxana API-ləri** – Tətbiqin daxilində istifadə olunan xüsusi funksiyalar və metodlar toplusudur, məsələn, Python-da `pandas` kitabxanası.\n",
    "\n",
    "API istifadə etməyin əsas üstünlükləri:\n",
    "- **Modulların ayrılığı:** Tətbiqlər bir-biri ilə kodu paylaşmadan ünsiyyət qura bilər.\n",
    "- **Təkrar istifadə:** Bir API müxtəlif tətbiqlər tərəfindən istifadə edilə bilər.\n",
    "- **Sadəlik və təhlükəsizlik:** Məlumatlara yalnız lazımi funksiyalar vasitəsilə çıxış təmin olunur.\n",
    "\n",
    "Məsələn, hava proqnozunu göstərən bir tətbiq, hava məlumatını birbaşa serverdən çəkmək üçün API-dən istifadə edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69e7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa79f1a6",
   "metadata": {},
   "source": [
    "## Objects nedir bes Pythonda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82918bd7",
   "metadata": {},
   "source": [
    "Python-da **object (obyekt)**, verilənləri və həmin verilənlər üzərində əməliyyatları bir arada saxlayan bir konstruksiyadır. Python-da hər şey bir obyekt hesab olunur — dəyişənlər, funksiyalar, siyahılar, hətta ən sadə verilənlər tipləri (nömrələr, mətnlər) belə obyektlərdir.\n",
    "\n",
    "Bir obyektin iki əsas komponenti var:\n",
    "1. **Attributes (xüsusiyyətlər)** — obyektin məlumatları (obyektin saxlaya biləcəyi verilənlər). Məsələn, bir obyektin rəngi və ya uzunluğu ola bilər.\n",
    "2. **Methods (metodlar)** — obyektin yerinə yetirə biləcəyi funksiyalar. Metodlar obyektin verilənləri üzərində əməliyyatlar aparır.\n",
    "\n",
    "### Python-da obyektlərin əsas xüsusiyyətləri:\n",
    "- **Class (sinif)**: Python obyektləri bir sinfə əsaslanır. Sinif obyektlərin sxemini təyin edir. Misal üçün, bir `Car` sinfi avtomobil obyektlərini təyin edə bilər, həmin sinfə əsaslanan obyektlər isə fərqli maşınlar ola bilər.\n",
    "  \n",
    "  ```python\n",
    "  class Car:\n",
    "      def __init__(self, brand, model):\n",
    "          self.brand = brand\n",
    "          self.model = model\n",
    "\n",
    "      def show_info(self):\n",
    "          print(f\"This car is a {self.brand} {self.model}\")\n",
    "  \n",
    "  # Car sinfinə əsaslanan obyekt yaratmaq\n",
    "  my_car = Car(\"Toyota\", \"Corolla\")\n",
    "  my_car.show_info()  # Output: This car is a Toyota Corolla\n",
    "  ```\n",
    "\n",
    "- **Instance (nüsxə)**: Bir sinifdən yaradılan fərdi obyektə instance deyilir. Məsələn, yuxarıdakı `my_car` bir instance-dır.\n",
    "  \n",
    "- **Inheritance (irsi əlaqə)**: Bir sinif digər sinifdən xüsusiyyətləri və metodları miras ala bilər. Bu, kodu təkrar istifadəni asanlaşdırır.\n",
    "\n",
    "Python-un obyekt yönümlü təbiəti proqramların daha modullu və idarəedilməsi asan olmasına kömək edir. Siniflərdən istifadə etməklə mürəkkəb tətbiqləri asanlıqla dizayn etmək və genişləndirmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3547f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360e832d",
   "metadata": {},
   "source": [
    "## Supervised ve Unsupervised arasinda ferq?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d37e3",
   "metadata": {},
   "source": [
    "Supervised (nəzarətli) və unsupervised (nəzarətsiz) öyrənmə, maşın öyrənməsi sahəsində istifadə olunan iki əsas yanaşmadır. Onlar arasında bir neçə mühüm fərq var:\n",
    "\n",
    "### Supervised Learning (Nəzarətli Öyrənmə)\n",
    "\n",
    "1. **Təyin Edilmiş Məlumat**: Supervised learning-də model, giriş verilənlərinin (x) və onların müvafiq çıxış etiketlərinin (y) olduğu bir dataset ilə öyrədilir. Bu etiketlər, modelin doğru proqnozlar etməsinə kömək edir.\n",
    "\n",
    "2. **Məqsəd**: Modelin məqsədi, verilən giriş verilənlərinə əsaslanaraq doğru çıxış etiketlərini proqnozlaşdırmaqdır. Məsələn, xəstələrin tibbi məlumatlarından istifadə edərək xəstəliyin olub-olmadığını proqnozlaşdırmaq.\n",
    "\n",
    "3. **Nümunələr**: Məsələn, sınıflandırma (classification) və regresiya (regression) tapşırıqları.\n",
    "\n",
    "4. **Tətbiq Sahələri**: E-poçt spam filtrasiya, müştəri davranışı proqnozu, maliyyə analizləri, tibbi diaqnozlar.\n",
    "\n",
    "5. **Örnek Algoritmlər**: Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), Neural Networks.\n",
    "\n",
    "### Unsupervised Learning (Nəzarətsiz Öyrənmə)\n",
    "\n",
    "1. **Təyin Edilməmiş Məlumat**: Unsupervised learning-də model, yalnız giriş verilənləri (x) olan bir dataset ilə öyrədilir, lakin çıxış etiketləri yoxdur. Yəni, model verilənlər arasında əlaqələri tapmağa çalışır.\n",
    "\n",
    "2. **Məqsəd**: Modelin məqsədi, verilənlər arasında gizli strukturları və ya klasterləri aşkar etməkdir. Məsələn, müştəri seqmentasiyası üçün müştəri məlumatlarını qruplaşdırmaq.\n",
    "\n",
    "3. **Nümunələr**: Məsələn, klasterləşdirmə (clustering) və əsas komponentlərin analizi (PCA).\n",
    "\n",
    "4. **Tətbiq Sahələri**: Müştəri seqmentasiyası, məlumatların analizi, anomaliya aşkar etmə, müstəqil öyrənmə.\n",
    "\n",
    "5. **Örnek Algoritmlər**: K-Means, Hierarchical Clustering, DBSCAN, Principal Component Analysis (PCA), t-SNE.\n",
    "\n",
    "### Qısaca Fərqlər\n",
    "\n",
    "- **Etiketlər**: Supervised learning-də etiketli verilənlər var, unsupervised learning-də isə yoxdur.\n",
    "- **Məqsəd**: Supervised learning-də modelin doğru proqnozlar etməsi üçün öyrədilməsi, unsupervised learning-də isə verilənlərdəki strukturları aşkar etmək.\n",
    "- **Nümunələr**: Supervised learning, sınıflandırma və regresiya, unsupervised learning, klasterləşdirmə və məlumat analizi.\n",
    "\n",
    "Bu fərqləri nəzərə alaraq, verilən tapşırığın təbiətinə uyğun öyrənmə metodunu seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df14f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6843d3bc",
   "metadata": {},
   "source": [
    "## Pythonda statistik testler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3d3fa",
   "metadata": {},
   "source": [
    "Python-da ən çox istifadə edilən statistik testlər aşağıdakı kimidir:\n",
    "\n",
    "1. **T-Test**:\n",
    "   - İki qrup arasındakı ortalamaların fərqlərini müqayisə etmək üçün istifadə olunur.\n",
    "   - `scipy.stats.ttest_ind()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "2. **ANOVA (Analysis of Variance)**:\n",
    "   - Bir neçə qrupun ortalamalarını müqayisə etmək üçün istifadə olunur.\n",
    "   - `scipy.stats.f_oneway()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "3. **Chi-Square Test (Chi-kvadrat testi)**:\n",
    "   - Kateqorial verilənlərin gözlənilən və faktiki dəyərləri arasındakı əlaqəni yoxlamaq üçün istifadə olunur.\n",
    "   - `scipy.stats.chi2_contingency()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "4. **Mann-Whitney U Test**:\n",
    "   - İki qrupun mediandakı fərqlərini yoxlamaq üçün istifadə olunur.\n",
    "   - `scipy.stats.mannwhitneyu()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "5. **Pearson Korrelyasiya Testi**:\n",
    "   - İki dəyişənin bir-biri ilə nə qədər əlaqəli olduğunu ölçmək üçün istifadə olunur.\n",
    "   - `scipy.stats.pearsonr()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "### Məşhur Dataset üzərində testlər tətbiqi\n",
    "**Titanic dataset**-i götürüb statistik testləri tətbiq edəcəyik. Əvvəlcə Titanic datasetini yükləyək və araşdırmaya başlayaq.\n",
    "\n",
    "### 1. Məlumatın Hazırlanması\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Titanic datasetini yükləyək\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# İlk beş sətri yoxlayaq\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 2. T-Test Indpendent (İki qrupun ortalamalarının müqayisəsi)\n",
    "**Sual**: 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarında fərq varmı?\n",
    "```python\n",
    "# 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarının fərqi\n",
    "first_class = df[df['Pclass'] == 1]['Age'].dropna()\n",
    "third_class = df[df['Pclass'] == 3]['Age'].dropna()\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(first_class, third_class)\n",
    "print(f\"T-Test nəticəsi: T-statistic = {t_stat}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- T-statistic = 7.934060672509778\n",
    "- P-value = 3.243665359138386e-15  \n",
    "Bu nəticə göstərir ki, 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamaları arasında statistik olaraq əhəmiyyətli fərq var (P-value çox kiçikdir).\n",
    "\n",
    "---\n",
    "###  3. T-Test Paired (bir qrupun ortalamalarının müqayisəsi)\n",
    "Paired t-test (cüt t-testi) adətən eyni qrup üzvlərinin əvvəl və sonra nəticələrini müqayisə etmək üçün istifadə olunur. Bu test iki qrup arasında fərqi yoxlamır, əksinə eyni fərdlərin iki müxtəlif şəraitdə (məsələn, eyni sərnişinlərin əvvəl və sonra yaşları kimi) nəticələrini müqayisə edir. \n",
    "\n",
    "Sizin Titanic datasetinizdə 1-ci və 3-cü sinif sərnişinləri müstəqil qruplardır, buna görə də `paired t-test` məntiqi olaraq uyğun deyil, çünki eyni sərnişinlərin müxtəlif şəraitdə yaş ortalamalarını müqayisə etmirik. Ancaq əgər siz bunu sadəcə məşq üçün etmək istəyirsinizsə, `paired t-test` tətbiqini aşağıdakı kimi yaza bilərsiniz:\n",
    "\n",
    "```python\n",
    "# Əgər iki qrupda eyni sayda müşahidə yoxdursa, kəsilmə edə bilərik\n",
    "min_length = min(len(first_class), len(third_class))\n",
    "\n",
    "# Paired t-test üçün ilk və üçüncü siniflərdə bərabər uzunluqlu datanı götürürük\n",
    "first_class_sample = first_class[:min_length]\n",
    "third_class_sample = third_class[:min_length]\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(first_class_sample, third_class_sample)\n",
    "print(f\"Paired T-Test nəticəsi: T-statistic = {t_stat}, P-value = {p_value}\")\n",
    "```\n",
    "\n",
    "Burada:\n",
    "\n",
    "- `stats.ttest_rel()` funksiyası paired t-test üçün istifadə olunur.\n",
    "- `first_class` və `third_class` sərnişin qruplarının uzunluqları fərqli ola bilər. Buna görə eyni sayda müşahidəni əldə etmək üçün onları kəsirik (`min_length` ilə eyni uzunluqlu nümunələr götürürük).\n",
    "\n",
    "Lakin unutmayın ki, bu dataset müstəqil qruplarla işləyir və **paired t-test** məntiqi olaraq doğru seçim deyil, çünki sərnişinlər eyni fərdlər deyil. Bu sadəcə təcrübə üçün bir tətbiqdir.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Chi-Square Test (Kateqorial dəyişənlər arasında əlaqə)\n",
    "**Sual**: Cinsiyyət və sağ qalma arasında əlaqə varmı?\n",
    "```python\n",
    "# Cinsiyyət və sağ qalma arasındakı əlaqə\n",
    "contingency_table = pd.crosstab(df['Sex'], df['Survived'])\n",
    "chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f\"Chi-Square Test nəticəsi: Chi2-statistic = {chi2_stat}, P-value = {p_val}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- Chi2-statistic = 260.71702016732104\n",
    "- P-value = 1.1973570627755645e-58  \n",
    "Bu nəticə göstərir ki, cinsiyyət və sağ qalma arasında güclü əlaqə var (P-value çox kiçikdir).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ANOVA Testi (Bir neçə qrupun ortalamalarının müqayisəsi)\n",
    "**Sual**: 1-ci, 2-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarında fərq varmı?\n",
    "```python\n",
    "# ANOVA testi: Siniflər üzrə yaş ortalamaları\n",
    "f_stat, p_value = stats.f_oneway(\n",
    "    df[df['Pclass'] == 1]['Age'].dropna(),\n",
    "    df[df['Pclass'] == 2]['Age'].dropna(),\n",
    "    df[df['Pclass'] == 3]['Age'].dropna()\n",
    ")\n",
    "print(f\"ANOVA nəticəsi: F-statistic = {f_stat}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- F-statistic = 36.16198155603057\n",
    "- P-value = 1.3983978326265877e-15  \n",
    "Bu nəticə göstərir ki, sərnişin siniflərinə görə yaş ortalamaları arasında statistik olaraq əhəmiyyətli fərq var.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Pearson Korrelyasiya Testi (İki dəyişən arasındakı əlaqə)\n",
    "**Sual**: Yaş və biletlərin qiyməti arasında əlaqə varmı?\n",
    "```python\n",
    "# Yaş və biletlərin qiyməti arasındakı korrelyasiya\n",
    "corr, p_value = stats.pearsonr(df['Age'].dropna(), df['Fare'].dropna())\n",
    "print(f\"Pearson Korrelyasiya Testi nəticəsi: Correlation = {corr}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- Correlation = 0.09606669176903893\n",
    "- P-value = 0.014676376252593772  \n",
    "Korrelyasiya əmsalı (0.096) çox zəif əlaqə olduğunu göstərir, lakin P-value 0.05-dən kiçik olduğu üçün bu zəif əlaqə statistik olaraq əhəmiyyətlidir.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.  Spearman Korrelyasiya Testi\n",
    "Hata, yaş və bilet qiyməti sütunlarının NaN dəyərlərini çıxardığımız zaman fərqli ölçülərə sahib olduqları üçün baş verdi. Hər iki sütundan eyni sayda verilənlərin olmasını təmin etmək üçün NaN dəyərlərini çıxardıb eyni indeksi olanları seçməliyik. Aşağıdakı kodu istifadə edərək Spearman korrelyasiya testini yenidən yerinə yetirəcəyik:\n",
    "\n",
    "###  Spearman Korrelyasiya Testi\n",
    "```python\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Titanic datasetini yükləyin\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# NaN dəyərləri olan sətrləri çıxarın\n",
    "filtered_df = titanic_df[['Age', 'Fare']].dropna()\n",
    "\n",
    "# Yaş və Bilet qiyməti məlumatlarını alın\n",
    "age = filtered_df['Age']\n",
    "fare = filtered_df['Fare']\n",
    "\n",
    "# Spearman korrelyasiya testi\n",
    "spearman_corr, p_value = stats.spearmanr(age, fare)\n",
    "\n",
    "# Nəticələri çap edin\n",
    "print(f\"Spearman Correlation Coefficient: {spearman_corr:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "İndi bunu yerinə yetirək.\n",
    "\n",
    "### Spearman Korrelyasiya Testinin Nəticələri\n",
    "- **Spearman Korrelasiya Koefisiyenti**: **0.1351**\n",
    "- **P-value**: **0.0003**\n",
    "\n",
    "### Nəticələrin İzahı\n",
    "1. **Spearman Korrelasiya Koefisiyenti**: 0.1351 dəyəri, yaş və bilet qiyməti arasında müsbət, lakin zəif bir əlaqənin olduğunu göstərir. Bu, yaş artdıqca bilet qiymətinin da artma ehtimalının olduğunu göstərir.\n",
    "\n",
    "2. **P-value**: 0.0003 dəyəri, 0.05-dən aşağı olduğu üçün, yaş və bilet qiyməti arasında statistik cəhətdən əhəmiyyətli bir əlaqənin olduğunu göstərir. Yəni, bu iki dəyişən arasında monotonic bir əlaqə vardır.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "Bu test, Titanic sərnişinlərinin yaş və bilet qiyməti arasında monotonic bir əlaqənin olduğunu ortaya qoyur. Hesablanmış koefisiyent, əlaqənin zəif olduğunu, amma statistik cəhətdən əhəmiyyətli olduğunu göstərir.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. One Sample T-test\n",
    "Titanic datasetindəki yaş ortalamasının 30 yaşdan fərqli olub olmadığını araşdırmaq üçün One Sample T-Test istifadə etdiniz. Aşağıda bu testin nəticələri, izahları və Python kodu ilə birlikdə təqdim olunur.\n",
    "\n",
    "### Titanic Datasetinin İlk Beş Sətri\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Titanic datasetini yükləyin\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# İlk beş sətri göstər\n",
    "print(titanic_df.head())\n",
    "```\n",
    "\n",
    "### One Sample T-Test Nəticələri\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Yaş məlumatlarını alın\n",
    "ages = titanic_df['Age'].dropna()  # NaN dəyərləri çıxar\n",
    "\n",
    "# Hipotez olunan ortalama\n",
    "hypothesized_mean = 30\n",
    "\n",
    "# One Sample T-Test\n",
    "t_statistic, p_value = stats.ttest_1samp(ages, hypothesized_mean)\n",
    "\n",
    "# Nəticələri çap edin\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "### One Sample T-Test Nəticələri\n",
    "- **T-statistic**: -0.5535\n",
    "- **P-value**: 0.5801\n",
    "\n",
    "### Nəticələrin İzahı\n",
    "1. **T-statistic**: Bu dəyər, nümunənin ortalamasının hipotez olunan ortalamadan nə qədər uzaq olduğunu göstərir. Negativ bir dəyər, nümunənin ortalamasının hipotez olunan ortalamadan (30 yaş) daha kiçik olduğunu göstərir.\n",
    "\n",
    "2. **P-value**: Bu dəyər, hipotezi qəbul etmək və ya rədd etmək üçün istifadə olunur. Ümumiyyətlə, p-value 0.05-dən aşağı olduqda, sıfır hipotezini (burada yaş ortalamasının 30-dan fərqli olduğu) rədd edirik. Burada p-value 0.5801 olduğu üçün, sıfır hipotezini rədd edə bilmirik. Yəni, sərnişinlərin yaş ortalamasının 30 yaşdan fərqli olduğuna dair statistik cəhətdən əhəmiyyətli bir dəlil yoxdur.\n",
    "\n",
    "### Nəticə\n",
    "Bu test, Titanic sərnişinlərinin yaş ortalamasının 30 yaşdan fərqli olub olmadığını araşdırmaq üçün istifadə edilmişdir. Lakin, nəticələr göstərir ki, yaş ortalaması 30 yaşdan əhəmiyyətli dərəcədə fərqlənmir.\n",
    "\n",
    "---\n",
    "\n",
    "Bu testlərin nəticələri Titanic datasetində fərqli dəyişənlər arasında statistik olaraq əhəmiyyətli əlaqələrin olub-olmamasını göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5984c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
