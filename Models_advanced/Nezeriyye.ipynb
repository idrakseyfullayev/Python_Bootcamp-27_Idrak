{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7e732e",
   "metadata": {},
   "source": [
    "## Data nedir?  Data analitika sahesi ucun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b4ce5",
   "metadata": {},
   "source": [
    "Data (məlumat), müəyyən bir mövzu, hadisə və ya obyekt haqqında əldə edilən faktlar, rəqəmlər və ya müşahidələrdir. Data həm strukturlu (məsələn, cədvəl formatında olan rəqəmlər və ya məlumatlar) həm də struktursuz (məsələn, mətnlər, şəkillər, audio) formada ola bilər.\n",
    "\n",
    "**Data analitika** isə bu məlumatları təhlil edərək qərar vermə, prosesləri təkmilləşdirmə və ya biznes strategiyalarını formalaşdırmaq üçün istifadə olunan sahədir. Data analitika müxtəlif üsullar və alətlər vasitəsilə datanın toplanması, təmizlənməsi, transformasiyası və təhlilini həyata keçirir. Bu sahənin əsas məqsədi məlumatlardan dəyərli nəticələr çıxarmaqdır.\n",
    "\n",
    "**Data analitika sahəsində bəzi əsas addımlar bunlardır:**\n",
    "\n",
    "1. **Datayı toplamaq:** Müxtəlif mənbələrdən (databazalar, veb səhifələr, sensorlar və s.) məlumat toplamaq.\n",
    "   \n",
    "2. **Datayı təmizləmək:** Yarımçıq və ya düzgün olmayan məlumatları düzəltmək və ya çıxarmaq.\n",
    "   \n",
    "3. **Datanı təhlil etmək:** Statistik üsullar, maşın öyrənmə modelləri və vizuallaşdırma ilə datanın içərisindəki nümunələri tapmaq.\n",
    "\n",
    "4. **Nəticələri vizuallaşdırmaq:** Müxtəlif qrafiklər, heatmap-lər, və ya dashboard-lar vasitəsilə məlumatları vizual formada təqdim etmək.\n",
    "\n",
    "5. **Qərar vermə:** Analizlər əsasında biznes və ya təşkilati qərarlar qəbul etmək.\n",
    "\n",
    "Data analitika həm biznes, həm maliyyə, həm də səhiyyə və digər sahələrdə geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816ef60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c045bc84",
   "metadata": {},
   "source": [
    "## Data analitika nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4f2c0",
   "metadata": {},
   "source": [
    "**Data analitika**, məlumatların toplanması, işlənməsi, təhlili və interpretasiyası prosesi olub, bu məlumatlardan istifadə edərək qərar qəbul etməyə və problemləri həll etməyə yönəlmişdir. Data analitika müxtəlif sahələrdə tətbiq olunur, məsələn, biznes, maliyyə, səhiyyə, marketinq və digər sahələrdə. Aşağıda data analitikasının əsas aspektləri təqdim olunur:\n",
    "\n",
    "### 1. **Məlumatların Toplanması**\n",
    "Data analitika prosesi, məlumatların müxtəlif mənbələrdən toplanması ilə başlayır. Bu, müştəri davranışları, satış məlumatları, sosial media və daha çoxu kimi müxtəlif növ məlumatları əhatə edə bilər.\n",
    "\n",
    "### 2. **Məlumatların Təmizlənməsi**\n",
    "Toplanan məlumatlar tez-tez çirklənmiş, yanlış və ya tam olmayan ola bilər. Bu mərhələdə məlumatlar təmizlənir, yəni yanlış dəyərlər aradan qaldırılır, eksik dəyərlər tamamlanır və məlumatlar uyğun formatda təşkil edilir.\n",
    "\n",
    "### 3. **Məlumatların Təhlili**\n",
    "Məlumat analitikası üçün müxtəlif metodlar və alətlər istifadə olunur. Bu mərhələdə məlumatlar statistik analiz, maşın öyrənmə, təhlil və vizuallaşdırma metodları ilə işlənir. Məsələn:\n",
    "- **Descriptive analytics**: Tarixi məlumatları təhlil edərək nə baş verdiyini anlamağa kömək edir.\n",
    "- **Predictive analytics**: Gələcəkdə nə baş verəcəyini proqnozlaşdırmaq üçün statistik modellərdən istifadə edir.\n",
    "- **Prescriptive analytics**: Müxtəlif seçimlərin nəticələrini qiymətləndirərək ən yaxşı qərarların verilməsinə kömək edir.\n",
    "\n",
    "### 4. **Məlumatların Vizuallaşdırılması**\n",
    "Məlumatların vizuallaşdırılması, məlumatların qrafik və diagramlarla təqdim edilməsi, məlumatların daha asan anlaşılmasını təmin edir. Məsələn, qrafiklər, cədvəllər və dashboardlar məlumatların daha vizual bir şəkildə nümayiş olunmasına kömək edir.\n",
    "\n",
    "### 5. **Qərar Qəbul Etmə**\n",
    "Məlumat analitikasının sonunda əldə olunan nəticələr qərar qəbul etmə prosesinə təsir edir. Bizneslər, təşkilatlar və fərdlər, analitika nəticələrinə əsaslanaraq daha məlumatlı qərarlar qəbul edə bilərlər.\n",
    "\n",
    "### 6. **Tətbiq Sahələri**\n",
    "Data analitika bir çox sahədə istifadə olunur, məsələn:\n",
    "- **Biznes**: Müştəri davranışını anlamaq və satışları artırmaq üçün.\n",
    "- **Maliyyə**: Risklərin qiymətləndirilməsi və investisiya strategiyalarının hazırlanması üçün.\n",
    "- **Səhiyyə**: Xəstəliklərin proqnozlaşdırılması və müalicə metodlarının təkmilləşdirilməsi üçün.\n",
    "- **Marketinq**: Hədəf auditoriyanı anlamaq və marketinq strategiyalarını optimallaşdırmaq üçün.\n",
    "\n",
    "### Nəticə\n",
    "Data analitika, müasir dövrdə məlumatların artması ilə daha da önəmli bir sahə halına gəlmişdir. Məlumatlardan istifadə edərək daha səmərəli qərarlar qəbul etmək və strateji planlama aparmaq üçün analitika mütləqdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa30b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315a04c2",
   "metadata": {},
   "source": [
    "## Machine Learning nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62faae38",
   "metadata": {},
   "source": [
    "**Machine Learning** (Maşın Öyrənməsi), kompüterlərin verilmiş məlumatlardan öyrənərək müəyyən tapşırıqları yerinə yetirmək üçün proqramlaşdırılmasına əsaslanan bir süni intellekt sahəsidir. Bu prosesdə maşın öyrənmə alqoritmləri, məlumatları analiz edərək və bu məlumatlardan qaydalar çıxararaq özünü inkişaf etdirir. Aşağıda maşın öyrənməsinin əsas aspektləri və tətbiq sahələri təqdim olunur:\n",
    "\n",
    "### 1. **Maşın Öyrənməsinin Tipləri**\n",
    "Maşın öyrənməsi əsasən üç tələbat üzrə təsnif edilir:\n",
    "\n",
    "- **Supervised Learning (Nəzarətli Öyrənmə)**:\n",
    "  - Məlumat dəstinin etiketli olduğu bir yanaşmadır. Yəni, hər bir giriş məlumatına uyğun bir çıxış dəyəri var. Məqsəd, bu etiketlərə əsaslanaraq yeni, görünməyən məlumatlar üçün proqnozlar verməkdir.\n",
    "  - Məsələn, müştəri kredit verilib-verilməyəcəyini müəyyənləşdirmək üçün verilənlərdəki müştəri məlumatlarını istifadə etmək.\n",
    "\n",
    "- **Unsupervised Learning (Nəzarətsiz Öyrənmə)**:\n",
    "  - Bu yanaşmada məlumatlar etiketlənməyib, yəni çıxış dəyərləri yoxdur. Məqsəd, məlumatlar arasında gizli strukturların və ya əlaqələrin aşkar edilməsidir.\n",
    "  - Məsələn, müştəriləri oxşar xüsusiyyətlərinə görə klasterlərə ayırmaq.\n",
    "\n",
    "- **Reinforcement Learning (Mükafatlandırma ilə Öyrənmə)**:\n",
    "  - Bu yanaşmada agent, mühit ilə qarşılıqlı təsir edir və hər bir hərəkətinin mükafatını (və ya cəzasını) alaraq öyrənir. Məqsəd, ən yüksək mükafatı əldə etmək üçün optimal strategiyaları inkişaf etdirməkdir.\n",
    "  - Məsələn, bir robotun müəyyən bir mühitdə tapşırıqları yerinə yetirməsi.\n",
    "\n",
    "### 2. **Maşın Öyrənməsinin Prosesləri**\n",
    "Maşın öyrənməsi prosesi adətən aşağıdakı mərhələləri əhatə edir:\n",
    "\n",
    "1. **Məlumat Toplanması**: Problemi anlamaq üçün müvafiq məlumatların toplanması.\n",
    "2. **Məlumatların Təmizlənməsi**: Məlumatların keyfiyyətini artırmaq üçün təmizlənməsi, eksik dəyərlərin tamamlanması və ya yanlış dəyərlərin düzəldilməsi.\n",
    "3. **Xüsusiyyətlərin Seçimi**: Modelin performansını artırmaq üçün müvafiq xüsusiyyətlərin seçilməsi.\n",
    "4. **Modelin Qurulması**: Seçilmiş alqoritmlərlə modelin yaradılması.\n",
    "5. **Modelin Testi**: Modelin performansının qiymətləndirilməsi üçün test dəstində yoxlanılması.\n",
    "6. **İcra**: Modelin real dünya tətbiqinə yönəldilməsi və nəticələrin izlənilməsi.\n",
    "\n",
    "### 3. **Tətbiq Sahələri**\n",
    "Maşın öyrənməsi müxtəlif sahələrdə geniş tətbiq olunur:\n",
    "\n",
    "- **Maliyyə**: Kredit riskinin qiymətləndirilməsi, fırıldaqçılıq aşkarlanması.\n",
    "- **Səhiyyə**: Xəstəliklərin proqnozlaşdırılması, tibbi şəkillərin analizi.\n",
    "- **Marketinq**: Hədəf auditoriyanın müəyyənləşdirilməsi, müştəri davranışlarının analizi.\n",
    "- **Robototexnika**: Avtomatlaşdırma, robotların mühitdə fəaliyyət göstərməsi.\n",
    "- **Tərcümə**: Tərcümə sistemlərində və təbii dil işləmə (NLP) tətbiqlərində.\n",
    "\n",
    "### Nəticə\n",
    "Maşın öyrənməsi, verilənlərdən avtomatik öyrənməyi və bu öyrənmələrə əsaslanaraq qərarlar verməyi mümkün edən güclü bir alət və metodologiyadır. Bu sahə, süni intellektin inkişafında və bir çox tətbiq sahəsində inqilabi dəyişikliklərə səbəb olmuşdur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6524dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d82ebba",
   "metadata": {},
   "source": [
    "## Normal paylanma nədir bəs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d720a",
   "metadata": {},
   "source": [
    "**Normal paylanma** statistikada çox mühüm bir anlayışdır və bir çox real dünya hadisələri təxmini normal paylanmaya uyğun gəlir. Bu paylanma **Gauss paylanması** və ya **zəng əyrisi** (bell curve) kimi də tanınır. Normal paylanmanın əsas xüsusiyyəti onun simmetrik olmasıdır və ortalama (mean), median (orta dəyər) və moda (ən çox təkrarlanan dəyər) eyni nöqtədə yerləşir.\n",
    "\n",
    "### Normal paylanmanın xüsusiyyətləri:\n",
    "1. **Simmetrik əyri**: Paylanmanın sol və sağ tərəfləri tamamilə simmetrikdir.\n",
    "2. **Ortalamaya yaxınlıq**: Dəyərlərin böyük bir hissəsi ortalama ətrafında cəmləşir. Dəyərlər ortalamadan uzaqlaşdıqca azalmağa başlayır.\n",
    "3. **Zəng əyrisi (bell-shaped curve)**: Qrafik olaraq normal paylanmanın əyrisi zəngə bənzəyir.\n",
    "4. **Standart sapma**: Normal paylanmada dəyərlərin 68%-i bir standart sapma (σ) daxilində, 95%-i iki standart sapma daxilində, 99.7%-i isə üç standart sapma daxilində olur. Bu qayda **68-95-99.7 qaydası** kimi tanınır.\n",
    "\n",
    "### Normal paylanmanın qrafiki:\n",
    "- **Orta** (mean) mərkəzdədir.\n",
    "- Əyrinin hər iki tərəfi simmetrik olaraq bərabərdir.\n",
    "- Paylanmanın böyük hissəsi orta qiymətə yaxın olan dəyərlərdir.\n",
    "- Hər iki ucda ekstremal dəyərlər çox nadir hallarda baş verir.\n",
    "\n",
    "### Normal paylanmanın formulu:\n",
    "Normal paylanmanın sıxlıq funksiyası aşağıdakı kimi göstərilir:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "Burada:\n",
    "- **μ**: Paylanmanın orta qiyməti,\n",
    "- **σ**: Standart sapma,\n",
    "- **x**: Təsadüfi dəyər,\n",
    "- **e**: Təbii logaritm əsası (təxminən 2.718).\n",
    "\n",
    "### Normal paylanmanın əhəmiyyəti:\n",
    "1. **Təbiət və sosial hadisələr**: Çoxlu fiziki və sosial hadisələr normal paylanmaya yaxındır. Məsələn, insanların boyu, çəkisi, test nəticələri və s. adətən normal paylanmaya uyğundur.\n",
    "   \n",
    "2. **Fərziyyə testləri və statistika**: Bir çox statistik metod və fərziyyə testləri, məsələn, Z-test və t-test, normal paylanmaya əsaslanır.\n",
    "   \n",
    "3. **Qeyri-müəyyənliyin ölçülməsi**: Normal paylanma qeyri-müəyyənliyi qiymətləndirmək üçün istifadə olunur və bir çox inferensial statistika üsulları onun əsasında qurulub.\n",
    "\n",
    "### 68-95-99.7 qaydası:\n",
    "Bu qayda normal paylanmada dəyərlərin nə qədərinin orta qiymətə yaxın olacağını göstərir:\n",
    "- Təxminən **68%** dəyər bir standart sapma (σ) daxilində (yəni ortalama ± 1σ).\n",
    "- Təxminən **95%** dəyər iki standart sapma (yəni ortalama ± 2σ).\n",
    "- Təxminən **99.7%** dəyər üç standart sapma (yəni ortalama ± 3σ).\n",
    "\n",
    "### Normal paylanmanın tətbiqi:\n",
    "- **Təhsil**: Tələbələrin imtahan nəticələri normal paylanmaya yaxın ola bilər.\n",
    "- **Tibb**: Müxtəlif tibbi parametrlər (məsələn, qan təzyiqi, bədən istiliyi) əhalidə normal paylanmaya uyğunlaşır.\n",
    "- **Maliyyə**: Qiymət dəyişiklikləri və səhmlərin gəlirləri tez-tez normal paylanma qəbul edilərək analiz edilir.\n",
    "\n",
    "Yekun olaraq, **normal paylanma** dəyərlərin ortalama ətrafında cəmləşdiyi, müəyyən qaydalara uyğun olaraq simmetrik paylanmadır və statistik analizdə geniş tətbiq olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6974d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5eb778",
   "metadata": {},
   "source": [
    "## Datanın normal paylanması nə deməkdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e868ad7",
   "metadata": {},
   "source": [
    "**Datanın normal paylanması**, verilmiş məlumatların **normal paylanmaya** uyğun olaraq bölündüyü anlamına gəlir. Yəni, məlumatların böyük bir hissəsi orta qiymət (mean) ətrafında cəmlənir və məlumatların dəyərləri simmetrik olaraq ortalamadan uzaqlaşdıqca azalmağa başlayır. Bu, məlumatların zəng əyrisi (bell-shaped curve) formasında qrafikləşməsinə səbəb olur.\n",
    "\n",
    "Datanın normal paylanması deməkdir ki:\n",
    "\n",
    "1. **Ortalamaya yaxın dəyərlər daha çoxdur**: Məlumat dəstəsinin böyük hissəsi ortalama ətrafında yerləşir. Məsələn, insanların boylarını ölçən bir dataset varsa, insanların çoxunun boyu orta boy ətrafında olacaq.\n",
    "   \n",
    "2. **Simmetrik paylanma**: Ortalamanın solunda və sağında dəyərlər bərabər sayda olur. Başqa sözlə, ortalamadan nə qədər uzaqlaşırsanız, həm kiçik, həm də böyük dəyərlərin tezliyi eyni dərəcədə azalır.\n",
    "\n",
    "3. **Outlier-lər azdır**: Ekstremal dəyərlər (çox kiçik və ya çox böyük dəyərlər) çox nadir hallarda olur. Normal paylanmada outlier-lər ortalamadan ən uzaq dəyərlər olur və nadir hallarda rast gəlinir.\n",
    "\n",
    "### Məlumatların normal paylanmasını anlamaq üçün bir neçə əsas nöqtə:\n",
    "\n",
    "#### 1. **Ortalamalar və standart sapmalar**:\n",
    "- **Orta (mean)**, **median** və **moda** normal paylanmada eyni nöqtədə olur.\n",
    "- Dəyərlərin təxminən 68%-i bir standart sapma daxilində (yəni, ortalamadan ±1 standart sapma məsafədə), 95%-i iki standart sapma daxilində, 99.7%-i isə üç standart sapma daxilində olur (68-95-99.7 qaydası).\n",
    "\n",
    "#### 2. **Qrafik təsviri**:\n",
    "Normal paylanmanı təsvir etmək üçün histogram və ya **Q-Q plot** (Quantile-Quantile plot) istifadə olunur. Histogramda zəng şəklində əyri varsa, bu, məlumatların normal paylanmaya uyğun olduğunu göstərir.\n",
    "\n",
    "#### 3. **Normal paylanmanın yoxlanılması**:\n",
    "Datanın normal paylanması olub-olmadığını yoxlamaq üçün bir neçə statistik test və qrafik metodu mövcuddur:\n",
    "- **Shapiro-Wilk testi** və ya **Kolmogorov-Smirnov testi**: Bu testlər datanın normal paylanmadan nə qədər sapdığını ölçmək üçün istifadə olunur.\n",
    "- **Histogram** və **Q-Q Plot**: Qrafik olaraq normal paylanmanı vizual şəkildə təsdiqləmək üçün istifadə olunur.\n",
    "\n",
    "#### 4. **Normal paylanmanın əhəmiyyəti**:\n",
    "Normal paylanmanın olduğu məlumat dəstələri statistik analizlər üçün uyğundur. Bir çox statistik metodlar (məsələn, t-test, ANOVA, regresyon analizləri) normal paylanmaya əsaslanır. Əgər məlumat normal paylanmırsa, bu analizlər düzgün nəticə verməyə bilər, buna görə də datanı normal paylanmaya yaxınlaşdırmaq üçün transformasiya metodlarından istifadə edilir (məsələn, log transformasiya).\n",
    "\n",
    "#### 5. **Normal paylanmanın real həyatda nümunələri**:\n",
    "- İnsanların boyu, çəkisi, qan təzyiqi kimi biometrik ölçülər adətən normal paylanmaya uyğun gəlir.\n",
    "- Bir çox sosial və iqtisadi göstəricilər də normal paylanmaya yaxındır, məsələn, test balları və ya məhsulun ölçüləri.\n",
    "\n",
    "### Datanın normal paylanmaması halları:\n",
    "Məlumatlar normal paylanmaya uyğun gəlmirsə, bu, müxtəlif hallarda baş verə bilər:\n",
    "- **Çox yüksək və ya aşağı outlier-lər** olduqda.\n",
    "- **Skewness** (asimetriya) olduqda: Sağ və ya sol tərəfə doğru qrafikdə əyilmə müşahidə oluna bilər.\n",
    "- **Kurtosis**: Paylanmanın zirvəsi normala nisbətən daha geniş və ya dar ola bilər.\n",
    "\n",
    "Yəni, **datanın normal paylanması**, məlumatların standart bir forma malik olduğu, böyük bir qisminin ortalama ətrafında cəmləşdiyi və ortalamadan uzaqlaşdıqca tezliyin azaldığı paylanma deməkdir. Bu, statistik analizlər üçün vacib şərtlərdən biridir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a64c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdcc9295",
   "metadata": {},
   "source": [
    "## Z-score haqqında geniş anlayış ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da445b",
   "metadata": {},
   "source": [
    "**Z-score** (və ya standart skor) statistikada müəyyən bir dəyərin ortalama ilə müqayisədə nə qədər uzaqda olduğunu ölçmək üçün istifadə olunur. Z-score, həmin dəyərin orta qiymətdən neçə standart sapma uzaqlıqda olduğunu göstərir. Bu üsul xüsusilə fərqli vahidlər və ölçülərdən istifadə edən məlumatları müqayisə etmək üçün əhəmiyyətlidir, çünki bütün məlumatları standartlaşdırılmış vahidlərə (z-skorlara) çevirir.\n",
    "\n",
    "### Z-Score-nu hesablamaq üçün formula:\n",
    "$$\n",
    "Z = \\frac{(X - \\mu)}{\\sigma}\n",
    "$$\n",
    "Burada:\n",
    "- **Z** – Z-score,\n",
    "- **X** – müşahidə olunan dəyər,\n",
    "- **$\\mu$** – orta dəyər (mean),\n",
    "- **$\\sigma$** – standart sapma.\n",
    "\n",
    "### Z-score-un mənası:\n",
    "- **Z-score = 0**: Dəyər tam ortalamaya bərabərdir.\n",
    "- **Z-score > 0**: Dəyər ortalamadan yuxarıdır (pozitiv sapma).\n",
    "- **Z-score < 0**: Dəyər ortalamadan aşağıdır (neqativ sapma).\n",
    "- **Z-score = 1**: Dəyər orta dəyərdən bir standart sapma yuxarıdır.\n",
    "- **Z-score = -2**: Dəyər orta dəyərdən iki standart sapma aşağıdır.\n",
    "\n",
    "### Z-score-un əhəmiyyəti:\n",
    "1. **Outlier-lərin tapılması**: Z-score böyük və ya çox kiçik olarsa, bu, dəyərin məlumat dəstəsindəki digər dəyərlərdən çox uzaq olduğunu göstərə bilər. Məsələn, Z-score -3 və ya +3-dən böyükdürsə, bu dəyər outlier kimi qəbul oluna bilər.\n",
    "   \n",
    "2. **Müqayisə etmək**: Z-score müxtəlif ölçü vahidlərində olan dəyərləri müqayisə etməyə imkan verir. Məsələn, bir tələbənin iki fərqli fəndən aldığı balları müqayisə etmək üçün Z-score istifadə oluna bilər.\n",
    "\n",
    "3. **Normal paylanma**: Normal paylanma üçün, müşahidələrin təxminən 68%-i ortalama ətrafında bir standart sapma daxilində, 95%-i isə iki standart sapma daxilində yerləşir. Z-score bu paylanmanı vizual olaraq görməyə kömək edir.\n",
    "\n",
    "### Z-Score istifadəsi:\n",
    "- **Hypothesis testing (təsadüfi fərziyyə testi)**: Z-score normallıq testlərində və fərziyyə testlərində istifadə olunur.\n",
    "- **Standartlaşdırma**: Fərqli vahidlərdəki dəyişənləri eyni ölçü vahidinə çevirir.\n",
    "- **Skorların təhlili**: Təhsildə və ya digər sahələrdə fərqli qiymətləndirmələr arasında müqayisə üçün istifadə olunur.\n",
    "\n",
    "Yəni, Z-score, məlumatlar içində bir dəyərin yerini standart sapmalar ilə müqayisə edərək izah edir və məlumatların təhlilində geniş istifadə edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a221c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e11b3f",
   "metadata": {},
   "source": [
    "## T-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e25b5f",
   "metadata": {},
   "source": [
    "**T-score** (və ya **t-skore**), statistikada kiçik nümunə ölçülərində istifadə olunan Z-score-un bir variantıdır. Xüsusilə, nümunə ölçüsü kiçik olduqda və ya populyasiyanın standart sapması bilinmədikdə, Z-score əvəzinə t-skordan istifadə olunur. T-skore **t-test** və digər statistik testlər zamanı paylanmanın normallığını yoxlamaq və fərziyyə testləri aparmaq üçün istifadə edilir.\n",
    "\n",
    "### T-score ilə Z-score fərqləri:\n",
    "- **Z-score** adətən böyük nümunə ölçüləri üçün (n > 30) və ya populyasiyanın standart sapması bilindiyi zaman istifadə olunur.\n",
    "- **T-score** isə kiçik nümunə ölçülərində (n < 30) və ya populyasiyanın standart sapması məlum olmadığı hallarda tətbiq edilir.\n",
    "\n",
    "### T-score-un formulu:\n",
    "T-score hesablamaq üçün aşağıdakı formula istifadə olunur:\n",
    "$$\n",
    "T = \\frac{(X - \\mu)}{s/\\sqrt{n}}\n",
    "$$\n",
    "Burada:\n",
    "- **T** – T-score,\n",
    "- **X** – müşahidə olunan dəyər (nümunə ortası),\n",
    "- **μ** – populyasiya ortası,\n",
    "- **s** – nümunə standart sapması,\n",
    "- **n** – nümunənin ölçüsü (müşahidə sayı).\n",
    "\n",
    "### T-score-un tətbiq sahələri:\n",
    "1. **Kiçik nümunələrdə fərziyyə testi**: T-score, xüsusən də kiçik nümunələr üçün populyasiya ortalaması ilə müqayisə aparmaq üçün istifadə edilir.\n",
    "2. **T-testlər**: İki qrupun ortalamalarını müqayisə etmək üçün **t-test** aparılarkən istifadə olunur. Bu test iki növ ola bilər:\n",
    "   - **One-sample t-test**: Nümunənin ortasını verilmiş populyasiya ortası ilə müqayisə etmək.\n",
    "   - **Two-sample t-test**: İki fərqli nümunənin ortalarını bir-biri ilə müqayisə etmək.\n",
    "3. **Nəticələrin güvən aralığının hesablanması**: Kiçik nümunələrdən əldə olunan nəticələrin nə qədər güvənlə populyasiyaya tətbiq oluna biləcəyini göstərmək üçün t-distribusiya istifadə olunur.\n",
    "\n",
    "### T-score-un əhəmiyyəti:\n",
    "- **Kiçik nümunələrdə dəqiqlik**: Nümunə ölçüsü kiçik olduqda və standart sapma məlum olmadıqda, Z-score istifadə etmək dəqiq nəticə verməyə bilər. T-score bu hallarda daha dəqiq təxminlər verir.\n",
    "- **Fərziyyə testləri**: Kiçik nümunə ölçüləri ilə işləyərkən, t-test və t-score, iki qrup arasında əhəmiyyətli bir fərqin olub-olmadığını müəyyənləşdirməyə kömək edir.\n",
    "\n",
    "Yəni, t-score, xüsusən kiçik nümunələrlə işləyərkən statistik analiz aparmaq üçün vacibdir və nəticələrin daha dəqiq olmasını təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03fe51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a246abb",
   "metadata": {},
   "source": [
    "## Outlier haqqında geniş məlumat ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1cc512",
   "metadata": {},
   "source": [
    "Outlier (kənar dəyər) verilənlər toplusunda statistik baxımdan normadan kənar, digər dəyərlərdən əhəmiyyətli dərəcədə fərqlənən məlumat nöqtəsidir. Bu dəyərlər verilənlərdə anormal dəyişikliklərin göstəricisi ola bilər və ya səhv məlumatın nəticəsi ola bilər. Kənar dəyərləri düzgün idarə etmək, verilənlərin keyfiyyətini artırmaq və modelləşdirmə prosesində dəqiqliyi təmin etmək üçün vacibdir.\n",
    "\n",
    "### Outlier-lərin təsnifatı\n",
    "Outlier-lər müxtəlif kateqoriyalara bölünə bilər:\n",
    "\n",
    "1. **Univariate outlier-lər**: Bir dəyişənə əsasən normal paylanmadan uzaq olan dəyərlərdir. Məsələn, yaş dəyişənində 120 yaşda bir dəyər normadan kənar sayıla bilər.\n",
    "   \n",
    "2. **Multivariate outlier-lər**: Birdən çox dəyişən birləşməsi əsasında kənar olan dəyərlərdir. Məsələn, hündürlüyü 2 metr olan bir şəxsin çəkisi 30 kiloqram olarsa, bu iki dəyişənin birgə qiymətləndirilməsində kənar dəyər sayıla bilər.\n",
    "\n",
    "3. **Contextual outlier-lər**: Bəzi kontekstlərdə normadan kənar hesab olunan dəyərlərdir. Məsələn, gecə temperaturu 40°C olarsa, bu çox güman ki, anormal hadisədir, amma gündüz bu dəyər normal ola bilər.\n",
    "\n",
    "4. **Collective outlier-lər**: Verilənlərdə bir qrup dəyərin birlikdə normadan kənar olmasıdır. Bu, xüsusilə zaman sıralarında və ya ardıcıl proseslərdə müşahidə edilir.\n",
    "\n",
    "### Outlier-lərin səbəbləri\n",
    "Outlier-lər müxtəlif səbəblərdən meydana gələ bilər:\n",
    "\n",
    "1. **Ölçmə və ya məlumat toplama səhvləri**: Verilənlərdə yanlış ölçmələr və ya qeyri-dəqiq məlumat yığımı kənar dəyərlərə səbəb ola bilər. Məsələn, cihazların səhv işləməsi nəticəsində qeyri-adi qiymətlər yaranır.\n",
    "\n",
    "2. **Təbiəti fərqli hadisələr**: Bəzi kənar dəyərlər həqiqətən anormal hadisələri əks etdirir. Məsələn, iqtisadi şoklar və ya təbii fəlakətlər zamanı qeyri-adi qiymətlər müşahidə oluna bilər.\n",
    "\n",
    "3. **Müəyyən fenomenlər**: Verilənlərdə kənar dəyərlər bəzi real həyatda xüsusi vəziyyətləri əks etdirə bilər. Məsələn, çox yüksək gəlirli insanlar.\n",
    "\n",
    "4. **Normal yayılmadan kənar paylanma**: Bəzi verilənlər müəyyən modelə əsaslanaraq paylanır, və bu model daxilində təbii kənar dəyərlər mövcud ola bilər.\n",
    "\n",
    "### Outlier-lərin təsiri\n",
    "Kənar dəyərlər verilənlərin analizinə ciddi təsir göstərə bilər:\n",
    "\n",
    "1. **Statistik nəticələrin təhrif olunması**: Outlier-lər ortalama və standart sapma kimi statistik göstəriciləri təhrif edə bilər. Məsələn, yüksək gəlirli bir nəfər, bütün qrupun orta gəlirini yüksəldə bilər.\n",
    "\n",
    "2. **Model performansının zəifləməsi**: Outlier-lər klassik statistika və maşın öyrənməsi modellərinin performansını mənfi təsir edə bilər. Linear regression kimi modellər outlier-lərə həssasdır və yanlış proqnozlar verə bilər.\n",
    "\n",
    "3. **Qərar qəbul etmənin pozulması**: Təhlil nəticələrini təhrif etməklə qərar qəbul etməni çaşdıra bilər. Məsələn, satış hesabatlarında anormal yüksək rəqəm gələcək strategiyanın səhv qurulmasına səbəb ola bilər.\n",
    "\n",
    "### Outlier-lərin aşkarlanması\n",
    "Kənar dəyərləri aşkar etmək üçün bir neçə metoddan istifadə olunur:\n",
    "\n",
    "1. **Vizual metodlar**:\n",
    "   - **Boxplot**: Verilənlərin paylanmasını qutular və saqqallar (whiskers) vasitəsilə təsvir edən qrafikdir. Saqqaldan (whiskers) kənarda qalan dəyərlər outlier-lər ola bilər.\n",
    "   - **Scatter Plot**: İki dəyişən arasındakı əlaqəni təsvir edir. Qrafikdəki anormal nöqtələr kənar dəyərləri göstərə bilər.\n",
    "\n",
    "2. **Statistik metodlar**:\n",
    "   - **Z-score**: Verilənlər nöqtələrinin ortalamadan neçə standart sapma uzaq olduğunu ölçən metoddur. Əgər Z-score çox yüksək və ya aşağıdırsa (məsələn, 3-dən böyük), bu, outlier ola bilər.\n",
    "   - **IQR (Interquartile Range)**: Verilənlər dəstindəki dəyərlərin ortasında yerləşən 50%-lik hissəni nəzərə alır. Birinci və üçüncü kvartillər arasındakı məsafədən çox kənarda olan dəyərlər outlier hesab oluna bilər. IQR metodunda outlier-lər aşağıdakı kimi təyin edilir:\n",
    "     \\[\n",
    "     \\text{Outliers} = [Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]\n",
    "     \\]\n",
    "     Burada Q1 birinci kvartildir, Q3 isə üçüncü kvartildir.\n",
    "\n",
    "3. **Maşın öyrənməsi metodları**:\n",
    "   - **Isolation Forest**: Decision tree əsaslı bir metoddur və anomaliyaları təyin etmək üçün istifadə olunur.\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Maşın öyrənməsində sıxlığa əsaslanan klasterləmə metodudur və kənar dəyərləri təyin edə bilər.\n",
    "   - **LOF (Local Outlier Factor)**: Hər bir verilənlər nöqtəsinin qonşuluqda digər nöqtələrlə müqayisədə anormal olub-olmadığını təyin edir.\n",
    "\n",
    "### Outlier-lərin idarə edilməsi\n",
    "Outlier-lər tapıldıqdan sonra onlarla necə davranılacağı çox vacibdir. Bəzi seçimlər bunlardır:\n",
    "\n",
    "1. **Kənar dəyərləri silmək**: Əgər kənar dəyərlərin səhv məlumatlardan qaynaqlandığı aşkarlanarsa, onları dataset-dən çıxarmaq olar. Amma məlumat itkisi ehtiyatla nəzərə alınmalıdır.\n",
    "\n",
    "2. **Kənar dəyərləri dəyişdirmək**: Bəzi hallarda kənar dəyərləri qrupun ortalama və ya median qiymətləri ilə əvəz etmək olar. Bu, verilənlərin ümumi paylanmasına daha uyğun nəticələr verə bilər.\n",
    "\n",
    "3. **Modeli kənar dəyərlərə davamlı etmək**: Outlier-ləri silmədən, modelləri bu dəyərlərə həssas olmayan metodlar üzərində qurmaq olar. Məsələn, decision tree, random forest, və ya robust regression kimi modellər outlier-lərə daha davamlıdır.\n",
    "\n",
    "4. **Log transformasiyası**: Dəyərlər arasındakı fərqi azaltmaq və kənar dəyərlərin təsirini yumşaltmaq üçün logaritmik transformasiya tətbiq etmək olar.\n",
    "\n",
    "### Nəticə\n",
    "Outlier-lər verilənlər analitikasında əhəmiyyətli rol oynayır. Onların düzgün aşkarlanması və idarə edilməsi statistika və maşın öyrənməsinin effektivliyini artırır, eyni zamanda məlumat keyfiyyətinin yaxşılaşdırılmasına xidmət edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862270e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2409ceac",
   "metadata": {},
   "source": [
    "## Null dəyərlər haqqında geniş məlumat ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cb6be",
   "metadata": {},
   "source": [
    "**Null dəyərlər** (boş dəyərlər) verilənlər toplusunda müəyyən sütun və ya dəyişən üçün məlumatların mövcud olmadığı vəziyyəti ifadə edir. Bu dəyərlər dataset-də itkin məlumat kimi qəbul edilir və \"NaN\" (Not a Number) və ya \"None\" kimi təqdim edilə bilər. Null dəyərlər düzgün təhlil aparmaq üçün düzgün idarə edilməli olan əsas məsələlərdən biridir, çünki onlar analiz və modelləşdirmə prosesini mənfi təsir edə bilər.\n",
    "\n",
    "### Null dəyərlərin səbəbləri\n",
    "\n",
    "Null dəyərlərin meydana çıxmasının bir neçə səbəbi ola bilər:\n",
    "\n",
    "1. **Məlumat toplama xətaları**: Verilənlər toplanarkən yanlış məlumat yığılması nəticəsində müəyyən dəyişənlər üçün məlumatların boş olması mümkündür.\n",
    "   \n",
    "2. **Məlumatların uyğunsuzluğu**: Müəyyən vəziyyətlərdə bəzi atributlar və ya dəyişənlər bəzi müşahidələr üçün tətbiq olunmaya bilər. Məsələn, tək yaşayan bir şəxs üçün 'Ailə üzvlərinin sayı' dəyişəni boş ola bilər.\n",
    "\n",
    "3. **Sensor və ya texnologiya xətaları**: Sensor və ya cihazlardan məlumat toplanarkən texniki problemlər nəticəsində bəzi məlumatlar itə bilər.\n",
    "\n",
    "4. **Müşahidənin qəsdən kənarlaşdırılması**: Araşdırmaçı və ya məlumat toplayan şəxs qəsdən müəyyən məlumatları daxil etməyə bilər, çünki bu məlumatlar həqiqətən mövcud olmaya bilər və ya müəyyən şərtlər daxilində məna kəsb etmir.\n",
    "\n",
    "### Null dəyərlərin təsiri\n",
    "\n",
    "Null dəyərlər verilənlər üzərində təhlillərin və modelləşdirmənin dəqiqliyinə birbaşa təsir edə bilər:\n",
    "\n",
    "1. **Məlumatın bütövlüyünün pozulması**: Null dəyərlər verilənlərin tamlığına mənfi təsir göstərir və statistikanın düzgün nəticələnməsinə mane ola bilər.\n",
    "   \n",
    "2. **Statistik analizlərdə problemlər**: Çoxsaylı null dəyərlər statistik göstəricilərin, məsələn, ortalama və medianın təhrif olmasına səbəb ola bilər.\n",
    "\n",
    "3. **Model performansına təsiri**: Maşın öyrənməsi modellərində null dəyərlər səhv nəticələrə gətirib çıxara bilər. Məsələn, bəzi modellər null dəyərləri qəbul etmir və onlar mövcud olduqda modellər effektiv işləmir.\n",
    "\n",
    "4. **İkincili təsirlər**: Null dəyərlər başqa dəyişənlərə də təsir edə bilər. Məsələn, korrelyasiya və ya statistik əlaqələrin təhlilində null dəyərlər dəqiq nəticələr əldə etməyə mane ola bilər.\n",
    "\n",
    "### Null dəyərlərin aşkarlanması\n",
    "\n",
    "Null dəyərləri müəyyən etmək üçün bir neçə metoddan istifadə olunur:\n",
    "\n",
    "1. **Təlimatçı komandalardan istifadə**: Python və R kimi proqramlaşdırma dillərində null dəyərləri aşkar etmək üçün təlimatlar mövcuddur.\n",
    "   - Python-da Pandas kitabxanası vasitəsilə:\n",
    "     ```python\n",
    "     df.isnull()  # DataFrame-də bütün null dəyərləri göstərir\n",
    "     df.isnull().sum()  # Hər bir sütunda neçə null dəyərin olduğunu göstərir\n",
    "     ```\n",
    "   - R-da:\n",
    "     ```r\n",
    "     is.na(df)  # DataFrame-də null dəyərləri göstərir\n",
    "     ```\n",
    "\n",
    "2. **Vizual metodlar**:\n",
    "   - **Missingno** kitabxanası (Python) null dəyərləri qrafik şəklində göstərməyə kömək edir.\n",
    "     ```python\n",
    "     import missingno as msno\n",
    "     msno.matrix(df)\n",
    "     ```\n",
    "   - **Heatmap**: Verilənlərin hansı hissəsində null dəyərlərin daha çox olduğunu vizual şəkildə göstərir.\n",
    "\n",
    "### Null dəyərlərin idarə edilməsi\n",
    "\n",
    "Null dəyərləri düzgün idarə etmək analitik və modelləşdirmə prosesinin uğurlu nəticələnməsi üçün vacibdir. Bunu etmək üçün müxtəlif üsullardan istifadə olunur:\n",
    "\n",
    "#### 1. Null dəyərləri çıxarmaq\n",
    "Ən sadə üsullardan biri dataset-dən null dəyərləri olan sətirləri və ya sütunları çıxarmaqdır. Bunun üçün bəzi funksiyalardan istifadə etmək mümkündür:\n",
    "   - Python-da Pandas vasitəsilə:\n",
    "     ```python\n",
    "     df.dropna()  # Null dəyərləri olan sətirləri silir\n",
    "     df.dropna(axis=1)  # Null dəyərləri olan sütunları silir\n",
    "     ```\n",
    "\n",
    "Bu metod dataset-də çox az null dəyər olduqda effektivdir. Lakin çoxsaylı null dəyərləri olan dataset-lərdə bu metod məlumat itkisinə səbəb ola bilər.\n",
    "\n",
    "#### 2. Dəyərləri doldurmaq (Imputation)\n",
    "Null dəyərləri silmək məlumat itkisinə səbəb olduğundan, bəzi hallarda dəyərləri təxmini rəqəmlərlə doldurmaq daha məqsədəuyğun ola bilər. Bunun üçün bir neçə yanaşma mövcuddur:\n",
    "\n",
    "   - **Orta və ya median ilə doldurmaq**: Ən geniş istifadə olunan yanaşmadır. Kəmiyyət dəyişənləri üçün null dəyərləri dataset-in ortalaması və ya medianı ilə doldurmaq olar.\n",
    "     ```python\n",
    "     df['column'].fillna(df['column'].mean(), inplace=True)  # Orta dəyərlə doldurur\n",
    "     df['column'].fillna(df['column'].median(), inplace=True)  # Median dəyərlə doldurur\n",
    "     ```\n",
    "\n",
    "   - **Mod ilə doldurmaq**: Kateqoriyalı dəyişənlər üçün null dəyərləri həmin dəyişənin ən çox rast gəlinən dəyəri ilə doldurmaq olar.\n",
    "     ```python\n",
    "     df['column'].fillna(df['column'].mode()[0], inplace=True)  # Mod ilə doldurur\n",
    "     ```\n",
    "\n",
    "   - **Linear regression və ya KNN**: Daha mürəkkəb yanaşmalar arasında regression modelləri və ya KNN (K-nearest neighbors) kimi metodlardan istifadə edərək, itkin dəyərləri proqnozlaşdırmaq mümkündür.\n",
    "\n",
    "   - **Forward və Backward Filling**: Zaman sıralarında null dəyərləri əvvəlki və ya növbəti dəyərlərlə doldurmaq mümkündür.\n",
    "     ```python\n",
    "     df.fillna(method='ffill', inplace=True)  # Əvvəlki dəyərlə doldurur\n",
    "     df.fillna(method='bfill', inplace=True)  # Növbəti dəyərlə doldurur\n",
    "     ```\n",
    "\n",
    "#### 3. Null dəyərləri xüsusi bir sinif olaraq təyin etmək\n",
    "Bəzi hallarda, xüsusilə kateqoriyalı dəyişənlər üçün null dəyərləri xüsusi bir kateqoriya kimi qəbul etmək mümkündür. Məsələn, cinsiyyət dəyişəni üçün null dəyərləri \"Məlumat yoxdur\" kimi təyin etmək olar.\n",
    "\n",
    "   ```python\n",
    "   df['column'].fillna('No Information', inplace=True)\n",
    "   ```\n",
    "\n",
    "#### 4. Modellərə null dəyərləri qarşı həssaslıq əlavə etmək\n",
    "Bəzi modellər (məsələn, decision tree və ya random forest) null dəyərlərlə işləməyi avtomatik həyata keçirə bilər. Bu modellər null dəyərləri olan verilənlərə qarşı daha davamlıdır.\n",
    "\n",
    "### Null dəyərlərin idarə edilməsi zamanı nəzərə alınmalı məqamlar\n",
    "\n",
    "- **Məlumat itkisi**: Null dəyərləri silmək data itkisinin artmasına səbəb ola bilər. Bu səbəbdən məlumatları itirmək əvəzinə, uyğun metodlarla doldurmaq daha məqsədəuyğun ola bilər.\n",
    "  \n",
    "- **Məlumatların təhrif edilməsi**: Dəyərləri doldurarkən təhlil və ya modelləşdirmə nəticələrinin təhrif edilməməsi üçün düzgün yanaşma seçilməlidir. Məsələn, ortalama ilə doldurmaq bəzən dəyərlərin təbii variasiyasını itirə bilər.\n",
    "\n",
    "- **İtkin məlumatların təsviri**: İtkin məlumatların səbəbini anlamaq və onların təbiətini qiymətləndirmək vacibdir. Məsələn, bəzi sütunlarda məlumatların olmaması xüsusi bir fenomeni göstərə bilər.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "Null dəyərlər verilənlərin analizi və modelləşdirilməsi zamanı diqqətlə idarə olunmalıdır. Onların səbəblərini düzgün anlamaq, uyğun metodlarla aşkarlamaq və düzgün idarə etmək analizin nəticələrini daha etibarlı və dəqiq edəcək. Null dəyərlərin yanlış idarə edilməsi isə verilənlərin təhlilində ciddi təhriflərə və yanlış qərarların qəbul edilməsinə səbəb ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d5e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0667bac8",
   "metadata": {},
   "source": [
    "## Supervised Learning Regression Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31acd3f",
   "metadata": {},
   "source": [
    "### R-squared: This measures the variation of a regression model. R-squared either increases or remains the same when new predictors are added to the model. ( R-kvadrat: Bu, reqressiya modelinin dəyişməsini ölçür. Modelə yeni proqnozlaşdırıcılar əlavə edildikdə R-kvadrat ya artır, ya da eyni qalır.)\n",
    "### Adjusted R-squared: This measures the variation for a multiple regression model, and helps you determine goodness of fit. Unlike R-squared, adjusted R-squared only adds new predictors to its model if it improves the model’s predicting power. (Düzəliş edilmiş R-kvadrat: Bu, çoxlu reqressiya modeli üçün variasiyanı ölçür və uyğunluğun yaxşılığını müəyyən etməyə kömək edir. R-kvadratdan fərqli olaraq, düzəliş edilmiş R-kvadrat modeli yalnız modelin proqnozlaşdırma gücünü yaxşılaşdırdığı halda modelinə yeni proqnozlaşdırıcılar əlavə edir.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76550179",
   "metadata": {},
   "source": [
    "#### https://images.app.goo.gl/Qgeqz1LPsGkEAfoD9\n",
    "#### https://images.app.goo.gl/xRogYdJjCpqKPc937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0b212c",
   "metadata": {},
   "source": [
    "## SST, SSR, SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d2871",
   "metadata": {},
   "source": [
    "**SST, SSR, və SSE** statistika və regresiya analizində istifadə olunan əsas ölçülərdir. Bu ölçülər modelin nə dərəcədə yaxşı uyğunlaşdığını və modeldəki variasiyaların nə qədərinin izah edildiyini qiymətləndirmək üçün istifadə olunur.\n",
    "\n",
    "**1. SST (Total Sum of Squares):** Ümumi kvadratlar cəmi (Total Sum of Squares) \n",
    "- Bu, `y` dəyişəninin ümumi dəyişkənliyini ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $ (y_i) $ həqiqi müşahidə olunan dəyərlər, $( \\bar{y})$ isə müşahidə olunan dəyərlərin orta qiymətidir.\n",
    "\n",
    "**2. SSR (Regression Sum of Squares):** Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- Bu, modelin nə dərəcədə uyğun gəldiyini ölçür, yəni modelin izah etdiyi variasiyadır.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $( \\hat{y}_i )$ model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**3. SSE (Error Sum of Squares):** Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- Bu, modelin izah edə bilmədiyi, qalıq variasiyanı ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\hat{y}_i )$ isə model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**Əlaqə**:\n",
    "Bu üç ölçü arasında belə bir əlaqə var:\n",
    "$$\n",
    "\\text{SST} = \\text{SSR} + \\text{SSE}\n",
    "$$\n",
    "Bu formula, ümumi variasiyanın izah edilən variasiyaya (SSR) və izah edilməmiş qalıq variasiyaya (SSE) bölünməsini göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345e34bc",
   "metadata": {},
   "source": [
    "## R2 and Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619e63b",
   "metadata": {},
   "source": [
    "**R-squared (R²)** və **Adjusted R-squared** regresiya modelinin uyğunluğunu qiymətləndirmək üçün istifadə olunan iki əsas ölçüdür. \n",
    "\n",
    "### 1. **R-squared (R²)**\n",
    "R², modelin nə qədər yaxşı izah etdiyini, yəni müstəqil dəyişənlərin asılı dəyişən üzərindəki variasiyanı nə qədər izah etdiyini göstərir. \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **SSR** - Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- **SSE** - Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- **SST** - Ümumi kvadratlar cəmi (Total Sum of Squares)\n",
    "\n",
    "R² dəyəri həmişə 0 ilə 1 arasında olur. 1-ə yaxın olduqda modelin yüksək uyğunluğunu, 0-a yaxın olduqda isə zəif uyğunluğu göstərir.\n",
    "\n",
    "### 2. **Adjusted R-squared**\n",
    "**Adjusted R²** (Düzəldilmiş R²) modelə əlavə edilən dəyişənlərin sayını nəzərə alır və modelin sadəcə dəyişənlərin sayına görə şişirdilməməsini təmin edir. Əgər modelə faydalı olmayan bir dəyişən əlavə edilərsə, Adjusted R² dəyəri azalacaq. Bu xüsusiyyət Adjusted R²-i daha etibarlı edir.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **R²** - R-squared dəyəri\n",
    "- **n** - Nümunə ölçüsü (müşahidələrin sayı)\n",
    "- **p** - Müstəqil dəyişənlərin sayı\n",
    "\n",
    "**Adjusted R²** dəyəri hər zaman **R²**-dən kiçik və ya ona bərabərdir. Bu, modelin sadəcə əlavə dəyişənlərin sayı ilə deyil, onların əhəmiyyəti ilə də əlaqəli olduğunu göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a60600",
   "metadata": {},
   "source": [
    "## R2 modelin mean-ne nezeren ne qeder yaxsi performans gosterdiyini olcen kemiyyetdir  ifadesi dogrudur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566c638",
   "metadata": {},
   "source": [
    "Bəli, bu ifadə doğrudur. $ R^2 $ dəyəri modelin performansını orta (mean) dəyərə nisbətən ölçür və modelin həmin orta dəyərə nəzərən nə qədər yaxşı işlədiyini göstərir.\n",
    "\n",
    "Daha dəqiq desək, $ R^2 $ göstəricisi modelin proqnozlarının sadəcə olaraq məlumatların ortalama dəyərinə əsaslanan proqnozlarla müqayisədə nə qədər üstün olduğunu ölçür. Yəni:\n",
    "\n",
    "- **SST** (Total Sum of Squares), bütün müşahidə dəyərlərinin ortalamadan nə qədər uzaq olduğunu ölçür.\n",
    "- **SSR** (Sum of Squared Residuals) isə model proqnozlarının həqiqi dəyərlərdən nə qədər uzaq olduğunu göstərir.\n",
    "\n",
    "$ R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} $\n",
    "\n",
    "əmsalı ilə, modelin qalıq səhvlərinin müşahidələrin sadəcə ortalama dəyərindən nə qədər az olduğunu müəyyən edir. Əgər $ R^2 $ yüksəkdirsə, deməli model müşahidələri orta dəyərə əsaslanan bir təxminə nisbətən daha yaxşı izah edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc784563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "377357b6",
   "metadata": {},
   "source": [
    "## R2 menfi ola biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022d314",
   "metadata": {},
   "source": [
    "Bəli, $ R^2 $ dəyəri mənfi ola bilər. Bu, modelin performansının orta dəyəri proqnoz kimi götürməkdən daha zəif olduğunu göstərir. \n",
    "\n",
    "Mənfi $ R^2 $ aşağıdakı vəziyyətlərdə ortaya çıxa bilər:\n",
    "\n",
    "- Model, verilən məlumatlarla yaxşı uyğunlaşmır və proqnoz səhvləri çox böyükdür.\n",
    "- Uyğun olmayan və ya səhv model seçilib.\n",
    "- Məlumat çox səs-küylüdür (noise) və sadə bir orta təxmin modeldən daha yaxşı nəticə verir.\n",
    "\n",
    "Məsələn, əgər $ R^2 = -1.5 $ çıxarsa, bu o deməkdir ki, modelin proqnozları orta dəyərə əsaslanan proqnozlarla müqayisədə 1.5 dəfə daha böyük səhvə malikdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe4b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "753a4038",
   "metadata": {},
   "source": [
    "## Noise modelde nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b57c03",
   "metadata": {},
   "source": [
    "Modeldə \"noise\" (səs-küy) verilənlərdəki təsadüfi və ya izah edilə bilməyən dəyişiklikləri ifadə edir. \"Noise\" real həyatda hər hansı bir sistemdən toplanmış verilənlərdəki təsadüfi dalğalanmaları və səhvləri əhatə edir. Bu dalğalanmalar modelin əsas trendlərini (təsirlərini) düzgün müəyyən etməsini çətinləşdirə bilər.\n",
    "\n",
    "**Noise-un mənbələri**:\n",
    "\n",
    "1. **Ölçmə səhvləri**: Məlumatın toplanması zamanı baş verən cihaz və ya metod səhvləri.\n",
    "2. **Çevrədə təsadüfi dəyişikliklər**: Çevrədəki təsadüfi hadisələr (hava şəraiti, istifadəçi davranışları və s.) verilənlərdə təsadüfi dəyişikliklərə səbəb ola bilər.\n",
    "3. **Modelə daxil edilməyən faktorlar**: Həqiqi nəticələrə təsir edən, lakin modellə izlənilməyən dəyişənlər də noise kimi görünə bilər.\n",
    "\n",
    "**Model üçün noise-un təsiri**:\n",
    "- Noise, modelin düzgün nümunələri müəyyənləşdirməsini çətinləşdirir, nəticədə proqnozlar daha qeyri-dəqiq olur.\n",
    "- Overfitting problemi yarana bilər, yəni model noiseyə uyğunlaşaraq verilənlərdəki əsas trendlər əvəzinə təsadüfi səhvləri öyrənir.\n",
    "\n",
    "Noise-u azaltmaq üçün daha çox və daha təmiz verilənlərdən istifadə etmək, uyğun dəyişənləri daxil etmək və ya modelləri düzgün seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db14327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e31207",
   "metadata": {},
   "source": [
    "## R2 ve Corellation ferqi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707ebeb",
   "metadata": {},
   "source": [
    "**R² (R-squared)**, həm də determinasiya əmsalı kimi tanınır, statistikada bir modelin izah gücünü ölçən bir göstəricidir. O, asılı (hədəf) dəyişənin dəyərlərinin neçə faizinin müstəqil dəyişənlər tərəfindən izah olunduğunu göstərir.\n",
    "\n",
    "R² ilə **korrelyasiya** arasında əlaqə var, amma onlar tam eyni məna daşımır:\n",
    "\n",
    "- **Korrelyasiya** (r) iki dəyişən arasındakı xətti əlaqəni ölçür və dəyərləri -1 ilə 1 arasında olur. Pozitiv korrelyasiya (r > 0) müsbət əlaqəni, mənfi korrelyasiya (r < 0) isə mənfi əlaqəni göstərir.\n",
    "  \n",
    "- **R²** isə korrelyasiya əmsalının kvadratı kimi hesablanır və dəyərləri 0 ilə 1 arasında olur. Bu, müstəqil dəyişənlərin modeldə asılı dəyişəni nə qədər yaxşı izah etdiyini göstərir. Məsələn, R² = 0.7 olarsa, bu o deməkdir ki, dəyişənlərin 70%-i model tərəfindən izah edilir.\n",
    "\n",
    "### R² necə işləyir?\n",
    "\n",
    "Məsələn, bir xətti regresiya modelində:\n",
    "\n",
    "- Əgər **R² = 1**, bu o deməkdir ki, model mükəmməl proqnozlaşdırır, bütün məlumat nöqtələri xəttin üzərindədir.\n",
    "- **R² = 0** olarsa, model heç bir proqnozlaşdırma dəqiqliyinə malik deyil və asılı dəyişən müstəqil dəyişənlər tərəfindən izah olunmur.\n",
    "\n",
    "Bunu belə də ifadə edə bilərik: R² korrelyasiya əmsalının kvadratı olsa da, korrelyasiya dəyişənlərin yalnız bir-biri ilə əlaqəsini göstərir, R² isə modelin nə qədər yaxşı uyğunlaşdığını ölçür."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adbad7",
   "metadata": {},
   "source": [
    "Həm **korrelyasiya əmsalı (r)**, həm də **R² (determinasiya əmsalı)** fərqli düsturlarla hesablanır.\n",
    "\n",
    "### 1. **Korrelyasiya (r)**\n",
    "\n",
    "Korrelyasiya iki dəyişən arasında əlaqəni ölçən statistik göstəricidir. Korrelyasiya əmsalı (r) -1 ilə 1 arasında dəyişir.\n",
    "\n",
    "Düstur belədir:\n",
    "$$\n",
    "r = \\frac{n(\\sum xy) - (\\sum x)(\\sum y)}{\\sqrt{[n\\sum x^2 - (\\sum x)^2][n\\sum y^2 - (\\sum y)^2]}}\n",
    "$$\n",
    "\n",
    "- \\(x\\) və \\(y\\) — dəyişənlərin dəyərləri.\n",
    "- \\(n\\) — müşahidələrin sayı.\n",
    "- $\\sum xy$ — iki dəyişənin dəyərlərinin hasilinin cəmi.\n",
    "- $\\sum x^2 $ və $\\sum y^2$ — dəyişənlərin dəyərlərinin kvadratlarının cəmi.\n",
    "\n",
    "Bu düstur iki dəyişən arasında xətti əlaqənin gücünü və istiqamətini ölçür. Pozitiv \\(r\\) müsbət xətti əlaqəni, mənfi \\(r\\) mənfi xətti əlaqəni göstərir. \\(r = 0\\) isə əlaqənin olmadığını göstərir.\n",
    "\n",
    "Bəli, tamamilə doğrudur! **Korrelyasiya əmsalının (r)** başqa bir daha sadə və məşhur formulu da var, o da kovariasiya ilə standart sapmaların istifadəsidir:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- $\\text{cov}(x, y)$ — \\(x\\) və \\(y\\) dəyişənlərinin kovariasiyasıdır.\n",
    "- $\\sigma_x$ — \\(x\\) dəyişəninin standart sapmasıdır.\n",
    "- $\\sigma_y$ — \\(y\\) dəyişəninin standart sapmasıdır.\n",
    "\n",
    "Bu düstur iki dəyişənin bir-biri ilə necə əlaqəli olduğunu ölçür:\n",
    "- Əgər **kovariasiya** müsbətdirsə, bu, \\(x\\) və \\(y\\)-nin birlikdə artdığını göstərir.\n",
    "- Əgər kovariasiya mənfidirsə, \\(x\\) artdıqca \\(y\\) azalır və ya əksinə.\n",
    "  \n",
    "Standart sapmalar isə dəyişənlərin öz dəyişkənliyini ölçür. Korrelyasiya bu iki statistik göstəricini istifadə edərək, dəyişənlər arasındakı əlaqənin gücünü normalizasiya edir və nəticəni -1 ilə 1 arasında bir əmsal şəklində göstərir.\n",
    "\n",
    "Beləliklə, korrelyasiyanı kovariasiya üzərindən də hesablamaq mümkündür:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) \\cdot \\sigma_x \\cdot \\sigma_y}\n",
    "$$\n",
    "\n",
    "Bu düstur kovariasiya ilə korrelyasiya arasındakı əlaqəni daha yaxşı izah edir.\n",
    "\n",
    "\n",
    "### 2. **R² (Determinasiya əmsalı)**\n",
    "\n",
    "**R²**, xətti regresiya modelində modelin nə qədər yaxşı uyğunlaşdığını ölçən göstəricidir. O, korrelyasiya əmsalının kvadratı kimi də hesablana bilər.\n",
    "\n",
    "Düstur:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- $y_i$ — faktiki müşahidələr (real dəyərlər).\n",
    "- $\\hat{y}_i$ — proqnozlaşdırılan dəyərlər (modelin verdiyi nəticələr).\n",
    "- $\\bar{y}$ — faktiki müşahidələrin orta qiyməti.\n",
    "- $\\sum (y_i - \\hat{y}_i)^2$ — modelin səhvlərinin kvadratlarının cəmi (residual sum of squares, SSR).\n",
    "- $\\sum (y_i - \\bar{y})^2$ — ümumi fərqlənmənin kvadratlarının cəmi (total sum of squares, SST).\n",
    "\n",
    "Bu düstur göstərir ki, model nə qədər yaxşı izah edir. \\(R^2\\) modelin proqnozlaşdırdığı və faktiki müşahidələr arasındakı fərqlənməni nə qədər yaxşı izah etdiyini ölçür.\n",
    "\n",
    "**Korrelyasiya və R² əlaqəsi:** Əgər yalnız bir müstəqil dəyişənli sadə xətti regresiya modeliniz varsa, o zaman R² korrelyasiya əmsalının kvadratı kimi də hesablana bilər:\n",
    "\n",
    "$$\n",
    "R^2 = r^2\n",
    "$$\n",
    "\n",
    "Bu yalnız sadə xətti regresiya modellərində doğrudur, çoxlu müstəqil dəyişənlərin olduğu hallarda isə bu düstur daha mürəkkəbləşir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a00ea63",
   "metadata": {},
   "source": [
    "## R, R2, Adjutsed R2  bunun hansi menfi ola biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaac43f",
   "metadata": {},
   "source": [
    "**R (korrelyasiya əmsalı)**, **R² (determinasiya əmsalı)** və **Adjusted R² (tənzimlənmiş determinasiya əmsalı)** göstəriciləri fərqli mənalar daşıyır və onların hansı hallarda mənfi olma ehtimalını izah etmək vacibdir:\n",
    "\n",
    "### 1. **R (korrelyasiya əmsalı)**\n",
    "- **Aralıq:** -1 ilə 1 arasında dəyişir.\n",
    "- **Mənfi ola bilərmi?**: Bəli, **R** mənfi ola bilər. Əgər iki dəyişən arasında mənfi xətti əlaqə varsa, yəni bir dəyişən artdıqca digəri azalırsa, onda **R** mənfi olacaq. **R = -1** tam mənfi xətti əlaqəni, **R = 1** isə tam müsbət xətti əlaqəni göstərir.\n",
    "\n",
    "### 2. **R² (determinasiya əmsalı)**\n",
    "- **Aralıq:** 0 ilə 1 arasında olur.\n",
    "- **Mənfi ola bilərmi?**: **Xeyr, R² mənfi ola bilməz.** Çünki o, korrelyasiya əmsalının kvadratı kimi hesablanır (r²), və kvadrat heç vaxt mənfi ola bilməz. **R² = 0** modelin heç bir izah gücü olmadığını göstərir, **R² = 1** isə mükəmməl uyğunlaşmanı göstərir.\n",
    "\n",
    "### 3. **Adjusted R² (tənzimlənmiş R²)**\n",
    "- **Aralıq:** Mənfi dəyərlər ala bilər və 1-ə qədər yüksələ bilər.\n",
    "- **Mənfi ola bilərmi?**: **Bəli, Adjusted R² mənfi ola bilər.** Adjusted R², modelin izah gücünü dəyişənlərin sayını nəzərə alaraq tənzimləyir. Əgər model çox zəifdirsə və modelin quraşdırdığı dəyişənlər hədəf dəyişəni yaxşı izah etmirsə, Adjusted R² mənfi ola bilər. Bu, modelin, hətta təsadüfi proqnozlaşdırmadan daha pis olduğunu göstərə bilər.\n",
    "\n",
    "Adjusted R² belə hesablansa da, əlavə dəyişənlərin sayını da nəzərə alır:\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "Burada:\n",
    "- \\( n \\) — müşahidələrin sayı,\n",
    "- \\( p \\) — müstəqil dəyişənlərin sayı.\n",
    "\n",
    "Əgər əlavə dəyişənlər modeli əhəmiyyətli dərəcədə yaxşılaşdırmazsa, Adjusted R² mənfi ola bilər.\n",
    "\n",
    "### Qısaca:\n",
    "- **R** mənfi ola bilər (-1 ilə 1 arasında).\n",
    "- **R² mənfi ola bilməz** (0 ilə 1 arasında).\n",
    "- **Adjusted R²** mənfi ola bilər, əgər model zəifdirsə və faydasız dəyişənlər əlavə edilibsə."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9a9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21ed24b",
   "metadata": {},
   "source": [
    "### R2 = SSregression / SST = 1 - SSresiduals / SST\n",
    "### R2 adjusted = 1 - (1 - R2) * (n - 1) / ( n - p - 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5052549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7111220768354529\n",
      "Adjusted R^2: 0.7110611578301462\n",
      "MAE: 2.15\n",
      "MSE: 7.17\n",
      "RMSE: 2.68\n",
      "Pearson Korelyasiya Əmsalı: 0.8434059453729525\n",
      "Pearson Korelyasiya Əmsalı (scipy ilə): 0.8434059453729523\n",
      "Intercept (kəsik): 7.603902211896262\n",
      "Slope (meyl əmsalı): 0.9982101225691038\n",
      "Xətti tənlik: y = 0.9982101225691038 * x + 7.603902211896262\n",
      "Yeni feature x_1 = 23\n",
      "Predict: y = 30.56273503098565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week4\\day1\\Weather.csv\")\n",
    "dataset = data.copy()\n",
    "\n",
    "# Data\n",
    "x = dataset['MinTemp'].values.reshape(-1,1)\n",
    "y = dataset['MaxTemp'].values.reshape(-1,1)\n",
    "\n",
    "# Train-test bölünməsi\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "# Model təlimi\n",
    "regressor = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Metriklərin hesablamaq\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "print('R^2:', R2)\n",
    "\n",
    "n = x_test.shape[0]\n",
    "p = x_test.shape[1]\n",
    "adjusted_R2 = 1 - (1-R2)*(n-1)/(n-p-1)\n",
    "print('Adjusted R^2:', adjusted_R2)\n",
    "\n",
    "MAE = round(mean_absolute_error(y_test, y_pred), 2)\n",
    "print(\"MAE:\", MAE)\n",
    "\n",
    "MSE = round(mean_squared_error(y_test, y_pred), 2)\n",
    "print(\"MSE:\", MSE)\n",
    "\n",
    "RMSE = round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Pearson korelyasiya əmsalını hesablamaq\n",
    "correlation_matrix = np.corrcoef(y_test.flatten(), y_pred.flatten())\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print('Pearson Korelyasiya Əmsalı:', pearson_corr)\n",
    "\n",
    "# Alternativ olaraq, scipy istifadə etməklə\n",
    "pearson_corr, _ = pearsonr(y_test.flatten(), y_pred.flatten())\n",
    "print('Pearson Korelyasiya Əmsalı (scipy ilə):', pearson_corr)\n",
    "\n",
    "# Intercept və Slope tapmaq\n",
    "intercept = regressor.intercept_[0]\n",
    "slope = regressor.coef_[0][0]\n",
    "print(\"Intercept (kəsik):\", intercept)\n",
    "print(\"Slope (meyl əmsalı):\", slope)\n",
    "\n",
    "# Xətti tənliyi qurmaq\n",
    "print(f\"Xətti tənlik: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Yeni feature\n",
    "x_1 = 23\n",
    "print(f'Yeni feature x_1 = {x_1}')\n",
    "\n",
    "# Predict\n",
    "print(f\"Predict: y = {slope * x_1 + intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e761747",
   "metadata": {},
   "source": [
    "## Recall ve Precision nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e03ff",
   "metadata": {},
   "source": [
    "**Recall** və **Precision**, təsnifat (classification) modellərində nəticələrin keyfiyyətini ölçmək üçün istifadə olunan iki əsas metrikdir. Bu anlayışlar, xüsusən balanssız datasetlərdə modelin performansını qiymətləndirmək üçün çox vacibdir.\n",
    "\n",
    "Bu, confusion matrix-i ən sadə formada göstərən cədvəl variantıdır.\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "\n",
    "### Recall (Həssaslıq)\n",
    "Recall, modelin gerçək pozitiv halların nə qədərini düzgün tapdığını göstərir. Başqa sözlə, gerçək \"pozitiv\" olan nümunələrin neçə faizinin düzgün şəkildə \"pozitiv\" kimi təsnif olunduğunu ölçür. Recall aşağıdakı formula ilə hesablanır:\n",
    "\n",
    "$$  \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}  $$\n",
    "\n",
    "- **True Positive (TP)**: Modelin düzgün şəkildə pozitiv olaraq təxmin etdiyi hallar.\n",
    "- **False Negative (FN)**: Modelin səhv şəkildə negativ olaraq təxmin etdiyi, əslində pozitiv olan hallar.\n",
    "\n",
    "Recall yüksəkdirsə, model əksər pozitiv halları düzgün təyin edir. Lakin bu zaman model yanlış pozitivlərə qarşı həssas ola bilər.\n",
    "\n",
    "### Precision (Dəqiqlik)\n",
    "Precision, modelin pozitiv olaraq təxmin etdiyi hallar arasından neçə faizinin həqiqətən də pozitiv olduğunu ölçür. Başqa sözlə, modelin \"pozitiv\" olaraq təsnif etdiyi nümunələrin düzgün olub-olmadığını göstərir. Precision aşağıdakı formula ilə hesablanır:\n",
    "\n",
    "$$  \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}} $$  \n",
    "\n",
    "- **True Positive (TP)**: Modelin düzgün şəkildə pozitiv olaraq təxmin etdiyi hallar.\n",
    "- **False Positive (FP)**: Modelin səhv şəkildə pozitiv olaraq təxmin etdiyi, əslində negativ olan hallar.\n",
    "\n",
    "Precision yüksəkdirsə, model pozitiv olaraq təyin etdiyi halların çoxunu düzgün təyin edir, yəni model daha \"dəqiq\"dir.\n",
    "\n",
    "### Recall və Precision arasındakı fərq\n",
    "- **Recall**: Gerçək pozitivlərin nə qədərini düzgün tapır.\n",
    "- **Precision**: Modelin pozitiv olaraq təsnif etdiyi halların nə qədərini düzgün tapır.\n",
    "\n",
    "İki metrik arasında ticarət əlaqəsi var. Recall-u artırmaq bəzən Precision-un azalmasına səbəb ola bilər və əksinə. Əgər model yüksək recall əldə etməyə çalışırsa, daha çox pozitiv təxmin edəcək və yanlış pozitivlər də arta bilər. Yüksək precision üçün isə model yalnız ən güvənli pozitivləri təyin edəcək, bu da bəzi gerçək pozitivlərin qaçırılmasına (FN artmasına) səbəb ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f47b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b065a",
   "metadata": {},
   "source": [
    "## Supervised Learning Classification Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2b083",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "#### Accuracy, modelin düzgün təsnif etdiyi nümunələrin ümumi nümunələrə olan nisbətini göstərir.\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394276f",
   "metadata": {},
   "source": [
    "#### AUC (Area Under the Curve):\n",
    "#### AUC, ROC (Receiver Operating Characteristic) əyrisinin altında qalan sahəni ölçür. ROC əyrisi modelin True Positive Rate (TPR) və False Positive Rate (FPR) göstəricilərini müxtəlif qərar hədlərinə (threshold) görə təyin edir. AUC dəyəri 0 ilə 1 arasında dəyişir və modelin təsnifat qabiliyyətini qiymətləndirir. Yüksək AUC dəyəri modelin müsbət və mənfi sinifləri yaxşı ayırdığını göstərir.\n",
    "#### TPR = TP / (TP+FN)\n",
    "#### FPR = FP / (FP+TN)\n",
    "\n",
    "#### AUC dəyərinin formulu birbaşa verilmir, amma AUC əsasən ROC əyrisinin altında qalan sahədir və inteqrasiya vasitəsilə hesablanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1310546",
   "metadata": {},
   "source": [
    "#### https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)\n",
    "#### PRECISON = TP / TP + FP\n",
    "#### RECALL = TP / TP + FN\n",
    "### F1 = 2 * ( (precision *  recall) / (precision+  recall) )\n",
    "### F0.5 = 1.25 * ( (precision * recall) / (0.25 * precision + recall) )\n",
    "### F2 = 5 * ( (precision * recall) / (4 * precision + recall) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e35012",
   "metadata": {},
   "source": [
    "### F1 score, modelin həm dəqiqlik, həm də recall göstəricilərində yaxşı performans göstərib-göstərmədiyini dəyərləndirir və balanslı bir metrikdir. Bir modelin F1 score-u yüksəkdirsə, demək olar ki, model həm yanlış müsbət, həm də yanlış mənfi nəticələri minimallaşdırmağa nail olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf5b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                     Predicted\n",
    "#                     *******************************************     \n",
    "#                     *  Positive        *     Negative         *\n",
    "#         ***************************************************************************************\n",
    "#         * Positive  *   True Positive  *    False Negative    *    Sensitivity                *\n",
    "#         *           *     (TP)         *  (FN)Type II error   *  TP / (TP + FN)               *  \n",
    "# Actual  ***************************************************************************************   \n",
    "#         * Negative  *  False Positive  *   True Negative      *    Specificity                *  \n",
    "#         *           * (FP)Type I error *      (TN)            *  TN / (TN + FP)               * \n",
    "#         *************************************************************************************** \n",
    "#         *           *  Precision       *  Negative Predictive *    Accuracy                   *   \n",
    "#         *           * TP / (TP + FP)   *    TN / (TN + FN)    * TP + TN / (TP + TN + FP + FN) *\n",
    "#         *************************************************************************************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20679742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b46ddda",
   "metadata": {},
   "source": [
    "## type error 1,  yoxsa type error 2 hansinda sehv etsek daha pisdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f85170",
   "metadata": {},
   "source": [
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "\n",
    "**Type I error** (False Positive) və **Type II error** (False Negative) arasında hansı daha pis olduğuna qərar vermək, konkret vəziyyətdən asılıdır, çünki hər bir vəziyyətdə bu səhvlərin təsirləri fərqli ola bilər. Gəlin hər iki səhvi izah edək:\n",
    "\n",
    "- **Type I error (False Positive)**: Əslində negativ olan bir halın səhvən pozitiv kimi təsnif edilməsi. Bu, \"yanlış alarm\" olaraq da bilinir.\n",
    "  - Məsələn: Səhvən bir şəxsi kredit riski yüksək olduğu halda kredit almağa layiq göstərmək.\n",
    "\n",
    "- **Type II error (False Negative)**: Əslində pozitiv olan bir halın səhvən negativ kimi təsnif edilməsi. Bu, \"qaçırılan fürsət\" olaraq bilinir.\n",
    "  - Məsələn: Əslində kredit almağa layiq bir şəxsin səhvən kredit riski yüksək olduğu üçün rədd edilməsi.\n",
    "\n",
    "### Hansı daha pisdir?\n",
    "Bu, kontekstdən asılıdır:\n",
    "\n",
    "1. **Tibb (xərçəng testi)**\n",
    "   - **Type I error**: Səhvən xəstəyə \"xərçəng var\" deyilir (həqiqətdə yoxdur).\n",
    "   - **Type II error**: Səhvən xəstəyə \"xərçəng yoxdur\" deyilir (həqiqətdə var).\n",
    "   - **Daha pis olan**: Type II error (çünki xəstə diaqnoz qoyulmadığı üçün müalicə olunmaya bilər, bu isə ölümcül ola bilər).\n",
    "\n",
    "2. **Maliyyə (kredit riski)**\n",
    "   - **Type I error**: Səhvən bir şəxsə kredit verilir (əslində ödəmə qabiliyyəti yoxdur).\n",
    "   - **Type II error**: Səhvən kredit verilməyən bir şəxsə (əslində ödəmə qabiliyyəti var).\n",
    "   - **Daha pis olan**: Type I error (çünki kredit qaytarılmadığı halda bank itkiyə məruz qala bilər).\n",
    "\n",
    "### Nəticə\n",
    "Hansı səhvin daha pis olduğunu qiymətləndirmək, tətbiq olunan sahəyə bağlıdır. Kritik səhvlərin daha ciddi təsir edə biləcəyi hallarda, bu səhvlərin qarşısını almaq üçün müxtəlif yanaşmalar istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cfdd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f146d27",
   "metadata": {},
   "source": [
    "## Bes ne vaxt Recall ve ya Precison onemlidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0075171",
   "metadata": {},
   "source": [
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) | \n",
    "\n",
    "Recall və precision fərqli hallarda önəmli ola bilər, və bunlar konkret problemlərə bağlı olaraq müxtəlif dərəcədə əhəmiyyətli ola bilər. \n",
    "\n",
    "1. **Recall** (Həqiqi müsbətlərin hissəsi) çox önəmlidir:\n",
    "   - **Sağlamlıq diaqnostikası**: Tibbi testlərdə, məsələn, xərçəng aşkar edən testlərdə recall çox önəmlidir, çünki əgər xəstəlik varsa, onu qaçırmamaq üçün testin mümkün qədər çox müsbət nəticə verməsi lazımdır.\n",
    "   - **Kiber təhlükəsizlik**: Həssas məlumatların sızdırılması və ya hücumları aşkarlama kimi vəziyyətlərdə, recall yüksək olmalıdır ki, mümkün olan bütün hücumları müəyyənləşdirmək mümkün olsun.\n",
    "\n",
    "2. **Precision** (Həqiqi müsbətlərin ümumi müsbət nəticələrə nisbəti) çox önəmlidir:\n",
    "   - **Məhsul təklifləri**: Təkliflər sistemlərində, məsələn, müştəriyə xüsusi məhsul tövsiyə edərkən, yalnız müştərinin maraqlandığı məhsulları təklif etmək daha yaxşıdır. Burada precision yüksək olmalıdır ki, təkliflər daha uyğun olsun.\n",
    "   - **Spam filtrləri**: E-poçt spam filtrləri üçün precision yüksək olmalıdır ki, istifadəçilərin əhəmiyyətli e-poçtları səhvən spam olaraq qeyd olunmasın.\n",
    "\n",
    "   - **Kredit başvurularının təsdiqlənməsi**: Kredit başvurularını təsdiqləyərkən, precision yüksək olmalıdır ki, yalnız əslində etibarlı müştərilərə kredit verilsin. Yüksək precision kreditlərin daha etibarlı müştərilərə verilməsini təmin edir, beləliklə qeyri-ödənilən kreditlərin sayı azalır.\n",
    "\n",
    "**F1 score** isə recall və precision arasındakı ticarət balansını təmin etmək üçün istifadə edilə bilər. İki metrik arasında tarazlığı təmin etmək istəyirsinizsə, F1 score hesablamaq faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9137859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487ad4d7",
   "metadata": {},
   "source": [
    "## True Positive Rate and  False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518be03",
   "metadata": {},
   "source": [
    "#### True Positive Rate (TPR) və False Positive Rate (FPR) təsnifat modellərinin performansını qiymətləndirmək üçün istifadə edilən iki mühüm göstəricidir. Onlar modelin müsbət və mənfi təsnifatlarla necə başa çıxdığını izah edirlər.\n",
    "\n",
    "#### 1. True Positive Rate (TPR)\n",
    "#### True Positive Rate (TPR), digər adı ilə Recall və ya Sensitivity (Sensitivlik), həqiqətən müsbət olan nümunələrin neçə faizinin model tərəfindən düzgün olaraq müsbət kimi təsnif edildiyini göstərir.\n",
    "\n",
    "#### TPR formulu:\n",
    "\n",
    "#### TPR = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n",
    "\n",
    " \n",
    "#### True Positives (TP): Modelin müsbət olaraq təsnif etdiyi və həqiqətən müsbət olan nümunələr.\n",
    "#### False Negatives (FN): Modelin mənfi olaraq təsnif etdiyi, lakin əslində müsbət olan nümunələr.\n",
    "#### TPR yüksəkdirsə, bu o deməkdir ki, model həqiqətən müsbət olanları yaxşı tanıyır və az sayda yanlış mənfi nəticə verir.\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "#### False Positive Rate (FPR), həqiqətən mənfi olan nümunələrin neçə faizinin model tərəfindən səhvən müsbət kimi təsnif edildiyini göstərir. FPR, modelin yanlış müsbət nəticələr vermə tendensiyasını göstərir.\n",
    "\n",
    "#### FPR formulu:\n",
    "#### FPR = 1 - Specificity = 1 - TN / (TN + FP)\n",
    "#### FPR = False Positives (FP) /  (False Positives (FP) + True Negatives (TN))\n",
    " \n",
    "#### False Positives (FP): Modelin müsbət olaraq təsnif etdiyi, lakin əslində mənfi olan nümunələr.\n",
    "#### True Negatives (TN): Modelin mənfi olaraq təsnif etdiyi və həqiqətən mənfi olan nümunələr.\n",
    "#### FPR yüksəkdirsə, bu, modelin çox sayda yanlış müsbət nəticə verdiyini göstərir.\n",
    "\n",
    "#### Nəticə:\n",
    "#### TPR yüksəkdirsə, bu, modelin həqiqətən müsbət olanları yaxşı tanıdığını göstərir.\n",
    "#### FPR aşağıdırsa, bu, modelin yanlış müsbət nəticələri azaldığını göstərir.\n",
    "#### Bu iki göstərici adətən ROC (Receiver Operating Characteristic) əyrisi ilə birlikdə istifadə edilir və AUC (Area Under the Curve) dəyəri ilə modelin performansı qiymətləndirilir. TPR və FPR dəyərlərinin tarazlaşdırılması modelin ümumi effektivliyini təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade81965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e34a568",
   "metadata": {},
   "source": [
    "## AUC (Area Under the Curve) və Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5cb0d",
   "metadata": {},
   "source": [
    "**AUC (Area Under the Curve)** və **Gini** əmsalı hər ikisi modelin performansını ölçmək üçün istifadə edilən göstəricilərdir. Xüsusilə, **ikili təsnifat (binary classification)** məsələlərində modelin nə qədər yaxşı işlədiyini başa düşmək üçün geniş istifadə olunur.\n",
    "\n",
    "### 1. **AUC (Area Under the ROC Curve)**\n",
    "- **AUC nədir?**\n",
    "  - **AUC** (ROC əyrisi altındakı sahə - Area Under the Receiver Operating Characteristic Curve) təsnifat modelinin performansını ölçən bir göstəricidir. ROC əyrisi modelin doğru müsbət nisbətini (True Positive Rate - TPR) səhv müsbət nisbətinə (False Positive Rate - FPR) qarşı çəkir.\n",
    "  - **AUC dəyəri** 0 ilə 1 arasında dəyişir:\n",
    "    - **0.5**: Model təsadüfi təxmin qədər yaxşıdır.\n",
    "    - **1.0**: Model mükəmməldir və heç bir səhv təxmin etmir.\n",
    "    - **0.5-1.0** aralığında: Dəyər artdıqca model daha yaxşıdır.\n",
    "\n",
    "  - **Misal:**\n",
    "    - AUC = 0.7 olarsa, bu o deməkdir ki, model təsadüfi bir şəkildə seçilmiş müsbət sinifləri (positives) mənfi siniflərdən (negatives) ayırmağı təxminən 70% doğru bacarır.\n",
    "\n",
    "- **ROC əyrisi**:\n",
    "  - ROC əyrisi modelin müxtəlif threshold dəyərlərində performansını qiymətləndirir. Threshold dəyişdikcə yanlış müsbət və doğru müsbət nisbətləri dəyişir və bu, əyri yaradır.\n",
    "  \n",
    "  **AUC dəyəri** bu əyrinin altında qalan sahəni ölçür. Yəni, AUC dəyəri modelin müsbət və mənfi sinifləri nə qədər dəqiq ayıra bildiyini göstərir.\n",
    "\n",
    "### 2. **Gini Əmsalı (Gini Coefficient)**\n",
    "- **Gini nədir?**\n",
    "  - **Gini** əmsalı modelin diskriminasiya qabiliyyətini ölçmək üçün istifadə olunur və AUC ilə sıx əlaqəlidir.\n",
    "  - **Gini** əmsalı belə hesablanır:\n",
    "    $$\n",
    "    \\text{Gini} = 2 \\times \\text{AUC} - 1\n",
    "    $$\n",
    "  - Gini dəyəri də 0 ilə 1 arasında dəyişir:\n",
    "    - **0**: Model təsadüfi təxmin qədər zəifdir (heç bir diskriminasiya qabiliyyəti yoxdur).\n",
    "    - **1**: Model mükəmməldir və müsbət/mənfi sinifləri tam olaraq düzgün təxmin edir.\n",
    "\n",
    "  **Misal:**\n",
    "  - Əgər AUC = 0.7-dirsə, onda Gini əmsalı belə olacaq:\n",
    "    $$\n",
    "    \\text{Gini} = 2 \\times 0.7 - 1 = 0.4\n",
    "    $$\n",
    "  Bu o deməkdir ki, modelin diskriminasiya qabiliyyəti orta səviyyədədir.\n",
    "\n",
    "### AUC ilə Gini arasındakı fərq:\n",
    "- **AUC** ROC əyrisi altında qalan sahəni ölçür və modelin düzgün müsbət təxminlər etmək qabiliyyətini göstərir.\n",
    "- **Gini** isə AUC əsasında hesablanır və diskriminasiya gücünü qiymətləndirir. **Gini** dəyəri **AUC**-nin lineer bir dönüşümüdür, buna görə hər iki göstərici eyni mənanı verir, sadəcə Gini AUC-dən fərqli bir ölçüdür.\n",
    "\n",
    "### Nəticə:\n",
    "- AUC modelin müsbət və mənfi sinifləri nə qədər yaxşı ayıra bildiyini göstərir, 0.5 təsadüfi, 1 isə mükəmməl təxmin deməkdir.\n",
    "- Gini əmsalı isə AUC əsasında modelin diskriminasiya qabiliyyətini ölçür, 0 təsadüfi, 1 mükəmməl nəticə göstərir.\n",
    "\n",
    "Hər iki göstərici təsnifat problemlərində modelin keyfiyyətini ölçmək üçün istifadə olunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4197a27c",
   "metadata": {},
   "source": [
    "Scikit-learn (`sklearn`) kitabxanasında **Gini** əmsalı birbaşa modelin metrikası kimi təklif edilmir, lakin **AUC** əsasında asanlıqla hesablana bilər, çünki **Gini** əmsalı AUC ilə əlaqəlidir və aşağıdakı düsturla hesablana bilər:\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 2 \\times \\text{AUC} - 1\n",
    "$$\n",
    "\n",
    "`sklearn.metrics` modulundan **ROC AUC** dəyərini tapıb Gini əmsalını hesablamaq üçün aşağıdakı addımları izləyə bilərsiniz:\n",
    "\n",
    "### Məsələn, Gini əmsalını hesablamaq üçün addımlar:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Məsələn, gerçək etiketlər (true labels) və proqnozlaşdırılan ehtimallar (predicted probabilities)\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "y_pred_prob = [0.1, 0.4, 0.35, 0.8, 0.7, 0.2, 0.6, 0.9, 0.5, 0.4]\n",
    "\n",
    "# AUC dəyərini hesabla\n",
    "auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "# Gini əmsalını hesabla\n",
    "gini = 2 * auc - 1\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"Gini: {gini}\")\n",
    "```\n",
    "\n",
    "### Nəticə:\n",
    "Bu kod nəticəsində həm AUC dəyərini, həm də Gini əmsalını əldə edə bilərsiniz. Əgər AUC dəyəriniz 0.7-dirsə, Gini təxminən 0.4 olacaq.\n",
    "\n",
    "### Alternativ Gini Metrikası\n",
    "Bəzən Gini əmsalı birbaşa olaraq modellər üçün **məqsəd funksiyası** (loss function) kimi istifadə edilir, məsələn, **LightGBM** və ya **XGBoost** modellərində bu parametr mövcuddur. Bu modellər Gini əmsalına əsaslanan optimallaşdırma həyata keçirə bilərlər.\n",
    "\n",
    "**Qeyd:** Sklearn-da hazır metriklər arasında Gini birbaşa mövcud deyil, lakin yuxarıdakı kimi kodlarla asanlıqla əldə edilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fd165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93abfc11",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f40ab1",
   "metadata": {},
   "source": [
    "#### https://www.saedsayad.com/images/MLR_r2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7a43a",
   "metadata": {},
   "source": [
    " Ridge :  (yi − ŷi)^2 + λ ∗ Slope^2  \n",
    " Lasso: (yi − ŷi)^2 + λ ∗ |Slope|\n",
    "\n",
    "Bu ifadə, ümumiyyətlə, cərimə terminləri (penalization terms) ilə birlikdə istifadə olunan optimallaşdırma məqsəd funksiyalarından biridir. Buradakı simvollar və ifadələrin mənası belədir:\n",
    "\n",
    "- `(yi − ŷi)^2`: Bu, müşahidə olunan dəyərlər (yi) ilə proqnozlaşdırılan dəyərlər (ŷi) arasındakı fərqlərin kvadratıdır. Bu fərq, adətən xəta və ya qalıq (residual) adlanır. Kvadrat halında ifadə edilməsinin səbəbi fərqlərin müsbət və mənfi olmasının fərqini aradan qaldırmaq və daha böyük fərqləri cəza olaraq daha çox nəzərə almaqdır.\n",
    "  \n",
    "- `λ ∗ Slope`: Bu, cərimə terminidir. Burada `λ` (lambda) cərimə əmsalıdır. Bu, modelin sadəliyinə nə qədər əhəmiyyət verildiyini göstərir. `Slope` isə modelin meyli və ya meyl əmsalının (məsələn, xətti reqressiyada meyl əmsalı beta) qiymətidir. Cərimə termini modelin çox mürəkkəb olmasının qarşısını almaq üçün əlavə olunur.\n",
    "\n",
    "Bu cür cərimə terminləri Ridge və ya Lasso kimi reqressiya modellərində istifadə olunur. Məqsəd, sadəcə modelin dəqiqliyini artırmaq deyil, həm də modelin sadəliyini və ümumiləşdirmə qabiliyyətini (overfitting-i aradan qaldırmaq) təmin etməkdir. Lambda (`λ`) çox böyük olarsa, model həddindən artıq sadə ola bilər; əksinə, çox kiçik olarsa, model mürəkkəb və ya overfit ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d74db131",
   "metadata": {},
   "source": [
    "## Ridge an Lasso haqqinda bilmek isteyirem. ne is gorurler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16efde",
   "metadata": {},
   "source": [
    "Ridge və Lasso reqressiyası, xətti reqressiya modellərində dəyişənlərin seçilməsi və modelin sadələşdirilməsi məqsədilə istifadə olunan iki məşhur cərimələndirmə (penalization) metodudur. Hər ikisi overfitting-in qarşısını almaq üçün reqressiya əmsallarına cərimə tətbiq edir, lakin bunu fərqli yollarla edirlər.\n",
    "\n",
    "### 1. **Ridge Regression**\n",
    "   - **Nədir?**\n",
    "     Ridge reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların kvadratlarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ (βj^2)` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Ridge reqressiyası, əmsalların böyük olmasının qarşısını almaq üçün onları kiçikləşdirir. Bu metod xüsusilə yüksək kollinear (bir-birinə bağlı) dəyişənlər olduğu hallarda effektivdir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L2 norması (kvadrat cərimə).\n",
    "     - **Əsas üstünlüyü:** Overfitting-in qarşısını alır və modelin stabilliyini artırır.\n",
    "     - **Dəyişənlər:** Bütün dəyişənlər saxlanılır, lakin əmsalları kiçikləşdirilir.\n",
    "\n",
    "### 2. **Lasso Regression**\n",
    "   - **Nədir?**\n",
    "     Lasso reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların modullarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ |βj|` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Lasso reqressiyası, bəzi əmsalların tamamilə sıfıra bərabər olmasına səbəb ola bilər. Bu, modeldə əhəmiyyətsiz olan dəyişənlərin tamamilə çıxarılmasına imkan verir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L1 norması (modul cərimə).\n",
    "     - **Əsas üstünlüyü:** Dəyişənlərin seçilməsi və modelin sadələşdirilməsi.\n",
    "     - **Dəyişənlər:** Bəzi dəyişənlər sıfırlanır və modeldən çıxarılır.\n",
    "\n",
    "### **Ridge və Lasso Arasındakı Fərqlər:**\n",
    "   - **Cərimə növü:** Ridge L2 cəriməsi istifadə edərək əmsalları kiçikləşdirir, Lasso isə L1 cəriməsi ilə bəzi əmsalları sıfıra çevirir.\n",
    "   - **Dəyişənlərin seçilməsi:** Ridge bütün dəyişənləri saxlayır, lakin Lasso bəzi dəyişənləri tamamilə çıxarır.\n",
    "   - **Yüksək korrelyasiya:** Ridge yüksək korrelyasiyalı dəyişənlər üçün daha uyğundur, çünki o, bu dəyişənləri bir arada saxlayır, Lasso isə yüksək korrelyasiyalı dəyişənlərin bir qismini sıfırlaya bilər.\n",
    "\n",
    "### **Ridge və Lasso'nun İstifadəsi:**\n",
    "   - Ridge reqressiyası daha çox dəyişən olduğu və onların əhəmiyyətinin hamısının qorunması lazım olduğu hallarda istifadə olunur.\n",
    "   - Lasso reqressiyası isə dəyişənlərin seçilməsi və daha sadə modellər qurmaq lazım olduğu hallarda istifadə olunur.\n",
    "\n",
    "Bu metodlar, xüsusilə böyük verilənlər dəstləri və yüksək miqdarda dəyişən olan problemlər üçün faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7e9fc0b",
   "metadata": {},
   "source": [
    "## Cross_Validation haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312bdd9",
   "metadata": {},
   "source": [
    "Cross-validation, maşın öyrənməsində modelin performansını qiymətləndirmək üçün istifadə olunan bir texnikadır. Bu üsul, modelin overfitting (modelin təlim məlumatlarına həddən artıq uyğunlaşması) və underfitting (modelin həm təlim, həm də test məlumatlarını yaxşı öyrənməməsi) probleminin qarşısını almağa kömək edir. Cross-validation, modeli müxtəlif alt qruplar üzərində təlim edərək və test edərək modelin ümumiləşdirmə qabiliyyətini qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Nədir?\n",
    "Cross-validation, ümumiyyətlə, məlumat dəstəsini bir neçə hissəyə (fold) bölmək və bu hissələrdən birini test dəsti kimi, qalanlarını isə təlim dəsti kimi istifadə etmək üsuludur. Bu proses bir neçə dəfə təkrarlanır və nəticədə hər bir hissə bir dəfə test dəsti kimi istifadə olunur. Bu təkrarlamalar nəticəsində əldə edilən nəticələr ortalanır və modelin performansı bu ortalama üzərindən qiymətləndirilir.\n",
    "\n",
    "### Növləri:\n",
    "1. **K-Fold Cross-Validation:**\n",
    "   - **Ən çox istifadə olunan üsuldur.** Məlumat dəstəsi K sayda hissəyə bölünür (məsələn, K=5 və ya K=10).\n",
    "   - **Proses:** Məlumat dəstəsi K hissəyə bölünür və model K dəfə təlim edilir. Hər dəfə bir hissə test dəsti kimi istifadə olunur, qalan K-1 hissə isə təlim üçün istifadə edilir. Nəticədə K dəfə modelin performansı ölçülür və bu nəticələr ortalanır.\n",
    "   - **Üstünlükləri:** Bütün məlumatlar həm təlim, həm də test üçün istifadə olunduğundan, bu üsul daha dəqiq və ümumi nəticələr verir.\n",
    "\n",
    "2. **Stratified K-Fold Cross-Validation:**\n",
    "   - K-Fold Cross-Validation-a bənzərdir, amma burada məlumat dəstəsi siniflərin paylanmasına uyğun olaraq bölünür. Bu, xüsusilə balanssız siniflər olan məlumat dəstələrində faydalıdır.\n",
    "   - Siniflərin hər fold içində təmsil olunmasını təmin edir.\n",
    "\n",
    "3. **Leave-One-Out Cross-Validation (LOOCV):**\n",
    "   - **Hər dəfə bir nümunə çıxarılaraq test üçün istifadə edilir.** Qalan bütün nümunələr təlim üçün istifadə olunur.\n",
    "   - **Proses:** Məlumat dəstəsinin hər bir nümunəsi bir dəfə test dəsti kimi istifadə olunur, qalan nümunələr isə təlim üçün istifadə edilir. Bu üsul çox vaxt aparsa da, daha dəqiq nəticələr verir.\n",
    "   - **Üstünlükləri:** Xüsusilə kiçik məlumat dəstələrində modelin performansını dəqiq ölçmək üçün yaxşıdır.\n",
    "\n",
    "4. **Time Series Cross-Validation:**\n",
    "   - Zaman sırasına görə məlumatları böldüyü üçün, zaman sıralı məlumatlarla işləyən modellər üçün istifadə olunur.\n",
    "   - Məlumatlar zamanla əlaqəli olduqda, gələcək məlumatların proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "5. **Nested Cross-Validation:**\n",
    "   - Bu üsul hiperparametr tənzimləməsi ilə model performansını qiymətləndirmək üçün istifadə olunur.\n",
    "   - Birinci dərəcəli cross-validation modeli tənzimləmək üçün, ikinci dərəcəli cross-validation isə performansı qiymətləndirmək üçün istifadə edilir.\n",
    "\n",
    "### Cross-Validation Üstünlükləri:\n",
    "- **Ümumiləşdirmə:** Modelin məlumatlara həddən artıq uyğunlaşmasını (overfitting) qarşısını almağa kömək edir.\n",
    "- **Dəqiqlik:** Məlumat dəstəsinin bütün hissələri həm təlim, həm də test üçün istifadə olunduğundan, modelin performansını daha dəqiq ölçür.\n",
    "- **Stabil qiymətləndirmə:** Bir çox təkrarlama ilə ortalama nəticələr əldə olunur, bu da modelin performansını daha stabil şəkildə qiymətləndirir.\n",
    "\n",
    "### Cross-Validation Zamanı Dikkat Edilmesi Gerekenler:\n",
    "- **Məlumatın Bərabər Bölünməsi:** Məlumat dəstəsinin fold-lara bərabər şəkildə bölündüyündən əmin olunmalıdır.\n",
    "- **Zaman Əlaqəsi:** Zamanla əlaqəli məlumatlar varsa, zaman sırası gözlənilməlidir.\n",
    "- **Hesablama Məsrəfləri:** Daha çox fold daha dəqiq nəticələr verir, lakin hesablama vaxtını artırır.\n",
    "\n",
    "Cross-validation, maşın öyrənməsində model performansını qiymətləndirmək və tənzimləmək üçün əsas vasitələrdən biridir. Bu texnika, modelin ümumiləşdirmə qabiliyyətini artırır və həm akademik, həm də sənaye tətbiqlərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655fb9a",
   "metadata": {},
   "source": [
    "Cross-validation prosesini cədvəl şəklində göstərmək üçün hər bir fold-un təlim və test dəstlərinə necə bölündüyünü və nəticələrin ortalanmasını cədvəl formasında təsvir edə bilərik.\n",
    "\n",
    "### 5-Fold Cross-Validation Cədvəli\n",
    "\n",
    "| Fold  | Təlim Dəsti                   | Test Dəsti      | F1-score |\n",
    "|-------|--------------------------------|-----------------|----------|\n",
    "| 1     | Fold 2, 3, 4, 5                | Fold 1          | F1-score_1|\n",
    "| 2     | Fold 1, 3, 4, 5                | Fold 2          | F1-score_2|\n",
    "| 3     | Fold 1, 2, 4, 5                | Fold 3          | F1-score_3|\n",
    "| 4     | Fold 1, 2, 3, 5                | Fold 4          | F1-score_4|\n",
    "| 5     | Fold 1, 2, 3, 4                | Fold 5          | F1-score_5|\n",
    "\n",
    "### Ümumi F1-score:\n",
    "$$ \n",
    "\\text{Ortalama F1-score} = \\frac{\\text{F1-score}_1 + \\text{F1-score}_2 + \\text{F1-score}_3 + \\text{F1-score}_4 + \\text{F1-score}_5}{5}\n",
    " $$\n",
    "\n",
    "Bu ifadə hər bir fold üçün hesablanmış F1-score dəyərlərinin ortalamasını verir.\n",
    "\n",
    "Bu cədvəl hər fold üçün təlim və test dəstlərinin necə bölündüyünü və nəticələrin (məsələn, F1-score) necə toplandığını göstərir. Nəticədə, bütün fold-ların performans göstəriciləri ortalanaraq modelin ümumi performansı müəyyən edilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218735b9",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import cross_validate\n",
    "accuracies = cross_validate(estimator=NB_model, X=X_train_scaled, \n",
    "                            y=y_train, cv=4, scoring = ['f1', 'roc_auc', 'recall', 'precision', 'accuracy'])\n",
    "accuracies\n",
    "\n",
    "output\n",
    "{'fit_time': array([0.004, 0.004, 0.004, 0.004]),\n",
    " 'score_time': array([0.033, 0.031, 0.03 , 0.048]),\n",
    " 'test_f1': array([0.8136, 0.8333, 0.8308, 0.8276]),\n",
    " 'test_roc_auc': array([0.866 , 0.8983, 0.8308, 0.9161]),\n",
    " 'test_recall': array([0.7742, 0.8065, 0.9   , 0.7742]),\n",
    " 'test_precision': array([0.8571, 0.8621, 0.7714, 0.8889]),\n",
    " 'test_accuracy': array([0.807 , 0.8246, 0.8036, 0.8214])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a47d1",
   "metadata": {},
   "source": [
    "**Cross-validation** (cv=5) istifadə edərkən, modelin performansını qiymətləndirmək üçün dataset-i 5 bərabər hissəyə (fold) bölürsünüz. Hər bir hissə, modelin test edildiyi zaman \"test seti\" kimi istifadə olunur, digərləri isə \"train seti\" olur. \n",
    "\n",
    "### Overfitting Analizinin İki Aspekti:\n",
    "\n",
    "1. **Individual Cross-validation Scores:**\n",
    "   - Əgər 5 cross-validation scores arasında böyük fərq varsa (məsələn, 0.8, 0.6, 0.9, 0.5, 0.95 kimi), bu, modelin verilənlərin müxtəlif hissələrinə görə qeyri-stabil olduğunu və bəlkə də overfitting problemindən əziyyət çəkdiyini göstərə bilər.\n",
    "   - Belə bir vəziyyətdə, model bəzi verilənlərdə çox yaxşı proqnozlaşdırır, amma başqalarında zəif performans göstərir, bu da onun yalnız konkret verilənlər üzərində öyrəndiyini (overfitting) göstərə bilər.\n",
    "\n",
    "2. **Mean Cross-validation Score vs. Test Set Score:**\n",
    "   - Cross-validation scores-un ortalaması (mean score) modelin daha ümumi bir performansını göstərir. Əgər bu ortalama score, test seti üzrə hesablanan score ilə böyük fərqə malikdirsə (məsələn, cv scores ortalaması 0.85, amma test set score 0.65), bu da overfitting əlaməti ola bilər.\n",
    "   - Bu vəziyyətdə, model tren setindəki məlumatları çox yaxşı öyrənmiş, amma görünməmiş test setindəki verilənlərə uyğunlaşa bilmir.\n",
    "\n",
    "### Nəticə:\n",
    "\n",
    "- **Böyük fərq varsa:** Həm individual cross-validation scores arasında, həm də mean cross-validation score ilə test score arasında fərq varsa, bu, overfitting əlaməti ola bilər.\n",
    "- **Stabil performans:** Əgər individual scores arasında kiçik fərqlər varsa və mean score ilə test score bir-birinə yaxınsa, bu modelin daha stabil olduğunu və overfitting probleminin olmadığını göstərir.\n",
    "\n",
    "Bu iki aspekti birlikdə qiymətləndirmək, modelin overfitting problemini anlamağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982d185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db5fba0",
   "metadata": {},
   "source": [
    "## bes λ nece tapilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c67a8",
   "metadata": {},
   "source": [
    "Lambda (`λ`) dəyəri, modelin cərimə əmsalıdır və onun optimal dəyəri modelin performansını yaxşılaşdırmaq üçün seçilir. Lambda-nı təyin etmək üçün bir neçə üsul mövcuddur:\n",
    "\n",
    "### 1. **Cross-Validation (Çapraz Doğrulama)**\n",
    "   - **Addım 1:** Verilənlər toplusunu bir neçə hissəyə bölmək (məsələn, 5 və ya 10 fold cross-validation).\n",
    "   - **Addım 2:** Müxtəlif lambda dəyərləri üçün modelin performansını hər bir hissədə qiymətləndirmək.\n",
    "   - **Addım 3:** Hər bir fold üçün modelin performansını ölçmək və ortalama performansı hesablayaraq ən yaxşı nəticə verən lambda dəyərini seçmək.\n",
    "\n",
    "### 2. **Grid Search**\n",
    "   - Müəyyən bir aralıqda lambda dəyərləri təyin olunur (məsələn, 0.001-dən 10-a qədər).\n",
    "   - Hər bir lambda dəyəri üçün model qurulur və performans ölçülür.\n",
    "   - Ən yaxşı performansı verən lambda seçilir.\n",
    "\n",
    "### 3. **Regularization Paths**\n",
    "   - Bu üsulda, müxtəlif lambda dəyərləri üçün reqressiya əmsallarının dəyişməsi analiz edilir.\n",
    "   - Bu qrafik üzərində, reqressiya əmsallarının stabilləşdiyi nöqtədəki lambda dəyəri seçilir.\n",
    "\n",
    "### 4. **Information Criteria (AIC, BIC)**\n",
    "   - Akaike Information Criterion (AIC) və ya Bayesian Information Criterion (BIC) kimi informasiya meyarları ilə lambda seçilə bilər. Bu meyarlar modeli uyğunlaşdırma keyfiyyətini və onun mürəkkəbliyini qiymətləndirir.\n",
    "\n",
    "Bu üsulların hər biri fərqli vəziyyətlər üçün uyğun ola bilər. Ən çox istifadə olunan metod isə Cross-Validation üsuludur, çünki bu metod həm modelin performansını yaxşı ölçməyə, həm də overfitting-in qarşısını almağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8eba0b",
   "metadata": {},
   "source": [
    "## penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0 bu paramertler haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4199",
   "metadata": {},
   "source": [
    "Bu parametrlər, `sklearn.linear_model.LogisticRegression` sinfində ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə olunur. Aşağıda hər bir parametrin detallı izahı verilmişdir:\n",
    "\n",
    "### 1. **`penalty='elasticnet'`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsini tətbiq edir. ElasticNet cərimələndirməsi həm L1 (Lasso) həm də L2 (Ridge) normlarını birləşdirir. Yəni, ElasticNet modeli həm bəzi əmsalları sıfıra yaxınlaşdırır (Lasso kimi), həm də digər əmsalları kiçikləşdirir (Ridge kimi).\n",
    "   - **Necə işləyir?**: Bu parametr təkbaşına işləməz, onu aktivləşdirmək üçün `l1_ratio` parametri və uyğun `solver` lazımdır.\n",
    "\n",
    "### 2. **`solver='saga'`**\n",
    "   - **İşlevi:** `solver` parametri Logistic Regression modelinin təlimi üçün istifadə edilən optimizasiya alqoritmini təyin edir. `saga`, həm L1, həm də ElasticNet cərimələndirmələrini dəstəkləyən bir solverdir. Bu alqoritm böyük verilənlər dəstlərində və sparse (sıx olmayan) xüsusiyyətlərə malik olan modellərdə çox effektivdir.\n",
    "   - **Nə üçün vacibdir?**: ElasticNet cərimələndirməsi üçün `saga` solverini seçmək məcburidir, çünki yalnız `saga` (və `liblinear`) L1 normunu dəstəkləyir, ElasticNet isə həm L1, həm də L2 normlarını birləşdirdiyindən, onun üçün də uyğundur.\n",
    "\n",
    "### 3. **`l1_ratio=0.5`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsi daxilində L1 və L2 cərimələri arasındakı balansı təyin edir. \n",
    "     - **`l1_ratio=0`** olduqda, ElasticNet yalnız L2 (yəni, Ridge) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=1`** olduqda, ElasticNet yalnız L1 (yəni, Lasso) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=0.5`** dəyəri həm L1, həm də L2 cərimələndirmələrinin qarışığını tətbiq edir, yəni hər ikisini bərabər şəkildə qarışdırır.\n",
    "   - **İstifadə məqsədi:** Bu parametr L1 və L2 normları arasında uyğun bir balans yaratmaq üçündür. Verilən problemə uyğun optimal balansı tapmaq üçün `l1_ratio` dəyərini dəyişdirərək təcrübə etmək mümkündür.\n",
    "\n",
    "### 4. **`C=1.0`**\n",
    "   - **İşlevi:** `C` parametri modelin cərimələndirmə əmsalını təyin edir. `C` dəyəri cərimələndirmənin tərs əmsalıdır (yəni, `C` nə qədər böyükdürsə, cərimə bir o qədər kiçikdir və əksinə).\n",
    "   - **Nə üçün vacibdir?**: \n",
    "     - **Kiçik `C` dəyəri:** Daha böyük cərimələndirməyə səbəb olur, yəni modelin sadələşdirilməsinə və əmsalların kiçikləşdirilməsinə (və ya sıfıra yaxınlaşmasına) gətirib çıxarır.\n",
    "     - **Böyük `C` dəyəri:** Daha az cərimələndirməyə səbəb olur və model daha çox uyğunlaşır (overfitting riskini artırır).\n",
    "   - **Standart dəyər:** `C=1.0` balanslı bir dəyərdir, lakin onu artırıb azaltmaqla modelin performansını dəyişdirə bilərsiniz.\n",
    "\n",
    "### **Yekun:**\n",
    "Bu parametrlər birlikdə ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə edilir. `penalty='elasticnet'` ElasticNet cərimələndirməsini tətbiq edir, `solver='saga'` bu cərimələndirməni dəstəkləyən optimizasiya alqoritmidir, `l1_ratio=0.5` L1 və L2 normları arasında balansı təyin edir, və `C=1.0` isə cərimələndirmənin dərəcəsini müəyyən edir. Bu parametrlər sizin modelinizin kompleksliyini idarə etmək və overfitting riskini azaldaraq daha dəqiq proqnozlar vermək üçün istifadə edilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab96bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe20b15",
   "metadata": {},
   "source": [
    "## solver nedir umumiyyetle ve imkan varsa formulasini yaz ve ya adi dilde izah ele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd31be",
   "metadata": {},
   "source": [
    "`Solver`, statistik modellərin və optimizasiya problemlərinin həlli üçün istifadə olunan bir alqoritmdir. Xüsusən, `sklearn.linear_model.LogisticRegression` sinifində olduğu kimi, xətti modellərdə (xətti reqressiya, logistika reqressiyası və s.) əmsalları tapmaq üçün bu alqoritmlərdən istifadə olunur. \n",
    "\n",
    "### **Solver Nədir?**\n",
    "\n",
    "- **Nə iş görür?**: Solver, optimizasiya problemini həll edərək modelin əmsallarını tapır. Bu prosesdə məqsəd, itki funksiyasını (loss function) minimallaşdırmaq və ən uyğun model parametrlərini tapmaqdır.\n",
    "- **İş prinsipi:** Solver modelin parametrlərini iterativ şəkildə dəyişdirir və itki funksiyasını tənzimləyərək ən yaxşı nəticəyə çatmağa çalışır. Hər bir iterasiyada modelin parametrləri yenilənir və daha az itki ilə nəticə əldə etməyə çalışılır.\n",
    "\n",
    "### **Populyar Solver-lər və İş Prinsipləri**\n",
    "\n",
    "1. **`liblinear`**\n",
    "   - **Xüsusiyyətləri:** Çoxlu miqdarda xüsusiyyətə malik olan (feature) kiçik və orta miqdarda verilənlər üçün yaxşıdır. L1 və L2 cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Koordinat Descent alqoritmindən istifadə edir. L1 və L2 cərimələndirməsi ilə səmərəlidir.\n",
    "\n",
    "2. **`newton-cg`**\n",
    "   - **Xüsusiyyətləri:** Daha böyük verilənlər dəstləri üçün uyğundur. L2 cərimələndirməsini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Newton-Raphson metodundan istifadə edir. Hər iterasiyada ikinci dərəcəli diferensial məlumatlardan istifadə edərək əmsalları yeniləyir.\n",
    "\n",
    "3. **`saga`**\n",
    "   - **Xüsusiyyətləri:** Böyük verilənlər dəstləri və sıx olmayan (sparse) verilənlər üçün yaxşıdır. L1, L2 və ElasticNet cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Stochastic Average Gradient Descent (SAG) metodunun inkişaf etdirilmiş formasıdır. Bu, gradient descend metodu ilə əmsalları iterativ şəkildə yeniləyir.\n",
    "\n",
    "4. **`lbfgs`**\n",
    "   - **Xüsusiyyətləri:** L2 cərimələndirməsini dəstəkləyir və böyük verilənlər üçün yaxşı nəticələr verir.\n",
    "   - **İş Prinsipi:** Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) alqoritmindən istifadə edir. İkinci dərəcəli optimizasiya üsulu olaraq parametrlərin optimallaşdırılmasına kömək edir.\n",
    "\n",
    "### **Solver-lərin İstifadəsi**\n",
    "\n",
    "Solver-lər müxtəlif optimizasiya alqoritmləri ilə modelin parametrlərini optimallaşdırmağa kömək edir. Onların əsas məqsədi itki funksiyasını minimallaşdırmaqdır.\n",
    "\n",
    "### **Formulalar**\n",
    "\n",
    "Solver-lərin istifadə etdiyi əsas formulalar ümumiyyətlə optimizasiya və gradient descend metodlarına əsaslanır. Burada ən çox istifadə olunan metodlardan biri olan gradient descent formulunu təqdim edirəm:\n",
    "\n",
    "**Gradient Descent Formula:**\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} $$\n",
    "\n",
    "- **$\\theta_{j}$**: Modelin parametridir.\n",
    "- **$\\alpha$**: Öğrenmə sürətidir (learning rate).\n",
    "- **$J(\\theta)$**: İtki funksiyasıdır (loss function).\n",
    "- **$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$**: İtki funksiyasının $\\theta_{j}$ parametrinə görə gradientidir.\n",
    "\n",
    "### **Adi Dildə İzahetmə**\n",
    "\n",
    "Solver-lər modelin \"öyrənmə\" mərhələsində əmsalları tapmağa kömək edir. Məsələn, Logistic Regression modelini qurduğunuz zaman, solver modelin düzgün parametrlərini tapmaq üçün müxtəlif üsullardan istifadə edir. Hər iterasiyada solver əmsalları dəyişdirərək itki funksiyasını (modelin səhvini) azaldır və nəticədə ən yaxşı parametrləri tapmağa çalışır. \n",
    "\n",
    "Solver-lərin seçimləri, verilənlərin ölçüsünə, cərimələndirmə növünə və modelin tələblərinə görə fərqlənə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82674380",
   "metadata": {},
   "source": [
    "## itki funksiayasi nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d078b70",
   "metadata": {},
   "source": [
    "İtki funksiyası (loss function), statistik modelləşdirmədə modelin nə qədər düzgün proqnoz verdiyini ölçmək üçün istifadə olunan bir funksiyadır. Bu funksiya modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqi kvantlaşdırır. İtki funksiyasının məqsədi modelin performansını qiymətləndirmək və modelin parametrlərini optimallaşdırmaq üçün istifadə olunur.\n",
    "\n",
    "İtki funksiyası müxtəlif modellərdə müxtəlif formullarla təyin olunur. Burada iki əsas növ itki funksiyasını təqdim edirəm: **Xətti Reqressiya** və **Logistik Reqressiya** üçün.\n",
    "\n",
    "### **1. Xətti Reqressiya üçün İtki Funksiyası (Mean Squared Error - MSE)**\n",
    "\n",
    "Xətti reqressiya modelində itki funksiyası adətən **Mean Squared Error (MSE)** ilə təyin olunur. MSE, modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqin kvadratlarının ortalamasını ölçür. \n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı (datanın ölçüsü).\n",
    "- **$y_i$**: Həqiqi dəyər (ground truth).\n",
    "- **$\\hat{y_i}$**: Modelin proqnozu (predicted value).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- MSE, modelin nə qədər yanıldığını ölçür. Kiçik MSE dəyərləri modelin daha yaxşı nəticə verdiyini göstərir.\n",
    "- Kvadratlı fərqlərin ortalaması olduğu üçün, böyük səhvləri daha çox vurğulayır.\n",
    "\n",
    "### **2. Logistik Reqressiya üçün İtki Funksiyası (Log Loss / Binary Cross-Entropy Loss)**\n",
    "\n",
    "Logistik reqressiya modelində itki funksiyası adətən **Log Loss** (və ya Binary Cross-Entropy Loss) ilə təyin olunur. Bu itki funksiyası modelin proqnozlarının doğru etiketlərlə nə qədər uyğun olduğunu ölçür.\n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı.\n",
    "- **$y_i$**: Həqiqi etiket (1 və ya 0).\n",
    "- **$\\hat{y_i}$**: Modelin ehtimal proqnozu (0 ilə 1 arasında).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- Log Loss, proqnozların həqiqi etiketlərlə nə qədər uyumlu olduğunu qiymətləndirir.\n",
    "- Əgər modelin proqnozu həqiqi etiketə yaxınsa, Log Loss dəyəri kiçik olacaqdır.\n",
    "- Log Loss, ehtimal dəyərləri ilə işləyir və aşağı ehtimallar üçün yüksək cəzalar verir.\n",
    "\n",
    "### **Əlavə Nümunələr:**\n",
    "\n",
    "- **Hinge Loss**: SVM (Support Vector Machine) modellərində istifadə olunur. Formulu belədir: \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y_i})\n",
    "  $$\n",
    "  burada **$y_i$** doğru etiketlərdir (1 və ya -1), **$\\hat{y_i}$** isə proqnoz edilən etiketlərdir.\n",
    "\n",
    "İtki funksiyası modelin öyrənmə prosesində əsas rol oynayır, çünki modelin parametrləri itki funksiyasını minimuma endirmək üçün optimallaşdırılır. Hər bir model və problem üçün itki funksiyası fərqli ola bilər və bu, modelin məqsədinə və xüsusiyyətlərinə əsaslanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcb53f6",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LogisticRegression daxilinde ridge lasso elasticnet tetbiqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987371e",
   "metadata": {},
   "source": [
    "`sklearn.linear_model.LogisticRegression` sinfi daxilində Ridge, Lasso və ElasticNet cərimələndirmələrini tətbiq etmək mümkündür. Bu cərimələndirmələr `penalty` parametri vasitəsilə seçilir. Aşağıda hər bir cərimələndirmə üsulunun necə tətbiq edildiyini göstərən nümunələr təqdim edirəm.\n",
    "\n",
    "### 1. **Ridge Regression (L2 Penalty)**\n",
    "Ridge cərimələndirməsi `penalty='l2'` parametri ilə həyata keçirilir. Bu, `LogisticRegression` sinfinin standart cərimələndirmə üsuludur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Məlumat dəstini bölmək\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli L2 cəriməsi ilə\n",
    "ridge_model = LogisticRegression(penalty='l2', C=1.0)  # C parametri lambda-nın tərsidir\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ridge Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed563de",
   "metadata": {},
   "source": [
    "### 2. **Lasso Regression (L1 Penalty)**\n",
    "Lasso cərimələndirməsi `penalty='l1'` parametri ilə həyata keçirilir. Bunun üçün `solver` parametrini də uyğun olaraq `liblinear` və ya `saga` seçmək lazımdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)  # SAGA və ya liblinear solver\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Lasso Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4951",
   "metadata": {},
   "source": [
    "### 3. **ElasticNet Regression**\n",
    "ElasticNet cərimələndirməsi üçün həm `l1_ratio`, həm də `penalty='elasticnet'` parametrləri istifadə olunur. Burada da `solver` olaraq `saga` istifadə edilməlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d461ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)\n",
    "elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = elasticnet_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ElasticNet Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb3bb",
   "metadata": {},
   "source": [
    "### Parametrlərin İzahı:\n",
    "- **`penalty`**: Cərimələndirmə növünü təyin edir (`l1`, `l2`, `elasticnet`).\n",
    "- **`C`**: Cərimə əmsalını təyin edir. Daha böyük `C` daha az cərimələndirmə deməkdir (`C` dəyəri lambda-nın tərsidir).\n",
    "- **`l1_ratio`**: ElasticNet üçün L1 və L2 arasında balans yaradır. `l1_ratio=0` yalnız Ridge, `l1_ratio=1` isə yalnız Lasso cəriməsi deməkdir.\n",
    "- **`solver`**: Problemi həll etmək üçün istifadə edilən alqoritmdir. `liblinear` və `saga` solverləri L1 və ElasticNet cərimələndirməsini dəstəkləyir.\n",
    "\n",
    "Bu nümunələrdə Logistic Regression daxilində Ridge, Lasso və ElasticNet cərimələndirmə üsullarını tətbiq edə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555e2d88",
   "metadata": {},
   "source": [
    "## Ridge, Lasso ve Elasticnet  svm, KNN, Decision tree-de de isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa227",
   "metadata": {},
   "source": [
    "Ridge, Lasso və ElasticNet kimi cərimələndirmə üsulları xətti modellərdə, xüsusilə xətti reqressiya və logistika reqressiya modellərində tətbiq olunan texnikalardır. Bu cərimələndirmə metodları, xətti modellərin daha yaxşı ümumiləşdirilməsini təmin etmək üçün nəzərdə tutulmuşdur. Ancaq digər maşın öyrənməsi alqoritmlərində (məsələn, SVM, KNN, Decision Tree) bu cərimələndirmə üsullarının eyni formada işlədilməsi mümkün deyil, çünki bu alqoritmlər fərqli optimizasiya və iş prinsiplərinə malikdir.\n",
    "\n",
    "### **Ridge, Lasso və ElasticNet Cərimələndirmələri**\n",
    "- **Ridge (L2 Penalty):** Əmsalların böyüklüyünü cərimələyir və onların kiçik olmasına səbəb olur. Overfitting riskini azaldır.\n",
    "- **Lasso (L1 Penalty):** Bəzi əmsalları tam sıfıra bərabər edir, yəni xüsusiyyət seçimi (feature selection) rolunu oynayır.\n",
    "- **ElasticNet:** Həm L1, həm də L2 cərimələndirməsini birləşdirir və xüsusiyyətlər arasındakı əlaqələri idarə edir.\n",
    "\n",
    "### **Bu Texnikalar Aşağıdakı Alqoritmlərdə Tətbiq Edilə Bilərmi?**\n",
    "\n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - **SVM Cərimələndirməsi:** SVM-də **cərimə parametri (C)** Ridge Regression-a bənzər şəkildə işləyir. Bu parametr səhv təsnifatları cərimələndirərək marjanı böyütməyə çalışır. Yəni, SVM-də L1 və L2 cəriməsi istifadə edilməsə də, itki funksiyası və cərimələndirmə prinsipləri cərimə parametri `C` ilə idarə olunur.\n",
    "   - **L1 və L2 Cərimələndirməsi:** SVM-də **LinearSVC** sinfi daxilində `penalty='l1'` və `penalty='l2'` parametrləri ilə L1 və L2 cərimələri tətbiq oluna bilər.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "   - **KNN-də Cərimələndirmə:** KNN alqoritmində cərimələndirmə tətbiq olunmur, çünki bu alqoritm xüsusiyyət əmsalları ilə deyil, məsafə metrikləri ilə işləyir. KNN sadəcə verilənlərdəki ən yaxın nöqtələrə əsaslanaraq qərar verir və əmsalların böyüklüyü kimi anlayışlar burada mövcud deyil.\n",
    "   \n",
    "3. **Decision Tree**\n",
    "   - **Decision Tree Cərimələndirməsi:** Decision Tree alqoritmlərində də Ridge və Lasso kimi cərimələndirmələr tətbiq edilmir, çünki bu alqoritm əmsallarla işləmək əvəzinə, verilənləri ağac strukturu şəklində bölür.\n",
    "   - **Overfitting-i İdarə Etmək:** Overfitting riskini azaltmaq üçün Decision Tree alqoritmində **ağacın dərinliyini məhdudlaşdırmaq** və ya **minimum nümunə sayını** təyin etmək kimi texnikalardan istifadə olunur.\n",
    "\n",
    "### **Nəticə**\n",
    "- **Ridge, Lasso və ElasticNet** xətti modellərə aid texnikalardır və onlar SVM-də müəyyən formada tətbiq oluna bilər.\n",
    "- **KNN** və **Decision Tree** kimi qeyri-xətti modellərdə isə bu texnikalar tətbiq edilmir, çünki bu alqoritmlər əmsallarla deyil, fərqli prinsiplərlə işləyirlər.\n",
    "\n",
    "Bu cərimələndirmə metodlarının işlədiyi modelləri və digər modellərdə alternativ üsulları başa düşmək maşın öyrənməsində doğru texnikaları seçmək üçün vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3321d294",
   "metadata": {},
   "source": [
    "## Bes Random Forest-de Ridge, Lasso ve Elasticnet isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e03f2f",
   "metadata": {},
   "source": [
    "**Random Forest** alqoritmində **Ridge**, **Lasso** və **ElasticNet** kimi cərimələndirmə üsulları birbaşa tətbiq olunmur. Bunun səbəbi Random Forest-in təməlində **Decision Tree** alqoritmi dayanmasıdır və Decision Tree əmsallarla işləmək əvəzinə, verilənləri qaydalara görə bölən bir alqoritmdir.\n",
    "\n",
    "### **Niyə Random Forest-də Ridge, Lasso və ElasticNet İşləmir?**\n",
    "\n",
    "1. **Decision Tree ilə İşləmə Prinsipi:**\n",
    "   - Random Forest bir çox decision tree-dən ibarət olan ansambl metodudur. Hər bir decision tree verilənlər dəstini qaydalara görə bölərək təsnifat və ya reqressiya işləri görür.\n",
    "   - Decision Tree-lər verilənləri əmsallarla təhlil etmədiyi üçün, Ridge, Lasso və ElasticNet cərimələri ilə işləmirlər. Bu cərimələr xətti modellərdə əmsalları tənzimləmək üçün istifadə olunur, lakin Decision Tree-lərdə belə bir əmsal yoxdur.\n",
    "\n",
    "2. **Overfitting-lə Mübarizə:**\n",
    "   - Random Forest alqoritmi decision tree-lərin təbii overfitting meylini azaldır. Bir çox ağacın nəticələrini birləşdirərək (və fərqli verilənlər dəstləri və xüsusiyyətlər üzərində hər bir ağacı təlim etdirərək), Random Forest daha sabit və ümumiləşdirilmiş nəticələr verir.\n",
    "   - Random Forest modelində overfitting-in qarşısını almaq üçün ağacın dərinliyini məhdudlaşdırmaq, minimum nümunə sayını təyin etmək və ağaclar arasında müxtəliflik yaratmaq kimi metodlardan istifadə olunur. Bu üsullar Ridge, Lasso və ElasticNet cərimələndirmələrinin funksiyasını əvəz edir.\n",
    "\n",
    "### **Random Forest üçün Alternativlər:**\n",
    "Ridge, Lasso və ElasticNet kimi cərimələndirmələr xətti modellərdə xüsusiyyətlərin seçimi və modelin sadələşdirilməsi üçün istifadə olunur. Random Forest-də isə xüsusiyyətlərin seçimi avtomatik şəkildə edilir, çünki hər ağac yalnız bir hissə xüsusiyyəti istifadə edir. \n",
    "\n",
    "Əgər xüsusiyyət seçimi və ya əmsallar üzərində müəyyən bir cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərdən istifadə etməlisiniz, məsələn:\n",
    "\n",
    "- **Linear Regression** üçün Ridge, Lasso və ElasticNet.\n",
    "- **Logistic Regression** üçün L1, L2 və ElasticNet.\n",
    "\n",
    "Random Forest-də belə bir ehtiyac yoxdur, çünki o, ansambl metodlarının təbiəti ilə artıq overfitting-ə qarşı dayanır və xüsusiyyət seçimi prosesini təbii olaraq həyata keçirir.\n",
    "\n",
    "### **Nəticə:**\n",
    "Random Forest-də Ridge, Lasso və ElasticNet kimi cərimələndirmələr işləməz, çünki bu cərimələndirmələr xətti modellər üçün nəzərdə tutulmuşdur və Random Forest-in iş prinsipinə uyğun gəlmir. Əgər modeldə xüsusiyyət seçimi və cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərə baxmaq daha uyğun olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d58a37",
   "metadata": {},
   "source": [
    "## Voting, Bagging və Boosting  Predict qaydasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9ca1f",
   "metadata": {},
   "source": [
    "Voting, Bagging və Boosting modellərinin hər biri ansambl (ensemble) metodlarıdır, yəni bir neçə modeli birləşdirərək nəticələri daha dəqiq etmək üçün istifadə edilirlər. Hər birinin fərqli işləmə qaydası var.\n",
    "\n",
    "### 1. **Voting (Səsvermə)**\n",
    "Voting metodu, bir neçə fərqli modelin nəticələrini birləşdirir və yekun nəticəni ya səs çoxluğuna (classification), ya da orta (regression) əsasında verir. Voting metodunda iki əsas növ var:\n",
    "\n",
    "- **Hard Voting** (Sərt Səsvermə): Hər model öz proqnozunu verir və ən çox təkrarlanan (əksər səs alan) nəticə yekun qərar olur. Məsələn, əgər 5 modelin 3-ü \"1\" deyirsə, o zaman yekun nəticə \"1\" olacaq.\n",
    "  \n",
    "- **Soft Voting** (Yumşaq Səsvermə): Hər model öz proqnoz ehtimalını verir və ehtimalların ortalaması hesablanır. Bu metodda hər bir proqnozun ehtimalı nəzərə alınır, yəni hər modelin verdiyi ehtimallar toplanır və ortalama ehtimallar hesablanır. Yekun qərar, bu ehtimalların ən yüksək olduğu sinif olur.\n",
    "\n",
    "**Voting modeli:**\n",
    "- Eyni və ya fərqli modellərdən istifadə edilə bilər.\n",
    "- **Classification** üçün həm \"hard\", həm də \"soft\" voting seçilə bilər.\n",
    "- **Regression** üçün modellərin proqnozlarının ortalaması götürülür.\n",
    "\n",
    "### 2. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging metodunda (məsələn, Random Forest), bir neçə model eyni tipli olur (ən çox Decision Tree istifadə olunur) və hər biri dataset-in fərqli subsample-larında (random seçilmiş hissələrində) öyrədilir. Bagging metodunun əsas məqsədi modeli daha stabil etməkdir.\n",
    "\n",
    "- **Classification** üçün: Hər bir modelin sinif proqnozu verilir və yekun proqnoz ən çox səs alan sinif olur (hard voting kimi). Məsələn, Random Forest-də hər ağacın verdiyi proqnozlar səs çoxluğu ilə seçilir.\n",
    "  \n",
    "- **Regression** üçün: Hər bir modelin çıxış nəticəsi toplanır və onların ortalaması götürülür.\n",
    "\n",
    "**Bagging modeli:**\n",
    "- Hər bir model dataset-in random hissələrində öyrədilir.\n",
    "- **Classification** üçün ən çox səs alan nəticə seçilir (ən çox təkrarlanan sinif).\n",
    "- **Regression** üçün orta nəticə götürülür (proqnozların ortalaması).\n",
    "\n",
    "### 3. **Boosting**\n",
    "Boosting metodunda modellər sıralı şəkildə işləyir və hər bir model əvvəlki modelin səhvlərini düzəltməyə çalışır. Yəni hər növbəti model, əvvəlki modelin yanlış proqnoz verdiyi nümunələrə daha çox fokuslanır.\n",
    "\n",
    "- **Classification** üçün: Hər bir modelin sinif proqnozları verilir və yekun nəticə, modellərin çəkiləri əsasında müəyyənləşdirilir (ən çox ağırlıq verdiyi sinif qalib olur). Məsələn, AdaBoost metodunda hər bir ağaca müəyyən çəki verilir və daha yaxşı nəticə verən modellər daha çox ağırlıq alır.\n",
    "  \n",
    "- **Regression** üçün: Hər bir modelin proqnoz nəticələri əvvəlki modellərin səhvlərini azaltmaq üçün düzəldilir. Yekun nəticə modellərin birləşmiş nəticələrinə əsaslanır.\n",
    "\n",
    "**Boosting modeli:**\n",
    "- Modellər bir-birini izləyir, hər biri əvvəlkilərin səhvlərini düzəltməyə çalışır.\n",
    "- **Classification** üçün çəkiləri çox olan modellərin proqnozları üstünlük təşkil edir.\n",
    "- **Regression** üçün modellər ardıcıl olaraq səhvləri düzəltməyə çalışır və yekun proqnoz modellərin birləşmiş nəticəsidir.\n",
    "\n",
    "### Nəticə:\n",
    "- **Voting**: Bir neçə modelin nəticələri birləşdirilir və səs çoxluğu və ya ehtimal ortalamasına əsaslanan yekun qərar verilir.\n",
    "- **Bagging**: Modellər dataset-in random hissələrində təkrar təlim keçirlər və nəticələr ortalanır (classification üçün ən çox səs).\n",
    "- **Boosting**: Modellər ardıcıl şəkildə işləyir və hər növbəti model əvvəlki modelin səhvlərini düzəldir, yekun nəticə çəkilərə əsasən hesablanır.\n",
    "\n",
    "Hər bir metodun məqsədi nəticələrin dəqiqliyini artırmaq və overfitting-in qarşısını almaqdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a9ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a911b4c3",
   "metadata": {},
   "source": [
    "## Types of Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3c35f",
   "metadata": {},
   "source": [
    "Ensemble learning metodları, bir neçə zəif modelin birləşdirilərək daha güclü və sabit nəticələr əldə edilməsinə imkan verir. Bu metodlar fərqli modelləri və ya eyni modelin fərqli versiyalarını istifadə edərək nəticələrin ortalamasını alır və ya səs çoxluğunu təyin edir. Aşağıda Ensemble learning-in əsas növləri haqqında geniş məlumat verilir:\n",
    "\n",
    "### 1. **Voting**\n",
    "Voting, müxtəlif modellərin proqnozlarının birləşdirilməsi üsuludur. Burada əsas məqsəd bir neçə fərqli modelin proqnozlarını birləşdirərək daha dəqiq nəticə əldə etməkdir.\n",
    "\n",
    "- **Hard Voting**: Hər modelin proqnozları nəzərə alınır və ən çox səs verilən sinif seçilir. Yəni, hansı sinif daha çox model tərəfindən seçilirsə, o sinif yekun proqnoz kimi təyin edilir.\n",
    "- **Soft Voting**: Hər modelin ehtimal proqnozları nəzərə alınır və ortalama ehtimallar əsasında yekun proqnoz verilir.\n",
    "\n",
    "### 2. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging, Bootstrap metodunu istifadə edərək təkrarlı olaraq verilənlərdən müxtəlif alt dəstələr seçir və hər bir alt dəstədə fərqli modellər qurur. Bu metod əsasən yüksək dəyişkənliyə malik modellərin stabilləşdirilməsi üçün istifadə edilir.\n",
    "\n",
    "- **Random Forest**: Bagging-in ən məşhur tətbiqi olan Random Forest, müxtəlif decision tree-lərdən ibarət bir ansambl yaradır. Hər bir decision tree fərqli verilənlər dəstəsində qurulur və nəticələr səs çoxluğuna əsasən təyin edilir.\n",
    "\n",
    "### 3. **Boosting**\n",
    "Boosting, ardıcıl olaraq zəif modellər qurur və hər yeni model əvvəlki modelin düzgün proqnoz verə bilmədiyi nümunələri düzəltməyə çalışır. Bu metod zəif modelləri güclü modelə çevirmək üçün nəzərdə tutulub.\n",
    "\n",
    "- **AdaBoost**: AdaBoost, hər modelin düzgün və ya səhv proqnozlarına görə ağırlıqlarını dəyişdirərək zəif modelləri ardıcıl qurur. Səhv proqnozlar daha çox ağırlıq alır, doğru proqnozlar isə daha az.\n",
    "- **Gradient Boosting**: Gradient Boosting, qalıqların üzərində işləyən modellər yaradır. Hər yeni model əvvəlki modelin səhvlərini düzəltməyə çalışır.\n",
    "- **XGBoost**: Gradient Boosting-in optimallaşdırılmış versiyasıdır. Hesablama performansını artırmaq və overfitting-i azaltmaq üçün müxtəlif optimallaşdırmalar təqdim edir.\n",
    "- **CatBoost**: KatBoost, əsasən kateqorik verilənlərlə işləmək üçün optimallaşdırılmış bir Boosting metodudur. Bu metod xüsusilə kateqorik dəyişənlərin avtomatik olaraq kodlaşdırılmasını və optimal performans təmin etməsini asanlaşdırır.\n",
    "- **LightGBM**: LightGBM, Gradient Boosting-in daha yüngül və sürətli bir variantıdır. Bu metod böyük verilənlər dəstələri üzərində effektiv şəkildə işləyir və hesablama müddətini əhəmiyyətli dərəcədə azaldır.\n",
    "\n",
    "### 4. **Model Stacking**\n",
    "Model Stacking, fərqli modellərin proqnozlarını birləşdirərək ikinci səviyyəli bir model qurur. Bu ikinci səviyyəli model ilkin modellərin proqnozlarına əsaslanaraq yekun proqnoz verir.\n",
    "\n",
    "- **Base Models**: İlk səviyyədə bir neçə müxtəlif model istifadə olunur və hər biri verilənlər dəstəsində təlim olunur.\n",
    "- **Meta-Model**: İkinci səviyyədə, ilkin modellərin proqnozlarını input olaraq istifadə edən bir meta-model qurulur. Bu meta-model yekun proqnozu təyin edir.\n",
    "\n",
    "### Nəticə\n",
    "Bu metodlar Ensemble learning-in müxtəlif tətbiq üsullarıdır və hər biri fərqli üstünlüklərə malikdir. Bagging yüksək dəyişkənlikli modelləri stabilləşdirmək üçün istifadə edilir, Boosting isə zəif modelləri güclü modellərə çevirmək üçün. Voting sadə birləşdirmə üsuludur, Stacking isə daha kompleks və güclü ansambl modellər yaratmağa imkan verir.\n",
    "\n",
    "CatBoost və LightGBM kimi spesifik alqoritmlər isə Boosting texnikalarının optimallaşdırılmış formalarını təklif edirlər və müxtəlif problem sahələrində yüksək performans təmin edə bilirlər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ab1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e37dec7",
   "metadata": {},
   "source": [
    "## 3. Boosting modellerin daha etrafli izahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869bd18",
   "metadata": {},
   "source": [
    "Boosting metodları maşın öyrənməsində güclü və effektiv modellər qurmaq üçün geniş istifadə olunur. AdaBoost, Gradient Boosting, XGBoost, CatBoost və LightGBM kimi müxtəlif Boosting alqoritmləri mövcuddur. Hər birinin özünəməxsus xüsusiyyətləri, üstünlükləri və məhdudiyyətləri vardır. Aşağıda bu modellərin detallı fərqlərini və hansı vəziyyətdə hansı modelin daha uyğun olacağını izah edirəm.\n",
    "\n",
    "## 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni zəif modelin əvvəlki modellərin səhvlərinə fokuslanaraq qurulmasını təmin edir. Yəni, səhv proqnoz verilən nümunələrə daha çox ağırlıq verilir.\n",
    "- **Ağırlıq Yeniləmə:** Hər iterasiyada nümunələrin ağırlıqları yenilənir. Səhv proqnoz verilən nümunələrin ağırlığı artırılır, doğru proqnoz verilənlərin isə azaldılır.\n",
    "- **Zəif Modellər:** Adətən, sadə qərar ağacları (decision stumps) kimi zəif modellər istifadə olunur.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sadə və asan tətbiq edilə biləndir.\n",
    "- Overfitting riskini azaldır.\n",
    "- Sürətli təlim prosesinə malikdir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Məsələn, çoxlu sayda outliers olan verilənlərdə performansı zəif ola bilər.\n",
    "- Kompleks verilənlər üçün optimal olmaya bilər.\n",
    "\n",
    "## 2. Gradient Boosting\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** Gradient Boosting, hər yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışdığı iterativ bir prosesdir.\n",
    "- **Qalıq Əsasında Təkmilləşdirmə:** Hər iterasiyada əvvəlki modellərin səhvləri üzərində işləyərək ümumi dəqiqliyi artırır.\n",
    "- **Loss Funksiyaları:** Müxtəlif itki funksiyalarını dəstəkləyir, bu da onu müxtəlif problemlərə uyğunlaşdırır.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Yüksək dəqiqlik və performans.\n",
    "- Çeşidli problemlərə tətbiq oluna bilir (regressiya, təsnifat və s.).\n",
    "- Parametrlərin geniş konfiqurasiyası ilə optimallaşdırma imkanı.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Hesablama baxımından bahalı ola bilər.\n",
    "- Overfitting riski yüksəkdir, xüsusilə çox sayda iterasiya ilə.\n",
    "\n",
    "## 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Optimallaşdırılmış Gradient Boosting:** Hesablama sürətini və yaddaş istifadəsini optimallaşdırır.\n",
    "- **Regularizasiya:** L1 və L2 regularizasiyası vasitəsilə overfitting-i azaldır.\n",
    "- **Paralel Təlim:** Paralel hesablama imkanları ilə təlim prosesini sürətləndirir.\n",
    "- **Tree Pruning:** Daha effektiv ağac kəsimi alqoritmləri istifadə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sürətli və effektiv, böyük verilənlər üzərində yaxşı işləyir.\n",
    "- Yüksək performans və dəqiqlik.\n",
    "- Çoxlu sayda parametr tuning imkanları.\n",
    "- Kaggle kimi yarışmalarda geniş istifadə olunur.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Parametr tuning mürəkkəb ola bilər.\n",
    "- Daha çox resurs tələb edir.\n",
    "\n",
    "## 4. CatBoost\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Kateqorik Dəyişənlərin Avtomatik Emalı:** Kateqorik verilənləri avtomatik olaraq kodlaşdırır, əlavə ön emal tələb etmir.\n",
    "- **Order Boosting:** Overfitting-i azaltmaq üçün xüsusi təlim strategiyası istifadə edir.\n",
    "- **Default Parametrlər:** Yaxşı performans üçün az parametr tuning tələb edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Kateqorik verilənlərlə asan işləyir.\n",
    "- Overfitting riski aşağıdır.\n",
    "- Performansı yüksək, xüsusən kateqorik xüsusiyyətlər çox olan verilənlər üçün.\n",
    "- Minimal ön emal tələb edir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Bəzi hallarda digər modellərə nisbətən daha yavaş ola bilər.\n",
    "- Bəzi platformalarda performans fərqləri ola bilər.\n",
    "\n",
    "## 5. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Sürətli Təlim:** Gradient Boosting metodunu daha sürətli və yüngül etmək üçün optimallaşdırılmışdır.\n",
    "- **Histogram-based:** Histogram əsasında təlim alqoritmi daha az yaddaş istifadə edir və sürətli təlim təmin edir.\n",
    "- **Leaf-wise Tree Growth:** Dərin ağaclar quraraq daha yaxşı dəqiqlik əldə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Böyük verilənlər dəstləri üçün idealdır.\n",
    "- Yüksək sürət və effektivlik.\n",
    "- Daha az yaddaş tələb edir.\n",
    "- Yaxşı performans və dəqiqlik.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Overfitting riski, xüsusilə dərin ağaclar istifadə edildikdə.\n",
    "- Kateqorik xüsusiyyətlər üçün əlavə ön emal tələb edə bilər (CatBoost-dan fərqli olaraq).\n",
    "\n",
    "## Hansını Necə Tətbiq Etmək?\n",
    "\n",
    "### **AdaBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Sadə problemlər və kiçik verilənlər üçün idealdır.\n",
    "- **Misal:** Klassik təsnifat problemləri, məsələn, spam filtrləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riski azaldılmışdır, lakin çoxlu outliers olan verilənlər üçün uyğun deyil.\n",
    "\n",
    "### **Gradient Boosting:**\n",
    "- **Uyğun Vəziyyətlər:** Yüksək dəqiqlik tələb edən müxtəlif problemlər üçün geniş istifadə olunur.\n",
    "- **Misal:** Proqnozlaşdırma modelləri, satış təhlili.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Optimal parametr tuning ilə yüksək performans əldə etmək mümkündür, amma hesablama baxımından bahalı ola bilər.\n",
    "\n",
    "### **XGBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Böyük verilənlər və yüksək performans tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Kaggle yarışmaları, böyük biznes məlumat analizi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Parametr tuning mürəkkəb ola bilər, amma yüksək performans təmin edir.\n",
    "\n",
    "### **CatBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Kateqorik xüsusiyyətlər çox olan verilənlər üçün ən yaxşı seçimdir.\n",
    "- **Misal:** Marketinq analizi, müştəri seqmentləşdirməsi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Minimal ön emal tələb edir və kateqorik verilənlərlə asan işləyir.\n",
    "\n",
    "### **LightGBM:**\n",
    "- **Uyğun Vəziyyətlər:** Çox böyük verilənlər və yüksək sürət tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Real-time təhlil sistemləri, böyük verilənlər dəstləri ilə işləyən biznes tətbiqləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riskinə diqqət edilməli, xüsusən dərin ağaclar istifadə edildikdə.\n",
    "\n",
    "## Nəticə\n",
    "\n",
    "Hansı Boosting metodunu seçməyiniz layihənizin tələblərinə və verilənlərinizin xüsusiyyətlərinə bağlıdır. Aşağıdakı tövsiyələrdən istifadə edə bilərsiniz:\n",
    "\n",
    "- **Kateqorik verilənlər çoxdursa:** **CatBoost** ən yaxşı seçimdir.\n",
    "- **Böyük verilənlər üzərində sürətli və effektiv performans istəyirsinizsə:** **LightGBM** uyğun olar.\n",
    "- **Optimal performans və geniş optimallaşdırma imkanları üçün:** **XGBoost** istifadə edilə bilər.\n",
    "- **Sadə və kiçik verilənlər üçün:** **AdaBoost** və ya **Gradient Boosting** istifadə edilə bilər.\n",
    "\n",
    "Əlavə olaraq, müxtəlif modelləri sınayıb onların performansını müqayisə etmək, layihəniz üçün ən uyğun olanını müəyyənləşdirməyə kömək edəcəkdir. Modellərin hər biri müxtəlif vəziyyətlərdə üstünlük göstərə bilər, buna görə də verilənlərinizin xüsusiyyətlərini və layihənizin tələblərini diqqətlə qiymətləndirmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b062ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd571f2",
   "metadata": {},
   "source": [
    "## Voting de regressor tetbiqi haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6587a",
   "metadata": {},
   "source": [
    "**Voting Regressor** də mövcuddur və bu metod, müxtəlif regresiya modellərinin nəticələrini birləşdirərək yekun proqnoz çıxarmaq üçün istifadə olunur. Voting Regressor, modellərin birləşdirilməsində fərqli strategiyalar istifadə edərək, bir neçə fərqli regressoru birləşdirir. \n",
    "\n",
    "### **Voting Regressor**\n",
    "Voting Regressor, bir neçə fərqli regressoru birləşdirərək yekun proqnoz əldə edir. Burada əsas ideya, müxtəlif modellərin proqnozlarını birləşdirərək ümumi nəticəni yaxşılaşdırmaqdır. Voting Regressor iki növ ola bilər:\n",
    "\n",
    "1. **Averaging (Orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır və onların ortalaması çıxarılır.\n",
    "   - Məsələn, üç fərqli model (Model 1, Model 2, Model 3) bir verilənlər dəstəsi üzərində tətbiq edildikdə, hər bir modelin proqnozu alınır və bu proqnozların arifmetik ortalaması nəticə kimi qəbul edilir.\n",
    "\n",
    "2. **Weighted Averaging (Çəkili orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır, lakin hər bir modelin proqnozuna müəyyən bir çəki verilir. Daha güclü performansa sahib modellərə daha çox çəki verilir.\n",
    "   - Bu metod, daha yaxşı performans göstərən modellərin nəticələrə daha çox təsir etməsini təmin edir.\n",
    "\n",
    "### **Voting Regressor-un üstünlükləri**\n",
    "- **Model müxtəlifliyi**: Fərqli modellərin birləşdirilməsi ilə, hər bir modelin zəif cəhətləri digər modellər tərəfindən kompensasiya edilə bilər.\n",
    "- **Təkmilləşdirilmiş performans**: Model müxtəlifliyi nəticəsində Voting Regressor daha sabit və dəqiq nəticələr təmin edə bilər.\n",
    "\n",
    "### **Nümunə: Voting Regressor**\n",
    "Tutaq ki, siz Linear Regression, Decision Tree Regressor və Random Forest Regressor modellərindən istifadə edirsiniz. Bu modellərin proqnozlarını Voting Regressor ilə birləşdirə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Ayrı-ayrı modelləri təyin edək\n",
    "model1 = LinearRegression()\n",
    "model2 = DecisionTreeRegressor()\n",
    "model3 = RandomForestRegressor()\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('lr', model1), \n",
    "    ('dt', model2), \n",
    "    ('rf', model3)\n",
    "])\n",
    "\n",
    "# Modeli təlim etdirin və proqnoz verin\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "```\n",
    "\n",
    "Bu yanaşma, modellərin hər birinin güclü tərəflərindən faydalanmağa imkan verir və beləliklə, yekun proqnoz daha dəqiq ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec88396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e038e07b",
   "metadata": {},
   "source": [
    "## bagging de regressor modelleri tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129672",
   "metadata": {},
   "source": [
    "Bəli, Bagging yalnız klassifikatorlar deyil, həm də regressor modelləri üçün tətbiq edilə bilər. Bagging metodunun əsas məqsədi, modelin təlim edildiyi verilənlər dəstələrini müxtəlif alt dəstələrə bölərək hər bir alt dəstədə fərqli modellər qurmaqdır. Bu metod, həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Bagging Regressor**\n",
    "- **Bagging Regressor**: Əsas model olaraq hər hansı bir regresor seçilə bilər, məsələn, Decision Tree Regressor, Linear Regression, və s. Bagging Regressor, müxtəlif verilənlər alt dəstələri üzərində bu əsas modelləri qurur və nəticələrin ortalamasını alaraq yekun proqnoz verir.\n",
    "  \n",
    "- **Random Forest Regressor**: Bagging-in spesifik bir tətbiqi olan Random Forest, Decision Tree Regressor-larının bir ansamblından ibarət olan bir regresor modeldir. Random Forest Regressor, verilənlər dəstəsindən təkrarən seçilmiş alt dəstələr üzərində bir neçə decision tree qurur və nəticədə bütün ağacların proqnozlarının ortalamasını alaraq yekun proqnoz verir.\n",
    "\n",
    "Bagging metodunun regressorlar üzərində istifadəsi, modelin variansını azaldır və daha stabil proqnozlar əldə etməyə kömək edir. Bu, xüsusilə decision tree kimi yüksək dəyişkənlikli modellərdə faydalıdır, çünki Bagging bu dəyişkənliyi stabilləşdirir və overfitting-in qarşısını alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe3b13a",
   "metadata": {},
   "source": [
    "## bes boosting de regressor tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845b0ce",
   "metadata": {},
   "source": [
    "Bəli, Boosting metodları da regressor modelləri üçün tətbiq oluna bilər. Boosting, əsas modelin ardıcıl olaraq qurulması ilə işləyir, burada hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu metod da həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Boosting Regressor**\n",
    "- **AdaBoost Regressor**: AdaBoost metodunun regresiya üçün tətbiqi. Burada hər bir ardıcıl model əvvəlki modelin proqnoz səhvlərinə daha çox diqqət yetirir. Nəticədə, modelin performansı ardıcıl olaraq yaxşılaşır.\n",
    "\n",
    "- **Gradient Boosting Regressor**: Gradient Boosting, hər yeni modelin qalıqları (yəni əvvəlki modellərin proqnoz səhvləri) üzərində qurulması ilə işləyir. Bu metod çox populyardır və yüksək dəqiqlik təmin edir. Scikit-learn kitabxanasında Gradient Boosting Regressor olaraq tətbiq edilir.\n",
    "\n",
    "- **XGBoost, LightGBM, və CatBoost**: Bunlar Boosting-in daha inkişaf etmiş versiyalarıdır və böyük verilənlər üzərində sürətli və effektiv təlim etmə qabiliyyəti ilə tanınır. Hər üç metod da regresiya problemləri üçün tətbiq oluna bilər və xüsusilə böyük ölçülü və mürəkkəb verilənlər dəstələri üzərində çox yaxşı nəticələr verir.\n",
    "\n",
    "Boosting metodları regressor modellərində də çox uğurlu ola bilər, xüsusən də verilənlər dəstəsində qeyri-xətti əlaqələr və mürəkkəb strukturlar olduqda. Bu metodlar modelin performansını artırmaq üçün ardıcıl olaraq səhvləri azaldır və nəticədə daha yaxşı proqnozlar təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8580f2f1",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96744d1",
   "metadata": {},
   "source": [
    "Aşağıdakı cədvəldə Voting, Bagging və Boosting metodlarının əsas fərqləri göstərilib:\n",
    "\n",
    "| **Xüsusiyyət**                | **Voting**                                      | **Bagging**                                   | **Boosting**                                      |\n",
    "|-------------------------------|-------------------------------------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **İş prinsipi**               | Fərqli modellərin nəticələrini birləşdirir      | Eyni modelin bir neçə varyantını təlim edir   | Modellər ardıcıl olaraq təlim edilir, səhvləri düzəldir |\n",
    "| **Modellərin birləşdirilməsi**| Bir neçə fərqli model (ən çox istifadə edilənlər) | Eyni modelin fərqli alt nümunələr üzərində təlimi | Hər bir növbəti model əvvəlkilərin səhvlərini düzəldir |\n",
    "| **İşləmə tərzi**              | Paralel                                         | Paralel                                        | Sequential (ardıcıl)                               |\n",
    "| **Overfitting riski**         | Aşağı                                           | Aşağı                                          | Daha yüksək, amma düzgün parametrlərlə idarə edilə bilər |\n",
    "| **Çeviklik**                  | Çox müxtəlif modelləri birləşdirə bilir         | Əsasən eyni modelin təkrarlanması               | Modellər daha çox özünü tənzimləyir, çətin nümunələrə fokuslanır |\n",
    "| **Hesablama mürəkkəbliyi**    | Orta                                            | Aşağı                                          | Yüksək, çünki ardıcıl təlim tələb olunur          |\n",
    "| **Məşhur alqoritmlər**        | VotingClassifier, VotingRegressor               | Random Forest, BaggingClassifier               | AdaBoost, XGBoost, Gradient Boosting, LightGBM, CatBoost |\n",
    "| **Paralel İşləmə**            | Bəli                                            | Bəli                                           | Xeyr                                              |\n",
    "| **Stabillik**                 | Artırır, modelləri birləşdirir                  | Artırır, varyansı azaldır                      | Riskli, ancaq yaxşı idarə olunarsa dəqiqlik artır  |\n",
    "| **Tətbiq sahələri**           | Hər iki klassifikasiya və reqressiya problemləri | Hər iki klassifikasiya və reqressiya problemləri | Əsasən klassifikasiya, bəzən reqressiya            |\n",
    "\n",
    "Bu cədvəl Voting, Bagging və Boosting metodlarının əsas xüsusiyyətlərini və fərqlərini ümumi şəkildə göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167de2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60b3719",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1071",
   "metadata": {},
   "source": [
    "| **Xüsusiyyət**                      | **Voting**                                      | **Bagging**                                      | **Boosting**                                  |\n",
    "|-------------------------------------|-------------------------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **Əsas İdeya**                      | Müxtəlif modellərin nəticələrinin birləşdirilməsi | Eyni modelin müxtəlif nümunələr və xüsusiyyətlər üzərində təlim edilməsi | Modellərin ardıcıl şəkildə təlim edilməsi və səhvlərin düzəldilməsi |\n",
    "| **Model Birləşdirmə**               | Fərqli modellər (məs. SVM, DT, LR)               | Eyni modelin fərqli versiyaları (məs. bir neçə decision tree) | Hər bir model əvvəlki modelin səhvlərini düzəldir |\n",
    "| **Nəticə Birləşdirmə**              | Səs çoxluğu və ya ortalama ilə                   | Orta/median (regressiya) və ya səs çoxluğu (klassifikasiya) | Ardıcıl modellərin nəticələri birləşdirilir |\n",
    "| **Paralel/Sequential İşləmə**       | Paralel                                         | Paralel                                          | Sequential, yəni ardıcıl |\n",
    "| **Overfitting Risk**                | Orta səviyyədə                                  | Aşağı                                            | Yüksək (əgər düzgün təlim edilməzsə)        |\n",
    "| **Əsas Məqsəd**                     | Modellərin ümumi dəqiqliyini artırmaq            | Varyansı azaltmaq və stabil nəticələr əldə etmək | Səhvləri azaltmaq və dəqiqliyi artırmaq     |\n",
    "| **İstifadə Edilən Məşhur Algoritmlər** | VotingClassifier                                | Random Forest, BaggingClassifier                 | AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost |\n",
    "| **Parametr Sayı**                   | Əsasən modellərin sayına bağlıdır               | Estimator, n_estimators, max_samples, max_features, bootstrap | Learning rate, n_estimators, base_estimator |\n",
    "| **Hesablama Çətinliyi**             | Aşağı və ya orta                                 | Orta                                             | Yüksək                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5443ad3",
   "metadata": {},
   "source": [
    "### X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size= 0.3, stratify = y, random_state=42) \n",
    "* stratify nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d473ff",
   "metadata": {},
   "source": [
    "`stratify` parametri, `train_test_split` funksiyasında verilən dataset-i təlim və test dəstlərinə bölərkən, target dəyişəninin (`y`) siniflərinin nisbətini qorumaq üçün istifadə edilir. Bu, dataset-dəki siniflərin balanslı qalmasını təmin edir.\n",
    "\n",
    "### Nümunə:\n",
    "Tutaq ki, dataset-də iki sinif var: `0` və `1`. Əgər dataset-inizdə `0` sinfi 70% və `1` sinfi 30% təşkil edirsə, `stratify=y` olaraq `train_test_split` funksiyasına verildikdə, həm təlim, həm də test dəstlərində bu nisbətlər təxminən qorunacaq. Bu, modelin hər iki sinifi daha yaxşı öyrənməsinə kömək edir, xüsusilə də sinif balansı qeyri-bərabər olduqda.\n",
    "\n",
    "Əgər `stratify` parametrini verməsəniz və dataset sinifləri qeyri-bərabərdirsə, təlim və test dəstlərində siniflərin nisbəti fərqli ola bilər, bu da modelin performansına mənfi təsir göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bb581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7202b8ed",
   "metadata": {},
   "source": [
    "## confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad768cf",
   "metadata": {},
   "source": [
    "`confusion_matrix` funksiyasının Sklearn-də qaytardığı qarışıqlıq matrisi aşağıdakı formada olur:\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | True Negative (TN)         | False Positive (FP)        |\n",
    "| **Əsl Sinif 1** | False Negative (FN)        | True Positive (TP)         |\n",
    "\n",
    "### Açıklama:\n",
    "- **True Negative (TN):** Əslində 0 olan nümunələr və düzgün 0 olaraq proqnozlaşdırılanlar.\n",
    "- **False Positive (FP):** Əslində 0 olan nümunələr, amma səhvən 1 olaraq proqnozlaşdırılanlar.\n",
    "- **False Negative (FN):** Əslində 1 olan nümunələr, amma səhvən 0 olaraq proqnozlaşdırılanlar.\n",
    "- **True Positive (TP):** Əslində 1 olan nümunələr və düzgün 1 olaraq proqnozlaşdırılanlar.\n",
    "\n",
    "### Confusion Matrix-dəki Hüceyrələr\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | TN (True Negative)         | FP (False Positive)        |\n",
    "| **Əsl Sinif 1** | FN (False Negative)        | TP (True Positive)         |\n",
    "\n",
    "Bu cədvəl confusion matrix-dəki hüceyrələrin yerləşməsini göstərir. Sklearn-də `confusion_matrix` funksiyasını istifadə edərək əldə etdiyiniz matrisi bu qaydada oxuya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b4016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1d0a",
   "metadata": {},
   "source": [
    "## xgbt.score(X_train, y_train)\n",
    "* bu neyin score dur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52720c",
   "metadata": {},
   "source": [
    "`xgbt.score(X_train, y_train)` metodu, `xgbt` modelinin (`XGBClassifier` obyektinin) təlim məlumat dəstində (`X_train`, `y_train`) əldə etdiyi dəqiqliyi (`accuracy`) qaytarır.\n",
    "\n",
    "### Ətraflı:\n",
    "- **`X_train`:** Təlim üçün istifadə olunan xüsusiyyətlər (features).\n",
    "- **`y_train`:** Təlim üçün istifadə olunan hədəf dəyişəni (labels).\n",
    "- **`xgbt.score`:** Dəqiqlik hesabını qaytarır, yəni təlim dəstində proqnozların neçə faizinin düzgün olduğunu göstərir.\n",
    "\n",
    "Əgər modelin proqnozları `y_train` ilə uyğun gəlirsə, `score` metodu təlim dəstindəki ümumi düzgün proqnozların nisbətini verir. Dəqiqlik hesabı 0 ilə 1 arasında dəyişir, burada 1 tam dəqiqliyi (bütün proqnozlar doğrudur) göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce0fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7da861e",
   "metadata": {},
   "source": [
    " xgbt = XGBClassifier( max_depth = 2,\n",
    "*             learning_rate = 0.2,\n",
    "*             objective  = \"multi:softmax\",\n",
    "*             num_class = 2,\n",
    "*             booster = \"gbtree\",\n",
    "*             n_estimators = 10,\n",
    "*             random_state = 123)\n",
    "\n",
    "### Parametrler haqqinda melumet ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fef5f",
   "metadata": {},
   "source": [
    "`XGBClassifier` üçün istifadə olunan parametrler aşağıdakılardır:\n",
    "\n",
    "### **Parametrlər və Təsvirləri:**\n",
    "\n",
    "1. **`max_depth`**\n",
    "   - **Təsvir:** Hər bir decision tree-nin maksimum dərinliyi.\n",
    "   - **Qeyd:** Dərinlik artdıqca model daha kompleks olur, amma overfitting riski də artır.\n",
    "\n",
    "2. **`learning_rate` (və ya `eta`)**\n",
    "   - **Təsvir:** Hər bir tree-nin təsirini azaldır. Kiçik dəyərlər daha yaxşı nəticələr verə bilər, amma daha çox tree tələb edə bilər.\n",
    "   - **Qeyd:** Düşük learning rate modelin öyrənməsini yavaşlatır və bu, daha çox `n_estimators` tələb edir.\n",
    "\n",
    "3. **`objective`**\n",
    "   - **Təsvir:** Məqsəd funksiyasını təyin edir. `multi:softmax` çox sinifli təsnifat üçün istifadə olunur.\n",
    "   - **Qeyd:** Məqsəd funksiyası, modelin öyrənmə məqsədini müəyyən edir. `multi:softmax` birbaşa siniflərin proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "4. **`num_class`**\n",
    "   - **Təsvir:** Siniflərin sayı. `objective=\"multi:softmax\"` istifadə edilərkən təyin edilir.\n",
    "   - **Qeyd:** Bu parametr çox sinifli təsnifat problemini həll edərkən mütləq olaraq təyin edilməlidir.\n",
    "\n",
    "5. **`booster`**\n",
    "   - **Təsvir:** Hansı növ boosterlə (tree-based, linear) modelləşdiriləcəyini müəyyən edir. `gbtree`, `gblinear` və `dart` seçimləri var.\n",
    "   - **Qeyd:** `gbtree` ağac əsaslı boosterlə daha kompleks problemləri həll edə bilər, `gblinear` isə xətləri modelləşdirmək üçün istifadə olunur.\n",
    "\n",
    "6. **`n_estimators`**\n",
    "   - **Təsvir:** Modeldə istifadə olunan tree-lərin sayı.\n",
    "   - **Qeyd:** Daha çox tree əlavə edildikdə model daha yaxşı öyrənə bilər, amma daha uzun təlim vaxtı və overfitting riski ilə birlikdə gəlir.\n",
    "\n",
    "7. **`random_state`**\n",
    "   - **Təsvir:** Modelin təsadüfi işlərini təyin edən ədədi dəyər.\n",
    "   - **Qeyd:** Təyin edilmiş dəyər modelin hər dəfə eyni nəticələr verməsini təmin edir, bu da nəticələrin təkrarolunmasını asanlaşdırır.\n",
    "\n",
    "### Parametrlərin Tətbiqi\n",
    "\n",
    "Yuxarıdakı parametr dəyərləri ilə XGBoost modeli qurarkən:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbt = XGBClassifier(\n",
    "    max_depth=2,                 # Tree dərinliyi\n",
    "    learning_rate=0.2,           # Təsir dərəcəsi\n",
    "    objective=\"multi:softmax\",   # Çox sinifli təsnifat\n",
    "    num_class=2,                 # İki sinif\n",
    "    booster=\"gbtree\",            # Tree-based boosting\n",
    "    n_estimators=10,             # 10 tree\n",
    "    random_state=123             # Təkrar nəticələr üçün təsadüfi dəyər\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri öz modelinizdə tənzimləməklə, performansını artırmaq və modelin tələblərinə uyğunlaşdırmaq mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e05a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173a3fd2",
   "metadata": {},
   "source": [
    "## Regression ucun en yaxsi model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff926d",
   "metadata": {},
   "source": [
    "Regressiya problemləri üçün ən yaxşı model müxtəlif amillərə, o cümlədən verilənlərin təbiətinə, modelin interpretasiya qabiliyyətinə və performansına bağlı olaraq dəyişir. Aşağıda ən çox istifadə olunan və yüksək performans verən regressiya modellərinin qısa izahı verilmişdir:\n",
    "\n",
    "### 1. **Linear Regression**\n",
    "   - **İstifadə Sahəsi:** Sadə, xətti əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Asan tətbiq edilir və interpretasiya olunur.\n",
    "   - **Məhdudiyyətləri:** Non-linear (qeyri-xətti) əlaqələri yaxşı tuta bilmir; outliers-ə həssasdır.\n",
    "\n",
    "### 2. **Ridge və Lasso Regression**\n",
    "   - **İstifadə Sahəsi:** Yüksək ölçülü verilənlər və ya multicollinearity olan problemlər.\n",
    "   - **Üstünlükləri:** Regularization (cərimələndirmə) istifadə edərək modelin sadələşdirilməsi və overfitting-in azaldılması.\n",
    "   - **Məhdudiyyətləri:** Non-linear əlaqələr üçün optimal deyil.\n",
    "\n",
    "### 3. **Polynomial Regression**\n",
    "   - **İstifadə Sahəsi:** Non-linear əlaqələr.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Model çox kompleks ola bilər və overfitting riski yüksəkdir.\n",
    "\n",
    "### 4. **Decision Tree Regressor**\n",
    "   - **İstifadə Sahəsi:** Həm xətti, həm də xətti olmayan əlaqələr üçün istifadə olunur.\n",
    "   - **Üstünlükləri:** Asan interpretasiya olunur və outliers-lərə daha az həssasdır.\n",
    "   - **Məhdudiyyətləri:** Overfitting riski yüksəkdir, xüsusilə ağac çox dərin olduqda.\n",
    "\n",
    "### 5. **Random Forest Regressor**\n",
    "   - **İstifadə Sahəsi:** Decision Tree-dən daha stabil və dəqiq proqnozlar tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Overfitting riski azdır; yaxşı ümumi performans verir.\n",
    "   - **Məhdudiyyətləri:** Ağacların sayı çox olduqda daha çox hesablama resursu tələb edir.\n",
    "\n",
    "### 6. **Gradient Boosting Regressor**\n",
    "   - **İstifadə Sahəsi:** Yüksək dəqiqlik tələb edən non-linear əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Yüksək performans; yaxşı ümumi dəqiqlik.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və parametr tuning mürəkkəbdir.\n",
    "\n",
    "### 7. **XGBoost, LightGBM, və CatBoost Regressor**\n",
    "   - **İstifadə Sahəsi:** Böyük verilənlər, kompleks əlaqələr və yüksək dəqiqlik tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Yüksək sürət və performans; müxtəlif optimallaşdırmalar və avtomatik kateqorik xüsusiyyətlərin emalı.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning mürəkkəb ola bilər.\n",
    "\n",
    "### 8. **Support Vector Regression (SVR)**\n",
    "   - **İstifadə Sahəsi:** Xətti olmayan və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri yaxşı tuta bilir və margin-based regressiya təmin edir.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning çətin ola bilər və böyük verilənlər dəstləri üçün yavaş ola bilər.\n",
    "\n",
    "### 9. **Neural Networks (MLP Regressor)**\n",
    "   - **İstifadə Sahəsi:** Çox böyük və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan kompleks əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və interpretasiya etmək çətindir.\n",
    "\n",
    "### Hansı Modeli Seçmək?\n",
    "- **Sadə, xətti əlaqələr:** Linear Regression, Ridge və ya Lasso Regression.\n",
    "- **Xətti olmayan əlaqələr:** Polynomial Regression, Decision Tree Regressor, SVR.\n",
    "- **Yüksək performans və kompleks modellər:** Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "- **Böyük və çox ölçülü verilənlər:** LightGBM, CatBoost, Neural Networks.\n",
    "\n",
    "Verilənlərinizin xüsusiyyətlərinə uyğun olaraq modelləri sınamaq və onların performansını müqayisə etmək daha yaxşı nəticələr əldə etməyə kömək edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e6dea1",
   "metadata": {},
   "source": [
    "## Polynomial Regression model qur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26e2bc",
   "metadata": {},
   "source": [
    "Aşağıda Polynomial Regression modelini qurmaq üçün tam kod nümunəsi verilmişdir:\n",
    "\n",
    "### Adım 1: Lazımi kitabxanaların yüklənməsi\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "\n",
    "### Adım 2: Verilənlərin yüklənməsi və hazırlanması\n",
    "```python\n",
    "# Verilənləri yükləyin\n",
    "credit = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week6\\day1\\CreditCardDefault.csv\")\n",
    "credit.drop([\"ID\"], axis=1, inplace=True)\n",
    "\n",
    "# X və y dəyişənlərini təyin edin\n",
    "X = credit.iloc[:, 0:23]\n",
    "y = credit.iloc[:, -1]\n",
    "\n",
    "# Verilənləri təlim və test dəstlərinə bölün\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### Adım 3: Polynomial Features yaratmaq\n",
    "```python\n",
    "# Polynomial features yaratmaq üçün PolynomialFeatures sinifini çağırın\n",
    "degree = 2  # Polynomial dərəcəsi\n",
    "poly = PolynomialFeatures(degree)\n",
    "\n",
    "# Təlim və test dəstləri üçün polynomial xüsusiyyətlər yaradın\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "```\n",
    "\n",
    "### Adım 4: Linear Regression modeli ilə polynomial regression tətbiqi\n",
    "```python\n",
    "# Linear Regression modelini qurun\n",
    "model = LinearRegression()\n",
    "\n",
    "# Polynomial verilənlər üzərində modeli təlim edin\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Test dəsti üzərində proqnozlar edin\n",
    "y_pred = model.predict(X_test_poly)\n",
    "```\n",
    "\n",
    "### Adım 5: Model performansının qiymətləndirilməsi\n",
    "```python\n",
    "# MSE və R2 dəyərlərini hesablayın\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "```\n",
    "\n",
    "### İcra\n",
    "Bu kod parçalarını addım-addım icra etdikdən sonra Polynomial Regression modelini qurmuş olacaqsınız. Kodun sonunda modelin performansını `Mean Squared Error (MSE)` və `R-squared (R2)` dəyərləri ilə ölçə biləcəksiniz.\n",
    "\n",
    "**Qeyd:** `degree` parametrini dəyişdirərək polynomial modelin dərəcəsini tənzimləyə bilərsiniz. Çox yüksək dərəcələr overfitting-ə səbəb ola bilər, buna görə dərəcəni diqqətlə seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a4509e",
   "metadata": {},
   "source": [
    "## Modellerde threshold-u nece deyisim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2571ed",
   "metadata": {},
   "source": [
    "Threshold (məhdud) dəyərini dəyişdirmək, xüsusilə təsnifat modelləri üçün vacibdir, çünki default olaraq modellər çox vaxt 0.5 threshold ilə işləyir. Threshold dəyərini dəyişdirməklə, modelin proqnoz nəticələrinin \"positive\" və \"negative\" olaraq təsnif edilmə meyarını dəyişdirə bilərsiniz.\n",
    "\n",
    "### Adımlar\n",
    "\n",
    "1. **Model proqnozlarını ehtimallar kimi əldə edin.**\n",
    "   - Təsnifat modellərində `.predict_proba()` metodu istifadə edərək ehtimal dəyərlərini ala bilərsiniz.\n",
    "\n",
    "2. **Threshold-a əsasən proqnozları təsnif edin.**\n",
    "   - İstədiyiniz threshold dəyərinə əsasən ehtimalları `positive` və ya `negative` olaraq təsnif edəcəksiniz.\n",
    "\n",
    "### Kod nümunəsi\n",
    "\n",
    "Məsələn, bir təsnifat modeli üçün threshold-u necə dəyişdirəcəyinizi göstərən bir nümunə:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Model proqnozları ehtimal kimi alındı\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Positive class üçün ehtimal\n",
    "\n",
    "# Threshold dəyərini təyin edin\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "# Threshold-a əsasən proqnozları təsnif edin\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### İzahat:\n",
    "- **`predict_proba`:** Bu metod modelin hər bir sinif üçün ehtimallarını qaytarır. `[:, 1]` pozitiv sinifin ehtimallarını seçir.\n",
    "- **`threshold = 0.3`:** İstədiyiniz threshold dəyərini təyin edirsiniz. Burada 0.3 olaraq seçilib.\n",
    "- **`np.where(y_probs > threshold, 1, 0)`:** Ehtimalları seçilən threshold dəyərinə əsasən 1 (positive) və ya 0 (negative) olaraq təsnif edir.\n",
    "\n",
    "Bu yanaşma, threshold dəyərini dəyişdirərək modelin performansını müxtəlif threshold dəyərləri üçün optimallaşdırmağa və balanslaşdırmağa imkan verir. Bu, xüsusilə sinif disbalansı olan verilənlər dəstlərində faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e4b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb3e2f3",
   "metadata": {},
   "source": [
    "## Butun modellerde threshold-u eyni cur deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6350af",
   "metadata": {},
   "source": [
    "Bəli, threshold dəyərini dəyişdirmə üsulu demək olar ki, bütün təsnifat modellərində tətbiq edilə bilər. Lakin bu proses yalnız ehtimal əsaslı təsnifat modelləri üçün keçərlidir. Aşağıda bəzi ümumi modellər üçün bu yanaşmanı necə tətbiq etmək barədə izahatlar verilmişdir:\n",
    "\n",
    "### 1. **Logistic Regression, Random Forest, Gradient Boosting və s.**  \n",
    "Bu modellər ehtimal proqnozları qaytarır və threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 2. **Support Vector Machine (SVM)**\n",
    "SVM modelləri default olaraq ehtimal vermir, lakin `probability=True` olaraq modeli qurmaqla ehtimal əsaslı proqnozlar əldə edə bilərsiniz. Sonra threshold-u dəyişdirmək mümkündür.\n",
    "\n",
    "### 3. **Neural Networks (MLPClassifier)**\n",
    "Sinif ehtimalları `.predict_proba()` metodu ilə əldə edilir və threshold-a əsasən dəyişdirilə bilər.\n",
    "\n",
    "### 4. **Naive Bayes**\n",
    "Naive Bayes modelləri də ehtimallar qaytarır, buna görə threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 5. **K-Nearest Neighbors (KNN)**\n",
    "KNN də `.predict_proba()` metodu ilə ehtimallar qaytarır və bu ehtimallara əsasən threshold dəyişdirilə bilər.\n",
    "\n",
    "### Tətbiq etmək üçün Ümumi Yanaşma:\n",
    "```python\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Pozitiv sinif ehtimalları\n",
    "\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### Qeyd:\n",
    "- **Decision Tree** kimi bəzi modellərdə ehtimal dəyərləri, leaf düyünlərinin təxminləri ilə hesablanır və `.predict_proba()` metodu vasitəsilə əldə edilə bilər.\n",
    "- **Threshold dəyərini dəyişdirmək**, modelin təsnifat balansını dəyişdirməyə imkan verir, bu da sinif disbalansı olan verilənlərdə xüsusilə faydalıdır.\n",
    "\n",
    "Beləliklə, ehtimal əsaslı təsnifat modellərində threshold dəyərini dəyişdirmək mümkün və çox vaxt faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a600c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6ead7e",
   "metadata": {},
   "source": [
    "## Boosting modellerinde, meselen XGBoost goturek. Bu modeli tetbiq ederken ozu arxa plnada hansi modeller qurur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab38e",
   "metadata": {},
   "source": [
    "Boosting modellərində, məsələn, XGBoost istifadə edərkən, modelin arxa planda qurduğu zəif modelləri (base learners) başa düşmək vacibdir. Boosting metodları ardıcıl olaraq zəif modelləri birləşdirərək daha güclü bir model qurur. XGBoost, Gradient Boosting metodunun optimallaşdırılmış bir versiyasıdır və bu arxa planda aşağıdakı addımları həyata keçirir:\n",
    "\n",
    "### XGBoost-un Arxa Planında Qurulan Modellər\n",
    "\n",
    "1. **Zəif Modellərin (Base Learners) Qurulması:**\n",
    "   - **Decision Trees:** XGBoost, ən çox Decision Tree-lərdən istifadə edir. Bu ağaclar, çox vaxt \"stumps\" olaraq da adlandırılan çox sadə qərar ağaclarıdır. XGBoost-da bu ağaclar çox vaxt qısa və çox dərin olmur; bu, hər bir ağacın çox sadə olduğunu bildirir.\n",
    "\n",
    "2. **Ardıcıl Tətbiq:**\n",
    "   - **Gradient Descent:** XGBoost, hər yeni ağacı əvvəlki modellərin səhvlərini düzəltmək üçün qurur. Bu, gradient descent optimallaşdırma metodu vasitəsilə həyata keçirilir. Hər bir ağacın yaradılması, əvvəlki ağacların səhvlərinə (residuals) əsaslanır.\n",
    "\n",
    "3. **Öyrənmə Sürəti (Learning Rate):**\n",
    "   - **Learning Rate:** Hər yeni ağacın təsirini tənzimləyən bir öyrənmə sürəti (learning rate) istifadə edilir. Bu, hər bir ağacın proqnozlarına əlavə ediləcək ağırlığı müəyyən edir. Daha kiçik öyrənmə sürəti ilə daha çox ağac tələb olunur.\n",
    "\n",
    "4. **Regularization:**\n",
    "   - **Regularization:** XGBoost-da modelin overfitting riskini azaltmaq üçün cərimələndirmə (regularization) tətbiq edilir. Bu, ağacların böyüməsini tənzimləmək və modelin kompleksliyini azaltmaq məqsədini güdür. XGBoost L1 (Lasso) və L2 (Ridge) cərimələndirmə üsullarını dəstəkləyir.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **Early Stopping:** XGBoost, modelin performansını qiymətləndirmək üçün validasiya dəstini istifadə edir və performansın yaxşılaşmadığı yerə qədər ağacları qurmağa davam edir. Bu, overfitting-i azaltmağa kömək edir.\n",
    "\n",
    "### XGBoost-un İş Prinsipi:\n",
    "\n",
    "1. **Başlanğıc Model:** Əvvəlcə bütün verilənlər üçün proqnozları təsadüfi bir başlanğıc dəyəri ilə başlatır.\n",
    "2. **Residual Hesablanması:** Təxmin edilmiş dəyərlərlə real dəyərlər arasındakı fərqi hesablamaqla qalıqları (residuals) tapır.\n",
    "3. **Yeni Ağacın Qurulması:** Qalıqları (residuals) düzəltmək üçün yeni bir Decision Tree qurur. Bu ağac qalıqların düzəldilməsinə kömək edir.\n",
    "4. **Proqnozlara Əlavə:** Yeni ağacın təxminləri əvvəlki proqnozlara əlavə edilir.\n",
    "5. **Yenidən Hesablamalar:** Bu proses təkrar olunur, hər dəfə yeni ağac əvvəlki ağacların səhvlərini düzəldir.\n",
    "\n",
    "Bu üsulla, XGBoost bir neçə zəif modelin (Decision Trees) ardıcıl olaraq birləşdirilməsi ilə güclü və dəqiq bir model qurur. Bu yanaşma, modelin öyrənmə qabiliyyətini artırır və daha yaxşı nəticələr əldə etməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdcfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a3e09c",
   "metadata": {},
   "source": [
    "## Boosting modelleri arxa planda hansi modelleri qurur. Bir bir hamisi haqqinda yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c83a6",
   "metadata": {},
   "source": [
    "Boosting modelləri arxa planda zəif modellər (base learners) qurur. Ən çox istifadə olunan boosting modelləri bunlardır: AdaBoost, Gradient Boosting, XGBoost, LightGBM, və CatBoost. Aşağıda hər birinin arxa planda qurduğu modellərin xüsusiyyətlərini izah edirəm:\n",
    "\n",
    "### 1. **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** Ən çox **Decision Trees** (adətən \"stumps\" adlanan çox sadə ağaclar) istifadə edilir. Hər bir ağac, əvvəlki ağacların səhvlərinə daha çox diqqət yetirir.\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni ağacın əvvəlki ağacların səhvlərini düzəltməyə çalışır. Səhv təsnif edilən nümunələrə daha çox ağırlıq verilir, beləliklə, yeni ağaclar bu nümunələrə daha çox diqqət yetirir.\n",
    "\n",
    "### 2. **Gradient Boosting**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** istifadə edilir. Bu ağaclar adətən daha dərin və daha kompleks ola bilər, lakin hələ də çox sadə modellərdir.\n",
    "- **İş Prinsipi:** Hər yeni ağac, əvvəlki ağacların qalıqları üzərində işləyərək səhvləri düzəltməyə çalışır. Hər ağac gradient descent metodunu istifadə edərək əvvəlki ağacların səhvlərini azaldır.\n",
    "\n",
    "### 3. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha kiçik və daha sadə).\n",
    "- **İş Prinsipi:** XGBoost, Gradient Boosting metodunun optimallaşdırılmış versiyasıdır. Bu metod eyni zamanda L1 (Lasso) və L2 (Ridge) cərimələndirmə tətbiq edir, və ağacların böyüməsini tənzimləyərək overfitting-i azaldır. Hər ağac əvvəlki ağacların səhvlərinə əsaslanaraq yaradılır və öyrənmə sürəti (learning rate) ilə tənzimlənir.\n",
    "\n",
    "### 4. **LightGBM (Light Gradient Boosting Machine)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha yüngül və daha sürətli öyrənən ağaclar).\n",
    "- **İş Prinsipi:** LightGBM, Gradient Boosting-in daha sürətli və daha az resurs tələb edən variantıdır. Ağacları \"leaf-wise\" bölmələrdən istifadə edərək qurur, bu da daha dərin və daha təkmil ağaclara gətirib çıxarır. Bu yanaşma, daha sürətli və daha yaxşı performans verir, xüsusilə böyük verilənlər dəstələrində.\n",
    "\n",
    "### 5. **CatBoost (Categorical Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (daha optimallaşdırılmış və kateqorik dəyişənləri avtomatik emal edən ağaclar).\n",
    "- **İş Prinsipi:** CatBoost, Gradient Boosting-in xüsusilə kateqorik verilənlərlə işləməyə optimallaşdırılmış versiyasıdır. Bu model kateqorik dəyişənlərin avtomatik kodlaşdırılmasını təmin edir və şumun qarşısını almaq üçün müxtəlif texnikalardan istifadə edir. Ağaclar adətən daha qısa və effektiv olur, modelin öyrənmə sürəti yüksəkdir.\n",
    "\n",
    "### Ümumi Yanaşma:\n",
    "Bütün bu boosting modelləri, zəif modelləri (adətən Decision Trees) bir-birinin ardınca quraraq, əvvəlki modellərin səhvlərini düzəltməyə çalışır. Hər yeni model əvvəlki modellərin qalıqları üzərində işləyir və modelin ümumi performansını artırır.\n",
    "\n",
    "Bu yanaşmaların hər biri, müxtəlif optimallaşdırma və cərimələndirmə üsulları ilə modelin performansını artırmağa və overfitting-i azaltmağa çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5b5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24406bd5",
   "metadata": {},
   "source": [
    "## Bagging-de eyni model qurulur? Boosting-de eyni model qurulur yoxsa bir nece model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a180d",
   "metadata": {},
   "source": [
    "Bagging və Boosting metodları arasında əsas fərq ondan ibarətdir ki, hər biri modelləri necə qurur və birləşdirir.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** metodunda bir neçə eyni model (həmin növ model) qurulur, lakin bu modellər fərqli verilənlər dəstləri üzərində öyrədilir. \n",
    "\n",
    "- **Eyni Model:** Bəli, **bagging** metodunda eyni növ modeldən bir neçə dəfə istifadə edilir. Məsələn, əgər bagging metodunda Decision Tree istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Verilənlər Dəstləri:** Bagging, orijinal verilənlər dəstinin təkrar nümunələri ilə yaradılan fərqli verilənlər dəstlərini (bootstrap samples) istifadə edərək hər bir modeli öyrədir.\n",
    "- **Final Nəticə:** Hər bir modelin proqnozları birləşdirilir. Təsnifat üçün bu, adətən səsvermə (majority voting) üsulu ilə, regresiyada isə ortalama (averaging) ilə həyata keçirilir.\n",
    "\n",
    "**Nəticə:** Bagging metodunda bir neçə eyni model yaradılır, lakin bu modellər fərqli verilənlər üzərində öyrədilir və final nəticə bütün modellərin nəticələrinin birləşdirilməsi ilə alınır.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Boosting** metodunda isə modellər ardıcıl olaraq qurulur, və hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu prosesdə modellərin hər biri eyni növ olsa da, onların hər biri müxtəlif verilənlər üzərində və fərqli səhvlər üzərində öyrədilir.\n",
    "\n",
    "- **Eyni Model:** Bəli, **boosting** metodunda da eyni növ model istifadə edilir. Məsələn, Gradient Boosting-də Decision Tree-lər istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Modellərin Qurulması:** Lakin burada fərq ondan ibarətdir ki, hər bir yeni model əvvəlki modellərin səhvlərini düzəltmək üçün qurulur, yəni hər model fərqli şəkildə optimallaşdırılmış bir variant kimi fəaliyyət göstərir.\n",
    "- **Final Nəticə:** Hər bir modelin nəticələri birləşdirilir, lakin bu birləşmə hər bir modelin səhvlərini düzəltmək üçün fərqli çəkilərdən istifadə edir.\n",
    "\n",
    "**Nəticə:** Boosting metodunda eyni növ model bir neçə dəfə qurulur, lakin bu modellər bir-birindən fərqlidir, çünki hər biri əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu modellər ardıcıl olaraq qurulur və nəticələr birləşdirilərək daha güclü bir model yaradılır.\n",
    "\n",
    "### Ümumi Qısa Xülasə:\n",
    "\n",
    "- **Bagging:** Eyni növ model, fərqli verilənlər dəstləri üzərində paralel olaraq öyrədilir və nəticələr birləşdirilir.\n",
    "- **Boosting:** Eyni növ model, ardıcıl olaraq qurulur və hər bir yeni model əvvəlki modellərin səhvlərini düzəldir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365bb8e2",
   "metadata": {},
   "source": [
    "## Modelin overfitting edib etmediyini nece bilim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0322b2",
   "metadata": {},
   "source": [
    "Modelin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə üsul və göstəricidən istifadə etmək mümkündür. Overfitting, modelin təlim məlumatlarına həddindən artıq uyğunlaşdığı, lakin test və ya yeni məlumatlarla yaxşı performans göstərmədiyi bir vəziyyətdir. Overfitting olub-olmadığını müəyyən etmək üçün aşağıdakı yanaşmaları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Train-Test Split və Performans Müqayisəsi**\n",
    "   - **Train Score:** Modelin təlim (train) məlumatlarındakı performansını ölçün. Yüksək dəqiqlik (accuracy) və ya digər performans metrikləri (məsələn, R² score) modelin təlim məlumatlarına çox yaxşı uyğunlaşdığını göstərə bilər.\n",
    "   - **Test Score:** Modelin test (validation) məlumatlarındakı performansını ölçün. Əgər test dəstindəki performans əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   train_score = model.score(X_train, y_train)\n",
    "   test_score = model.score(X_test, y_test)\n",
    "   \n",
    "   print(\"Train Score:\", train_score)\n",
    "   print(\"Test Score:\", test_score)\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train score çox yüksək, test score isə əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsidir.\n",
    "\n",
    "### 2. **Learning Curve**\n",
    "   - **Learning Curve:** Train və test məlumatları üzərində modelin performansını müxtəlif təlim məlumatları ilə vizuallaşdıran bir əyri çəkin. Əgər train əyrisi yuxarıda qalır və test əyrisi daha aşağıda qalırsa, bu da overfitting göstəricisidir.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import learning_curve\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "   plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "   plt.xlabel(\"Training examples\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Əgər train əyrisi yüksək performans göstərir, lakin test əyrisi daha aşağı qalırsa və aradakı fərq böyükdürsə, bu da overfitting-i göstərir.\n",
    "\n",
    "### 3. **Cross-Validation**\n",
    "   - **Cross-Validation:** Verilənlər dəstini bir neçə hissəyə bölərək modelin performansını müxtəlif hissələr üzərində yoxlayın. Əgər modelin performansı müxtəlif hissələr arasında çox dəyişirsə, bu da overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "   print(\"Cross-Validation Scores:\", scores)\n",
    "   print(\"Mean Score:\", np.mean(scores))\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Cross-validation nəticələri arasında böyük fərqlər varsa, bu, modelin müəyyən verilənlər dəstlərinə həddən artıq uyğunlaşdığını göstərə bilər.\n",
    "\n",
    "### 4. **Validation Curve**\n",
    "   - **Validation Curve:** Modelin müəyyən parametrləri üzrə performansını yoxlamaq üçün validation curve çəkə bilərsiniz. Əgər train performansı yüksəkdir, lakin validation performansı aşağıdırsa, overfitting ehtimalı var.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import validation_curve\n",
    "\n",
    "   param_range = np.logspace(-6, -1, 5)\n",
    "   train_scores, test_scores = validation_curve(model, X, y, param_name=\"alpha\", param_range=param_range, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "   plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "   plt.xlabel(\"Parameter\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train əyrisi yüksək, validation əyrisi isə daha aşağı qalırsa, overfitting ehtimalı var.\n",
    "\n",
    "### 5. **Regularization Yoxlamaq**\n",
    "   - **Regularization:** Regularization (L1, L2 cərimələndirmə) metodlarından istifadə etməklə modelin overfitting edilib-etmədiyini yoxlaya bilərsiniz. Regularization overfitting-in qarşısını almağa kömək edir.\n",
    "\n",
    "   - **Overfitting əlaməti:** Regularization tətbiq etdikdə modelin performansı daha sabit və balanslı olursa, bu, overfitting-in qarşısını almağa kömək edə bilər.\n",
    "\n",
    "### 6. **Modelin Kompleksliyi**\n",
    "   - **Modelin Kompleksliyi:** Çox kompleks modellər (çox dərin decision tree-lər, çox sayda hidden layer-ləri olan neyron şəbəkələr və s.) asanlıqla overfitting edə bilər. Sadə modellər istifadə edərək performansı müqayisə etmək də overfitting-i müəyyən etməyə kömək edə bilər.\n",
    "\n",
    "   - **Overfitting əlaməti:** Çox kompleks model çox yüksək train performansı və aşağı test performansı göstərə bilər.\n",
    "\n",
    "Bu metodlar və yanaşmalarla modelin overfitting edib-etmədiyini müəyyən edə bilərsiniz və ehtiyac olduqda daha yaxşı performans əldə etmək üçün modelin quruluşunu və parametrlərini tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c0dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a3f95",
   "metadata": {},
   "source": [
    "## Bu overfitting ucun kifayetdir?\n",
    "* Cross-Validation Scores: [0.81, 0.76, 0.82, 0.77, 0.79]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33d3bb",
   "metadata": {},
   "source": [
    "Verilən cross-validation nəticələrinə əsasən, modellərinizin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə göstəriciyə diqqət yetirmək lazımdır:\n",
    "\n",
    "1. **Dəyişkənlik (Variance):** Cross-validation skorları arasında çox böyük bir fərq yoxdursa, bu yaxşı bir göstəricidir. Verilən skorlar 0.76 ilə 0.82 arasında dəyişir, bu isə bir qədər dəyişkənlik göstərsə də, ciddi bir overfitting işarəsi deyil. \n",
    "\n",
    "2. **Test və Təlim Dəqiqlikləri (Train vs. Test Accuracy):** Əgər təlim dəstindəki dəqiqlik çox yüksək, amma test dəstindəki dəqiqlik nisbətən aşağıdırsa, bu overfitting işarəsi ola bilər. Siz bu məlumatı paylaşmamısınız, amma əgər test və train dəqiqlikləri arasında böyük bir fərq varsa, overfitting ehtimalı daha yüksəkdir.\n",
    "\n",
    "3. **Kompromis:** Skorların orta səviyyədə olması (məsələn, 0.79) modeli qismən balanslaşdırılmış göstərə bilər. Amma, modellərinizi daha da optimallaşdırmaq və overfitting-in qarşısını almaq üçün modeli sadələşdirmək və ya daha çox məlumat toplamaq barədə düşünə bilərsiniz.\n",
    "\n",
    "**Nəticə:** Bu skorlar çox böyük bir dəyişkənlik göstərmədiyi üçün ciddi bir overfitting işarəsi deyildir. Lakin, modelin təlim və test dəstində necə performans göstərdiyini daha dərindən analiz etmək daha dəqiq bir qərar verməyə kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a92e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b126fc89",
   "metadata": {},
   "source": [
    "### train_score = model.score(X_train, y_train)\n",
    "### test_score = model.score(X_test, y_test)\n",
    "## Bunlar accuracy dir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66317e3",
   "metadata": {},
   "source": [
    "Bəli, `model.score(X_train, y_train)` və `model.score(X_test, y_test)` metodları adətən **accuracy** dəyərini qaytarır, amma bu, yalnız təsnifat (classification) modelləri üçün keçərlidir. \n",
    "\n",
    "### Təfsilatlı izah:\n",
    "- **Classification Modelləri üçün:** \n",
    "  - `model.score(X_train, y_train)` və `model.score(X_test, y_test)` təlim və test dəstlərində modelin təsnifat dəqiqliyini (accuracy) qaytarır. Bu, düzgün proqnozların ümumi nümunələrə nisbətidir.\n",
    "\n",
    "  ```python\n",
    "  accuracy = correct_predictions / total_predictions\n",
    "  ```\n",
    "\n",
    "- **Regression Modelləri üçün:** \n",
    "  - Əgər model regresiya modelidirsə (məsələn, Linear Regression, Ridge, Lasso), `model.score()` metodunun qaytardığı dəyər **R² score** olacaq. R² score, modelin verilənlər dəstinə nə qədər yaxşı uyğun gəldiyini ölçür və proqnozların faktiki dəyərlərə nə qədər yaxın olduğunu göstərir.\n",
    "\n",
    "  ```python\n",
    "  R² = 1 - (SS_res / SS_tot)\n",
    "  ```\n",
    "  Burada:\n",
    "  - `SS_res`: Proqnozlaşdırılmış dəyərlərin və faktiki dəyərlərin kvadrat fərqlərinin cəmi.\n",
    "  - `SS_tot`: Faktiki dəyərlərin və onların ortalama dəyərinin kvadrat fərqlərinin cəmi.\n",
    "\n",
    "### Nəticə:\n",
    "Əgər təsnifat modeli istifadə edirsinizsə, `model.score()` metodu **accuracy** dəyərini, əgər regresiya modeli istifadə edirsinizsə, **R² score** qaytaracaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8df425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b7828c",
   "metadata": {},
   "source": [
    "## Bank datalari ucun ve elektronika satan sirket datalari ucun en cox hansi modeller istifade olunur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ba52d",
   "metadata": {},
   "source": [
    "Bank dataları və elektronika satan şirkətlərin məlumatları üçün ən uyğun modellər seçilərkən, bu dataların xarakteristikası və məqsəd nəzərə alınır. Hər bir sektorun özünəməxsus xüsusiyyətləri var, buna görə də istifadə olunan modellər fərqlənə bilər.\n",
    "\n",
    "### Bank Dataları Üçün\n",
    "Bank sektoru əsasən müştərilərin maliyyə məlumatları, kredit riski, fırıldaqçılıq aşkar edilməsi, müştəri seqmentasiyası kimi mövzulara yönəlir. Bu səbəbdən bank məlumatları üçün geniş istifadə olunan modellər:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - Əsasən kredit riski, müştəri davranışı və fırıldaqçılıq aşkar etmək üçün istifadə olunur.\n",
    "   - Məqsəd: İkili təsnifat (binary classification) məsələləri.\n",
    "\n",
    "2. **Random Forest:**\n",
    "   - Fərqli atributlar üzərində güclü performans göstərir və feature importance üçün istifadə edilir.\n",
    "   - Məqsəd: Təsnifat və regresiya məsələləri.\n",
    "\n",
    "3. **Gradient Boosting (XGBoost, LightGBM, CatBoost):**\n",
    "   - Bank dataları üçün xüsusilə effektivdir, çünki bu metodlar mürəkkəb əlaqələri və nümunələri öyrənə bilir.\n",
    "   - Məqsəd: Kredit skoring, risk analizi, müştəri churn təxminatı.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - Daha az məlumat üçün güclü performans göstərir, lakin böyük datalar üçün hesablama baxımından bahalı ola bilər.\n",
    "   - Məqsəd: Kredit riski və fırıldaqçılıq aşkar edilməsi.\n",
    "\n",
    "5. **K-Means Clustering:**\n",
    "   - Müştəri seqmentasiyası üçün istifadə olunur.\n",
    "   - Məqsəd: Müştəri qruplarını tapmaq və onlara fərqli maliyyə məhsulları təklif etmək.\n",
    "\n",
    "### Elektronika Satan Şirkətlərin Dataları Üçün\n",
    "Elektronika satan şirkətlər əsasən satış təxminatı, müştəri davranışı, stok idarəsi və qiymət optimallaşdırması üzərində işləyirlər. Bu sahə üçün istifadə olunan əsas modellər:\n",
    "\n",
    "1. **Linear Regression:**\n",
    "   - Satış proqnozları və qiymət optimallaşdırılması üçün istifadə olunur.\n",
    "   - Məqsəd: Kəmiyyət (quantitative) proqnozlar.\n",
    "\n",
    "2. **Time Series Analysis (ARIMA, SARIMA):**\n",
    "   - Satış və tələbat proqnozları üçün zaman seriyası analizləri geniş istifadə olunur.\n",
    "   - Məqsəd: Gələcək satışların və tələbin proqnozlaşdırılması.\n",
    "\n",
    "3. **Random Forest və Gradient Boosting:**\n",
    "   - Elektronika məhsullarının satışlarına təsir edən müxtəlif faktorları təhlil etmək üçün istifadə olunur.\n",
    "   - Məqsəd: Təsnifat və proqnozlaşdırma.\n",
    "\n",
    "4. **Collaborative Filtering (Recommendation Systems):**\n",
    "   - Məhsul təklifləri və müştəriyə fərdi təkliflərin hazırlanması üçün geniş istifadə olunur.\n",
    "   - Məqsəd: Müştəriyə uyğun məhsul təklifləri vermək.\n",
    "\n",
    "5. **Clustering (K-Means, DBSCAN):**\n",
    "   - Müştəri seqmentasiyası və marketinq strategiyalarının tərtibi üçün.\n",
    "   - Məqsəd: Müştəri qruplarının aşkar edilməsi və hədəfli reklam strategiyalarının inkişafı.\n",
    "\n",
    "### Nəticə\n",
    "Bank məlumatları üçün daha çox təsnifat modelləri (logistic regression, random forest, gradient boosting) istifadə olunur. Elektronika satan şirkətlər isə satış proqnozları və tələbat analizi üçün zaman seriyaları modelləri və regresiya modellərinə üstünlük verirlər. Hər iki sahədə də clustering müştəri seqmentasiyası üçün geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46597909",
   "metadata": {},
   "source": [
    "## CatBoost-da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba7ccd",
   "metadata": {},
   "source": [
    "CatBoost modeli daxilində işləyən spesifik modelləri dəyişdirmək mümkün deyil, çünki CatBoost özü qərar ağacları (decision trees) əsasında işləyən bir **Gradient Boosting** metodudur və daxili arxitekturası istifadəçilər üçün dəyişdirilə bilən deyil. CatBoost, xüsusilə kateqorik verilənlərlə effektiv işləmək üçün optimallaşdırılmışdır və daxili olaraq xüsusi bir **symmetrical tree** strukturu istifadə edir.\n",
    "\n",
    "Bununla belə, siz CatBoost modelinin performansını və davranışını təsir edən müxtəlif hiperparametrləri tənzimləməklə modelin necə işlədiyini dəyişə bilərsiniz. Məsələn:\n",
    "\n",
    "### 1. **`depth` (ağacın dərinliyi):**\n",
    "   - Modelin hər bir ağacının maksimum dərinliyini təyin edir. Daha böyük dərinlik daha kompleks modellər və overfitting riskini artırır.\n",
    "\n",
    "### 2. **`learning_rate`:**\n",
    "   - Modelin hər bir iterasiyada addım böyüklüyünü təyin edir. Daha aşağı qiymətlər daha sabit, amma daha yavaş öyrənməyə səbəb olur.\n",
    "\n",
    "### 3. **`iterations`:**\n",
    "   - Ümumi ağac sayını müəyyən edir. Daha çox iterasiya adətən daha yüksək dəqiqlik, amma overfitting riski ilə nəticələnə bilər.\n",
    "\n",
    "### 4. **`l2_leaf_reg`:**\n",
    "   - Ağacların budaqlanmasının qarşısını almaq üçün cərimələndirmə əmsalıdır (regularization term). Daha böyük dəyər daha az kompleks modellərə və overfitting riskinin azalmasına səbəb ola bilər.\n",
    "\n",
    "### 5. **`one_hot_max_size`:**\n",
    "   - Kateqorik dəyişənlər üçün one-hot encoding tətbiq ediləcək maksimum unikal dəyər sayını təyin edir. Bu, xüsusilə çox unikal kateqorik dəyərə malik atributlar üçün faydalıdır.\n",
    "\n",
    "### 6. **`boosting_type`:**\n",
    "   - CatBoost modeli əsasən **Ordered Boosting** və **Plain Boosting** kimi iki növ boosting metodu təklif edir:\n",
    "     - **Ordered Boosting**: Overfitting-in qarşısını almaq üçün daha yaxşıdır.\n",
    "     - **Plain Boosting**: Daha sürətli öyrənməyə səbəb ola bilər, amma overfitting riskini artıra bilər.\n",
    "\n",
    "Bu hiperparametrləri dəyişərək CatBoost modelinin performansını və davranışını təyin edə bilərsiniz, amma daxildəki modeli fərqli bir model ilə əvəz etmək mümkün deyil, çünki CatBoost özü bir boosting alqoritmi kimi çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69438d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d07da7",
   "metadata": {},
   "source": [
    "## XGBoost da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e74fbc",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) modelində də spesifik olaraq işləyən \"daxili model\" anlayışı yoxdur, çünki XGBoost özü qərar ağacları (decision trees) üzərində qurulmuş bir **Gradient Boosting** metodudur. Bununla belə, siz XGBoost modelində bəzi hiperparametrləri dəyişdirərək modelin necə işlədiyini təsir edə bilərsiniz. Amma bu, istifadə olunan modelin növünü deyil, qərar ağaclarının quruluşunu və iş rejimini dəyişdirir.\n",
    "\n",
    "### XGBoost Modelində Dəyişdirilə Bilən Əsas Hiperparametrlər:\n",
    "\n",
    "1. **`booster`:**\n",
    "   - **`gbtree`**: Standart qərar ağaclarına əsaslanan boosting (ən geniş yayılmış).\n",
    "   - **`gblinear`**: Qərar ağacları əvəzinə xətti modellər (linear boosters) istifadə edilir.\n",
    "   - **`dart`**: Dropouts təklif edir ki, bu da gbtree-də olan bəzi ağacları tərk edərək overfitting-i azalda bilər.\n",
    "\n",
    "   Yəni, XGBoost-da \"model\" əvəzinə, fərqli boosting metodlarını seçə bilərsiniz.\n",
    "\n",
    "2. **`max_depth`:**\n",
    "   - Hər bir ağacın maksimum dərinliyini təyin edir. Dərinlik nə qədər böyükdürsə, model o qədər kompleks və potensial olaraq overfitting riski böyükdür.\n",
    "\n",
    "3. **`learning_rate`:**\n",
    "   - Yeni ağacın verdiyi töhfənin ölçüsünü təyin edir. Aşağı dəyər daha stabillik gətirir, amma modelin öyrənmə sürətini azaldır.\n",
    "\n",
    "4. **`n_estimators`:**\n",
    "   - Ağacların ümumi sayını təyin edir. Daha çox ağac daha yaxşı performansa gətirib çıxara bilər, amma overfitting riski də artar.\n",
    "\n",
    "5. **`subsample`:**\n",
    "   - Hər bir ağac üçün istifadə edilən təlim məlumatlarının faizini təyin edir. Bu, modelin daha az məlumat üzərində öyrənməsinə və overfitting riskinin azalmasına kömək edir.\n",
    "\n",
    "6. **`colsample_bytree`, `colsample_bylevel`, `colsample_bynode`:**\n",
    "   - Hər bir ağac üçün (və ya səviyyə və ya düyün üçün) istifadə edilən atributların faizini təyin edir. Bu da overfitting riskini azaldır.\n",
    "\n",
    "7. **`gamma`:**\n",
    "   - Ağacların bölünməsinə nəzarət edən cərimələndirmə əmsalıdır. Yüksək dəyər daha az bölünmə və daha sadə modellərlə nəticələnir.\n",
    "\n",
    "### Yekun\n",
    "XGBoost-da da daxildə işləyən model dəyişdirilə bilməz, çünki o, Gradient Boosting-ə əsaslanan bir metoddur. Lakin siz modelin necə öyrəndiyini və performansını dəyişdirmək üçün müxtəlif hiperparametrləri tənzimləyə bilərsiniz. Məsələn, **`booster`** parametri ilə \"gbtree\", \"gblinear\", və \"dart\" arasında seçim edə bilərsiniz ki, bu da modelin davranışını xeyli dərəcədə dəyişə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c889867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7eee2b",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansinda daxili modeli deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13699f",
   "metadata": {},
   "source": [
    "Boosting modellərində ümumiyyətlə \"daxili model\" anlayışı yoxdur, çünki bu modellərin strukturu müəyyən bir əsas modelə, adətən qərar ağaclarına (decision trees), əsaslanır. Bu səbəbdən, boosting metodlarında daxili modeli dəyişdirmək mümkün olmur. \n",
    "\n",
    "### Əsas Boosting Modelləri və Daxili Model Seçimi:\n",
    "\n",
    "1. **XGBoost, LightGBM, və CatBoost:**\n",
    "   - Bu modellər qərar ağacları əsasında işləyir və istifadəçilər üçün daxili modelin (məsələn, qərar ağacları əvəzinə xətti modellər və ya başqa bir model) dəyişdirilməsi imkanı təqdim etmir.\n",
    "   - Yeganə istisna XGBoost-da **`booster`** parametridir, burada \"gbtree\" (qərar ağacları) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz. Lakin bu, daha çox modelin strukturu ilə əlaqədardır və tam mənada \"daxili model\" dəyişimi kimi qəbul edilmir.\n",
    "\n",
    "2. **AdaBoost:**\n",
    "   - AdaBoost-da istifadə olunan zəif modellər adətən qərar ağaclarıdır, lakin bu modelin təbiətinə görə zəif model olaraq fərqli algoritmlər (məsələn, xətti regresiya, SVM və ya başqa modellər) istifadə oluna bilər. Bu, AdaBoost-un digər boosting metodlarından fərqli olmasının əsas səbəbidir.\n",
    "   - Yəni, **AdaBoost** modelində istifadə olunan zəif modelləri dəyişdirmək mümkündür.\n",
    "\n",
    "### Nəticə\n",
    "Əgər daxili modeli dəyişmək imkanını istəyirsinizsə, **AdaBoost** istifadə etməlisiniz. AdaBoost-da zəif model olaraq fərqli algoritmlərdən istifadə edə bilərsiniz. Lakin digər məşhur boosting metodları olan XGBoost, LightGBM, və CatBoost-da daxili model dəyişdirilməsi mümkün deyil, çünki bunlar xüsusi olaraq qərar ağacları üzərində işləmək üçün dizayn olunub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eccd75",
   "metadata": {},
   "source": [
    "## Gradient Boosting-de daxili modeli deyismek olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b54636",
   "metadata": {},
   "source": [
    "Bəli, Gradient Boosting metodunda da daxili model dəyişdirilmir. Gradient Boosting, adətən qərar ağacları (decision trees) istifadə edərək işləyən bir boosting metodudur və bu, metodun əsas strukturunu təşkil edir. \n",
    "\n",
    "### Əsas Nöqtələr:\n",
    "- **Gradient Boosting** metodunun əsas ideyası, zəif modellərin (adətən qərar ağacları) ardıcıl olaraq qurulması və hər bir yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışmasıdır.\n",
    "- **Qərar ağacları** bu metodun əsasında durur və buna görə də daxili model olaraq dəyişdirilə bilməz.\n",
    "\n",
    "### İstisnalar və Seçimlər:\n",
    "- **XGBoost** və **LightGBM** kimi Gradient Boosting-in optimallaşdırılmış versiyaları da əsasən qərar ağacları üzərində işləyir. Bu modellərdə də daxili modeli dəyişdirmək mümkün deyil.\n",
    "- Yalnız **XGBoost** modelində `booster` parametri ilə \"gbtree\" (qərar ağacı) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz, amma bu tam olaraq daxili modelin dəyişdirilməsi deyil. Bu parametr sadəcə olaraq modelin xətti və ya ağac əsaslı olacağını təyin edir.\n",
    "\n",
    "### Nəticə:\n",
    "Gradient Boosting metodunda daxili model adətən dəyişdirilmir və əsas olaraq qərar ağacları istifadə olunur. Bu, metodun fundamental xüsusiyyətlərindən biridir və onun effektivliyini təmin edən əsas səbəblərdən biridir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938f93d4",
   "metadata": {},
   "source": [
    "## Robust Scale haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3a003",
   "metadata": {},
   "source": [
    "**Robust Scaler** məlumat dəstələrinin (datasets) miqyasını dəyişdirmək üçün istifadə olunan bir normalizasiya texnikasıdır. Bu metod ənənəvi miqyaslandırma üsulları (məsələn, Standart Scaler, Min-Max Scaler) ilə müqayisədə məlumat dəstələrindəki outlier-lərə (məlumatların ümumi tendensiyasından uzaq olan dəyərlərə) qarşı daha davamlıdır. \n",
    "\n",
    "### Robust Scaler-in Əsas Xüsusiyyətləri:\n",
    "\n",
    "1. **Median Və İQR Əsaslı Miqyaslandırma:**\n",
    "   - **Median**: Məlumat dəstəsinin ortasında olan dəyərdir və bu dəyər məlumat dəstəsindəki dəyərlərin yarısının bu dəyərdən az, yarısının isə çox olduğunu göstərir.\n",
    "   - **IQR (Interquartile Range)**: Məlumat dəstəsinin 25-ci və 75-ci percentilləri arasındakı fərqdir. Bu aralıq mərkəzləşmiş dəyərləri əhatə edir və outlier-lərdən daha az təsirlənir.\n",
    "   - Robust Scaler məlumat dəstəsini median ətrafında mərkəzləşdirir və dəyərləri IQR ilə miqyaslandırır. Yəni, hər bir dəyərdən median çıxılır və sonra IQR-ə bölünür:\n",
    "     $$ \n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{Median}(X)}{\\text{IQR}(X)}\n",
    "      $$\n",
    "   - Bu yanaşma məlumat dəstəsində outlier-lər olduğu halda belə, məlumatların düzgün miqyaslandırılmasını təmin edir.\n",
    "\n",
    "2. **Outlier-lərə Qarşı Dayanıqlıq:**\n",
    "   - Outlier-lər, məlumatların əksəriyyətinin uzaqında yerləşən ekstremal dəyərlərdir. Ənənəvi miqyaslandırma üsulları bu dəyərlərə həssas ola bilər və nəticədə miqyaslandırılmış dəyərlər qeyri-adekvat ola bilər.\n",
    "   - Robust Scaler, median və IQR istifadə etdiyi üçün outlier-lərin təsirini azaldır və məlumatların miqyaslandırılmasını daha sabit və dəqiq edir.\n",
    "\n",
    "3. **Tətbiq Sahələri:**\n",
    "   - **Maşın Öyrənməsi (Machine Learning):** Məlumat dəstələrinin outlier-lərlə zəngin olduğu vəziyyətlərdə xüsusən istifadə olunur. Bu, modellərin (məsələn, linear regression, SVM) performansını artırmağa kömək edə bilər.\n",
    "   - **Peyk Məlumatları:** Məsələn, peyk görüntüləri və ya uzaqdan zondlama (remote sensing) məlumatları çox sayda outlier-lərlə işləyə bilər.\n",
    "   - **Finans Məlumatları:** Finansal məlumatlarda outlier-lər tez-tez rast gəlinir. Robust Scaler bu məlumatları miqyaslandırmaq üçün əlverişlidir.\n",
    "\n",
    "### Nümunə: Python ilə Robust Scaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Random data with outliers\n",
    "data = np.array([[1, 2, 2], \n",
    "                 [3, 6, 4], \n",
    "                 [7, 18, 8], \n",
    "                 [9, 10, 12], \n",
    "                 [20, 50, 90]])\n",
    "\n",
    "# RobustScaler-in tətbiqi\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nScaled Data with RobustScaler:\\n\", scaled_data)\n",
    "```\n",
    "\n",
    "### Üstünlüklər:\n",
    "- **Outlier-lərə Dayanıqlıq:** Median və IQR istifadə edildiyi üçün məlumat dəstələrindəki outlier-lər bu miqyaslandırmaya az təsir edir.\n",
    "- **Daha Sabit Nəticələr:** Ənənəvi miqyaslandırma metodları ilə müqayisədə, outlier-lərin səbəb olduğu qeyri-stabilliklər azaldılır.\n",
    "\n",
    "### Çatışmazlıqlar:\n",
    "- **Küçük Məlumat Dəstələrində Problemlər:** Əgər məlumat dəstəsi çox kiçikdirsə və ya çox az sayda nümunədən ibarətdirsə, median və IQR düzgün hesablanmaya bilər.\n",
    "- **Yalnız Düzgün Paylanmış Məlumatlarda Effektivdir:** Məlumatların paylanışı çox qeyri-bərabərdirsə, Robust Scaler bəzən qeyri-adekvat miqyaslandırma apara bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Robust Scaler outlier-lərlə zəngin olan məlumat dəstələrinin miqyaslandırılması üçün ideal bir vasitədir. Median və IQR kimi statistik ölçülərdən istifadə edərək məlumatları miqyaslandırmaqla, bu metod outlier-lərə qarşı daha sabit və dəqiq nəticələr verir. Bu səbəbdən, Robust Scaler maşın öyrənməsi tətbiqlərində və digər məlumat təhlili proseslərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a586bc43",
   "metadata": {},
   "source": [
    "## Feature Importance Riyaziyyatda formulu? Hesablanma qaydasi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebb069",
   "metadata": {},
   "source": [
    "Decision Tree modellərində **feature importance** hər bir xüsusiyyətin (feature) modeldə nə qədər vacib olduğunu göstərir. Bu, xüsusiyyətin ağacın qurulmasında (tree-building process) nə qədər təsirli olduğunu qiymətləndirir. Decision Tree-də **feature importance** ümumiyyətlə impurity-based importance metodu ilə hesablanır.\n",
    "\n",
    "### Feature Importance Hesablanması\n",
    "\n",
    "1. **Impurity Measurement (Gini impurity, Entropy):** \n",
    "   - Hər bir düyündə (node) impurity (məsələn, Gini impurity, Entropy) hesablanır. Impurity, həmin düyündəki qarışıqlığın ölçüsüdür.\n",
    "\n",
    "2. **Impurity Reduction (Information Gain):**\n",
    "   - Hər bir xüsusiyyət üçün, həmin xüsusiyyətə əsaslanaraq ağacın düyünü bölündükdən sonra impurity azalması hesablanır. Bu azalma həmin xüsusiyyətin nə qədər məlumat verdiyini (information gain) göstərir.\n",
    "   \n",
    "   - Impurity reduction = impurity_before_split - impurity_after_split\n",
    "\n",
    "3. **Cumulative Importance:**\n",
    "   - Hər bir xüsusiyyət üçün, ağacın müxtəlif düyünlərindəki impurity reduction-lar toplanır. Bu, həmin xüsusiyyətin ümumi əhəmiyyətini (total importance) göstərir.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - Hər bir xüsusiyyətin əhəmiyyəti normallaşdırılır (0 ilə 1 arasında) və nəticədə hər bir xüsusiyyətin final feature importance dəyəri əldə edilir.\n",
    "\n",
    "### Riyazi Formula\n",
    "\n",
    "Feature importance $  FI_j  $ hər bir xüsusiyyət $ j $ üçün aşağıdakı kimi hesablanır:\n",
    "\n",
    "$$ \n",
    "FI_j = \\sum_{t=1}^{T} \\Delta i_t \\cdot 1 \\{v(t) = j\\}\n",
    " $$\n",
    "\n",
    "burada:\n",
    "- $  \\Delta i_t  $ - düyün $ t $-də impurity reduction-dur (impurity_before_split - impurity_after_split).\n",
    "- $ 1 \\{v(t) = j\\}  $ - düyün $ t $-nin xüsusiyyət $ j $-yə aid olub-olmamasını göstərən göstərici funksiyasıdır.\n",
    "- $ T $ - ümumi düyünlərin sayıdır.\n",
    "\n",
    "Bu formula ağacın bütün düyünlərində xüsusiyyətin nə qədər məlumat verdiyini toplamaqla xüsusiyyətin ümumi əhəmiyyətini hesablayır. Nəticədə, daha çox informasiya verən xüsusiyyətlərin əhəmiyyəti yüksək olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e04d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a46511d",
   "metadata": {},
   "source": [
    "* scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "* X_train = scaler.transform(X_train)\n",
    "* X_test = scaler.transform(X_test)\n",
    "\n",
    "### Muellif scale -i bu formada edib. Niye X_test-i de X_traine gore scale edib? Umimiyyetle  train test splitden evvel ya sonra yaxsidir scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cc696",
   "metadata": {},
   "source": [
    "`StandardScaler` və digər skalarlardan istifadə edərkən `train` və `test` setlərinin uyğunlaşdırılması (scaling) çox önəmlidir. Niyə `X_test` də `X_train`-ə görə standartlaşdırılır və bu prosesin ən doğru vaxtı haqqında izahat verəcəyəm.\n",
    "\n",
    "### Niyə `X_test` `X_train`-ə görə standartlaşdırılır?\n",
    "\n",
    "1. **Məlumat Sızması (Data Leakage) Əngəllənməsi:**\n",
    "   - **Qarşısını Almaq:** Test məlumatlarına birbaşa müdaxilə etmək modelin qiymətləndirilməsini səhv edə bilər. Test məlumatlarına əvvəlcədən baxmaq, modelin test dəstinə uyğunlaşmasına və nəticədə daha yaxşı nəticələr əldə etməsinə səbəb ola bilər. Bu, məlumat sızması (data leakage) olaraq adlandırılır.\n",
    "   - **Nə Üçün:** Test setini `X_train`-ə görə standartlaşdırmaq, test məlumatlarının yalnız təlim məlumatları əsasında miqyaslandırılması və beləliklə, test dəstinin düzgün qiymətləndirilməsi təmin edilir.\n",
    "\n",
    "2. **Hesablama Uyğunluğu:**\n",
    "   - **Standartlaşdırma:** Təlim məlumatları (`X_train`) üzərində fit edilmiş `StandardScaler`, müvafiq olaraq bu məlumatların ortalaması və standart sapması ilə `X_train` və `X_test`-i miqyaslandırır.\n",
    "   - **Nə Üçün:** Bu, modelin test məlumatlarının dəqiq və obyektiv qiymətləndirilməsi üçün lazımdır. `X_test`-i `X_train`-ə görə miqyaslandırmaq, test dəstinin eyni skala ilə işlənməsini təmin edir.\n",
    "\n",
    "### Train Test Splitdən Əvvəl Yoxsa Sonra Scale Etmək?\n",
    "\n",
    "- **Testdən Sonra:** Test və təlim məlumatlarını ayırdıqdan sonra, `StandardScaler` fit edildikdə, yalnız təlim məlumatlarına əsaslanaraq fit edilir və sonra bu transformasiyanı test məlumatlarına tətbiq etmək məsləhətdir. Bu, test məlumatlarının təlim məlumatlarının xüsusiyyətlərindən istifadə etmədən miqyaslandırılmasını təmin edir.\n",
    "\n",
    "### Məsələn\n",
    "\n",
    "**Train Test Splitdən Sonra Skalarlama:**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DataFrame və target dəyişənini ayırmaq\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler obyektini yaratmaq və fit etmək\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### Əsas Məqamlar\n",
    "\n",
    "1. **Train Test Splitdən Sonra Skalarlama:**\n",
    "   - `StandardScaler`-in `fit()` metodu yalnız `X_train` üzərində çalışır.\n",
    "   - `transform()` metodu həm `X_train`, həm də `X_test` üzərində tətbiq olunur.\n",
    "\n",
    "2. **Test Seti Üzərində Fit Etmək:**\n",
    "   - Test setinə fit edilməməlidir, çünki bu, test məlumatlarının məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "Bu yanaşma test dəstinin düzgün qiymətləndirilməsini təmin edir və modelin yalnız təlim məlumatlarına əsaslanaraq necə performans göstərdiyini daha doğru qiymətləndirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a1f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f552a7",
   "metadata": {},
   "source": [
    "## RobustScaler haqqinda genis melumat ver?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bc9aa",
   "metadata": {},
   "source": [
    "`RobustScaler`, xüsusilə outlier-lərlə zəngin məlumat dəstləri üçün nəzərdə tutulmuş bir miqyaslandırma metodudur. Bu metod, xüsusiyyətləri `median` və `interquartile range` (IQR) əsasında standartlaşdıraraq daha dayanıqlı və etibarlı nəticələr əldə etməyə imkan verir.\n",
    "\n",
    "### `RobustScaler` Nədir?\n",
    "\n",
    "`RobustScaler`, `sklearn.preprocessing` kitabxanasında olan bir vasitədir. Bu metod, xüsusiyyətləri aşağıdakı addımlar ilə miqyaslandırır:\n",
    "\n",
    "1. **Median ilə Mərkəzləşdirmə:**\n",
    "   - **Median**: Hər bir xüsusiyyət üçün orta dəyəri çıxarmaq məqsədilə istifadə olunur. Median outlier-lərdən təsirlənməz, çünki o, 50%-lik kvantildir və outlier-lərin təsirini azaltmağa kömək edir.\n",
    "\n",
    "2. **Interquartile Range (IQR) ilə Miqyaslandırma:**\n",
    "   - **IQR**: Bu metod, 25%-lik (Q1) və 75%-lik (Q3) kvantil arasındakı məsafəni ölçür. Bu məsafə xüsusiyyətlərin yayılmasını ölçmək üçün istifadə edilir və outlier-lərin təsirini azaltmaq məqsədilə miqyaslandırma prosesində tətbiq olunur.\n",
    "\n",
    "### Formula\n",
    "\n",
    "`RobustScaler`-in tətbiq etdiyi standartlaşdırma aşağıdakı kimi təyin olunur:\n",
    "\n",
    "- **Mərkəzləşdirmə (Mean subtraction):**\n",
    "  \n",
    "  \\[\n",
    "  X_{\\text{centered}} = X - \\text{Median}\n",
    "  \\]\n",
    "\n",
    "- **Miqyaslandırma (Scaling):**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{scaled}} = \\frac{X_{\\text{centered}}}{\\text{IQR}}\n",
    "  \\]\n",
    "\n",
    "  Burada:\n",
    "  - \\( X \\) - Xüsusiyyətlərin dəyərləri\n",
    "  - **Median** - Xüsusiyyətlərin medianı\n",
    "  - **IQR** - Interquartile range (Q3 - Q1)\n",
    "\n",
    "### İstifadə Misalı\n",
    "\n",
    "`RobustScaler`-dən istifadə edərkən adətən aşağıdakı addımlar izlənir:\n",
    "\n",
    "1. **DataFrame və target dəyişənini ayırmaq:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "   # Məsələn, DataFrame\n",
    "   data = {\n",
    "       'feature1': [1, 2, 3, 1000],  # Outlier daxil olan xüsusiyyətlər\n",
    "       'feature2': [4, 5, 6, 7]\n",
    "   }\n",
    "   df = pd.DataFrame(data)\n",
    "   X = df[['feature1', 'feature2']]\n",
    "   y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "   # Train və Test dəstini ayırmaq\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **`RobustScaler`-i tətbiq etmək:**\n",
    "\n",
    "   ```python\n",
    "   # RobustScaler obyektini yaratmaq və fit etmək\n",
    "   scaler = RobustScaler()\n",
    "   scaler.fit(X_train)\n",
    "\n",
    "   # Təlim və test məlumatlarını transformasiya etmək\n",
    "   X_train_scaled = scaler.transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "   # Dəyişdirilmiş məlumatları çap etmək\n",
    "   print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "   print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "   ```\n",
    "\n",
    "### Nə Zaman İstifadə Edilməlidir?\n",
    "\n",
    "- **Outlier-lərlə Mübarizə:** `RobustScaler` xüsusilə outlier-lərin olduğu məlumat dəstləri üçün uyğundur, çünki median və IQR outlier-lərdən təsirlənmir.\n",
    "- **Model Performansı:** Outlier-lərdən təsirlənməyən daha düzgün nəticələr və daha stabil model performansı təmin edir.\n",
    "\n",
    "### Müqayisə\n",
    "\n",
    "- **`StandardScaler` vs. `RobustScaler`:**\n",
    "  - `StandardScaler`-in istifadə etdiyi ortalama və standart sapma outlier-lərlə təsirlənə bilər, amma `RobustScaler`-in istifadə etdiyi median və IQR outlier-lərdən daha dayanıqlıdır.\n",
    "  - `StandardScaler` daha çox \"normal\" paylanma olan məlumat dəstləri üçün uyğundur, `RobustScaler` isə outlier-lərin olduğu hallarda daha uyğundur.\n",
    "\n",
    "Bu şəkildə, `RobustScaler` xüsusiyyətlərin daha dayanıqlı şəkildə standartlaşdırılmasını təmin edir və xüsusiyyətlərdəki outlier-lərin təsirini azaltmağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9397e4e4",
   "metadata": {},
   "source": [
    "## RobustScaler de train test split den sonra olmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7fc8f",
   "metadata": {},
   "source": [
    "Bəli, `RobustScaler` və digər miqyaslandırma metodları train-test splitdən sonra tətbiq olunmalıdır. Bu, məlumat sızmasının (data leakage) qarşısını almaq və modelin təlim və test mərhələlərində düzgün qiymətləndirilməsini təmin etmək üçün vacibdir.\n",
    "\n",
    "### Niyə Train-Test Splitdən Sonra Scalarlama?\n",
    "\n",
    "1. **Məlumat Sızmasının Qarşısını Almaq:**\n",
    "   - **Niyə:** Test məlumatlarına birbaşa təsir etmədən yalnız təlim məlumatlarından miqyaslandırma parametrləri (median və IQR) əldə etmək lazımdır. Bu, test dəstinin yalnız təlim məlumatları əsasında miqyaslandırıldığını təmin edir.\n",
    "   - **Nəticə:** Test məlumatlarına əvvəlcədən baxmaq və ya onları miqyaslandırmaq modelin nəticələrini təhrif edə bilər və məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "2. **Miqyaslandırma Parametrlərinin Düzgün Tətbiqi:**\n",
    "   - **Təlim Məlumatları:** Miqyaslandırma parametrləri yalnız təlim məlumatlarına əsaslanaraq fit edilir.\n",
    "   - **Test Məlumatları:** Eyni parametrlər (mediana və IQR) test məlumatlarına tətbiq edilir.\n",
    "\n",
    "### Misal Kodu\n",
    "\n",
    "**1.** **Train-Test Splitdən Sonra Miqyaslandırma:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Nümunə DataFrame\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 1000],  # Outlier olan xüsusiyyət\n",
    "    'feature2': [4, 5, 6, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RobustScaler obyektini yaratmaq və fit etmək\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)  # Yalnız X_train üzərində fit edilir\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dəyişdirilmiş məlumatları çap etmək\n",
    "print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "```\n",
    "\n",
    "**2.** **Nümunənin İslahatı:**\n",
    "\n",
    "- **DataFrame Yaratma:** Nümunə məlumatlar yaradılır.\n",
    "- **Train-Test Split:** `X` və `y` məlumatları train və test dəstlərinə bölünür.\n",
    "- **`RobustScaler` İstifadəsi:** `RobustScaler` yalnız təlim dəstində fit edilir, sonra həm təlim, həm də test dəstinə tətbiq edilir.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "`RobustScaler` və digər miqyaslandırma üsulları üçün düzgün təcrübə train-test splitdən sonra miqyaslandırma tətbiq etməkdir. Bu yanaşma, məlumat sızmasının qarşısını alır və modelin daha düzgün qiymətləndirilməsini təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15586d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61de99c",
   "metadata": {},
   "source": [
    "## Bagging parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5a4a",
   "metadata": {},
   "source": [
    "* bag = BaggingClassifier(estimator=svm,\n",
    "*                        n_estimators=10,\n",
    "*                        max_samples=0.4,\n",
    "*                        max_features=35,\n",
    "*                        bootstrap=True,\n",
    "*                        bootstrap_features=True,\n",
    "*                        random_state=123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61254f",
   "metadata": {},
   "source": [
    "`BaggingClassifier` parametrleri aşağıda izah olunur:\n",
    "\n",
    "1. **`estimator=svm`**: Burada `svm`, bagging ilə istifadə olunan əsas modelin növünü göstərir. SVM (Support Vector Machine) burada əsas model kimi istifadə olunur. `BaggingClassifier`, bu SVM modellərindən bir neçə nüsxə yaradaraq onları birləşdirir.\n",
    "\n",
    "2. **`n_estimators=10`**: Bu parametr, neçə dənə baza modelinin (bu halda SVM) yaradılacağını göstərir. `n_estimators=10` təyin edildikdə, `BaggingClassifier` 10 fərqli SVM modeli qurur və onları birləşdirir.\n",
    "\n",
    "3. **`max_samples=0.4`**: Bu parametr, hər bir baza modelinin təlimi üçün orijinal məlumat dəstindən seçilən nümunələrin faizi və ya sayı ilə bağlıdır. `max_samples=0.4` olduqda, hər baza modeli üçün orijinal məlumat dəstinin 40%-i təlim üçün istifadə olunur. Bu seçim prosesi \"bootstrapping\" metodu ilə həyata keçirilir.\n",
    "\n",
    "4. **`max_features=35`**: Bu parametr, hər bir baza modelinin qurulmasında istifadə olunan xüsusiyyətlərin maksimum sayını göstərir. `max_features=35` təyin edildikdə, hər baza modelinin təlimində 35 xüsusiyyət seçilir.\n",
    "\n",
    "5. **`bootstrap=True`**: Bu parametr, nümunələrin \"bootstrapping\" (yəni, geri qoyaraq nümunə götürmə) metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap=True` olduqda, hər bir baza modeli üçün təlim nümunələri geri qoyularaq seçilir, yəni eyni nümunə bir neçə dəfə seçilə bilər.\n",
    "\n",
    "6. **`bootstrap_features=True`**: Bu parametr, xüsusiyyətlərin \"bootstrapping\" metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap_features=True` olduqda, hər bir baza modeli üçün xüsusiyyətlər geri qoyularaq seçilir.\n",
    "\n",
    "7. **`random_state=123`**: Bu parametr, nəticələrin təkrarlanabilirliyini təmin etmək üçün istifadə olunur. `random_state=123` təyin edildikdə, modelin təsadüfi seçimləri eyni şəkildə həyata keçirilir və nəticələr hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `BaggingClassifier` modelinin müxtəlif aspektlərini nəzarət edir və modelin qurulması, nümunə götürülməsi və xüsusiyyətlərin seçilməsi prosesi üzərində tam nəzarət təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafb507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bfaed9",
   "metadata": {},
   "source": [
    "## XGBoost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31aee",
   "metadata": {},
   "source": [
    "* xgbt = XGBClassifier(max_depth = 2,\n",
    "*              learning_rate = 0.2,\n",
    "*              objective  = \"multi:softmax\",\n",
    "*              num_class = 2,\n",
    "*              booster = \"gbtree\",\n",
    "*              n_estimators = 10,\n",
    "*              random_state = 123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16db875",
   "metadata": {},
   "source": [
    "`XGBClassifier` modelinin parametrləri aşağıda izah olunur:\n",
    "\n",
    "1. **`max_depth = 2`**:\n",
    "   - Bu parametr hər bir fərdi ağacın maksimal dərinliyini təyin edir. `max_depth = 2` təyin edildikdə, hər ağac ən çox 2 dərinlik səviyyəsinə malik olacaq. Daha kiçik dərinlik overfitting-i azaldır, lakin çox kiçik olarsa, model kifayət qədər mürəkkəbliyi tuta bilməz.\n",
    "\n",
    "2. **`learning_rate = 0.2`**:\n",
    "   - Bu parametr, hər ağacın öyrənmə sürətini təyin edir. `learning_rate = 0.2` olduqda, hər bir ağacın modelə töhfəsi 0.2 faktorla azalır. Kiçik learning rate daha dəqiq və stabil model təlimi verir, amma daha çox iterasiya tələb edir.\n",
    "\n",
    "3. **`objective = \"multi:softmax\"`**:\n",
    "   - Bu parametr modelin məqsədini təyin edir. `multi:softmax` təyin edildikdə, model çoxlu sinifli təsnifat (multi-class classification) üçün istifadə edilir və proqnozlaşdırılan dəyər ən yüksək ehtimala malik sinif kimi verilir.\n",
    "\n",
    "4. **`num_class = 2`**:\n",
    "   - Bu parametr, modelin neçə sinif olduğunu təyin edir. `num_class = 2` təyin edildikdə, model iki sinifli (binary) təsnifat məsələsini həll edir. (Qeyd: `multi:softmax` ilə birlikdə istifadə olunduqda, bu parametr faktiki olaraq daha çox sinif sayı üçün istifadə olunur.)\n",
    "\n",
    "5. **`booster = \"gbtree\"`**:\n",
    "   - Bu parametr, modelin hansı boosting algoritmini istifadə edəcəyini təyin edir. `gbtree` təyin edildikdə, model qərar ağacları ilə qurulur. Alternativ olaraq `gblinear` (linear model) və ya `dart` (Dropouts meet Multiple Additive Regression Trees) kimi digər algoritmlər də istifadə edilə bilər.\n",
    "\n",
    "6. **`n_estimators = 10`**:\n",
    "   - Bu parametr, modeldə neçə dənə ağacın qurulacağını göstərir. `n_estimators = 10` təyin edildikdə, model 10 ağac qurur. Daha çox ağac modelin dəqiqliyini artıra bilər, lakin eyni zamanda təlim müddətini uzada bilər.\n",
    "\n",
    "7. **`random_state = 123`**:\n",
    "   - Bu parametr, təsadüfi proseslər üçün toxum (seed) təyin edir ki, nəticələr təkrarlanabilsin. `random_state = 123` təyin edildikdə, modelin nəticələri hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `XGBClassifier` modelinin qurulması və işləməsi zamanı onun davranışını təyin edir və modelin düzgün performans göstərməsi üçün müxtəlif aspektləri optimallaşdırmağa imkan verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5475dc",
   "metadata": {},
   "source": [
    "### XGBoots-da \n",
    "* scale_pos_weight=[default=1]\n",
    "### ???????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c25fe",
   "metadata": {},
   "source": [
    "`scale_pos_weight` parametri `XGBoost` modelində balanssız verilənlər dəstlərində (yəni, bir sinifin digərindən çox üstün olduğu hallarda) istifadə olunur. Bu parametr siniflər arasındakı balanssızlığı tənzimləmək və modelin daha yaxşı performans göstərməsini təmin etmək üçün istifadə edilir.\n",
    "\n",
    "### `scale_pos_weight` Parametrinin İzahı\n",
    "\n",
    "- **Məqsəd:** Əgər verilənlər dəstində siniflərdən biri (məsələn, pozitiv sinif) çox az sayda nümunə ilə təmsil olunursa, `scale_pos_weight` parametri bu sinifə daha çox əhəmiyyət vermək üçün istifadə olunur.\n",
    "- **Dəyər:** `scale_pos_weight = nöqativ_sinif_nümunələri / pozitiv_sinif_nümunələri` şəklində təyin edilir.\n",
    "\n",
    "### Necə İşləyir?\n",
    "\n",
    "- **Balanssızlıq Problemi:** Əgər siniflər arasında balanssızlıq varsa, model çoxluqda olan sinifi daha çox öyrənir və bu da nadir sinifin düzgün proqnozlaşdırılmasına mane ola bilər.\n",
    "- **`scale_pos_weight` Parametrinin Təyini:** Bu parametr, azlıqda olan sinifin təsirini artırmaq üçün istifadə olunur. Məsələn, pozitiv sinifdə 100 nümunə, neqativ sinifdə isə 1000 nümunə varsa, `scale_pos_weight = 1000/100 = 10` təyin etmək olar. Bu, modelin azlıqda olan sinifə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Misal:\n",
    "\n",
    "```python\n",
    "xgbt = XGBClassifier(max_depth=2,\n",
    "                     learning_rate=0.2,\n",
    "                     objective=\"binary:logistic\",\n",
    "                     n_estimators=10,\n",
    "                     scale_pos_weight=10,  # Siniflər arasındakı balanssızlıq nəzərə alınır\n",
    "                     random_state=123)\n",
    "```\n",
    "\n",
    "Burada `scale_pos_weight=10` təyin edilib. Bu, modelin pozitiv sinifə neqativ sinifdən 10 dəfə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Nəticə\n",
    "`scale_pos_weight` parametri balanssız verilənlər dəstlərində siniflər arasındakı balansı tənzimləyərək, modelin azlıqda olan sinifi düzgün proqnozlaşdırmasını təmin etmək üçün istifadə edilir. Bu parametr xüsusilə tibbi diaqnostika, fırıldaqçılığın aşkarlanması kimi sahələrdə faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e67694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0befd682",
   "metadata": {},
   "source": [
    "## Data Understanding etmek ucun nelere baxim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b969",
   "metadata": {},
   "source": [
    "Bu məlumat dəstini yaxşı anlamaq üçün aşağıdakı addımları ata bilərsiniz:\n",
    "\n",
    "### 1. **Veri Tipini Yoxlayın**\n",
    "   - **Hər bir sütunun** (dəyişənin) tipini yoxlayın (məsələn, integer, float, object). Bu, hər bir dəyişənin necə işlənməli olduğunu anlamağa kömək edəcək. Məsələn, `LoanAmount` və `ApplicantIncome` sayısal dəyərlərdir, amma `Gender`, `Married` və `Loan_Status` kimi dəyişənlər kateqoriyalardır.\n",
    "\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "\n",
    "### 2. **Məlumatların Yoxluğu (Missing Data)**\n",
    "   - Məlumat dəstində boş dəyərlərin olub-olmadığını yoxlayın. Bu, model qurmaq üçün məlumatların keyfiyyətini artırmağa kömək edəcək. Boş dəyərlər varsa, onları doldurma (imputation) və ya silmə metodları tətbiq edə bilərsiniz.\n",
    "\n",
    "   ```python\n",
    "   df.isnull().sum()\n",
    "   ```\n",
    "\n",
    "### 3. **Məlumatların Dağılımı**\n",
    "   - Sayısal dəyişənlərin (məsələn, `ApplicantIncome`, `LoanAmount`) və kateqoriyal dəyişənlərin (`Gender`, `Education`) dağılımını yoxlayın. Bu, məlumatların normal paylanıb-paylanmadığını və ya bəzi dəyişənlərdə outlier-ların olub-olmadığını anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.describe()\n",
    "   df['ApplicantIncome'].hist()\n",
    "   ```\n",
    "\n",
    "### 4. **Kateqoriyal Dəyərlərin Paylanması**\n",
    "   - Kateqoriyal dəyişənlərin (məsələn, `Gender`, `Married`, `Education`) müxtəlif siniflər üzrə paylanmasını yoxlayın. Bu, məlumat dəstində hansı siniflərin dominant olduğunu və modelin necə performans göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df['Gender'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 5. **Hədəf Dəyişən (Target Variable)**\n",
    "   - `Loan_Status` dəyişəninin paylanmasını yoxlayın. Hədəf dəyişənin balanslı və ya balanssız olub-olmadığını bilmək vacibdir. Balanssızlıq varsa, xüsusi metodlar tətbiq etmək lazım ola bilər (məsələn, undersampling, oversampling).\n",
    "\n",
    "   ```python\n",
    "   df['Loan_Status'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 6. **Məlumatların Əlaqəsi (Correlation)**\n",
    "   - Sayısal dəyişənlər arasındakı əlaqəni yoxlayın. Bu, hansı dəyişənlərin bir-biri ilə daha çox əlaqəli olduğunu və hansı dəyişənlərin modelə daha çox təsir göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.corr()\n",
    "   ```\n",
    "\n",
    "### 7. **Pivotal Dəyişənlərin Təsiri**\n",
    "   - Hədəf dəyişən (`Loan_Status`) ilə digər dəyişənlər arasındakı əlaqəni yoxlayın. Məsələn, `ApplicantIncome`, `Credit_History` və `Education` dəyişənlərinin kredit təsdiqinə necə təsir etdiyini araşdırın.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "   ```\n",
    "\n",
    "### 8. **Outlier-ları Yoxlamaq**\n",
    "   - Sayısal dəyişənlərdə outlier-ları yoxlayın və onları necə idarə edəcəyinizi qərar verin. Outlier-lar modeli yanıltmaq riski daşıya bilər.\n",
    "\n",
    "   ```python\n",
    "   df.boxplot(column='LoanAmount')\n",
    "   ```\n",
    "\n",
    "Bu addımları yerinə yetirdikdən sonra məlumat dəstini daha yaxşı anlayacaq və model qurmağa başlamaq üçün yaxşı bir zəmin yaradacaqsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b084870e",
   "metadata": {},
   "source": [
    "## Data Preparation usullari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564113cf",
   "metadata": {},
   "source": [
    "Loan eligibility prediction üçün data preparation mərhələsi çox vacibdir və müvəffəqiyyətli modellər qurmaq üçün düzgün hazırlıq tələb olunur. Aşağıdakı addımlar bu prosesdə sizə kömək edə bilər:\n",
    "\n",
    "### 1. **Veri Yükləmə və İnspeksiya**\n",
    "   - Veriləri yükləyin və ilk baxışda statistik göstəricilərə baxın.\n",
    "   - Datalarda hər hansı bir boş (missing) dəyərin olub olmadığını yoxlayın.\n",
    "   \n",
    "   ```python\n",
    "import pandas as pd\n",
    "\n",
    "# CSV faylı yüklə\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# İlk 5 sətirə baxış\n",
    "print(df.head())\n",
    "\n",
    "# Verinin ümumi təsviri\n",
    "print(df.info())\n",
    "\n",
    "# Statistik məlumat\n",
    "print(df.describe())\n",
    "```\n",
    "\n",
    "### 2. **Boş Dəyərlərin İdarə Edilməsi**\n",
    "   - Boş dəyərləri necə idarə edəcəyinizə qərar verin. Ən çox istifadə olunan metodlar:\n",
    "     - **İlkin Təsnifat**: Boş dəyərləri təyin olunmuş bir dəyər ilə əvəzləmək (məsələn, median, mod, və s.)\n",
    "     - **Sıxlıqla Təyin Edilməsi**: Boş dəyərləri sıxlıqla təyin edilmiş dəyərlərlə əvəzləmək.\n",
    "     \n",
    "```python\n",
    "# Boş dəyərlərin yoxlanması\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Median ilə boş dəyərlərin doldurulması (nümunə: LoanAmount sütunu üçün)\n",
    "df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())\n",
    "\n",
    "# Ən çox istifadə olunan dəyərlə boş dəyərlərin doldurulması (nümunə: Gender sütunu üçün)\n",
    "df['Gender'] = df['Gender'].fillna(df['Gender'].mode()[0])\n",
    "```\n",
    "\n",
    "### 3. **Kateqoriyalı Dəyərlərin Kodlaşdırılması**\n",
    "   - Kateqoriyalı dəyişənləri (Gender, Married, Education, Self_Employed, Property_Area) rəqəmlə ifadə etməlisiniz.\n",
    "     - **Label Encoding**: Hər bir kateqoriyanı təmsil edən bir rəqəm verilir.\n",
    "     - **One-Hot Encoding**: Hər bir kateqoriya üçün ayrılıqda sütun yaradılır və müvafiq sütunda 1, digərlərində isə 0 olur.\n",
    "        \n",
    "**Label Encoding:**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "df['Married'] = le.fit_transform(df['Married'])\n",
    "df['Education'] = le.fit_transform(df['Education'])\n",
    "df['Self_Employed'] = le.fit_transform(df['Self_Employed'])\n",
    "df['Property_Area'] = le.fit_transform(df['Property_Area'])\n",
    "```\n",
    "\n",
    "**One-Hot Encoding:**\n",
    "```python\n",
    "df = pd.get_dummies(df, columns=['Property_Area', 'Education'], drop_first=True)\n",
    "```        \n",
    "\n",
    "### 4. **Miqdar Dəyərlərin Miqyaslandırılması**\n",
    "   - **Standardization**: Orta və standart sapma əsasında dəyərlərin normallaşdırılması.\n",
    "   - **Normalization**: Dəyərlərin [0, 1] intervalına gətirilməsi.\n",
    "    \n",
    "**Standardization:**\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df['LoanAmount'] = scaler.fit_transform(df[['LoanAmount']])\n",
    "```\n",
    "\n",
    "**Normalization:**\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['LoanAmount'] = scaler.fit_transform(df[['LoanAmount']])\n",
    "```    \n",
    "\n",
    "### 5. **Boş Dəyərlərin Aşağı Səviyyədə Dəyərləndirilməsi**\n",
    "   - Verilərin doğruluğunu və əlaqəliliyini artırmaq üçün boş dəyərləri analiz edin.\n",
    "\n",
    "```python\n",
    "# Boş dəyərlərin olub-olmadığını yenidən yoxla\n",
    "print(df.isnull().sum())\n",
    "```    \n",
    "    \n",
    "### 6. **Feature Engineering**\n",
    "   - **Yeni Dəyişənlərin Yaradılması**: Yeni dəyərləri yaratmaq üçün mövcud dəyişənləri birləşdirə bilərsiniz (məsələn, **TotalIncome** = **ApplicantIncome** + **CoapplicantIncome**).\n",
    "   - **Dəyişənlərin Seçilməsi**: Modeliniz üçün ən əhəmiyyətli olan dəyişənləri seçin.\n",
    "    \n",
    "**Yeni Dəyişənlərin Yaradılması:**\n",
    "```python\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "```    \n",
    "\n",
    "**Dəyişənlərin Seçilməsi (Feature Selection):**\n",
    "```python\n",
    "# Ən əhəmiyyətli xüsusiyyətləri müəyyən etmək üçün korelyasiya matrisi\n",
    "corr = df.corr()\n",
    "print(corr['Loan_Status'].sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "### 7. **Outlierların İdarə Edilməsi**\n",
    "   - **Outlier Analizi**: Outlierları aşkar edin və onların təsirini azaltmaq üçün uyğun üsullar tətbiq edin.\n",
    "\n",
    "**Outlier aşkarlanması və onların idarə edilməsi:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# LoanAmount sütunu üçün outlier-ları yoxla\n",
    "Q1 = df['LoanAmount'].quantile(0.25)\n",
    "Q3 = df['LoanAmount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Outlier-ları filtr etmə\n",
    "outliers = (df['LoanAmount'] < (Q1 - 1.5 * IQR)) | (df['LoanAmount'] > (Q3 + 1.5 * IQR))\n",
    "df = df[~outliers]\n",
    "```        \n",
    "             \n",
    "### 8. **Target Variableın İdarə Edilməsi**\n",
    "   - **Label Encoding**: Kreditin təsdiqi dəyişənini (Loan_Status) 0 və 1 ilə təmsil edin.\n",
    "        \n",
    "```python\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
    "```      \n",
    "        \n",
    "### 9. Data Split\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Loan_Status', axis=1)\n",
    "y = df['Loan_Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### 10. Cross-validation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV score:\", np.mean(cv_scores))\n",
    "```\n",
    "\n",
    "### 11. Feature Importance Analizi\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Xüsusiyyətlərin əhəmiyyəti\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "print(feature_importance)\n",
    "```\n",
    "\n",
    "Bu addımları izləyərək, loan eligibility prediction layihəsində data preparation mərhələsini tamamlaya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab60505b",
   "metadata": {},
   "source": [
    "## Men burda bezi data preparation isleri gormisem.  ne yaddan cixib elemimisem sen onlari izah ele ve kodu yaz. Yani sonra men model quracam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d7cd",
   "metadata": {},
   "source": [
    "Mən notebook-dan aşağıdakı data preparation addımlarını aşkar etdim. Gördüyünüz işlər bunlardır:\n",
    "\n",
    "### 1. **Verilənlərin Yüklənməsi**\n",
    "   İlk addım olaraq, verilənlər `loan_prediction.csv` faylından yüklənib və `df` DataFrame-i yaradılıb.\n",
    "\n",
    "### 2. **Sütun Adlarının və Məlumatın Araşdırılması**\n",
    "   `df.columns` və `df.info()` metodları istifadə edilərək verilənlərin sütun adları və strukturu araşdırılıb.\n",
    "\n",
    "### 3. **Verilənlərin İzlənməsi**\n",
    "   - **Dəyərlərin sayılması**: `Dependents` və `Loan_Status` sütunlarında dəyərlərin sayı yoxlanılıb (`value_counts`).\n",
    "   - **Balansın yoxlanılması**: `Loan_Status` sütununda balansın olub-olmadığı yoxlanılıb. Görünür, 192/612 və 422/612 hesabatları göstərir ki, balans problemi yoxdur.\n",
    "\n",
    "### 4. **Boş Dəyərlərin Yoxlanılması və Doldurulması**\n",
    "   - **Boş Dəyərlərin Araşdırılması**: `df.isnull().sum()` metodu ilə dataset-də boş dəyərlər yoxlanılıb.\n",
    "   - **Boş Dəyərlərin Doldurulması**: Aşağıdakı sütunlarda boş dəyərlər doldurulub:\n",
    "     - `Gender`: Mod (ən çox təkrarlanan) dəyər ilə doldurulub.\n",
    "     - `Married`, `Dependents`, `Self_Employed`, `Credit_History`: Bu sütunlar da mod dəyəri ilə doldurulub.\n",
    "\n",
    "### 5. **Hələ Yadınızdan Çıxmış Ola Bilən Data Preparation Addımları**\n",
    "   - **Outlier Detection**: Məsələn, `ApplicantIncome` və `LoanAmount` üçün outlier-lərin yoxlanılması və ya uyğunlaşdırılması (log transformasiya və s.).\n",
    "   - **Sütunların Normalizasiyası/Standartizasiyası**: `ApplicantIncome`, `LoanAmount` kimi numeric dəyişənlər üçün normalizasiya və ya standartizasiya aparıla bilər.\n",
    "   - **Categorical Encoding**: Kategorik dəyişənlər (`Gender`, `Married`, `Education`, `Property_Area`, və s.) üçün one-hot encoding və ya label encoding tətbiq oluna bilər.\n",
    "   - **Feature Engineering**: Yeni dəyişənlərin yaradılması, məsələn, `TotalIncome` (müraciət edən və birgə müraciət edənin gəliri toplamı).\n",
    "\n",
    "Bu addımları tətbiq etmək üçün sizə uyğun kodları təqdim edə bilərəm. Hansı xüsusi addımlar haqqında məlumat əldə etmək istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133173aa",
   "metadata": {},
   "source": [
    "Aşağıda, qeyd etdiyiniz dataya əlavə olaraq həyata keçirə biləcəyiniz data preparation addımlarını təqdim edirəm:\n",
    "\n",
    "### 1. **Outlier-ların Yoxlanılması və Müxtəlifləşdirilməsi**\n",
    "Numeric dəyişənlər (`ApplicantIncome`, `LoanAmount`) üçün outlier-ların olub-olmadığını yoxlayıb, ehtiyac varsa log transformasiya tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ApplicantIncome üçün outlier-ların yoxlanılması\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['ApplicantIncome'])\n",
    "plt.title('ApplicantIncome Outliers')\n",
    "\n",
    "# Log transformasiya ilə outlier-ların uyğunlaşdırılması\n",
    "df['Log_ApplicantIncome'] = np.log1p(df['ApplicantIncome'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_ApplicantIncome'])\n",
    "plt.title('Log Transformed ApplicantIncome')\n",
    "plt.show()\n",
    "\n",
    "# LoanAmount üçün eyni proseduru həyata keçirə bilərsiniz\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)  # Boş dəyərləri median ilə doldurmaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['LoanAmount'])\n",
    "plt.title('LoanAmount Outliers')\n",
    "\n",
    "df['Log_LoanAmount'] = np.log1p(df['LoanAmount'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_LoanAmount'])\n",
    "plt.title('Log Transformed LoanAmount')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. **Categorical Encoding (One-Hot Encoding)**\n",
    "Kategorik dəyişənlər üçün one-hot encoding tətbiq edərək, onları model üçün istifadə edə biləcəyiniz numerik formaya çevirmək.\n",
    "\n",
    "```python\n",
    "# One-hot encoding tətbiq etmək\n",
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents'], drop_first=True)\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 3. **Yeni Dəyişənlər Yaratmaq (Feature Engineering)**\n",
    "Müraciət edən şəxsin və birgə müraciət edən şəxsin gəlirini toplam olaraq yeni bir dəyişən yaratmaq.\n",
    "\n",
    "```python\n",
    "# TotalIncome adlı yeni bir dəyişən yaratmaq\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "\n",
    "# TotalIncome üçün log transformasiya tətbiq etmək\n",
    "df['Log_TotalIncome'] = np.log1p(df['TotalIncome'])\n",
    "\n",
    "# Vizual olaraq log transformasiyanın təsirini yoxlamaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['TotalIncome'])\n",
    "plt.title('TotalIncome Outliers')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_TotalIncome'])\n",
    "plt.title('Log Transformed TotalIncome')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4. **Numeric Dəyərlərin Normalizasiyası və ya Standartizasiyası**\n",
    "Numeric dəyişənlər üçün normalizasiya (Min-Max scaling) və ya standartizasiya (Z-score normalization) tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Numeric dəyişənləri müəyyənləşdirin\n",
    "numeric_features = ['Log_ApplicantIncome', 'Log_LoanAmount', 'Log_TotalIncome']\n",
    "\n",
    "# Standartizasiya tətbiq etmək\n",
    "scaler = StandardScaler()\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Bu addımların hamısı datanızı model üçün hazır etmək məqsədi daşıyır. Bu addımlardan sonra model qurma mərhələsinə keçə bilərsiniz. Hansısa spesifik addım haqqında daha çox məlumat və ya əlavə izah istəsəniz, mənə bildirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa7140f",
   "metadata": {},
   "source": [
    "## Mene paired sample t test et bu iki sutunu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eab99b",
   "metadata": {},
   "source": [
    "Paired sample t-test, eyni subyektlərin iki müstəqil dəyişən üzərində ölçülən nəticələrinin statistik olaraq əhəmiyyətli fərqini yoxlamaq üçün istifadə olunur. Lakin sizin nümunədə `Loan_Status` (binary) və `ApplicantIncome` (numeric continuous) dəyişənləri arasında birbaşa `paired t-test` tətbiq etmək məntiqli olmayacaq, çünki `Loan_Status` müstəqil dəyişən, `ApplicantIncome` isə davamlı dəyişəndir. \n",
    "\n",
    "Bununla belə, eyni subyektlərdə iki ölçüm arasındakı fərqi təhlil etmək üçün `paired t-test` istifadə oluna bilər. Məsələn, iki müxtəlif zaman nöqtəsində eyni subyektlərdəki `ApplicantIncome` dəyişəni ölçülə bilər. Bu halda, `paired t-test` aşağıdakı şəkildə tətbiq oluna bilər:\n",
    "\n",
    "### Python Kod Nümunəsi\n",
    "\n",
    "1. **Bir Dataset Yaradın**: İki ölçüm arasındakı fərqi təsvir etmək üçün iki fərqli gəlir sütunu yaradın.\n",
    "2. **Paired Sample t-Test Tətbiq Edin**: `scipy.stats` kitabxanasını istifadə edərək.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset yaratmaq üçün funksiyalar\n",
    "def generate_data(num_samples):\n",
    "    np.random.seed(42)\n",
    "    # Təsadüfi numeric continuous dəyərlər\n",
    "    applicant_income_1 = np.random.rand(num_samples) * 10000  # 0-10000 arasında gəlir\n",
    "    applicant_income_2 = applicant_income_1 + np.random.randn(num_samples) * 2000  # ikinci ölçüm, biraz fərqli\n",
    "    \n",
    "    # DataFrame yaratmaq\n",
    "    df = pd.DataFrame({\n",
    "        'ApplicantIncome_1': applicant_income_1,\n",
    "        'ApplicantIncome_2': applicant_income_2\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Data yaratmaq\n",
    "df = generate_data(1000)\n",
    "\n",
    "# Paired sample t-test tətbiq etmək\n",
    "t_statistic, p_value = ttest_rel(df['ApplicantIncome_1'], df['ApplicantIncome_2'])\n",
    "\n",
    "print(f'T-statistic: {t_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Korrelyasiya matrisini hesablamaq\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Korrelyasiya matrisini vizualizasiya etmək\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Açıklama\n",
    "\n",
    "1. **Data Yaratmaq**: `generate_data` funksiyası iki fərqli `ApplicantIncome` sütunu ilə bir dataset yaradır.\n",
    "2. **Paired Sample t-Test**: `ttest_rel` funksiyasını istifadə edərək iki müstəqil gəlir sütunu arasındakı fərqi test edir.\n",
    "3. **Korrelyasiya Matrisini Hesablamaq**: Yaradılan data üzərində korrelyasiya matrisini və heatmap vizualizasiyasını təqdim edir.\n",
    "\n",
    "Bu kod nümunəsi `ApplicantIncome` dəyişəninin iki fərqli ölçüm arasında əhəmiyyətli bir fərqin olub olmadığını yoxlamaq üçün `paired sample t-test` tətbiq edir. Əgər siz `Loan_Status` ilə `ApplicantIncome` arasında fərq yoxlamaq istəyirsinizsə, fərqli statistik testlər (məsələn, t-test, ANOVA) və ya metodlar istifadə etməlisiniz, çünki bu iki dəyişən fərqli növlərdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9586093",
   "metadata": {},
   "source": [
    "## plt.hist(np.log(df['CoapplicantIncome'] + 1))\n",
    "* niye +1 edirik?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d96f9",
   "metadata": {},
   "source": [
    "`np.log(df['CoapplicantIncome'] + 1)` əməliyyatında `+1` etməyin səbəbi, logaritma funksiyasının tərifinə görə sıfır və mənfi dəyərlərin logaritmı tərif olunmur. Əgər `CoapplicantIncome` sütununda sıfır dəyəri varsa, `np.log(0)` undefined (müəyyən edilməmiş) olacaq və bu da səhvlərə səbəb ola bilər.\n",
    "\n",
    "`+1` əlavə etməklə, bütün dəyərlər ən azı 1 olacaq və logaritmik transformasiyanın tətbiqi mümkün olacaq. Bu həmçinin dəyərləri bir qədər dəyişir, lakin ümumi tendensiyanı və ya datanın strukturu əhəmiyyətli dərəcədə dəyişmir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3901c039",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansina random_state vermek lazimdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bcab5",
   "metadata": {},
   "source": [
    "Boosting modellərində `random_state` parametri modelin təkrarlanabilməsini təmin etmək üçün istifadə olunur. Bu, təlim zamanı təsadüfi elementlərin (məsələn, təlim məlumatlarının təsadüfi seçilməsi, təsadüfi başlanğıc ağırlıqları və s.) nəzarət edilməsi üçün bir toxum dəyəri (seed value) təmin edir.\n",
    "\n",
    "### Hər hansı bir `random_state` dəyəri təyin edərkən:\n",
    "\n",
    "- **Sabit bir dəyər təyin etmək** (məsələn, `random_state=42`): Təlim və test proseslərinin hər dəfə eyni nəticəni verməsini təmin edər. Bu, modeli təkrarlamaq və nəticələri yoxlamaq üçün faydalıdır.\n",
    "  \n",
    "- **`random_state`-i `None` olaraq buraxmaq**: Modelin hər dəfə fərqli bir təsadüfi dəyər seçməsinə səbəb olar. Bu da modelin hər dəfə fərqli nəticələr verməsi ilə nəticələnir.\n",
    "\n",
    "### Fərqli Boosting Modellərində `random_state`:\n",
    "\n",
    "1. **XGBoost (`xgboost.XGBClassifier` və ya `xgboost.XGBRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Sabit bir dəyər təyin edərək (məsələn, `random_state=42`), modeli təkrarlamaq mümkündür.\n",
    "\n",
    "2. **LightGBM (`lightgbm.LGBMClassifier` və ya `lightgbm.LGBMRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Burada da sabit bir dəyər təyin etməklə modeli təkrarlamaq mümkündür.\n",
    "\n",
    "3. **CatBoost (`catboost.CatBoostClassifier` və ya `catboost.CatBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Eyni təkrarlanabilirlik üçün sabit bir dəyər istifadə edilə bilər.\n",
    "\n",
    "4. **AdaBoost (`sklearn.ensemble.AdaBoostClassifier` və ya `sklearn.ensemble.AdaBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Təkrarlanabilirlik üçün sabit bir dəyər verilməsi məsləhətlidir.\n",
    "\n",
    "### Hansı `random_state` təyin edilməlidir?\n",
    "\n",
    "- **Sabit dəyər olaraq `42`**: Bu, elm sahəsində çox yaygın olaraq istifadə olunur və təkrarlanabilirlik təmin etmək üçün yaxşı bir seçimdir.\n",
    "- **Başqa bir sabit dəyər**: Əgər xüsusi olaraq fərqli bir toxum dəyəri istəyirsinizsə, istənilən ədəd seçilə bilər.\n",
    "- **`None` olaraq buraxmaq**: Hər dəfə təsadüfi dəyərlər istəyirsinizsə, `random_state=None` olaraq buraxıla bilər.\n",
    "\n",
    "Nəticədə, hansı `random_state` istifadə edəcəyiniz sizin proyektinizin tələblərindən asılıdır. Əgər nəticələrin hər dəfə təkrarlanmasını istəyirsinizsə, sabit bir `random_state` dəyəri təyin etmək yaxşı bir təcrübədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f53ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad97c34",
   "metadata": {},
   "source": [
    "## [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "\n",
    "## Nece edim bu warning olmasin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee933e3",
   "metadata": {},
   "source": [
    "Bu xəbərdarlıqlar, `LightGBM` modelini təlim etdirərkən yaranır və bu xəbərdarlıqların əsas səbəbi budur ki, model təlim zamanı bəzi node-lar üçün (qovşaqlar üçün) mənfi və ya sıfır dəyərində \"gain\" əldə edir. Bu halda, əlavə bir split (qovşaqda yeni bir bölmə) yaratmaq mənasız olur, çünki bu heç bir yaxşılaşma gətirməyəcək.\n",
    "\n",
    "### Bu xəbərdarlıqların səbəbləri:\n",
    "\n",
    "1. **Məlumatların Paylanması**: Məlumatların dəyər spektri çox dar ola bilər və ya bəzi dəyərlər çox nadir ola bilər. Bu halda, hər hansı bir split üçün müsbət \"gain\" əldə etmək çətin ola bilər.\n",
    "\n",
    "2. **Parametr Dəyərləri**: Modelin parametr dəyərləri (məsələn, `min_child_samples`, `min_split_gain`, `max_depth`) çox sərt olaraq təyin edilmiş ola bilər, bu da modelin çox sayda split yaratmasına maneə ola bilər.\n",
    "\n",
    "3. **Overfitting**: Model təlim zamanı çox sayda split etməyə çalışarkən overfitting-i önləmək üçün bu xəbərdarlıqları verir.\n",
    "\n",
    "### Çözüm Yolları:\n",
    "\n",
    "1. **Parametrləri Yenidən Dəyərləndirin**:\n",
    "   - `min_child_samples` dəyərini azaldaraq daha çox split imkanı verə bilərsiniz.\n",
    "   - `min_split_gain` dəyərini azaldaraq daha kiçik split-ləri icazə verə bilərsiniz.\n",
    "   - `max_depth` dəyərini artırmaq da bəzi hallarda kömək edə bilər.\n",
    "\n",
    "2. **Məlumatları Təkrardan Analiz Edin**:\n",
    "   - Məlumatlarınızı yoxlayın və fərqli qruplar arasında balansı təmin etməyə çalışın.\n",
    "   - Ehtiyac varsa, məlumatları transformasiya edərək (məsələn, log transformasiyası) modelin daha yaxşı öyrənməsi üçün şərait yaradın.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Məlumat dəstinizə əlavə xüsusiyyətlər əlavə etməyi və ya bəzi xüsusiyyətləri yenidən yaratmağı nəzərdən keçirə bilərsiniz. Bu, modelin daha yaxşı öyrənməsinə kömək edə bilər.\n",
    "\n",
    "Bu xəbərdarlıqlar modelin nəticələrinə birbaşa mənfi təsir göstərməyə bilər, ancaq məlumatların və parametrlərin diqqətlə incələnməsi modelin performansını artırmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082dd5a4",
   "metadata": {},
   "source": [
    "LightGBM modelini qurarkən bu xəbərdarlıqların qarşısını almaq üçün bəzi parametrləri aşağıdakı kimi tənzimləyə bilərsiniz:\n",
    "\n",
    "1. **`min_split_gain`** dəyərini azaldaraq daha kiçik split-ləri icazə vermək.\n",
    "2. **`min_child_samples`** dəyərini artırmaq, beləliklə model yalnız müəyyən sayda nümunələr üçün split edər.\n",
    "3. **`max_depth`** dəyərini artırmaq və ya başqa dəyərlərini tənzimləməklə modelin daha yaxşı performans göstərməsini təmin etmək.\n",
    "\n",
    "Aşağıda bir nümunə LightGBM kodu verilmişdir:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',  # Klassifikasiya üçün\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,  # Split üçün minimum \"gain\"\n",
    "    min_child_samples=20,  # Split üçün minimum nümunə sayı\n",
    "    max_depth=7,  # Maksimum dərinlik\n",
    "    random_state=42  # Reproducibility üçün\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### İpuçları:\n",
    "- **`min_split_gain`**: Bu dəyəri artırmaq splitlərin daha da mənalı olmasını təmin edəcək, beləliklə bu xəbərdarlıqların sayı azalacaq.\n",
    "- **`min_child_samples`**: Bu dəyəri artırmaq, çox kiçik nümunələr üçün split yaratmamaq üçün istifadə edilə bilər.\n",
    "- **`max_depth`**: Dərinliyi artırmaq, modelin daha çox split etməsinə icazə verəcək, ancaq overfitting riskini də nəzərə almaq lazımdır.\n",
    "\n",
    "Bu parametrlər xəbərdarlıqların qarşısını ala bilər və ya sayını azalda bilər. Ayrıca, bu parametrləri layihənizin spesifik tələblərinə uyğun tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13485373",
   "metadata": {},
   "source": [
    "## yene warning gelir...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492efc3",
   "metadata": {},
   "source": [
    "LightGBM-də xəbərdarlıqları azaltmaq üçün `min_split_gain`, `min_child_samples` və `max_depth` kimi parametrləri dəyişdirmək kifayət etmədikdə, digər variantları da yoxlamaq olar. Xəbərdarlıqları tamamilə aradan qaldırmaq üçün əlavə parametrlərdən istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`verbosity` və ya `verbose` Parametrindən İstifadə Edin:**\n",
    "LightGBM modellərində `verbosity` (və ya `verbose`) parametrini istifadə edərək xəbərdarlıqların səviyyəsini tənzimləyə bilərsiniz. Xəbərdarlıqların qarşısını almaq üçün `verbosity` dəyərini `-1` təyin edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1  # Warning-ları tamamilə söndürmək\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### 2. **`early_stopping_rounds` İstifadə Edərək Overfitting-i Azaltmaq:**\n",
    "`early_stopping_rounds` parametrini istifadə edərək, təlim zamanı modelin lazımsız splitlər etməsinin qarşısını ala bilərsiniz. Bu, xəbərdarlıqların azalmasına kömək edə bilər.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "```\n",
    "\n",
    "### 3. **`min_child_weight` Parametrini Dəyişdirin:**\n",
    "`min_child_weight` parametrini artırmaqla xəbərdarlıqların qarşısını almaq mümkündür. Bu, splitlərin daha da məntiqli olmasını təmin edəcək.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=1.0,  # Dəyəri artırmaqla xəbərdarlıqların qarşısını almaq\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri birləşdirərək və tənzimləyərək xəbərdarlıqların sayını azalda bilərsiniz. Ancaq `verbosity=-1` parametri bu xəbərdarlıqları tamamilə gizlədəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cfcc75a",
   "metadata": {},
   "source": [
    "## SVC \n",
    "* kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, default=’rbf’,  \n",
    "* C : float,  default=1.0,  \n",
    "* degree: int, default=3, \n",
    "* gamma: {‘scale’, ‘auto’} or float, default=’scale’  \n",
    "### Bu parameterleri haqqinda genis meulumat yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd3460",
   "metadata": {},
   "source": [
    "SVC (Support Vector Classifier) modelində **kernel** funksiyası, verilənlər arasındakı qeyri-xətti əlaqələri müəyyən etmək üçün istifadə olunur. Kernel funksiyası, verilənlərin daha yüksək ölçülü bir fəzaya proyeksiya edilməsini təmin edir, beləliklə verilənlər xətti olaraq ayrıla bilməyən hallarda belə effektiv bir ayrıcı xətt tapıla bilir.\n",
    "\n",
    "### Kernel Növləri\n",
    "\n",
    "#### 1. **Linear Kernel (‘linear’)**\n",
    "- **Təsvir:** Verilənlər xətti olaraq ayrılabilirsə, linear kernel istifadə olunur. Xətti kernel SVC modelində xüsusiyyətlərin (features) xətti birləşməsini hesablayır.\n",
    "- **Tətbiq Sahəsi:** Sadə və xətti olan verilənlər üçün effektivdir. Məsələn, eyni tipli verilənlər iki sinif arasında çoxluqda ayrılabilirsə, linear kernel uyğundur.\n",
    "- **Riyazi İzah:** Xətti kernel funksiyası \\( K(x_i, x_j) = x_i^T x_j \\) olaraq göstərilir. Burada \\( x_i \\) və \\( x_j \\) verilən nöqtələrdir.\n",
    "\n",
    "#### 2. **Polynomial Kernel (‘poly’)**\n",
    "- **Təsvir:** Polinomial kernel, verilənlər arasında qeyri-xətti əlaqələri təyin etmək üçün istifadə olunur. Bu kernel funksiyası, xüsusiyyətlərin çoxsahəli kombinasiya edilərək daha kompleks bir model yaratmağa imkan verir.\n",
    "- **Tətbiq Sahəsi:** Verilənlərdə dərəcəli bir münasibət varsa (məsələn, bir sinifin xüsusiyyətləri digərindən polinomial olaraq asılıdırsa), bu kernel effektivdir.\n",
    "- **Riyazi İzah:** Polinomial kernel funksiyası \\( K(x_i, x_j) = (x_i^T x_j + c)^d \\) olaraq göstərilir. Burada \\( d \\) polinomial dərəcədir, \\( c \\) isə sabitdir.\n",
    "\n",
    "#### 3. **RBF Kernel (Radial Basis Function, ‘rbf’)**\n",
    "- **Təsvir:** RBF kernel funksiyası, xüsusən verilənlər arasında kompleks və qeyri-xətti əlaqələr varsa, çox güclü bir seçimdir. Bu kernel funksiyası, hər bir verilən nöqtənin məsafəsini nəzərə alaraq, daha yüksək ölçülü fəzada onların təsir sahəsini genişləndirir.\n",
    "- **Tətbiq Sahəsi:** Çoxlu xüsusiyyətləri olan və onların arasında əlaqənin mürəkkəb olduğu verilənlərdə RBF kernel çox istifadə edilir.\n",
    "- **Riyazi İzah:** RBF kernel funksiyası \\( K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\) olaraq göstərilir. Burada \\( \\gamma \\) kernelin təsir sahəsini təyin edən parametrdir.\n",
    "\n",
    "#### 4. **Sigmoid Kernel (‘sigmoid’)**\n",
    "- **Təsvir:** Sigmoid kernel, sinir şəbəkələrindəki aktivasiya funksiyasına bənzər bir funksiyadır və iki verilən nöqtə arasındakı məsafəni qeyri-xətti olaraq modelləşdirir.\n",
    "- **Tətbiq Sahəsi:** Sigmoid kernel, xüsusən logistik əlaqələr olan verilənlərdə effektivdir. Bu kernel logistik reqlamentasiya və sinir şəbəkələri ilə oxşarlıq göstərir.\n",
    "- **Riyazi İzah:** Sigmoid kernel funksiyası \\( K(x_i, x_j) = \\tanh(\\alpha x_i^T x_j + c) \\) olaraq göstərilir. Burada \\( \\alpha \\) və \\( c \\) tənzimləmə parametrləridir.\n",
    "\n",
    "#### 5. **Precomputed Kernel (‘precomputed’)**\n",
    "- **Təsvir:** Bu seçimdə kernel funksiyası əvvəlcədən hesablanır və SVC modelinə ötürülür. Bu, xüsusən xüsusi kernel funksiyaları istifadə edildikdə və ya verilənlərin çox olduğu hallarda istifadə olunur.\n",
    "- **Tətbiq Sahəsi:** Kernel matrisini əvvəldən hesablamağa ehtiyac olduğu hallarda və ya xüsusi kernel funksiyaları tətbiq edilən vəziyyətlərdə istifadə edilir.\n",
    "\n",
    "### Kernel Funksiyalarının Seçimi\n",
    "\n",
    "- **Linear kernel** verilənlərdə sadə və xətti bir əlaqə varsa uyğundur.\n",
    "- **Polynomial kernel** qeyri-xətti, lakin mürəkkəb olmayan münasibətləri modelləşdirmək üçün istifadə olunur.\n",
    "- **RBF kernel** daha mürəkkəb və qeyri-xətti verilənlərdə optimal performans göstərir.\n",
    "- **Sigmoid kernel** logistik münasibətləri modelləşdirmək üçün yaxşı seçimdir.\n",
    "- **Precomputed kernel** xüsusi kernel funksiyaları istifadə edilərkən və ya kernel matrixini əvvəlcədən hesablamaq istədikdə istifadə olunur.\n",
    "\n",
    "### Qrafik Təsviri\n",
    "Kernel funksiyalarının hər birinin təsirini göstərən qrafiklərlə izah edə bilərəm. Məsələn, verilənlərə tətbiq edilən linear, polynomial və RBF kernel funksiyalarının qərar sərhədlərini necə dəyişdiyini göstərə bilərəm. Bu, müxtəlif kernel növlərinin verilənlər üzərində necə işlədiyini və fərqli təsir göstərdiyini daha aydın şəkildə başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb9972",
   "metadata": {},
   "source": [
    "## C parametri\n",
    "C parametri, Support Vector Classifier (SVC) modelində cəza tənzimləyicisi (regularization parameter) olaraq istifadə olunur. Bu parametr modelin margin (ayırıcı xəttin yaxınlığında olan nümunələr) genişliyi ilə səhvlər arasında bir balans yaradır. C parametri aşağıdakı iki əsas amilə təsir edir:\n",
    "\n",
    "### 1. **Margin Genişliyi və Klassifikasiya Səhvləri**\n",
    "C parametri marginin genişliyini və modelin səhvlərə olan dözümlülüyünü tənzimləyir:\n",
    "\n",
    "- **Kiçik C Dəyəri (C < 1):** \n",
    "  - Model daha geniş margin yaratmağa çalışır. Bu, daha çox səhv qəbul edilməsinə səbəb olur, lakin model daha ümumiləşdirilmiş (generalized) olur. Yəni, model overfitting (həddindən artıq uyğunlaşma) riskini azaldır.\n",
    "  - Məsələn, qərar sərhədi çox yaxın olan iki fərqli sinif varsa, kiçik C dəyəri ilə model bu sərhədin hər iki tərəfindəki bəzi səhvləri göz ardı edərək daha hamar bir ayrıcı xətt çəkər.\n",
    "\n",
    "- **Böyük C Dəyəri (C > 1):**\n",
    "  - Model səhvləri minimuma endirmək üçün daha dar margin yaratmağa çalışır. Bu, modelin training setinə çox uyğunlaşmasına və overfitting etməsinə səbəb ola bilər.\n",
    "  - Böyük C dəyərində model hər bir səhvi ciddi şəkildə cəzalandırır və səhvləri aradan qaldırmaq üçün qərar sərhədini həmin səhvlərə uyğunlaşdırır.\n",
    "\n",
    "### 2. **Overfitting və Underfitting**\n",
    "- **Kiçik C Dəyəri:** Underfitting riski artır, yəni model verilənlərə kifayət qədər uyğunlaşmır və bəzi məlumat nümunələrini düzgün klassifikasiya edə bilmir.\n",
    "- **Böyük C Dəyəri:** Overfitting riski artır, yəni model verilənlərə həddindən artıq uyğunlaşır və yeni verilənlərə ümumiləşdirməkdə çətinlik çəkir.\n",
    "\n",
    "### Qrafik İzahı\n",
    "Qrafikdə C parametri ilə margin genişliyi arasındakı əlaqəni belə göstərə bilərik:\n",
    "\n",
    "- **Kiçik C dəyərində** margin daha geniş olacaq, lakin bəzi nümunələr marginin içində qalacaq (yəni səhvlər göz ardı ediləcək).\n",
    "- **Böyük C dəyərində** margin daha dar olacaq, nümunələr marginə çox yaxın olacaq və model hər bir nümunəni düzgün klassifikasiya etməyə çalışacaq.\n",
    "\n",
    "Bu izahı qrafiklə də göstərməyimi istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d7378",
   "metadata": {},
   "source": [
    "## Margin genisliyi\n",
    "**Margin**, Support Vector Classifier (SVC) modelində verilənlər sinifləri arasındakı ən yaxın nöqtələrə qədər olan məsafədir. Daha dəqiq desək, margin iki sinif arasında çəkilmiş ayrıcı xətt (decision boundary) ilə həmin siniflərə aid olan ən yaxın nümunələr (support vectors) arasındakı məsafəni ifadə edir. SVC modelində əsas məqsəd bu margin-i maksimuma çatdırmaqdır, çünki daha böyük bir margin ümumiləşdirilmənin daha yaxşı olmasına və modelin daha yaxşı performans göstərməsinə səbəb ola bilər.\n",
    "\n",
    "### Margin-in Əsas Məqamları:\n",
    "\n",
    "1. **Maksimum Margin:** \n",
    "   - SVC modelinin əsas məqsədi verilən iki sinif arasındakı margin-i maksimuma çatdırmaqdır. Yəni, ayrıcı xətt bu sinifləri mümkün qədər uzaq tutmalıdır. Bu, modelin daha çox ümumiləşdirmə qabiliyyətinə sahib olmasına kömək edir.\n",
    "   - Məsələn, aşağıdakı şəkildə iki sinif arasındakı ayrıcı xətt ən geniş margin-ə malikdir və bu xətt margin-i maksimum edən ayrıcı xətdir.\n",
    "\n",
    "2. **Support Vectors:** \n",
    "   - Margin-i təyin edən verilən nöqtələrə **support vectors** deyilir. Bu nöqtələr margin-in sərhədlərini müəyyənləşdirir. SVC modeli bu nöqtələr üzərində işləyir və digər nöqtələr margin-in genişliyini təsir etmir.\n",
    "   - Əgər support vectors modelləşdirilmə zamanı dəyişərsə, margin və ayrıcı xətt də dəyişə bilər.\n",
    "\n",
    "3. **Margin-in Genişliyi və Performans:**\n",
    "   - Daha geniş bir margin, verilənləri daha yaxşı ayırır və bu, overfitting riskini azaldır, çünki model daha ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "   - Əgər margin çox dardırsa, model verilənlərə çox yaxınlaşır və overfitting riski artır, yəni model training setində yaxşı performans göstərsə də, yeni verilənlərdə zəif performans göstərə bilər.\n",
    "\n",
    "4. **Margin və `C` Parametri:**\n",
    "   - `C` parametri margin-in ölçüsünü idarə edir. Kiçik `C` dəyəri daha geniş margin-i təşviq edir və daha az səhv nümunələrə icazə verir. Yüksək `C` dəyəri isə margin-i daraldır və modelin verilənlərə daha çox uyğunlaşmasına səbəb olur, lakin overfitting riski artır.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "Bir qrafikdə margin-i təsvir etmək olar:\n",
    "- İki sinif nöqtələrinin olduğu bir qrafikdə, bir ayrıcı xətt (decision boundary) çəkilir və bu xəttin hər iki tərəfində margin göstərilir.\n",
    "- Ayrıcı xəttə ən yaxın olan nöqtələr support vectors olaraq göstərilir.\n",
    "- Margin-in ölçüsü bu nöqtələr ilə ayrıcı xətt arasındakı məsafə ilə təyin olunur.\n",
    "\n",
    "İstəyirsinizsə, margin və support vectors-i göstərən belə bir qrafik hazırlaya bilərəm. Bu, margin-in necə işlədiyini və onun model performansına necə təsir etdiyini daha aydın izah edəcək."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad67c5",
   "metadata": {},
   "source": [
    "## degree parametri\n",
    "`degree` parametri, Support Vector Classifier (SVC) modelində yalnız **polinomial kernel** (`'poly'`) üçün istifadə edilir və polinomial kernelin dərəcəsini (degree) təyin edir. Bu parametr verilənlərin xüsusiyyətlərini daha yüksək dərəcədə olan bir fəzaya proyeksiya etməyə imkan verir, beləliklə verilənlərdəki qeyri-xətti əlaqələri daha yaxşı modelləşdirmək mümkündür.\n",
    "\n",
    "### `degree` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Polinomial Kernelin Kompleksliyi**\n",
    "- **Kiçik dərəcə (d = 2, 3):**\n",
    "  - Nisbətən sadə, lakin qeyri-xətti əlaqələri modelləşdirmək üçün uyğundur.\n",
    "  - Məsələn, iki sinif arasında sadə, lakin qeyri-xətti bir sərhəd varsa, dərəcə 2 və ya 3 istifadə edilə bilər. Bu vəziyyətdə polinomial kernel ikinci və ya üçüncü dərəcəli polinomlara əsaslanacaq.\n",
    "\n",
    "- **Böyük dərəcə (d > 3):**\n",
    "  - Daha kompleks və mürəkkəb əlaqələri modelləşdirmək üçün istifadə olunur.\n",
    "  - Dərəcənin artırılması, kernel funksiyasının daha yüksək dərəcəli polinomlara əsaslanmasını təmin edir və beləliklə, çoxsaylı xüsusiyyətlər arasındakı mürəkkəb əlaqələri modelləşdirir.\n",
    "\n",
    "#### 2. **Overfitting Riski**\n",
    "- **Yüksək `degree` (dərəcə) dəyəri:**\n",
    "  - Overfitting riski artır, çünki model training datasetindəki hər bir nümunəni düzgün uyğunlaşdırmağa çalışır. Bu, modelin training setinə çox yaxşı uyğunlaşması, lakin yeni verilənlərdə ümumiləşdirmədə çətinlik çəkməsi deməkdir.\n",
    "  - Məsələn, çox yüksək dərəcəli polinomial kernel hər bir nümunə üçün fərdi qərar sərhədləri çəkə bilər, lakin bu, modelin daha sonra səhvlərlə üzləşməsinə səbəb ola bilər.\n",
    "\n",
    "- **Aşağı `degree` (dərəcə) dəyəri:**\n",
    "  - Underfitting riski artır, çünki model verilənlərdəki daha mürəkkəb əlaqələri düzgün modelləşdirə bilmir. Bu halda, model siniflər arasındakı daha kompleks münasibətləri nəzərə almadan sadə bir sərhəd çəkə bilər.\n",
    "\n",
    "#### 3. **Dərəcənin Seçilməsi**\n",
    "- `degree` parametrini seçərkən verilənlərin xüsusiyyətlərinə və onların arasındakı əlaqələrə diqqət yetirmək vacibdir.\n",
    "- Nəzərə alınmalıdır ki, dərəcənin artırılması modelin kompleksliyini və hesablama tələblərini artıracaq.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Aşağı dərəcə** (d = 2): Polinomial kernel, iki sinif arasında nisbətən sadə və əyri bir sərhəd çəkəcək.\n",
    "- **Orta dərəcə** (d = 3): Kernel funksiyası daha mürəkkəb, lakin hələ də idarə edilə bilən bir sərhəd çəkəcək.\n",
    "- **Yüksək dərəcə** (d > 3): Kernel funksiyası çox mürəkkəb bir sərhəd çəkəcək, bu da overfittingə səbəb ola bilər.\n",
    "\n",
    "Bu təsiri qrafiklərlə göstərmək üçün, müxtəlif dərəcələr üçün polinomial kernelin necə işlədiyini qrafiklərlə göstərə bilərəm. Bu cür qrafiklər dərəcə parametri ilə decision boundary-nin necə dəyişdiyini daha yaxşı başa düşməyə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9557d7f",
   "metadata": {},
   "source": [
    "## gamma parametri\n",
    "`gamma` parametri, Support Vector Classifier (SVC) modelində **RBF (Radial Basis Function)**, **polinomial**, və **sigmoid** kernel funksiyaları üçün istifadə edilir. Bu parametr, verilənlər arasında məsafənin təsirini tənzimləyir və modelin qərar sərhədlərinin nə qədər yaxın və ya geniş olacağını müəyyən edir.\n",
    "\n",
    "### `gamma` Parametrinin Əsas Funksiyaları\n",
    "\n",
    "#### 1. **Verilənlər Arasındakı Məsafə**\n",
    "- **Yüksək `gamma` dəyəri:**\n",
    "  - Verilənlərdəki hər bir nöqtənin təsir sahəsini daraldır, yəni hər bir nöqtənin təsiri çox yaxın məsafədə olur. Bu, modelin verilənlərə çox yaxşı uyğunlaşmasına və beləliklə, overfitting etməsinə səbəb ola bilər.\n",
    "  - Bu halda, model hər bir nümunəni çox yaxından izləyir və qərar sərhədləri çox qısa məsafələrdə dəyişir.\n",
    "  \n",
    "- **Aşağı `gamma` dəyəri:**\n",
    "  - Verilənlərdəki nöqtələrin təsir sahəsi genişlənir, yəni hər bir nöqtənin təsiri daha uzaq məsafədə hiss olunur. Bu, modelin daha hamar və ümumiləşdirilmiş bir sərhəd yaratmasına imkan verir.\n",
    "  - Bu vəziyyətdə, model fərdi nümunələrə çox yaxınlaşmır və daha geniş, ümumiləşdirilmiş bir qərar sərhədi çəkir.\n",
    "\n",
    "#### 2. **Overfitting və Underfitting**\n",
    "- **Yüksək `gamma`:** \n",
    "  - Model verilənlərə çox uyğunlaşır (overfitting), bu da training setində yüksək dəqiqlik əldə edə bilər, lakin yeni verilənlərə qarşı zəif performans göstərə bilər.\n",
    "  - Qərar sərhədi çox qarmaqarışıq və mürəkkəb olur, belə ki, hətta kiçik fərqlər də sərhədi dəyişə bilər.\n",
    "  \n",
    "- **Aşağı `gamma`:** \n",
    "  - Model verilənlərə kifayət qədər uyğunlaşmır (underfitting), bu da training setində aşağı dəqiqlik, lakin yeni verilənlərə qarşı daha yaxşı ümumiləşdirmə ilə nəticələnə bilər.\n",
    "  - Qərar sərhədi daha sadə və hamar olur.\n",
    "\n",
    "### `gamma` Parametrinin Dəyərləri\n",
    "\n",
    "#### 1. **‘scale’ (default dəyəri):**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına və məlumatların yayılmasına əsasən avtomatik olaraq təyin olunur.\n",
    "  - Riyazi olaraq, `scale` dəyəri $  \\frac{1}{n \\times \\text{X.var}}  $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayı, `X.var` isə məlumatın dispersiyasını (variance) ifadə edir.\n",
    "  - Bu, orta bir gamma dəyəri verir və modelin çox uyğunlaşmasının (overfitting) qarşısını almaq üçün yaxşı balanslaşdırılmışdır.\n",
    "\n",
    "#### 2. **‘auto’:**\n",
    "  - `gamma` dəyəri verilənlərdəki xüsusiyyətlərin sayına (n_features) əsasən təyin olunur.\n",
    "  - Riyazi olaraq, `auto` dəyəri $ \\frac{1}{n} $ kimi hesablanır. Burada `n` xüsusiyyətlərin sayıdır.\n",
    "  - Bu, daha az xüsusiyyətli verilənlər üçün uyğundur və daha geniş təsir sahəsi verir. Lakin böyük datasetlərdə underfitting riski artır.\n",
    "\n",
    "#### 3. **Float dəyəri:**\n",
    "  - `gamma` dəyərini özünüz təyin edə bilərsiniz. Məsələn, `gamma=0.1` və ya `gamma=1.0` kimi.\n",
    "  - Bu, verilənlərə xüsusi uyğunlaşdırılmış gamma dəyəri ilə daha dəqiq idarə etmə imkanı verir.\n",
    "\n",
    "### Qrafik Təsviri ilə İzah\n",
    "\n",
    "- **Yüksək `gamma` dəyəri:** Qərar sərhədi çox mürəkkəb və dar olacaq, hər bir nümunəyə uyğunlaşaraq sərhədin tez-tez dəyişməsinə səbəb olacaq.\n",
    "- **Aşağı `gamma` dəyəri:** Qərar sərhədi daha geniş və hamar olacaq, fərdi nümunələrin təsirindən daha az asılı olacaq.\n",
    "\n",
    "Bu təsirləri qrafiklərlə də göstərə bilərəm. `gamma` parametrinin fərqli dəyərləri üçün decision boundary-lərin necə dəyişdiyini qrafiklərlə təsvir edə bilərəm. Bu, `gamma` parametrinin modelin performansına necə təsir etdiyini daha aydın başa düşməyinizə kömək edəcək.\n",
    "\n",
    "Bu qrafikləri yaratmağımı istəyirsiniz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1eae8ac",
   "metadata": {},
   "source": [
    "## Cross validation-a X, Y verilmelidir, yoxsa X_train, Y_train ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe845f",
   "metadata": {},
   "source": [
    "Cross-validation (çarpaz doğrulama) prosesində modelin performansını qiymətləndirmək üçün **orijinal verilənlər dəsti** (`X` və `Y`) istifadə olunur. Bu prosesdə, orijinal verilənlər dəsti bir neçə alt dəstəyə bölünür, daha sonra bu alt dəstələrdən biri test dəsti kimi istifadə edilir, qalanları isə train (öyrədici) dəsti kimi istifadə olunur.\n",
    "\n",
    "### Ətraflı İzahetmə\n",
    "\n",
    "- **X və Y:** Bu, sizin bütün verilənlər dəstinizdir. `X` bağımsız dəyişənləri (features) və `Y` isə bağımlı dəyişəni (target, label) təmsil edir.\n",
    "\n",
    "- **Cross-Validation Prosesində:**\n",
    "  - Verilənlər `k` alt dəstəyə bölünür (`k`-fold cross-validation).\n",
    "  - Hər dövrədə (`fold`) bir alt dəstə test dəsti kimi istifadə olunur, qalan `k-1` alt dəstə isə train dəsti kimi istifadə olunur.\n",
    "  - Bu dövrə `k` dəfə təkrarlanır və hər dəfə fərqli bir alt dəstə test dəsti kimi istifadə edilir.\n",
    "  - Bu prosesin nəticəsində modelin performansı hər bir fold üçün ölçülür və son nəticə olaraq bu performans ölçülərinin ortalaması götürülür.\n",
    "\n",
    "**Nəticə:**\n",
    "Cross-validation tətbiq edərkən, bütün verilənlər dəstinizi (`X` və `Y`) funksiyaya ötürməlisiniz. Model bu verilənlər dəstini daxildə train və test dəstlərinə böləcək və cross-validation prosesini icra edəcək. Beləliklə, siz train/test bölünməsini özünüz etmirsiniz; bunu cross-validation avtomatik olaraq edir.\n",
    "\n",
    "### İstifadə Nümunəsi:\n",
    "\n",
    "Python-da `sklearn` kitabxanasından `cross_val_score` funksiyasını istifadə edərkən, verilənlər dəstini aşağıdakı kimi daxil edirsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Bütün verilənlər dəstinizi istifadə edin\n",
    "model = SVC(kernel='linear')\n",
    "scores = cross_val_score(model, X, Y, cv=5)\n",
    "```\n",
    "\n",
    "Burada `X` və `Y` tam verilənlər dəstidir və `cv=5` isə 5-fold cross-validation həyata keçirdiyinizi göstərir. Bu funksiyada `cross_val_score` verilənlər dəstini avtomatik olaraq train və test dəstlərinə böləcək və cross-validation-u icra edəcək.\n",
    "\n",
    "**Diqqət:** Əgər `X_train` və `Y_train` artıq bölünmüş verilənlər dəstinə sahibsinizsə və cross-validation tətbiq etmək istəyirsinizsə, o zaman həmin verilənlər dəstini də `X` və `Y` kimi istifadə edə bilərsiniz, lakin bu halda yalnız həmin bölünmüş train dəstiniz üzərində cross-validation icra ediləcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64e706b",
   "metadata": {},
   "source": [
    "## Code izah ele?\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "# define the mask to set the values in the upper triangle to True\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88052caa",
   "metadata": {},
   "source": [
    "Bu kod parçası bir korelyasiya heatmap-i (istilik xəritəsi) yaratmaq üçün istifadə olunur. İndi hər sətirə nəzər salaq:\n",
    "\n",
    "### 1. Şəkil ölçüsünü təyin edir:\n",
    "```python\n",
    "plt.figure(figsize=(16, 12))\n",
    "```\n",
    "- `figsize` parametri vasitəsilə çıxan şəkilin ölçüsünü (en və hündürlük) təyin edir. Burada 16x12 ölçülərində bir şəkil yaradır.\n",
    "\n",
    "### 2. Üst üçbucağı gizlətmək üçün maska yaradılır:\n",
    "```python\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n",
    "```\n",
    "- Bu sətir korelyasiya matrisinin üst üçbucaqlı hissəsini gizlətmək üçün bir maska yaradır. \n",
    "- `np.ones_like(df.corr())` matrisin bütün elementlərini 1 ilə doldurur.\n",
    "- `np.triu` funksiyası isə bu 1-lik matrisin yuxarı üçbucağını saxlayır (alt üçbucağını 0 edir).\n",
    "- Bu maska, korelyasiya matrisinin yarısını (üst üçbucağı) gizlədərək yalnız alt yarısını görməyə imkan verir.\n",
    "\n",
    "### 3. Korelyasiya heatmap-i yaradılır:\n",
    "```python\n",
    "heatmap = sns.heatmap(round(df.corr(),1), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "```\n",
    "- `df.corr()` DataFrame-in sütunları arasındakı korelyasiya matrisini qaytarır.\n",
    "- `round(df.corr(),1)` korelyasiya dəyərlərini 1 ondalık rəqəmlə yuvarlayır.\n",
    "- `mask=mask` parametri ilə yaratdığınız maska tətbiq edilir, beləliklə üst üçbucaqdakı dəyərlər gizlədilir.\n",
    "- `vmin` və `vmax` parametrləri heatmap-də minimum və maksimum dəyərləri təyin edir (burada -1 və 1 arasındadır).\n",
    "- `annot=True` parametri hüceyrələrin içində dəyərlərin göstərilməsini təmin edir.\n",
    "- `cmap='BrBG'` rəng xəritəsini təyin edir. Burada \"BrBG\" palitrası istifadə olunur.\n",
    "\n",
    "### 4. Heatmap başlığı və formatlama:\n",
    "```python\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16)\n",
    "```\n",
    "- `set_title` funksiyası heatmap-in başlığını \"Correlation Heatmap\" olaraq təyin edir.\n",
    "- `fontdict={'fontsize':18}` başlığın şrift ölçüsünü 18 olaraq təyin edir.\n",
    "- `pad=16` başlıq ilə heatmap arasında boşluğun ölçüsünü təyin edir.\n",
    "\n",
    "Bu kod ilə, DataFrame-in sütunları arasındakı korelyasiyaları asanlıqla görsəl formada təqdim edə bilərsiniz. Korelyasiya heatmap-ləri, xüsusən dəyişənlər arasındakı əlaqələri təhlil etmək üçün faydalıdır. Yuxarı üçbucağı maskalamaq isə məlumatın təkrarlanmasını və vizual qarışıqlığı azaldır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b884cb07",
   "metadata": {},
   "source": [
    "## iki feature arasinda corr 1 dir. bu ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48708488",
   "metadata": {},
   "source": [
    "İki dəyişən (feature) arasındakı korelyasiyanın (corr) 1 olması onların tam müsbət əlaqədə olduğunu göstərir. Bu vəziyyətin mənası aşağıdakılardır:\n",
    "\n",
    "1. **Tam Müsbət Xətti Əlaqə:** Korelyasiya əmsalı 1 olan iki dəyişən bir-birinə tam xətti şəkildə bağlıdır. Yəni, bir dəyişən artdıqca, digəri də eyni nisbətdə artır. Məsələn, bir dəyişən 10% artarsa, digəri də mütləq şəkildə 10% artacaq.\n",
    "\n",
    "2. **Biri Digərini Tamamilə Təyin Edir:** Əgər iki dəyişən arasında korelyasiya 1-dirsə, bu o deməkdir ki, bir dəyişənin dəyərləri digər dəyişənin dəyərlərindən tamamilə asılıdır. Bir dəyişən digərini eyni şəkildə təkrarlayır və ya onun sadə bir xətti funksiyasıdır.\n",
    "\n",
    "3. **Multikollinearlıq Problemi:** Əgər bir modeldə iki dəyişən arasında korelyasiya 1-dirsə, bu multikollinearlıq problemi yarada bilər. Multikollinearlıq modelin performansını və etibarlılığını mənfi təsir edə bilər, çünki dəyişənlərdən biri digərini tam əvəz edir, və nəticədə modeldə lazımsız təkrarlanma yaranır.\n",
    "\n",
    "### Nəticə:\n",
    "Korelyasiyanın 1 olması iki dəyişənin eyni məlumatı daşıdığı mənasına gəlir. Birini modeldə saxlamaq kifayətdir; ikincisi isə redundant ola bilər və modelin performansını yaxşılaşdırmaq üçün çıxarılmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e951b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4c22e4",
   "metadata": {},
   "source": [
    "## Bes categoryc deyerler arasinda Multikollinearlıq nece teyin etmek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4935c",
   "metadata": {},
   "source": [
    "Kategorik (category) dəyişənlər üçün tam uyğunluq və ya çox oxşarlığı müəyyən etmək numeric (rəqəmsal) dəyişənlərdəki kimi sadə deyil, lakin müəyyən üsullar və yanaşmalar mövcuddur:\n",
    "\n",
    "### 1. **Crosstab (Çapraz cədvəl) və ya Contingency Table:**\n",
    "   - İki və ya daha çox kategorik dəyişəni analiz etmək üçün `pd.crosstab()` funksiyasını istifadə edə bilərsiniz. Bu, dəyişənlər arasındakı əlaqələri görmək üçün faydalıdır.\n",
    "   - Əgər iki kategorik dəyişənin crosstab cədvəli eynidirsə, bu onların bir-birinə çox oxşar olduğunu göstərir.\n",
    "   - Məsələn, eyni dəyərləri ehtiva edən iki dəyişənin bütün crosstab dəyərləri (yəni sıxlıqları) eyni olacaq.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   ```\n",
    "\n",
    "### 2. **Khi-kvadrat Testi (Chi-Square Test):**\n",
    "   - Khi-kvadrat testi iki kategorik dəyişənin bir-birindən asılı olub-olmadığını yoxlamaq üçün istifadə olunur. Əgər iki dəyişənin arasında çox güclü bir əlaqə varsa (yüksək Chi-square dəyəri və çox aşağı p-value), bu onların bir-birinə çox oxşar olduğuna işarə edə bilər.\n",
    "   \n",
    "   ```python\n",
    "   from scipy.stats import chi2_contingency\n",
    "   contingency_table = pd.crosstab(df['category_feature1'], df['category_feature2'])\n",
    "   chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "   ```\n",
    "\n",
    "### 3. **Mutual Information (Özəl İnformasiya):**\n",
    "   - Mutual Information metodu kategorik dəyişənlər arasında nə qədər informasiya paylaşdığını ölçür. Yüksək mutual information dəyəri onların güclü bir əlaqəyə malik olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.feature_selection import mutual_info_classif\n",
    "   mi = mutual_info_classif(df[['category_feature1']], df['category_feature2'])\n",
    "   print(mi)\n",
    "   ```\n",
    "\n",
    "### 4. **Label Encoding və Korelyasiya:**\n",
    "   - Kategorik dəyişənləri öncə `Label Encoding` və ya `One-Hot Encoding` ilə rəqəmsal formaya çevirərək, daha sonra onların arasında korelyasiya təhlili apara bilərsiniz. Bu, xüsusilə ordinal (sıralı) kategorik dəyişənlər üçün daha mənalı ola bilər.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "   le = LabelEncoder()\n",
    "   df['encoded_feature1'] = le.fit_transform(df['category_feature1'])\n",
    "   df['encoded_feature2'] = le.fit_transform(df['category_feature2'])\n",
    "   \n",
    "   correlation = df[['encoded_feature1', 'encoded_feature2']].corr()\n",
    "   ```\n",
    "\n",
    "### 5. **Jaccard Similarity:**\n",
    "   - İki kategorik dəyişənin kəsişməsini və birləşməsini müqayisə etmək üçün Jaccard bənzərlik metrikasını istifadə edə bilərsiniz. Əgər Jaccard bənzərliyi 1-dirsə, bu, iki dəyişənin tam eyni olduğunu göstərir.\n",
    "   \n",
    "   ```python\n",
    "   from sklearn.metrics import jaccard_score\n",
    "   similarity = jaccard_score(df['category_feature1'], df['category_feature2'], average='macro')\n",
    "   ```\n",
    "\n",
    "### 6. **Mode və ya Value Counts:**\n",
    "   - Hər bir kategorik dəyişənin `mode` (ən çox təkrarlanan dəyər) və ya `value_counts` statistikalarını müqayisə edərək oxşarlığı təhlil edə bilərsiniz. Əgər iki dəyişənin dəyərləri və onların sıxlıqları çox bənzərdirsə, bu oxşarlıq göstəricisi ola bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Numeric dəyişənlərdəki kimi, kategorik dəyişənlər arasında da tam uyğunluq və güclü oxşarlığı müəyyən etmək üçün müxtəlif statistik metodlardan istifadə edilə bilər. Bu yanaşmalarla kategorik dəyişənlərin redundant (təkrarlanan) olub-olmadığını müəyyənləşdirə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217e6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b96ad99d",
   "metadata": {},
   "source": [
    "## VIf score hem numeric,  hem categoryc ucun isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ffe37",
   "metadata": {},
   "source": [
    "VIF (Variance Inflation Factor) skoru yalnız numeric (rəqəmsal) dəyişənlər üçün tətbiq olunur. Bu metod multikollinearlığı, yəni bir rəqəmsal dəyişənin digər rəqəmsal dəyişənlərlə nə qədər əlaqəli olduğunu qiymətləndirir. VIF, rəqəmsal dəyişənlər arasında xətti asılılıqların olub-olmamasını təhlil etmək üçün istifadə olunur.\n",
    "\n",
    "### Niyə VIF yalnız numeric dəyişənlər üçün işləyir?\n",
    "\n",
    "1. **Linear Regression Əsaslı:** \n",
    "   - VIF, bir dəyişəni digər rəqəmsal dəyişənlərə qarşı bir lineer regresiya modeli ilə proqnozlaşdırmaqla hesablanır. Kategorik dəyişənlər birbaşa linear regressiyada mənalı bir şəkildə istifadə edilə bilməz, çünki onlar saylı deyil.\n",
    "\n",
    "2. **Multikollinearlıq:**\n",
    "   - Multikollinearlıq problemi xətti modellərdə yaranır, çünki bu modellər dəyişənlər arasında xətti əlaqələrdən asılıdır. Kategorik dəyişənlər isə qeyri-xətti xüsusiyyətlərə malikdir, buna görə də multikollinearlıq VIF ilə qiymətləndirilmir.\n",
    "\n",
    "### Kategorik dəyişənlər üçün nə istifadə etmək olar?\n",
    "\n",
    "Kategorik dəyişənlər üçün VIF-ə bənzər bir üsul yoxdur, lakin onların redundant olub olmadığını müəyyənləşdirmək üçün aşağıdakı metodlar istifadə edilə bilər:\n",
    "\n",
    "1. **Chi-Square Testi:**\n",
    "   - Kategorik dəyişənlər arasında asılılıq dərəcəsini ölçmək üçün Chi-Square testi istifadə edilə bilər. Bu, kategorik dəyişənlərin bir-birinə nə qədər bağlı olduğunu göstərir.\n",
    "\n",
    "2. **Mutual Information:**\n",
    "   - Mutual Information kategorik dəyişənlər arasında nə qədər məlumatın paylaşıldığını ölçür. Yüksək mutual information dəyəri onların bir-birinə çox oxşar və ya əlaqəli olduğunu göstərir.\n",
    "\n",
    "3. **Label Encoding ilə Korelyasiya:**\n",
    "   - Kategorik dəyişənlər Label Encoding və ya One-Hot Encoding ilə numeric formaya çevrilə bilər, daha sonra bu dəyişənlər arasında korelyasiya hesablanıb redundant olanlar müəyyən edilə bilər.\n",
    "\n",
    "### Nəticə:\n",
    "VIF yalnız numeric dəyişənlər üçün istifadə edilir. Kategorik dəyişənlər üçün isə digər statistik metodlardan istifadə edərək, redundantlıq və oxşarlığı qiymətləndirmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad8228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdbb49d9",
   "metadata": {},
   "source": [
    "* from sklearn.model_selection import train_test_split\n",
    "* X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "## stratify=y  ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67be01f",
   "metadata": {},
   "source": [
    "`stratify=y` parametri, `train_test_split` funksiyasında verilən məlumatların sınıflar üzrə balanslı şəkildə bölünməsini təmin edir. Yəni, orijinal `y` target (hədəf) dəyişənindəki sınıfların nisbəti necədirsə, həm təlim, həm də test dəstində də həmin sınıf nisbətləri qorunacaq.\n",
    "\n",
    "### Misal:\n",
    "Tutaq ki, sizin `y` hədəf dəyişəninizdə iki sınıf (məsələn, 0 və 1) var və bunların nisbəti belədir:\n",
    "- `0` sınıfı: 80%\n",
    "- `1` sınıfı: 20%\n",
    "\n",
    "Əgər siz `stratify=y` parametri ilə bölmə aparsanız, həm təlim dəstində, həm də test dəstində bu sınıfların nisbəti eyni qalacaq (80% və 20%). Bu, modelin hər iki dəstdə məlumatların təmsilçiliyini qoruması üçün faydalıdır.\n",
    "\n",
    "Əgər `stratify=y` olmasa, bölünmə təsadüfi olar və sınıfların nisbəti təlim və test dəstlərində qeyri-bərabər ola bilər, bu da modelin performansını mənfi təsir edə bilər, xüsusilə də sınıflar arasında balanssızlıq varsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082402e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513f1e35",
   "metadata": {},
   "source": [
    "## XGBoost esas parameterleri ve o parameterlere hyperparameter tuning ucun hansi deyerler vermek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe8fdd",
   "metadata": {},
   "source": [
    "XGBoost modellərində əsas parametrlər və onların hyperparameter tuning (hiperparametr tənzimlənməsi) üçün dəyərləri aşağıda göstərilmişdir. Bu dəyərlər `GridSearchCV` və ya `BayesSearchCV` kimi alətlərlə tuning üçün istifadə oluna bilər.\n",
    "\n",
    "### Əsas XGBoost Parametrləri\n",
    "\n",
    "1. **`n_estimators`**:\n",
    "   - Açıklama: Modelin istifadə etdiyi ağacların sayı.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `100, 200, 500, 1000`\n",
    "\n",
    "2. **`learning_rate`** (və ya `eta`):\n",
    "   - Açıklama: Modelin öyrənmə sürəti. Kiçik dəyərlər öyrənməni yavaşladır, ancaq nəticəni yaxşılaşdırır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.01, 0.05, 0.1, 0.2`\n",
    "\n",
    "3. **`max_depth`**:\n",
    "   - Açıklama: Hər bir qərar ağacının maksimum dərinliyi. Daha böyük dərinliklər modelin mürəkkəbliyini artırır, lakin overfitting-ə səbəb ola bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `3, 5, 7, 10`\n",
    "\n",
    "4. **`min_child_weight`**:\n",
    "   - Açıklama: Bir yarpaq qovşağının ən az nə qədər müşahidəyə sahib olmalı olduğunu təyin edir. Böyük dəyərlər underfitting yarada bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `1, 3, 5, 7`\n",
    "\n",
    "5. **`subsample`**:\n",
    "   - Açıklama: Təlim üçün istifadə edilən məlumatların nisbəti. Overfitting-i azaltmaq üçün faydalıdır.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "6. **`colsample_bytree`**:\n",
    "   - Açıklama: Hər bir ağac üçün təsadüfi seçilən xüsusiyyətlərin nisbəti.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0.5, 0.7, 0.8, 1.0`\n",
    "\n",
    "7. **`gamma`**:\n",
    "   - Açıklama: Ağacın budaqlanmasına qərar verən minimal itki azalma dəyəri. Dəyər nə qədər yüksəkdirsə, budaqlanma daha sərt olur.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.1, 0.2, 0.5`\n",
    "\n",
    "8. **`reg_alpha`** (L1 tənzimlənməsi):\n",
    "   - Açıklama: L1 regularization-u. Daha yüksək dəyərlər sadə modellərə gətirib çıxara bilər və overfitting-i azalda bilər.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "9. **`reg_lambda`** (L2 tənzimlənməsi):\n",
    "   - Açıklama: L2 regularization-u. Overfitting-in qarşısını almağa kömək edir.\n",
    "   - Hyperparameter tuning üçün dəyərlər: `0, 0.01, 0.1, 1.0`\n",
    "\n",
    "### Hyperparameter Tuning Misalı (`GridSearchCV`):\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "```\n",
    "\n",
    "Bu tuning prosesini modelinizə uyğun olaraq dəyişə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4458f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33468f9c",
   "metadata": {},
   "source": [
    "## GridSearchCV, RandomizedSearchCV və BayesSearchCV Parameters cedveli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f25710",
   "metadata": {},
   "source": [
    "Əlbəttə! Aşağıda `GridSearchCV`, `RandomizedSearchCV` və `BayesSearchCV` metodlarının əsas parametrlərini Jupyter Notebook üçün uyğun şəkildə cədvəl şəklində təqdim edirəm.\n",
    "\n",
    "| **Parameter**               | **GridSearchCV (scikit-learn)**                                   | **RandomizedSearchCV (scikit-learn)**                     | **BayesSearchCV (scikit-optimize)**                        |\n",
    "|-----------------------------|-------------------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n",
    "| **estimator**               | Optimallaşdırılacaq maşın öyrənmə modeli                          | Optimallaşdırılacaq maşın öyrənmə modeli                    | Optimallaşdırılacaq maşın öyrənmə modeli                     |\n",
    "| **param_grid / param_distributions / search_spaces** | `param_grid`: dict – Parametrlərin tam qrafiki                | `param_distributions`: dict və ya list – Parametrlərin paylanması | `search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər |\n",
    "| **n_iter**                  | Tətbiq olunmur                                                    | `n_iter`: int – Nümunə parametrlərin sayı                  | `n_iter`: int – Optimallaşdırma üçün iterasiya sayı         |\n",
    "| **scoring**                 | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası | `scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası |\n",
    "| **cv**                      | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası | `cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası |\n",
    "| **n_jobs**                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı                  | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı            | `n_jobs`: int – Paralel işləyəcək nüvələrin sayı             |\n",
    "| **refit**                   | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma      | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma | `refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma |\n",
    "| **verbose**                 | `verbose`: int – Verbosity səviyyəsi                              | `verbose`: int – Verbosity səviyyəsi                      | `verbose`: int – Verbosity səviyyəsi                        |\n",
    "| **pre_dispatch**            | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı  | `pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı | Tətbiq olunmur, implementasiyaya bağlı                      |\n",
    "| **random_state**            | Tətbiq olunmur                                                    | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması | `random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması |\n",
    "| **error_score**             | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | `error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər | Implementasiyaya bağlı ola bilər                              |\n",
    "| **return_train_score**      | `return_train_score`: bool – Təlim skorlarını daxil etmək         | `return_train_score`: bool – Təlim skorlarını daxil etmək   | `return_train_score`: bool – Təlim skorlarını daxil etmək     |\n",
    "| **optimizer**               | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `optimizer`: str – Optimallaşdırma alqoritmi, məsələn \"auto\", \"GP\" və s. |\n",
    "| **n_points**                | Tətbiq olunmur                                                    | Tətbiq olunmur                                              | `n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı |\n",
    "\n",
    "### Qısa İzahat\n",
    "\n",
    "- **GridSearchCV**:\n",
    "  - Bütün mümkün parametrlərin kombinasyonlarını sınayır.\n",
    "  - `param_grid` vasitəsilə tam qrafik təyin olunur.\n",
    "  - Daha çox hesablama vaxtı tələb edir, xüsusilə parametrlərin sayı çox olduqda.\n",
    "\n",
    "- **RandomizedSearchCV**:\n",
    "  - Parametrlərin müəyyən paylanmalardan təsadüfi seçilmiş kombinasiyalarını sınayır.\n",
    "  - `n_iter` ilə sınanacaq kombinasiyaların sayını təyin edə bilərsiniz.\n",
    "  - Hesablama baxımından daha səmərəlidir, xüsusilə geniş parametrlər sahəsində.\n",
    "\n",
    "- **BayesSearchCV**:\n",
    "  - Bayes optimallaşdırması istifadə edərək daha ağıllı axtarış aparır.\n",
    "  - `search_spaces` ilə axtarış sahəsi təyin olunur.\n",
    "  - Daha az iterasiya ilə optimal nəticələr əldə etməyə çalışır.\n",
    "\n",
    "### Jupyter Notebook-da Görüntüləmək\n",
    "\n",
    "Bu cədvəli Jupyter Notebook-da gözəl şəkildə görüntüləmək üçün Markdown hüceyrəsində yuxarıdakı cədvəl formatını istifadə edə bilərsiniz. Alternativ olaraq, `pandas` kitabxanasından istifadə edərək DataFrame şəklində də təqdim edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Parameter\": [\n",
    "        \"estimator\",\n",
    "        \"param_grid / param_distributions / search_spaces\",\n",
    "        \"n_iter\",\n",
    "        \"scoring\",\n",
    "        \"cv\",\n",
    "        \"n_jobs\",\n",
    "        \"refit\",\n",
    "        \"verbose\",\n",
    "        \"pre_dispatch\",\n",
    "        \"random_state\",\n",
    "        \"error_score\",\n",
    "        \"return_train_score\",\n",
    "        \"optimizer\",\n",
    "        \"n_points\"\n",
    "    ],\n",
    "    \"GridSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_grid`: dict – Parametrlərin tam qrafiki\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"RandomizedSearchCV (scikit-learn)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`param_distributions`: dict və ya list – Parametrlərin paylanması\",\n",
    "        \"`n_iter`: int – Nümunə parametrlərin sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"`pre_dispatch`: str – Paralel icra zamanı göndərilən işlərin sayı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"`error_score`: 'raise' və ya rəqəm – Estimator fit zamanı xəta baş verərsə veriləcək dəyər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"Tətbiq olunmur\",\n",
    "        \"Tətbiq olunmur\"\n",
    "    ],\n",
    "    \"BayesSearchCV (scikit-optimize)\": [\n",
    "        \"Optimallaşdırılacaq maşın öyrənmə modeli\",\n",
    "        \"`search_spaces`: dict və ya list – Axtarış sahəsinin tərifi, paylanmalar daxil ola bilər\",\n",
    "        \"`n_iter`: int – Optimallaşdırma üçün iterasiya sayı\",\n",
    "        \"`scoring`: string, callable və ya list/dict – Qiymətləndirmə metrikası\",\n",
    "        \"`cv`: int, cross-validation generator və ya iterable – Çarpaz doğrulama strategiyası\",\n",
    "        \"`n_jobs`: int – Paralel işləyəcək nüvələrin sayı\",\n",
    "        \"`refit`: bool və ya string – Ən yaxşı modeli yenidən quraşdırma\",\n",
    "        \"`verbose`: int – Verbosity səviyyəsi\",\n",
    "        \"Tətbiq olunmur, implementasiyaya bağlı\",\n",
    "        \"`random_state`: int, RandomState instance və ya None – Təsadüfi say generatorunun toxuması\",\n",
    "        \"Implementasiyaya bağlı ola bilər\",\n",
    "        \"`return_train_score`: bool – Təlim skorlarını daxil etmək\",\n",
    "        \"`optimizer`: str – Optimallaşdırma alqoritmi, məsələn 'auto', 'GP' və s.\",\n",
    "        \"`n_points`: int – Hər iterasiya üçün qiymətləndiriləcək nöqtələrin sayı\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "```\n",
    "\n",
    "Bu kodu işlədərək daha interaktiv və oxunaqlı bir cədvəl əldə edə bilərsiniz.\n",
    "\n",
    "Ümid edirəm ki, bu məlumatlar sizin üçün faydalı oldu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c9773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "034bc5a1",
   "metadata": {},
   "source": [
    "## imbalance problemi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa70613",
   "metadata": {},
   "source": [
    "Imbalance problemi, xüsusilə təsnifat (classification) problemlərində rast gəlinən vəziyyətdir. Bu zaman siniflər arasında böyük fərqlilik olur, yəni bir sinif çox az, digəri isə çox sayda nümunə ehtiva edir. Məsələn, xəstəlik diaqnozu ilə bağlı bir datasetdə xəstəlik olan insanların sayı çox az, sağlam olan insanların sayı isə çox ola bilər.\n",
    "\n",
    "Imbalance problemi modelin performansını mənfi təsir edir, çünki model çox vaxt dominant sinifin nümunələrini düzgün təsnif etməyə meyilli olur. Bunun nəticəsində, model azlıqda olan sinifin nümunələrini düzgün öyrənə bilmir və bu da overall performansı zəiflədir.\n",
    "\n",
    "Bu problemi həll etmək üçün bir neçə metod istifadə olunur:\n",
    "\n",
    "### 1. **Resampling (Nümunələrin Dəyişdirilməsi):**\n",
    "   - **Oversampling (Çoxaldma):** Azlıqda olan sinifin nümunələrini çoxaltmaqla balans yaratmaq olar. `SMOTE (Synthetic Minority Over-sampling Technique)` kimi metodlar istifadə olunur.\n",
    "   - **Undersampling (Azaldılma):** Çoxluqda olan sinifin nümunələrini azaldaraq balans təmin etmək olar. Lakin bu üsul datanın bir hissəsini itirmək riskini yaradır.\n",
    "\n",
    "### 2. **Class Weights (Sinif Çəki Dəyərləri):**\n",
    "   Təsnifat modellərində (məsələn, `RandomForest`, `SVM`, `XGBoost`) sinif çəki dəyərlərini tənzimləməklə azlıqda olan sinifin daha vacib olduğunu göstərmək olar. Bununla model azlıq sinifə daha çox diqqət yetirir.\n",
    "\n",
    "### 3. **Anomaly Detection (Anomaliya Aşkarlanması):**\n",
    "   Əgər azlıq sinifi anomaliyaya bənzəyirsə, anomaly detection metodları ilə bu sinifi tapmaq mümkün ola bilər.\n",
    "\n",
    "### 4. **Evaluation Metrics (Qiymətləndirmə Metodları):**\n",
    "   Klassik ölçüm metrikləri (məsələn, accuracy) imbalance datada yanıltıcı ola bilər. Bunun yerinə `precision`, `recall`, `F1-score`, `ROC-AUC` kimi metriklərdən istifadə etmək daha uyğundur.\n",
    "\n",
    "Imbalance problemi ilə qarşılaşanda bu metodlardan birini və ya bir neçəsini sınamaq balanslı və daha performanslı bir model yaratmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d354699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786294d8",
   "metadata": {},
   "source": [
    "## Nece faiz nisbetden sonra imblance sayilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2bbeb",
   "metadata": {},
   "source": [
    "Imbalance problemi adətən siniflər arasında əhəmiyyətli fərqlər olduqda meydana çıxır. Ümumiyyətlə, siniflər arasında 1:1.5 və ya 1:2 nisbəti qədər fərq olduqda balanslı (balanced) hesab edilir. Əgər bir sinif digərindən 1:4 və ya daha çox dəfə böyükdürsə, bu artıq imbalance problemi kimi qiymətləndirilə bilər.\n",
    "\n",
    "Daha dəqiq desək, bəzi tədqiqatçılar siniflər arasında 70% və ya daha çox fərqin (yəni bir sinifin nümunələrinin 70%-dən çoxunu təşkil etməsi) imbalance problemi yaratdığını deyirlər. Məsələn, bir datasetdə bir sinif 90% və digər sinif 10% təşkil edirsə, bu ciddi bir imbalance olaraq qəbul edilir.\n",
    "\n",
    "Əsas qayda odur ki, əgər azlıqda olan sinifin modellə düzgün öyrənilməsi çətinləşirsə və model çoxluqda olan sinifə üstünlük verirsə, bu zaman imbalance problemi var deməkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372da750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9d5322b",
   "metadata": {},
   "source": [
    "## Butun feature sutunlari etsem ne bas verecek ki?  1 ve 0 olanlar deyise biler. yoxsa oldugu kimi saxlayacaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e99c66",
   "metadata": {},
   "source": [
    "Əgər datada categorical dəyişənləri artıq `1` və `0` olaraq çevirmisinizsə, həmin dəyişənlər artıq binary formatdadır və onları yenidən scale etmək lazım deyil. Bu tip binary dəyişənlər onsuz da məhdud aralıqda olur (0 və 1), ona görə də scale etməyə ehtiyac yoxdur.\n",
    "\n",
    "Numeric dəyişənləri isə scale etməniz məqsədəuyğundur, çünki onlar fərqli böyüklükdə ola bilər və bəzi modellər (məsələn, SVM, KNN və ya linear regression modelləri) fərqli miqyaslarda olan dəyişənlərdən mənfi təsirlənə bilər.\n",
    "\n",
    "Yəni, ümumi tövsiyə belədir:\n",
    "- Numeric dəyişənləri scale edin.\n",
    "- Binary (0 və 1 olan categorical) dəyişənləri scale etməyin.\n",
    "\n",
    "Bu addımları tətbiq etməklə datanızı daha optimal şəkildə modelə təqdim edə bilərsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357584b",
   "metadata": {},
   "source": [
    "Əgər bütün feature sütunlarını (həm numeric, həm də binary olanları) scale etsəniz, 0 və 1 olan binary sütunlar da dəyişəcək. Məsələn, `StandardScaler` və ya `MinMaxScaler` tətbiq edildikdə bu binary sütunlar 0 və 1 arasında başqa qiymətlər alacaq, çünki scale prosesi bütün dəyişənləri yeni miqyasda sıxışdırmağa çalışır.\n",
    "\n",
    "Məsələn, əgər `MinMaxScaler` tətbiq edirsinizsə:\n",
    "- `0` olan dəyərlər dəyişməz qalacaq.\n",
    "- `1` olan dəyərlər isə `min` və `max` aralığında yeni dəyər ala bilər (məsələn, 0.5 və ya başqa bir qiymət).\n",
    "\n",
    "Bu, modelin davranışını mənfi təsir edə bilər, çünki bəzi modellər binary dəyişənləri orijinal 0 və 1 aralığında gözləyir. Ona görə də binary sütunları scale etməmək daha məqsədəuyğundur.\n",
    "\n",
    "Əgər ümumi datanı scale etmək istəyirsinizsə, binary sütunları istisna etmək üçün kodunuzda bu dəyişənləri seçmədən yalnız numeric sütunlara scale tətbiq etməniz daha doğru olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53417cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34302a69",
   "metadata": {},
   "source": [
    "## Bes nece edim axi. nece kod yazim ki binary olanalra toxunmasin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0765452",
   "metadata": {},
   "source": [
    "Binary sütunlara toxunmadan yalnız numeric sütunları scale etmək üçün əvvəlcə numeric və binary sütunları ayırmalısınız. Python və `pandas` istifadə edərək bunu etmək üçün aşağıdakı addımları izləyə bilərsiniz:\n",
    "\n",
    "1. **Binary və numeric sütunları ayırın.**\n",
    "2. **Yalnız numeric sütunlara `scaler` tətbiq edin.**\n",
    "3. **Yenidən datafraimi birləşdirin.**\n",
    "\n",
    "Məsələn, aşağıdakı kimi bir kod yaza bilərsiniz:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Məsələn, data frame-iniz olsun\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# Target və features sütunlarını müəyyən edin (məsələn, 'target' adlı bir sütun)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split edin\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Numeric sütunları seçin\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Binary sütunları müəyyən edin\n",
    "binary_cols = [col for col in numeric_cols if X_train[col].nunique() == 2 and sorted(X_train[col].unique()) == [0, 1]]\n",
    "\n",
    "# Binary olmayan numeric sütunları tapın\n",
    "numeric_cols_to_scale = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Scaler tətbiq edin yalnız numeric sütunlara\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Yalnız train datasında fit edin, amma həm train, həm də test datasında transform edin\n",
    "X_train[numeric_cols_to_scale] = scaler.fit_transform(X_train[numeric_cols_to_scale])\n",
    "X_test[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
    "\n",
    "# Artıq yalnız numeric sütunlar scale olundu, binary sütunlara toxunulmadı\n",
    "```\n",
    "\n",
    "### Açıqlama:\n",
    "- `select_dtypes(include=['float64', 'int64'])`: Numeric sütunları seçir.\n",
    "- `df[col].nunique() == 2`: Sütunda yalnız 2 unikal dəyər (0 və 1) olduğunu yoxlayır.\n",
    "- `sorted(df[col].unique()) == [0, 1]`: Sütundakı dəyərlərin həqiqətən 0 və 1 olmasını yoxlayır.\n",
    "- Son olaraq, yalnız numeric sütunlar scale olunur, binary sütunlara toxunulmur.\n",
    "\n",
    "Beləliklə, yalnız numeric sütunları scale edə bilərsiniz, binary sütunlar isə dəyişməz qalacaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eccd2960",
   "metadata": {},
   "source": [
    "## Outlier sixmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f32fe",
   "metadata": {},
   "source": [
    "Outlier-ləri sıxmaq üçün bir neçə fərqli metod var. Bu metodlar datasetin növündən, təbiətindən və məqsədlərinizdən asılı olaraq dəyişir. Aşağıda outlier-ləri sıxmaq üçün ən çox istifadə olunan üsullar və onların Python-da tətbiqi nümunələri ilə tanış ola bilərsiniz.\n",
    "\n",
    "### 1. **Z-Score Metodu ilə Sıxmaq**\n",
    "Z-score, hər bir dəyərin ortalamadan neçə standart sapma uzaqda olduğunu göstərir. Əgər bir dəyər müəyyən bir həddən böyükdürsə (məsələn, 3 standart sapmadan uzaqdırsa), onu həmin həddə sıxmaq olar.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "df\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score hesablayın (sayısal sütunlar üçün)\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# Z-scores hesablayın\n",
    "z_scores = zscore(df_)\n",
    "\n",
    "# Threshold təyin edin\n",
    "threshold = 3\n",
    "\n",
    "# Z-scores dəyəri 3-dən böyük olanları 3-ə sıxın, -3-dən kiçik olanları isə -3-ə sıxın\n",
    "z_scores_compressed = np.where(z_scores > threshold, threshold, \n",
    "                               np.where(z_scores < -threshold, -threshold, z_scores))\n",
    "\n",
    "# Z-scores əsasında sıxılmış dəyərləri orijinal df_-dəki dəyərlərlə əvəz edin\n",
    "df_compressed = df_.copy()\n",
    "\n",
    "for i, col in enumerate(df_.columns):\n",
    "    # Hər sütun üçün sıxılmış Z-scores dəyərlərini yenidən orijinal miqyasına çevirin\n",
    "    df_compressed[col] = (z_scores_compressed[:, i] * df_[col].std()) + df_[col].mean()\n",
    "\n",
    "df_compressed\n",
    "```\n",
    "\n",
    "### 2. **IQR (Interquartile Range) Metodu ilə Sıxmaq**\n",
    "IQR metodu, dəyərləri 1-ci və 3-cü kvartillər (Q1 və Q3) əsasında müəyyənləşdirir. Outlier-lər Q1 - 1.5 * IQR və Q3 + 1.5 * IQR intervalından kənarda olan dəyərlərdir. Bu hədləri aşan dəyərləri həmin həddə sıxışdıra bilərsiniz.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# CSV faylını oxuyun\n",
    "df = pd.read_csv(r'C:\\Users\\dell\\Desktop\\Python\\week3\\day2\\train.csv')\n",
    "\n",
    "# Sayısal sütunları seçin və lazımsız sütunları silin\n",
    "df_ = df.select_dtypes(include=[np.number]).drop(columns=['Credit_History', 'Loan_Amount_Term'])\n",
    "\n",
    "# Nəmissing dəyərləri sütun ortalamaları ilə doldurun\n",
    "df_.fillna(df_.mean(), inplace=True)\n",
    "\n",
    "# IQR istifadə edərək outlier-ləri sıxın\n",
    "df_clipped = df_.copy()\n",
    "\n",
    "for col in df_clipped.columns:\n",
    "    Q1 = df_clipped[col].quantile(0.25)\n",
    "    Q3 = df_clipped[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Outlier sərhədlərini müəyyən edin\n",
    "    lower_fence = Q1 - 1.5 * IQR\n",
    "    upper_fence = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Outlier-ləri sıxın\n",
    "    df_clipped[col] = df_clipped[col].clip(lower=lower_fence, upper=upper_fence)\n",
    "\n",
    "df_clipped\n",
    "```\n",
    "\n",
    "### 3. **Winsorization (Sərhəd Dəyərlərə Sıxmaq)**\n",
    "Winsorization metodu, outlier-ləri müəyyən bir həddə sıxışdırır, yəni outlier-lər alt və üst limitlərdən kənara çıxa bilmir.\n",
    "\n",
    "```python\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize tətbiq edin (1% alt və 1% üst həddə sıxışdırma)\n",
    "df_winsorized = winsorize(df, limits=[0.01, 0.01])\n",
    "\n",
    "# Bu dəyərləri müəyyən bir həddə sıxışdırmış olursunuz\n",
    "```\n",
    "\n",
    "### 4. **Percentile Metodu ilə Sıxmaq**\n",
    "Bəzi hallarda, outlier-ləri percentil əsaslı metodlarla müəyyən etmək faydalı ola bilər. 1%-dən aşağı və ya 99%-dən yuxarı olan dəyərləri sıxmaq üçün aşağıdakı koddan istifadə edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "# Alt və üst hədləri təyin edin\n",
    "lower_percentile = df.quantile(0.01)\n",
    "upper_percentile = df.quantile(0.99)\n",
    "\n",
    "# Outlier-ləri sıxın\n",
    "df_clipped = np.where(df < lower_percentile, lower_percentile, df)\n",
    "df_clipped = np.where(df > upper_percentile, upper_percentile, df)\n",
    "```\n",
    "\n",
    "### 5. **Log Transformasiya**\n",
    "Əgər outlier-lər data massivində çox geniş fərqlənirsə, log transformasiya dəyərləri daha kiçik miqyasda sıxmağa kömək edə bilər.\n",
    "\n",
    "```python\n",
    "df_log_transformed = np.log1p(df)  # 1 əlavə edərək log transformasiya edin\n",
    "```\n",
    "\n",
    "### 6. **Clipping (Sərhədli Dəyərlərlə Əvəz Etmək)**\n",
    "Bir çox hallarda dəyərləri müəyyən bir hədd ilə sıxmaq üçün birbaşa `numpy`-ın `clip()` funksiyasından istifadə edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "df_clipped = np.clip(df, lower_bound, upper_bound)\n",
    "```\n",
    "\n",
    "### Hansı Metodu Seçməli?\n",
    "- **Z-score**: Daha çox normal paylanma üçün uyğundur.\n",
    "- **IQR**: Paylanma asimmetrikdirsə və outlier-lər çox genişdirsə istifadə edilir.\n",
    "- **Winsorization və Percentile**: Daha stabildir və sərhəd dəyərləri təyin etmək asandır.\n",
    "- **Log Transformasiya**: Exponential (eksponential) və ya geniş miqyaslı paylanmalar üçün uyğundur.\n",
    "\n",
    "Hansı metoddan istifadə edəcəyiniz datasetin təbiətindən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf446d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2677df6d",
   "metadata": {},
   "source": [
    "## Missing deyerleri doldurmaq yollari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1816a7",
   "metadata": {},
   "source": [
    "Missing dəyərləri doldurmaq (imputation) analitik və maşın öyrənməsi işlərində tez-tez qarşılaşılan problemlərdən biridir. Aşağıda bir neçə effektiv yanaşma və onların Python vasitəsilə həyata keçirilməsi yollarını görə bilərsiniz.\n",
    "\n",
    "### 1. **Ortalama (Mean) ilə Doldurma**\n",
    "Bu üsul sadədir və əsasən **numerik** verilənlər üçün tətbiq olunur. Missing dəyərləri həmin sütunun ortalama (mean) dəyəri ilə əvəz edirsiniz.\n",
    "\n",
    "```python\n",
    "# Numerik sütundakı missing dəyərləri ortalama ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mean())\n",
    "```\n",
    "\n",
    "### 2. **Median (Orta Əsaslı) Doldurma**\n",
    "Median üsulu, xüsusilə outlier-lərin çox olduğu datasetlərdə daha sabit nəticələr verir. Bu üsul da əsasən **numerik** verilənlər üçün istifadə olunur.\n",
    "\n",
    "```python\n",
    "# Missing dəyərləri median ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].median())\n",
    "```\n",
    "\n",
    "### 3. **Mod (Modus) ilə Doldurma**\n",
    "**Kategorik** (category) verilənlər üçün daha çox istifadə edilən üsuldur. Mod üsulu, sütundakı ən çox təkrarlanan dəyəri missing dəyərlərə əlavə edir.\n",
    "\n",
    "```python\n",
    "# Kategorik sütunu mod ilə doldurmaq\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(df['sütun_adi'].mode()[0])\n",
    "```\n",
    "\n",
    "### 4. **Sabit Dəyər ilə Doldurma**\n",
    "Bəzi hallarda missing dəyərləri sabit bir dəyərlə əvəz etmək lazım ola bilər (məsələn, 0 və ya müəyyən kateqoriya).\n",
    "\n",
    "```python\n",
    "# Sabit dəyərlə doldurmaq (məsələn, 0 ilə)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(0)\n",
    "\n",
    "# Kategorik verilənlər üçün sabit kateqoriya təyin etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna('Unknown')\n",
    "```\n",
    "\n",
    "### 5. **Linear İnteqrasiya ilə Doldurma (Interpolation)**\n",
    "Linear inteqrasiya metodu zaman seriyası məlumatları və ya ardıcıl numerik məlumatlar üçün uyğundur. Məlumatın əvvəlki və sonrakı dəyərləri əsasında missing dəyərləri interpolasiya edərək hesablayır.\n",
    "\n",
    "```python\n",
    "# Linear interpolation tətbiq etmək\n",
    "df['sütun_adi'] = df['sütun_adi'].interpolate()\n",
    "```\n",
    "\n",
    "### 6. **K-NN (K-Nearest Neighbors) İmputation**\n",
    "K-NN imputation hər bir missing dəyəri ən yaxın qonşu dəyərlər əsasında tamamlayır. Bu üsul həm numerik, həm də kategorik dəyərlər üçün istifadə oluna bilər.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)  # 5 ən yaxın qonşu ilə doldurmaq\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 7. **MICE (Multiple Imputation by Chained Equations)**\n",
    "MICE çoxlu itkin dəyərlərin olduğu datasetlər üçün istifadə edilən bir üsuldur. Bu metod hər bir missing dəyəri digər dəyişənlərdən istifadə edərək təkrarlanmış şəkildə təxmin edir.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# MICE ilə doldurma\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "### 8. **Fərqli Sütunlar Əsasında Doldurma**\n",
    "Bəzi hallarda missing dəyərləri doldurmaq üçün başqa bir sütunun məlumatından istifadə etmək lazım ola bilər.\n",
    "\n",
    "```python\n",
    "# Əgər 'sütun_adi1' missingdirsə, onu 'sütun_adi2' ilə doldur\n",
    "df['sütun_adi1'] = df['sütun_adi1'].fillna(df['sütun_adi2'])\n",
    "```\n",
    "\n",
    "### 9. **Foward Fill və Backward Fill (Zaman Seriyasında)**\n",
    "Zaman seriyasında əvvəlki və ya sonrakı dəyərləri missing dəyərlə əvəz etməyə imkan verir.\n",
    "\n",
    "```python\n",
    "# Əvvəlki dəyərlə doldurmaq (forward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='ffill')\n",
    "\n",
    "# Sonrakı dəyərlə doldurmaq (backward fill)\n",
    "df['sütun_adi'] = df['sütun_adi'].fillna(method='bfill')\n",
    "```\n",
    "\n",
    "### 10. **Model Əsaslı Doldurma**\n",
    "Daha kompleks yanaşma kimi, missing dəyərləri təxmin etmək üçün maşın öyrənməsi modellərindən istifadə edə bilərsiniz. Məsələn, regression və ya classification modelləri ilə missing dəyərləri təxmin etmək mümkündür.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Missing dəyərləri təxmin etmək üçün model istifadə edin\n",
    "model = LinearRegression()\n",
    "X_train = df.dropna(subset=['sütun_adi'])  # Missing olmayan dəyərlərlə modeli öyrədin\n",
    "y_train = X_train.pop('sütun_adi')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Missing dəyərləri təxmin edin\n",
    "X_missing = df[df['sütun_adi'].isnull()].drop('sütun_adi', axis=1)\n",
    "df.loc[df['sütun_adi'].isnull(), 'sütun_adi'] = model.predict(X_missing)\n",
    "```\n",
    "---\n",
    "\n",
    "### İki və daha çox sütunda `null` dəyərləri varsa\n",
    "\n",
    "İki və daha çox sütunda `null` dəyərləri varsa, onları təxmin etmək üçün bir neçə yanaşma istifadə edə bilərsiniz. Burada bir neçə üsul təqdim edirəm:\n",
    "\n",
    "### Yanaşma 1: Hər bir sütun üçün ayrıca model qurmaq\n",
    "\n",
    "Bu yanaşma ilə hər bir sütun üçün ayrı-ayrı modellər quraraq `null` dəyərləri təxmin edə bilərsiniz. Aşağıdakı nümunə bu yanaşmanı göstərir:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Missing dəyərləri təxmin etmək üçün model\n",
    "def fill_missing_values(df, target_columns):\n",
    "    for column in target_columns:\n",
    "        model = LinearRegression()\n",
    "        \n",
    "        # Missing olmayan dəyərlərlə modeli öyrədin\n",
    "        X_train = df.dropna(subset=[column])  # Missing olmayan dəyərlər\n",
    "        y_train = X_train.pop(column)          # Target sütunu\n",
    "        \n",
    "        # Modeli öyrədin\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Missing dəyərləri təxmin edin\n",
    "        X_missing = df[df[column].isnull()].drop(column, axis=1)\n",
    "        df.loc[df[column].isnull(), column] = model.predict(X_missing)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Nümunə üçün sütunları seçin\n",
    "target_columns = ['sütun1', 'sütun2', 'sütun3']  # Missing dəyərləri olan sütunlar\n",
    "df = fill_missing_values(df, target_columns)\n",
    "```\n",
    "\n",
    "### Yanaşma 2: MultiOutputRegressor istifadə etmək\n",
    "\n",
    "`scikit-learn` kitabxanasında `MultiOutputRegressor` istifadə edərək birdən çox target sütununu bir model ilə təxmin etmək mümkündür. Bu yanaşma ilə eyni anda bir neçə sütunu təxmin edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Missing dəyərləri təxmin etmək üçün model\n",
    "model = MultiOutputRegressor(LinearRegression())\n",
    "\n",
    "# Missing olmayan dəyərlərlə modeli öyrədin\n",
    "X_train = df.dropna(subset=['sütun1', 'sütun2'])  # Missing olmayan dəyərlər\n",
    "y_train = X_train[['sütun1', 'sütun2']]           # Target sütunları\n",
    "\n",
    "# Modeli öyrədin\n",
    "model.fit(X_train.drop(['sütun1', 'sütun2'], axis=1), y_train)\n",
    "\n",
    "# Missing dəyərləri təxmin edin\n",
    "X_missing = df[df[['sütun1', 'sütun2']].isnull().any(axis=1)].drop(['sütun1', 'sütun2'], axis=1)\n",
    "predictions = model.predict(X_missing)\n",
    "\n",
    "# Missing dəyərləri doldur\n",
    "df.loc[df[['sütun1', 'sütun2']].isnull().any(axis=1), ['sütun1', 'sütun2']] = predictions\n",
    "```\n",
    "\n",
    "### Qeyd:\n",
    "- Hər iki yanaşma üçün `X_train` və `X_missing` verilənlər dəstinin bir hissəsini `dropna()` ilə `null` dəyərlərindən təmizləyir.\n",
    "- Birinci yanaşmada, hər bir sütun üçün ayrı-ayrı modellər qurulur, ikincidə isə bir model bir neçə sütunun `null` dəyərlərini eyni anda təxmin edir.\n",
    "- Sizin tətbiq sahənizdən asılı olaraq bu yanaşmalardan birini seçə bilərsiniz. Həmçinin, daha mürəkkəb modellər və ya digər maşın öyrənməsi alqoritmləri istifadə etməyi də düşünə bilərsiniz.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Linear Regression / Random Forest Imputation**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Random Forest ilə itkin dəyərlərin doldurulması\n",
    "rf = RandomForestRegressor()\n",
    "df_filled = SimpleImputer(strategy=\"mean\").fit_transform(df)\n",
    "```\n",
    "\n",
    "\n",
    "### Hansı Metodun Seçilməsi:\n",
    "- **Ortalama və Median**: Numerik sütunlar üçün tez-tez istifadə olunur.\n",
    "- **Modus və Sabit Dəyər**: Kategorik sütunlar üçün uyğundur.\n",
    "- **K-NN və MICE**: Çox dəyişkənli imputation tələb edən datasetlərdə daha dəqiqdir.\n",
    "- **Interpolation**: Zaman seriyaları üçün ideal metodlardan biridir.\n",
    "\n",
    "### Ümumi Yekun:\n",
    "- **Sadə datasetlər üçün**: **Mean/Median/Mode imputation** üsulları kifayət qədər sadə və tezdir.\n",
    "- **Kateqorial dəyişənlər üçün**: **Mode imputation** və ya **KNN imputation** uyğun gəlir.\n",
    "- **Daha mürəkkəb modellər üçün**: **MICE**, **KNN**, və ya **Random Forest imputation** üsulları çox ölçülü məlumatlar üçün daha dəqiq nəticələr verir, çünki dəyişənlər arasındakı əlaqələri nəzərə alır.\n",
    "\n",
    "### Ən yaxşı üsulun seçilməsi üçün ipuçları:\n",
    "- **Məlumatın ölçüsü**: Kiçik datasetlər üçün MICE və ya KNN daha yaxşı nəticə verə bilər.\n",
    "- **İtkin dəyərlərin faizi**: Əgər çox sayda itkin dəyər varsa, mürəkkəb modellər (MICE, KNN, Random Forest) daha dəqiq nəticə verəcək.\n",
    "- **Dəyişənlər arasındakı əlaqə**: Əgər dəyişənlər arasında güclü əlaqə varsa, bu əlaqələri nəzərə alan metodlar (MICE, Regression, Random Forest) daha yaxşı işləyir.\n",
    "\n",
    "Hansı metodu seçməyiniz datasetin növündən və analiz məqsədinizdən asılıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c71e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09a576b",
   "metadata": {},
   "source": [
    "## SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea309cc",
   "metadata": {},
   "source": [
    "**SimpleImputer** sklearn kitabxanasında mövcud olan sadə və effektiv bir imputation vasitəsidir. O, datasetdəki itkin (null) dəyərləri müəyyən bir strategiya əsasında doldurmaq üçün istifadə olunur. **SimpleImputer** müxtəlif dəyərləri, məsələn, **orta (mean)**, **median**, **ən çox təkrarlanan dəyər (mode)** və ya xüsusi bir dəyər ilə itkin dəyərləri əvəz etməyə imkan verir.\n",
    "\n",
    "### SimpleImputer-in istifadəsi və xüsusiyyətləri:\n",
    "\n",
    "- **Strategiyalar**:\n",
    "  - **mean**: İtkin dəyərləri sütundakı orta dəyərlə doldurur. Davamlı dəyişənlər üçün istifadə olunur.\n",
    "  - **median**: İtkin dəyərləri median dəyəri ilə doldurur. Xüsusilə asimmetrik paylanmalar üçün faydalıdır.\n",
    "  - **most_frequent**: İtkin dəyərləri ən çox təkrarlanan dəyərlə əvəz edir. Bu üsul həm kateqorial, həm də davamlı dəyişənlər üçün istifadə oluna bilər.\n",
    "  - **constant**: İtkin dəyərləri müəyyən edilən sabit bir dəyər ilə doldurur. Məsələn, `0` və ya `missing` kimi xüsusi bir dəyər istifadə edilə bilər.\n",
    "\n",
    "### SimpleImputer-in tətbiqi:\n",
    "\n",
    "Burada **SimpleImputer**-in müxtəlif strategiyalarla necə istifadə ediləcəyini göstərən bir nümunə var:\n",
    "\n",
    "#### 1. **Orta dəyərlə (mean) doldurmaq:**\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Datasetin yaradılması\n",
    "data = {'age': [25, 30, None, 35, 40, None, 50],\n",
    "        'salary': [50000, 60000, 55000, None, 70000, 80000, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# SimpleImputer ilə orta dəyərlə (mean) doldurmaq\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 2. **Median ilə doldurmaq:**\n",
    "\n",
    "```python\n",
    "# Median strategiyası\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df['salary'] = imputer_median.fit_transform(df[['salary']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 3. **Ən çox təkrarlanan dəyərlə (most_frequent) doldurmaq:**\n",
    "\n",
    "```python\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df['age'] = imputer_mode.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### 4. **Sabit bir dəyərlə doldurmaq:**\n",
    "\n",
    "```python\n",
    "imputer_constant = SimpleImputer(strategy='constant', fill_value=0)\n",
    "df['age'] = imputer_constant.fit_transform(df[['age']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Üstünlükləri:\n",
    "- **Sadə və sürətli**: Datasetdə çox sayda itkin dəyər varsa belə, sadə üsullarla onları doldurmaq imkanı verir.\n",
    "- **Çox yönlülük**: Həm davamlı (numeric) dəyişənlər, həm də kateqorial (categorical) dəyişənlər üçün istifadə edilə bilər.\n",
    "- **Asan tətbiq oluna bilir**: sklearn kitabxanası ilə birgə istifadə olunur və model qurma prosesinə asanlıqla daxil edilir.\n",
    "\n",
    "### Mənfi cəhətləri:\n",
    "- **Sadə yanaşma**: **SimpleImputer** dəyişənlər arasındakı əlaqəni nəzərə almır. Bu, mürəkkəb datasetlər üçün dəyərlərin doldurulmasında kifayət qədər dəqiq olmaya bilər.\n",
    "- **Dəyər dispersiyasını azaldır**: Orta və ya median ilə doldurulduqda, məlumatın dispersiyası (müxtəlifliyi) azalır, bu da model performansına təsir göstərə bilər.\n",
    "\n",
    "SimpleImputer sadə datasetlərdə və ya ilkin təmizləmə mərhələsində effektivdir, lakin daha mürəkkəb datasetlərdə MICE, KNN və ya Random Forest kimi daha inkişaf etmiş metodlarla müqayisədə məhdudiyyətləri var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f97376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d81d7a0",
   "metadata": {},
   "source": [
    "## Fillna ve SimpleImputer ferqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fab80",
   "metadata": {},
   "source": [
    "Bəli, hər iki yanaşma mahiyyətcə oxşar iş görür — **itkin dəyərləri (NaN)** sütundakı **orta dəyərlə (mean)** doldurur, lakin işləmə prinsiplərində bəzi fərqlər var. Gəlin fərqləri detallı şəkildə izah edim:\n",
    "\n",
    "### 1. **`df['age'].fillna(df['age'].mean())` ilə doldurmaq**:\n",
    "\n",
    "Bu üsul **Pandas**-ın daxili metodunu istifadə edir və yalnız sadə bir əməliyyatdır. Bu metod itkin dəyərləri **kolonun ortalama dəyəri ilə** doldurur.\n",
    "\n",
    "```python\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "```\n",
    "\n",
    "- **Pros**:\n",
    "  - Çox sadədir və sürətlidir.\n",
    "  - Kiçik datasetlər və sadə hallarda rahatdır.\n",
    "  \n",
    "- **Cons**:\n",
    "  - Yalnız **statistik metodlar** ilə işləyir (mean, median və s.).\n",
    "  - Yalnız bir sütunu nəzərə alır və heç bir əlavə analiz və ya dəyişənlər arasındakı əlaqəni istifadə etmir.\n",
    "\n",
    "### 2. **`SimpleImputer(strategy='mean')` ilə doldurmaq**:\n",
    "\n",
    "`SimpleImputer` isə **scikit-learn** kitabxanasından olan bir metoddur və itkin dəyərlərin doldurulması üçün daha geniş imkanlar yaradır. Burada `SimpleImputer` həmçinin daha mürəkkəb üsullarla, məsələn, **median**, **mode** və ya **sabit bir dəyər** ilə itkin dəyərləri doldura bilər.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['age'] = imputer.fit_transform(df[['age']])\n",
    "```\n",
    "\n",
    "- **Pros**:\n",
    "  - Daha çevikdir və müxtəlif strategiyaları (mean, median, mode və s.) dəstəkləyir.\n",
    "  - Digər maşın öyrənməsi (ML) metodları ilə birgə rahat istifadə oluna bilir.\n",
    "  - **Multiple column imputation** dəstəklənir — yəni bir neçə sütunu eyni anda doldura bilərsiniz.\n",
    "\n",
    "- **Cons**:\n",
    "  - Pandas metoduna nisbətən bir qədər daha mürəkkəbdir.\n",
    "  - Pandas fillna metoduna nisbətən daha çox hesablama vaxtı tələb edə bilər (böyük datasetlərdə).\n",
    "\n",
    "### Fərqli cəhətlər:\n",
    "\n",
    "1. **Pandas `fillna()`** yalnız bir sütunun ortalamasını istifadə edir və dəyişənlər arasındakı heç bir əlaqəni nəzərə almır. O, daha sadə yanaşmadır və yalnız bir sütun üzərində işləyir.\n",
    "\n",
    "2. **SimpleImputer** isə xüsusilə bir neçə sütun üçün işləyə bilər və maşın öyrənməsi prosesində daha sistematik bir yanaşma tələb edir. Eyni zamanda `SimpleImputer` ilə birdən çox sütunda imputation tətbiq edərək işləyə bilərsiniz.\n",
    "\n",
    "#### Əlavə Misal (Bir neçə sütunu doldurmaq):\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])\n",
    "```\n",
    "Bu, həm `age`, həm də `salary` sütunlarındakı NaN dəyərlərini **orta** ilə dolduracaq.\n",
    "\n",
    "### Yekun:\n",
    "- **Kiçik və sadə hallarda**: Pandas-ın `fillna()` metodu daha sadədir və bir qədər daha sürətli işləyir.\n",
    "- **Böyük və mürəkkəb hallarda**: `SimpleImputer` çox yönlü və daha uyğun metoddur, xüsusilə maşın öyrənməsi modellərində və birdən çox sütunlu datasetlərdə imputation etmək lazım olduqda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc367e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27769fab",
   "metadata": {},
   "source": [
    "## Null value doldurmaq advanced metodlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ff525",
   "metadata": {},
   "source": [
    "Məlumat dəstindəki boş (null) dəyərləri doldurmaq üçün bir neçə **advanced metod** mövcuddur. Aşağıda bəzi əsas və inkişaf etmiş metodları və onların kod nümunələrini paylaşacağam.\n",
    "\n",
    "### 1. **Forward/Backward Filling (Time Series üçün uyğun)**\n",
    "Bu metodda null dəyərlər əvvəlki və ya sonrakı qeydlərlə doldurulur.\n",
    "\n",
    "#### Forward Filling (`ffill`)\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nümunə dataframe\n",
    "data = {\n",
    "    'A': [1, np.nan, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Forward fill\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(df_ffill)\n",
    "```\n",
    "\n",
    "#### Backward Filling (`bfill`)\n",
    "```python\n",
    "# Backward fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(df_bfill)\n",
    "```\n",
    "\n",
    "### 2. **Linear Interpolation (Xətti İnterpolasiya)**\n",
    "Bu metodda boş dəyərlər, əvvəlki və sonrakı qeydlərin ortasında yerləşən xətti bir əlaqə əsasında doldurulur.\n",
    "\n",
    "```python\n",
    "# Xətti interpolasiya\n",
    "df_interpolated = df.interpolate(method='linear')\n",
    "print(df_interpolated)\n",
    "```\n",
    "\n",
    "### 3. **Polynomial Interpolation (Çoxhəddli İnterpolasiya)**\n",
    "Çoxhəddli interpolasiya daha kompleks və qeyri-xətti məlumat dəstləri üçün tətbiq olunur. Bu metodda xətti deyil, daha yüksək dərəcəli çoxhəddli funksiyalar vasitəsilə interpolasiya edilir.\n",
    "\n",
    "```python\n",
    "# Polynomial interpolasiya (dərəcə = 2)\n",
    "df_poly_interpolated = df.interpolate(method='polynomial', order=2)\n",
    "print(df_poly_interpolated)\n",
    "```\n",
    "\n",
    "### 4. **K-Nearest Neighbors (KNN) ilə Boş Dəyərlərin Doldurulması**\n",
    "KNN boş dəyərlərin doldurulmasında istifadə edilən inkişaf etmiş metodlardan biridir. Bu metodda hər bir boş dəyər, ona ən yaxın olan `k` sayda qonşunun orta dəyəri ilə doldurulur.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Nümunə dataset\n",
    "data = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [5, 3, 4, np.nan, 6],\n",
    "    'C': [np.nan, 8, 6, 7, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# KNN Imputer tətbiq edirik\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_knn_imputed)\n",
    "```\n",
    "\n",
    "### 5. **MICE (Multiple Imputation by Chained Equations)**\n",
    "MICE metodunda hər boş dəyər bir neçə dəfə proqnozlaşdırılır və daha sonra bu proqnozlar birləşdirilərək doldurulmuş dəyərlər təyin edilir.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Iterative Imputer tətbiq edirik (MICE)\n",
    "mice_imputer = IterativeImputer(random_state=42)\n",
    "df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_mice_imputed)\n",
    "```\n",
    "\n",
    "### 6. **Random Forest ilə Boş Dəyərlərin Proqnozlaşdırılması**\n",
    "Boş dəyərlər `RandomForest` kimi bir modeldən istifadə edərək proqnozlaşdırıla bilər. Bu zaman digər sütunlardakı məlumatlara əsasən, boş dəyərlər təxmin edilir.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Nümunə dataframe\n",
    "df_rf = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, 5],\n",
    "    'C': [5, 6, 7, 8, 9]\n",
    "})\n",
    "\n",
    "# Boş dəyərlərin olduğu sütun (hədəf) və digər sütunlar\n",
    "X_train = df_rf.dropna(subset=['A']).drop(columns=['A'])  # Boş olmayan qeydlər\n",
    "y_train = df_rf.dropna(subset=['A'])['A']\n",
    "X_missing = df_rf[df_rf['A'].isna()].drop(columns=['A'])\n",
    "\n",
    "# Random Forest modeli\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnoz\n",
    "df_rf.loc[df_rf['A'].isna(), 'A'] = rf_model.predict(X_missing)\n",
    "print(df_rf)\n",
    "```\n",
    "\n",
    "### 7. **Mean/Median/Mode Doldurma**\n",
    "Sadə, lakin effektiv metodlardan biri də orta (mean), median və ya mod (mode) dəyərlərlə boş hüceyrələri doldurmaqdır.\n",
    "\n",
    "```python\n",
    "# Mean ilə doldurma\n",
    "df_mean_filled = df.fillna(df.mean())\n",
    "print(df_mean_filled)\n",
    "\n",
    "# Median ilə doldurma\n",
    "df_median_filled = df.fillna(df.median())\n",
    "print(df_median_filled)\n",
    "\n",
    "# Mode (ən çox rast gəlinən dəyər) ilə doldurma\n",
    "df_mode_filled = df.fillna(df.mode().iloc[0])\n",
    "print(df_mode_filled)\n",
    "```\n",
    "\n",
    "### 8. **Group-Based Imputation**\n",
    "Bu metodda boş dəyərlər müəyyən bir qruplaşma əsasında, məsələn, kateqoriyalara görə, doldurulur.\n",
    "\n",
    "```python\n",
    "# Nümunə dataframe\n",
    "df_group = pd.DataFrame({\n",
    "    'Category': ['A', 'A', 'B', 'B', 'C'],\n",
    "    'Value': [10, np.nan, 20, np.nan, 30]\n",
    "})\n",
    "\n",
    "# Hər bir kateqoriya üçün orta dəyər ilə doldurma\n",
    "df_group['Value'] = df_group.groupby('Category')['Value'].transform(lambda x: x.fillna(x.mean()))\n",
    "print(df_group)\n",
    "```\n",
    "\n",
    "### Xülasə:\n",
    "Boş dəyərlərin doldurulması müxtəlif üsullarla həyata keçirilə bilər və seçilən metod məlumatların növünə və problemlərin xüsusiyyətlərinə görə dəyişir. İnkişaf etmiş metodlar, xüsusilə `KNN`, `MICE` və model əsaslı yanaşmalar, boş dəyərləri daha dəqiq və kompleks şəkildə doldurmaq üçün istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fdd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e015d50e",
   "metadata": {},
   "source": [
    "## Outlier problemini hell etmek advanced metodlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f564867",
   "metadata": {},
   "source": [
    "Outlier (kənar dəyərlər) problemini həll etmək üçün bir çox inkişaf etmiş metod mövcuddur. Kənar dəyərlər modelin performansını pisləşdirə bilər, ona görə də onları müəyyənləşdirmək və düzgün idarə etmək vacibdir. Aşağıda outlier-ları idarə etmək üçün istifadə olunan müxtəlif **advanced metodları** və onların **Python** ilə tətbiqini izah edəcəm:\n",
    "\n",
    "### 1. **IQR (Interquartile Range) ilə Outlier-ların Aşkarlanması və Silinməsi**\n",
    "Interquartile Range (IQR) metodu ilə kənar dəyərlər aşkarlanıb müəyyən həddlər daxilində silinə bilər.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Nümunə dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Value': [10, 12, 14, 16, 18, 1000, 20, 22, 24, 26, 10000]\n",
    "})\n",
    "\n",
    "# IQR hesablayırıq\n",
    "Q1 = df['Value'].quantile(0.25)\n",
    "Q3 = df['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Həddlər\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Outlier-ları silirik\n",
    "df_filtered = df[(df['Value'] >= lower_bound) & (df['Value'] <= upper_bound)]\n",
    "print(df_filtered)\n",
    "```\n",
    "\n",
    "### 2. **Z-Score (Standard Score) ilə Aşkarlanma**\n",
    "Z-Score hər bir müşahidənin standart sapmadan neçə dəfə uzaq olduğunu göstərir. Bu metod normal paylanmış verilənlər üçün uyğundur.\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Nümunə dataframe\n",
    "df['Z_Score'] = np.abs(stats.zscore(df['Value']))\n",
    "\n",
    "# Z-score dəyəri > 3 olan kənar dəyərləri silirik\n",
    "df_no_outliers = df[df['Z_Score'] < 3]\n",
    "print(df_no_outliers)\n",
    "```\n",
    "\n",
    "### 3. **Isolation Forest (İzolyasiya Meşəsi)**\n",
    "Bu metod, outlier-ları aşkarlamaq üçün `ensemble learning` metodudur. Məlumatları izolyasiya etmək üçün təsadüfi meşə alqoritmasından istifadə edir.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Nümunə dataframe\n",
    "df_iso = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 4, 5, 100],\n",
    "    'Feature2': [2, 3, 4, 5, 6, 200]\n",
    "})\n",
    "\n",
    "# Isolation Forest modelini tətbiq edirik\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df_iso['Outlier'] = iso_forest.fit_predict(df_iso)\n",
    "\n",
    "# Kənar dəyərləri silirik (Outlier=-1 olanları)\n",
    "df_cleaned = df_iso[df_iso['Outlier'] == 1]\n",
    "print(df_cleaned)\n",
    "```\n",
    "\n",
    "### 4. **Local Outlier Factor (LOF)**\n",
    "LOF metodu qonşuluq analizindən istifadə edərək outlier-ları müəyyən edir. Hər bir nöqtənin kənar olub-olmadığını həmin nöqtənin yaxın qonşuları ilə müqayisə edərək müəyyən edir.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Local Outlier Factor modelini tətbiq edirik\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "df['LOF_Score'] = lof.fit_predict(df[['Value']])\n",
    "\n",
    "# Kənar dəyərləri silirik\n",
    "df_no_lof_outliers = df[df['LOF_Score'] == 1]\n",
    "print(df_no_lof_outliers)\n",
    "```\n",
    "\n",
    "### 5. **Elliptic Envelope (Covariance Estimation)**\n",
    "Bu metod çoxölçülü paylanmaları təxmin etmək üçün istifadə olunur və kənar dəyərləri belə çoxölçülü mühitlərdə müəyyən edir.\n",
    "\n",
    "```python\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Nümunə dataframe\n",
    "df_elliptic = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 4, 5, 100],\n",
    "    'Feature2': [2, 3, 4, 5, 6, 200]\n",
    "})\n",
    "\n",
    "# Elliptic Envelope modelini tətbiq edirik\n",
    "elliptic = EllipticEnvelope(contamination=0.1)\n",
    "df_elliptic['Outlier'] = elliptic.fit_predict(df_elliptic[['Feature1', 'Feature2']])\n",
    "\n",
    "# Kənar dəyərləri silirik\n",
    "df_no_elliptic_outliers = df_elliptic[df_elliptic['Outlier'] == 1]\n",
    "print(df_no_elliptic_outliers)\n",
    "```\n",
    "\n",
    "### 6. **Winsorization (Kənar Dəyərləri Truncation)**\n",
    "Winsorization, kənar dəyərləri sadəcə silmək yerinə onları müəyyən bir həddə qədər “trim” etməklə həlledir. Bu metod modelin daha sabit nəticələr verməsinə kömək edir.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nümunə dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Value': [10, 12, 14, 16, 18, 1000, 20, 22, 24, 26, 10000]\n",
    "})\n",
    "\n",
    "# Winsorization (5%-lik üst və alt sərhəd trim edilir)\n",
    "df['Value'] = df['Value'].clip(lower=df['Value'].quantile(0.05), upper=df['Value'].quantile(0.95))\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 7. **Transformasiya**\n",
    "Məlumatların log və ya başqa transformasiyalara məruz qalması outlier-ların təsirini azalda bilər. Bu metod xüsusilə çox böyük kənar dəyərlərə malik olan paylanmalarda faydalıdır.\n",
    "\n",
    "#### Log Transformasiyası\n",
    "```python\n",
    "df['Log_Value'] = np.log(df['Value'] + 1)  # Log transformasiyası\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 8. **Box-Cox Transformasiyası**\n",
    "Box-Cox transformasiyası müxtəlif paylanmalar üçün outlier-ları idarə etməkdə təsirli olan güclü bir metoddur.\n",
    "\n",
    "```python\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Məlumatların Box-Cox ilə transformasiyası\n",
    "df['BoxCox_Value'], _ = boxcox(df['Value'] + 1)  # +1 sıfır və ya mənfi dəyərlər üçün əlavə edilir\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### 9. **Robust Scaler (Dayanıqlı Ölçəkləndirmə)**\n",
    "`Robust Scaler` outlier-ların təsirini azaltmaq üçün məlumatları median və interquartile range əsasında ölçəkləndirir.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Nümunə dataframe\n",
    "df_robust = pd.DataFrame({\n",
    "    'Value': [10, 12, 14, 16, 18, 1000, 20, 22, 24, 26, 10000]\n",
    "})\n",
    "\n",
    "# Robust Scaler tətbiq edirik\n",
    "scaler = RobustScaler()\n",
    "df_robust_scaled = pd.DataFrame(scaler.fit_transform(df_robust), columns=['Value'])\n",
    "print(df_robust_scaled)\n",
    "```\n",
    "\n",
    "### Xülasə:\n",
    "- **Outlier-ların silinməsi** üçün metodlar: **IQR**, **Z-Score**, **LOF**, **Isolation Forest**.\n",
    "- **Outlier-ları proqnozlaşdırma** və **düzəltmək** üçün metodlar: **Winsorization**, **Elliptic Envelope**, **Log və Box-Cox Transformasiya**.\n",
    "- Məlumat dəstində **outlier-ların təsirini azaltmaq** üçün isə **Robust Scaler** kimi metodlardan istifadə olunur.\n",
    "\n",
    "Bu metodlar arasından seçmək üçün məlumat dəstinin strukturu, paylanması və tələbləri nəzərə alınmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3bafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d9866c",
   "metadata": {},
   "source": [
    "## XGBOOST esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e625c",
   "metadata": {},
   "source": [
    "XGBoost-un əsas parametrləri və onların təklif olunan dəyərləri aşağıdakılardır:\n",
    "\n",
    "### 1. `booster`\n",
    "- **Izah**: Modelin öyrənmə üsulunu təyin edir. \n",
    "- **Dəyərlər**:\n",
    "  - `'gbtree'`: Ən çox istifadə edilən ağac əsaslı modeldir. Çoxsaylı qərar ağacları yaradır.\n",
    "  - `'gblinear'`: Xətti modeldir. Xətti birbaşa nəticələr təxmin edir.\n",
    "  - `'dart'`: Dropouts ilə ağac modelləri, daha çox tənzimləmə təmin edir.\n",
    "\n",
    "### 2. `learning_rate` (və ya `eta`)\n",
    "- **Izah**: Hər iterasiyada öyrənmə sürətini müəyyən edir. Kiçik dəyərlər daha yavaş öyrənmə sürəti təmin edir və daha çox iterasiya tələb edə bilər.\n",
    "- **Dəyərlər**: 0.01 ilə 0.3 arasında, məsələn, 0.01, 0.1, 0.2.\n",
    "\n",
    "### 3. `n_estimators`\n",
    "- **Izah**: Təlimdə istifadə olunan ağacların sayını göstərir.\n",
    "- **Dəyərlər**: 50 ilə 1000 arasında, məsələn, 100, 200, 500.\n",
    "\n",
    "### 4. `max_depth`\n",
    "- **Izah**: Hər bir qərar ağacının maksimum dərinliyini müəyyən edir. Daha böyük dəyərlər daha kompleks ağaclar yaradır.\n",
    "- **Dəyərlər**: 3 ilə 15 arasında, məsələn, 3, 6, 10.\n",
    "\n",
    "### 5. `min_child_weight`\n",
    "- **Izah**: Bir düyündəki minimum nümunə çəkisini təyin edir. Bu, bir düyünün bölünməsi üçün tələb olunan minimal nümunə çəkisini göstərir.\n",
    "- **Dəyərlər**: 1 ilə 10 arasında, məsələn, 1, 3, 5.\n",
    "\n",
    "### 6. `subsample`\n",
    "- **Izah**: Hər iterasiyada istifadə olunan nümunələrin hissəsini müəyyən edir. Kiçik dəyərlər daha çox tənzimləmə təmin edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 7. `colsample_bytree`\n",
    "- **Izah**: Hər ağac üçün istifadə olunan xüsusiyyətlərin hissəsini müəyyən edir. Bu parametr xüsusiyyətlərin təsadüfi alt qruplarını istifadə edir.\n",
    "- **Dəyərlər**: 0.5 ilə 1.0 arasında, məsələn, 0.5, 0.8, 1.0.\n",
    "\n",
    "### 8. `lambda` (və ya `reg_lambda`)\n",
    "- **Izah**: L2 (Ridge) cərimələmə parametridir. Modelin sadələşdirilməsinə kömək edir və aşırı uyğunlaşmanı azaldır.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 9. `alpha` (və ya `reg_alpha`)\n",
    "- **Izah**: L1 (Lasso) cərimələmə parametridir. Bu, bəzi xüsusiyyətlərin çəkilərini sıfıra endirərək modelin sadələşdirilməsinə kömək edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 10.\n",
    "\n",
    "### 10. `gamma`\n",
    "- **Izah**: Bölünmə üçün tələb olunan minimal qazanma dəyərini təyin edir. Daha yüksək dəyərlər ağacların bölünməsini daha sərt edir.\n",
    "- **Dəyərlər**: 0 ilə 10 arasında, məsələn, 0, 1, 5.\n",
    "\n",
    "### 11. `scale_pos_weight`\n",
    "- **Izah**: Sinif balansı məsələlərində istifadə olunur, çünki modelin siniflərin tarazlığına uyğunlaşdırılmasına kömək edir.\n",
    "- **Dəyərlər**: 1 ilə (nöqtələr arasındakı fərqə əsasən dəyişir), məsələn, 1, 10, 20.\n",
    "\n",
    "### 12. `objective`\n",
    "- **Izah**: Məqsəd funksiyasını müəyyən edir. Problemin növünə görə dəyişir, məsələn, `reg:squarederror` (regressiya üçün), `binary:logistic` (ikili təsnifat üçün).\n",
    "- **Dəyərlər**:\n",
    "  - `'reg:squarederror'`: Regressiya üçün.\n",
    "  - `'binary:logistic'`: İkili təsnifat üçün.\n",
    "  - `'multi:softmax'`: Çoxsinifli təsnifat üçün (proqnozlar sinif etiketləri şəklində verilir).\n",
    "\n",
    "### 13. `eval_metric`\n",
    "- **Izah**: Modelin qiymətləndirilməsi üçün istifadə olunan ölçü vahidini müəyyən edir.\n",
    "- **Dəyərlər**:\n",
    "  - `'rmse'`: Kök orta kvadrat səhv (regressiya üçün).\n",
    "  - `'logloss'`: Loqaritmik itki (ikili təsnifat üçün).\n",
    "  - `'mlogloss'`: Çoxsinifli loqaritmik itki (çoxsinifli təsnifat üçün).\n",
    "\n",
    "Parametrləri tənzimləyərkən, ən yaxşı nəticələri əldə etmək üçün çapraz təsdiqləmə (cross-validation) istifadə edərək, hər parametrin müxtəlif dəyərlərini yoxlamaq tövsiyə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664514d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06f0c06",
   "metadata": {},
   "source": [
    "## GridSearchCV esas parametrler , onlarin izahi ve deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eea088",
   "metadata": {},
   "source": [
    "`GridSearchCV`, verilən parametrlər üçün bütün mümkün kombinasiyaları sınayaraq ən yaxşı nəticəni tapmaq üçün istifadə olunur. Bu alqoritm hər bir parametri müxtəlif dəyərlərlə sınayır və bu, daha dəqiq bir optimallaşdırma aparmağa imkan verir. Lakin, çox sayda parametr və dəyərlər olduqda bu proses çox vaxt ala bilər.\n",
    "\n",
    "### Əsas Parametrlər və Onların İzahı\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **Açıqlama**: Parametrlərini optimallaşdırmaq istədiyiniz modeldir. Bu model, `fit` və `predict` metodlarına sahib olmalıdır.\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()`, `LogisticRegression()`, və s.\n",
    "\n",
    "2. **`param_grid`**:\n",
    "   - **Açıqlama**: Modelin optimallaşdırılacaq hiperparametrlərinin dəyərlərini təyin edən grid (tor) strukturdur. Bütün parametrlərin dəyərlərini sınamaq üçün istifadə olunur.\n",
    "   - **Dəyərlər**: Parametrlərin mümkün dəyərlərini ehtiva edən `dict` formatında olmalıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     param_grid = {\n",
    "         'n_estimators': [50, 100, 200],\n",
    "         'learning_rate': [0.01, 0.1, 0.2],\n",
    "         'max_depth': [3, 5, 7],\n",
    "         'subsample': [0.6, 0.8, 1.0],\n",
    "         'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün istifadə olunan qiymətləndirmə kriteriyasıdır.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`\n",
    "\n",
    "4. **`cv`** (Cross-validation):\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə prosesində məlumatları neçə qrupa bölmək lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri (məsələn, 3, 5, 10).\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "5. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir. Bütün nüvələri istifadə etməklə paralelləşdirilmiş hesablama mümkündür.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "6. **`verbose`**:\n",
    "   - **Açıqlama**: Axtarış prosesi zamanı nə qədər detallı geri bildirim göstəriləcəyini təyin edir.\n",
    "   - **Dəyərlər**: 0 (səssiz rejim), 1 (minimum məlumat), 2 və daha yüksək (daha çox məlumat).\n",
    "   - **Nümunə**: `verbose=2`\n",
    "\n",
    "7. **`refit`**:\n",
    "   - **Açıqlama**: Ən yaxşı tapılan parametrlərlə modelin yenidən uyğunlaşdırılıb-uyğunlaşdırılmayacağını təyin edir.\n",
    "   - **Dəyərlər**: `True` (model refit ediləcək), `False` (model refit edilməyəcək).\n",
    "   - **Nümunə**: `refit=True`\n",
    "\n",
    "8. **`pre_dispatch`**:\n",
    "   - **Açıqlama**: Paralelləşdirilmiş hesablama zamanı nə qədər iş tapşırığının əvvəlcədən göndərilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `int` dəyəri və ya `'2*n_jobs'` kimi nisbət.\n",
    "   - **Nümunə**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "9. **`error_score`**:\n",
    "   - **Açıqlama**: Modelin bir parametri ilə səhv baş verərsə, o vəziyyətdə neçə xal verilməli olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `float` və ya `np.nan` dəyəri (nəticə olmaması).\n",
    "   - **Nümunə**: `error_score=np.nan`\n",
    "\n",
    "### GridSearchCV ilə XGBClassifier tətbiqi\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr grid-i\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# XGBClassifier modeli\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# GridSearchCV obyektinin yaradılması\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Modelin uyğunlaşdırılması (fit)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və nəticəni çap etmək\n",
    "print(f\"Ən yaxşı parametrlər: {grid_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {grid_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### Parametrlərin Dəyərlər Aralıqları\n",
    "\n",
    "1. **`n_estimators`** (Ağac sayı):\n",
    "   - Aralıq: [50, 100, 200, 500]\n",
    "   \n",
    "2. **`max_depth`** (Ağacların dərinliyi):\n",
    "   - Aralıq: [3, 5, 7, 10]\n",
    "   \n",
    "3. **`learning_rate`** (Öyrənmə sürəti):\n",
    "   - Aralıq: [0.01, 0.05, 0.1, 0.2]\n",
    "   \n",
    "4. **`subsample`** (Nümunə payı):\n",
    "   - Aralıq: [0.5, 0.7, 1.0]\n",
    "   \n",
    "5. **`colsample_bytree`** (Xüsusiyyət payı):\n",
    "   - Aralıq: [0.6, 0.8, 1.0]\n",
    "\n",
    "6. **`gamma`** (Bölünmə üçün minimum itki azaldılması):\n",
    "   - Aralıq: [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "Bu aralıqlar verilən problemə görə genişlədilə və ya daraldıla bilər. `GridSearchCV` bütün bu aralıqlardakı mümkün kombinasiyaları sınayaraq ən optimal parametrləri təyin edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889bfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bace169f",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1859a5",
   "metadata": {},
   "source": [
    "`RandomizedSearchCV` istifadə edərkən, müxtəlif parametrlər var və hər birinin müəyyən funksiyası var. Hər parametrin də nə dəyər ala biləcəyini və bu dəyərlərin aralıqlarını başa düşmək vacibdir. Gəlin, əsas parametrləri və onların izahını nəzərdən keçirək.\n",
    "\n",
    "### `RandomizedSearchCV` əsas parametrləri və izahı:\n",
    "\n",
    "1. **`estimator`**:\n",
    "   - **İzah**: Axtarılacaq model. Məsələn, `XGBClassifier`, `SVC`, `RandomForestClassifier`, və s.\n",
    "   - **Dəyər aralığı**: Bir machine learning modeli (`estimator`) obyektini alır.\n",
    "\n",
    "2. **`param_distributions`**:\n",
    "   - **İzah**: Hiperparametrlərin təsadüfi axtarışı üçün parametr dəyərlərinin aralıqları.\n",
    "   - **Dəyər aralığı**: Hiperparametrlərin mümkün dəyərlərinin siyahısını və ya numpy array-i kimi obyektlər alır.\n",
    "   - **Misal**:\n",
    "     ```python\n",
    "     param_dist = {\n",
    "         'n_estimators': np.arange(50, 400, 50),\n",
    "         'max_depth': [3, 4, 5, 6],\n",
    "         'learning_rate': np.linspace(0.01, 0.3, 10)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **`n_iter`**:\n",
    "   - **İzah**: Axtarış zamanı təsadüfi olaraq neçə kombinasiya yoxlanılacaq.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (adətən 10-100 arası). Axtarışın sürəti və keyfiyyəti arasında balans yaradılır.\n",
    "   - **Misal**: `n_iter=50`\n",
    "\n",
    "4. **`cv`**:\n",
    "   - **İzah**: Çarpaz təsdiqləmə qatlarının sayı (neçə fold ilə model yoxlanılacaq).\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd (məsələn, `cv=3`, `cv=5`). Daha çox qat dəqiq nəticələr verə bilər, lakin təlim müddətini uzadır.\n",
    "   - **Misal**: `cv=5` (beş qatlı çarpaz təsdiqləmə)\n",
    "\n",
    "5. **`scoring`**:\n",
    "   - **İzah**: Modelin performansını qiymətləndirmək üçün ölçü.\n",
    "   - **Dəyər aralığı**: `accuracy`, `f1`, `roc_auc`, `precision`, `recall`, və s.\n",
    "   - **Misal**: `scoring='accuracy'`, `scoring='roc_auc'`\n",
    "\n",
    "6. **`random_state`**:\n",
    "   - **İzah**: Təkrarlanabilirlik üçün təsadüfi toxum. Nəticələrin təkrarlanabilməsi üçün istifadə edilir.\n",
    "   - **Dəyər aralığı**: İstənilən tam ədəd (məsələn, `random_state=42`).\n",
    "   - **Misal**: `random_state=42`\n",
    "\n",
    "7. **`verbose`**:\n",
    "   - **İzah**: Təlim prosesi haqqında ekrana nə qədər məlumat çıxarılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: `0`, `1`, `2`, və ya daha yüksək (daha yüksək dəyərlər daha çox məlumat göstərir).\n",
    "   - **Misal**: `verbose=1` (orta dərəcədə məlumat)\n",
    "\n",
    "8. **`n_jobs`**:\n",
    "   - **İzah**: Paralel işləmə üçün istifadə ediləcək CPU nüvələrinin sayı.\n",
    "   - **Dəyər aralığı**: Müsbət tam ədəd, `-1` bütün nüvələri istifadə etmək deməkdir.\n",
    "   - **Misal**: `n_jobs=-1` (bütün mövcud CPU nüvələrini istifadə etmək üçün)\n",
    "\n",
    "9. **`pre_dispatch`**:\n",
    "   - **İzah**: Paralel axtarış üçün neçə iş prosesinin əvvəlcədən yaradılacağını təyin edir.\n",
    "   - **Dəyər aralığı**: Tam ədəd və ya `all` kimi dəyərlər ala bilər.\n",
    "   - **Misal**: `pre_dispatch='2*n_jobs'`\n",
    "\n",
    "### XGBClassifier ilə `RandomizedSearchCV` misalı:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parametr aralığı\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 400, 50),       # Ağac sayı\n",
    "    'max_depth': np.arange(3, 10),                # Ağacların dərinliyi\n",
    "    'learning_rate': np.linspace(0.01, 0.3, 10),  # Öyrənmə sürəti\n",
    "    'subsample': np.linspace(0.5, 1.0, 5),        # Nümunə payı\n",
    "    'colsample_bytree': np.linspace(0.5, 1.0, 5), # Xüsusiyyət payı\n",
    "    'gamma': np.linspace(0, 0.5, 5),              # Bölünmə üçün minimum itki azaldılması\n",
    "    'reg_alpha': np.linspace(0, 1, 5),            # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': np.linspace(1, 2, 5),           # L2 cəriməsi (Ridge)\n",
    "    'booster': ['gbtree', 'dart']                 # Booster növü\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# RandomizedSearchCV obyektinin yaradılması\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50,           # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=5,                 # Çarpaz təsdiqləmə qatları\n",
    "    verbose=1,            # Geri bildirim səviyyəsi\n",
    "    random_state=42,      # Təsadüfi toxum\n",
    "    n_jobs=-1             # Bütün CPU nüvələrini istifadə et\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {random_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {random_search.best_score_}\")\n",
    "```\n",
    "\n",
    "Bu kod `RandomizedSearchCV` istifadə edərək `XGBClassifier` modelinin hiperparametrlərini optimallaşdırır və ən yaxşı parametrləri çıxarır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9486f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3063f1",
   "metadata": {},
   "source": [
    "## BayesSearchCV esas parametrler , onlarin izahi ve bu parametrlerin deyer araliqlari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07a07e",
   "metadata": {},
   "source": [
    "`BayesSearchCV` modeli, hiperparametr optimallaşdırması üçün Bayesian Optimization metodundan istifadə edən bir alqoritmdir. Bu metod, hiperparametr arama prosesini daha ağıllı və sürətli edir, çünki hər bir axtarış addımı əvvəlki nəticələrə əsaslanaraq həyata keçirilir.\n",
    "\n",
    "### 1. **`estimator`**:\n",
    "   - **Açıqlama**: Optimallaşdırmaq istədiyiniz model obyektidir (məsələn, `XGBClassifier`).\n",
    "   - **Nümunə**: `XGBClassifier()`, `RandomForestClassifier()`, `SVC()` və s.\n",
    "\n",
    "### 2. **`search_spaces`**:\n",
    "   - **Açıqlama**: Bu parametr, optimallaşdırılacaq hiperparametrləri və onların dəyərlər aralığını təyin edir. `BayesSearchCV` bu aralığa əsaslanaraq axtarış həyata keçirir.\n",
    "   - **Dəyərlər**: Müxtəlif parametrlərin dəyərlər aralığıdır.\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     search_spaces = {\n",
    "         'n_estimators': (50, 400),\n",
    "         'learning_rate': (0.01, 0.3, 'log-uniform'),\n",
    "         'max_depth': (3, 10),\n",
    "         'subsample': (0.5, 1.0),\n",
    "         'colsample_bytree': (0.5, 1.0),\n",
    "         'gamma': (0, 0.5),\n",
    "         'reg_alpha': (0, 1),\n",
    "         'reg_lambda': (1, 2)\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### 3. **`n_iter`**:\n",
    "   - **Açıqlama**: Axtarış zamanı edilməsi planlaşdırılan iterasiya sayı. Bu dəyər Bayesian Optimization ilə təyin olunan fərqli hiperparametr dəstlərinin sayını bildirir.\n",
    "   - **Dəyərlər**: Müxtəlif iterasiya sayı.\n",
    "   - **Nümunə**: `n_iter=30` (30 iterasiya).\n",
    "\n",
    "### 4. **`cv`**:\n",
    "   - **Açıqlama**: Çarpaz təsdiqləmə (Cross-validation) qatlarının sayını təyin edir. Məlumatları neçə qrupa böləcəyini göstərir.\n",
    "   - **Dəyərlər**: 3, 5 və ya 10.\n",
    "   - **Nümunə**: `cv=5` (5 qatlı çarpaz təsdiqləmə).\n",
    "\n",
    "### 5. **`scoring`**:\n",
    "   - **Açıqlama**: Model performansını qiymətləndirmək üçün hansı metrikdən istifadə olunacağını təyin edir.\n",
    "   - **Dəyərlər**: `'accuracy'`, `'f1'`, `'roc_auc'`, `'precision'`, `'recall'` və s.\n",
    "   - **Nümunə**: `scoring='accuracy'`.\n",
    "\n",
    "### 6. **`n_points`**:\n",
    "   - **Açıqlama**: Hər iterasiyada eyni anda neçə fərqli parametri sınamaq lazım olduğunu təyin edir.\n",
    "   - **Dəyərlər**: `n_points=1` (hər iterasiyada bir parametr dəsti sınanır), `n_points=2` (hər iterasiyada iki parametr dəsti sınanır).\n",
    "   - **Nümunə**: `n_points=3` (hər iterasiyada 3 fərqli parametr dəsti sınanır).\n",
    "\n",
    "### 7. **`n_jobs`**:\n",
    "   - **Açıqlama**: Modeli təlim edərkən neçə CPU nüvəsini istifadə edəcəyini təyin edir.\n",
    "   - **Dəyərlər**: `-1` bütün CPU nüvələrini istifadə edəcək, `1` tək CPU nüvəsi ilə işləyəcək.\n",
    "   - **Nümunə**: `n_jobs=-1` (bütün nüvələri istifadə etmək).\n",
    "\n",
    "### 8. **`random_state`**:\n",
    "   - **Açıqlama**: Nəticələrin təkrar istehsal edilə bilməsi üçün təsadüfi toxum dəyəri təyin edir.\n",
    "   - **Dəyərlər**: İstənilən integer dəyəri.\n",
    "   - **Nümunə**: `random_state=42`.\n",
    "\n",
    "### 9. **`optimizer_kwargs`**:\n",
    "   - **Açıqlama**: Bayesian Optimization alqoritmi ilə bağlı əlavə seçimləri idarə edir (məsələn, akvizisiya funksiyası).\n",
    "   - **Nümunə**:\n",
    "     ```python\n",
    "     optimizer_kwargs = {\n",
    "         'base_estimator': 'GP',  # Gaussian Process (GP) ilə optimizasiya edəcək\n",
    "         'acq_func': 'EI'         # Expected Improvement (EI) akvizisiya funksiyası\n",
    "     }\n",
    "     ```\n",
    "\n",
    "### XGBClassifier ilə `BayesSearchCV` tətbiqi nümunəsi:\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Hiperparametr aralığı\n",
    "search_spaces = {\n",
    "    'n_estimators': (50, 400),                # Ağac sayı\n",
    "    'learning_rate': (0.01, 0.3, 'log-uniform'), # Öyrənmə sürəti\n",
    "    'max_depth': (3, 10),                     # Maksimum dərinlik\n",
    "    'subsample': (0.5, 1.0),                  # Nümunə payı\n",
    "    'colsample_bytree': (0.5, 1.0),           # Xüsusiyyət payı\n",
    "    'gamma': (0, 0.5),                        # Minimum itki azaldılması\n",
    "    'reg_alpha': (0, 1),                      # L1 cəriməsi (Lasso)\n",
    "    'reg_lambda': (1, 2)                      # L2 cəriməsi (Ridge)\n",
    "}\n",
    "\n",
    "# Modelin yaradılması\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# BayesSearchCV obyektinin yaradılması\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=xgb, \n",
    "    search_spaces=search_spaces, \n",
    "    n_iter=30,            # Təsadüfi kombinasiya sayı\n",
    "    scoring='accuracy',   # Dəyərləndirmə kriteriyası\n",
    "    cv=3,                 # Çarpaz təsdiqləmə qatları\n",
    "    n_jobs=-1,            # Bütün CPU nüvələrini istifadə et\n",
    "    random_state=42       # Təsadüfi toxum\n",
    ")\n",
    "\n",
    "# Modeli uyğunlaşdırmaq (fit)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Ən yaxşı parametrləri və performansı çıxarmaq\n",
    "print(f\"Ən yaxşı parametrlər: {bayes_search.best_params_}\")\n",
    "print(f\"Ən yaxşı skor: {bayes_search.best_score_}\")\n",
    "```\n",
    "\n",
    "### BayesSearchCV-nin əsas üstünlükləri:\n",
    "- **Təsadüfi olmaması**: Bayesian Optimization əvvəlki nəticələrə əsaslanaraq növbəti parametri ağıllı şəkildə seçir.\n",
    "- **Daha az iterasiya**: RandomizedSearchCV ilə müqayisədə daha az iterasiya ilə daha optimal nəticələr əldə etmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df419cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40fef1d9",
   "metadata": {},
   "source": [
    "## GridSearchCV, RandomizedSearchCV, BayesSearchCV ferqler cedveli?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8222994",
   "metadata": {},
   "source": [
    "Aşağıda metodları yuxarıda göstərən cədvəl formasını təqdim edirəm:\n",
    "\n",
    "| **Metod**             | **GridSearchCV**                                                            | **RandomizedSearchCV**                                                   | **BayesSearchCV**                                                        |\n",
    "|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
    "| **İş Prinsipi**        | Bütün parametrlərin mümkün kombinasiyalarını yoxlayır.                       | Parametrlərin mümkün dəyərlərini təsadüfi seçir və müəyyən sayda kombinasiyanı yoxlayır. | Keçmiş nəticələrə əsasən növbəti sınanacaq parametrləri daha ağıllı seçir. |\n",
    "| **Üstünlüklər**        | Tam dəqiqliklə optimal parametrləri tapır.                                   | Daha sürətli və resurs qənaətcilidir.                                     | Daha ağıllı seçimlər edir, daha səmərəli və tez converge olur.             |\n",
    "| **Çatışmazlıqlar**     | Hesablama baxımından çox vaxt və resurs tələb edir.                          | Tam optimal parametrləri tapmaya bilər.                                   | Implementasiyası daha mürəkkəb ola bilər.                                  |\n",
    "| **Nə vaxt istifadə olunur** | Kiçik parametrlər sahəsində tam optimal parametrləri tapmaq istəyəndə.       | Böyük datasetlər və çoxlu sayda parametrlərlə işləyərkən.                 | Daha böyük parametrlər sahəsi üçün və daha səmərəli hyperparameter tuning üçün. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3679672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267a3c5b",
   "metadata": {},
   "source": [
    "## K-means ve AgglomerativeClustering ferqi cedvel seklinde cap ele?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68addf",
   "metadata": {},
   "source": [
    "Aşağıda K-Means və Agglomerative Clustering modellərinin fərqlərini cədvəl şəklində göstərə bilərik:\n",
    "\n",
    "| **Xüsusiyyət**                    | **K-Means**                                 | **Agglomerative Clustering**                        |\n",
    "|-----------------------------------|---------------------------------------------|----------------------------------------------------|\n",
    "| **Metod**                         | Mərkəzə əsaslanan klasterləşdirmə            | Hierarchical (Aqqlomerativ) klasterləşdirmə        |\n",
    "| **İşləmə Prosesi**                | Klaster mərkəzləri təyin edilir və təkrarən yenilənir. | Aşağıdan yuxarıya doğru qruplar birləşdirilir.      |\n",
    "| **Klaster Sayı**                  | Əvvəldən müəyyən edilməlidir (`n_clusters`).  | Əvvəldən müəyyən edilməlidir (`n_clusters`).        |\n",
    "| **Hesablama Mürəkkəbliyi**        | Nisbətən sürətli, \\(O(n \\cdot k \\cdot d)\\)   | Nisbətən yavaş, \\(O(n^3)\\)                          |\n",
    "| **Klaster Forması**               | Sferik (klasterlər dairəvi olur)             | Daha çox forma elastikliyi, qeyri-sferik klasterlər |\n",
    "| **Məlumat Strukturu**             | Yalnız nöqtələr və klaster mərkəzləri        | Dendrogram (ağac strukturu)                        |\n",
    "| **Məsafə Ölcüsü**                 | Adətən Euclidean məsafəsi                    | Müxtəlif məsafə metrikləri (Euclidean, Manhattan)  |\n",
    "| **Klaster Mərkəzi**               | Hər klaster üçün bir mərkəz nöqtəsi vardır   | Klasterlərin mərkəzi anlayışı yoxdur               |\n",
    "| **Böyük Məlumatlar**              | Daha çox böyük məlumat dəstləri üçün uyğundur | Kiçik məlumat dəstləri üçün daha uyğundur          |\n",
    "| **Təsirli Klasterlərin Sayı**     | Sabit klaster sayı verir                     | Hierarchical model olaraq müxtəlif səviyyələrdə klasterlər yarada bilir |\n",
    "| **İşləmə Sürəti**                 | Daha sürətli                                 | Daha yavaş                                          |\n",
    "| **Təsadüfi Başlanğıc Problemi**    | Bəli, təsadüfi başlanğıc nöqtələri istifadə edir, nəticələr dəyişə bilər | Xeyr, başlanğıc nöqtələri sabitdir                |\n",
    "\n",
    "Bu cədvəl K-Means və Agglomerative Clustering modelləri arasında əsas fərqləri əks etdirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11275027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51b820f",
   "metadata": {},
   "source": [
    "## K-means ve AgglomerativeClustering isleme prinsipleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a298c02",
   "metadata": {},
   "source": [
    "Aşağıda K-Means və Agglomerative Clustering modellərinin işləmə prinsiplərini cədvəl şəklində verək:\n",
    "\n",
    "| **Xüsusiyyət**                  | **K-Means İşləmə Prinsipi**                                                                 | **Agglomerative Clustering İşləmə Prinsipi**                                                        |\n",
    "|---------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **Başlanğıc**                   | Klaster mərkəzləri (centroids) təsadüfi olaraq seçilir.                                      | Hər bir nöqtə əvvəlcə öz-özünə klasterdir.                                                           |\n",
    "| **Addım 1: Klasterləşdirmə**     | Hər bir nöqtə ona ən yaxın mərkəzə (centroid) təyin edilir.                                 | Ən yaxın nöqtələr (və ya klasterlər) cüt-cüt birləşdirilir.                                          |\n",
    "| **Addım 2: Mərkəz Yenilənməsi** | Hər bir klasterin mərkəzi (centroid) həmin klasterdəki nöqtələrin ortası olaraq yenilənir.  | Qonşu klasterlər birləşdirildikcə klaster mərkəzləri təyin olunmur, klasterlər ardıcıl birləşdirilir. |\n",
    "| **Təkrarlama**                  | Nöqtələr yenidən ən yaxın mərkəzə təyin edilir, bu proses mərkəzlər sabit qalana qədər təkrarlanır. | Bütün nöqtələr birləşdirilənə qədər bu birləşmə prosesi davam edir.                                   |\n",
    "| **Nəticə**                      | Sabit klaster mərkəzləri və ona uyğun nöqtələr ilə bitir.                                    | Klasterlər aralarındakı məsafələrə əsasən hierarchical (ağacvari) formada təşkil olunur.               |\n",
    "| **Klaster Sayı**                | İstifadəçi əvvəlcədən klaster sayını müəyyən etməlidir.                                      | İstifadəçi əvvəlcədən klaster sayını müəyyən etməli, ya da dendrogram vasitəsilə qərar verə bilər.     |\n",
    "| **Məsafə Ölçüsü**               | Nöqtələr mərkəzlərdən Euclidean məsafəsinə görə təyin edilir.                                | Nöqtələr arası Euclidean, Manhattan və ya digər məsafə metriklərinə görə ən yaxın klaster tapılır.     |\n",
    "| **Təkrarlanma Sayı**            | Mərkəzlər sabitləşənə qədər təkrarlanır.                                                    | Bütün nöqtələr tək bir klasterə birləşdirilənə qədər təkrarlanır.                                     |\n",
    "\n",
    "Bu cədvəl, hər iki modelin işləmə prinsiplərini sadə və qarşılaşdırmalı şəkildə izah edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc42a2d0",
   "metadata": {},
   "source": [
    "## Autocorrelation (Avtokorrelyasiya) ve  Multicollinearity (Çoxxəttlik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb1d7c",
   "metadata": {},
   "source": [
    "**Autocorrelation** və **Multicollinearity** statistik modellərdə fərqli problemlər və anlayışlar olaraq çıxış edir. Onları ayrı-ayrılıqda izah edim:\n",
    "\n",
    "### 1. **Autocorrelation (Avtokorrelyasiya):**\n",
    "Avtokorrelyasiya, bir zaman seriyasında müşahidələrin öz əvvəlki dəyərləri ilə bağlı olduğu vəziyyətdir. Yəni, bir dəyişənin cari dəyəri əvvəlki zaman nöqtələrindəki dəyərləri ilə korrelyasiyadadır. Avtokorrelyasiya zamanı ardıcıl müşahidələr bir-birindən asılı ola bilər.\n",
    "\n",
    "- **Məqsəd:** Avtokorrelyasiyanı müəyyən etmək üçün əsasən zaman seriyası analizində istifadə edilir.\n",
    "- **Nümunə:** Hava istiliyi seriyasında bugünkü istilik dəyərinin dünənki və ondan əvvəlki günlərlə bağlı olması.\n",
    "- **Problem:** Avtokorrelyasiya olduqda statistik modellər (məsələn, regresiya modelləri) düzgün nəticələr verməyə bilər, çünki müşahidələr arasında müstəqillik pozulur.\n",
    "\n",
    "#### Avtokorrelyasiyanı aşkar etmək üçün metodlar:\n",
    "- **Durbin-Watson testi**: Bu test avtokorrelyasiyanın olub-olmamasını yoxlayır. Dəyərlər 0-dan 4-ə qədər olur, 2 isə heç bir avtokorrelyasiyanın olmadığını göstərir.\n",
    "\n",
    "### 2. **Multicollinearity (Çoxxəttlik):**\n",
    "Çoxxəttlik, bir neçə müstəqil dəyişənin bir-biri ilə güclü korrelyasiya göstərdiyi vəziyyətdir. Bu, əsasən çoxsaylı xətti regresiya modellərində ortaya çıxır və modelin dəqiq interpretasiyasını çətinləşdirir.\n",
    "\n",
    "- **Məqsəd:** Multicollinearity-dən qaçınmaq, modelin sadəliyini və proqnoz gücünü qorumaq üçün vacibdir.\n",
    "- **Nümunə:** \"Ev sahəsinin ölçüsü\" və \"Otaq sayı\" kimi dəyişənlər bir-birinə güclü korrelyasiya göstərə bilər.\n",
    "- **Problem:** Çoxxəttlik olduqda regresiya əmsalları qeyri-sabit ola bilər, yəni kiçik dəyişikliklər modeldəki əmsalları kəskin dəyişdirə bilər.\n",
    "\n",
    "#### Multicollinearity aşkar etmək üçün metodlar:\n",
    "- **Variance Inflation Factor (VIF)**: VIF dəyərləri dəyişənlərin çoxxəttlik probleminin nə qədər ciddi olduğunu göstərir. VIF dəyəri 5-dən böyük olduqda, çoxxəttlik ciddi bir problem sayılır.\n",
    "\n",
    "Bu anlayışlar fərqli olsa da, hər ikisi statistik modellərin düzgün işləməsi üçün nəzərə alınmalı olan amillərdir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3be0e",
   "metadata": {},
   "source": [
    "## Avtokorrelyasiya və çoxxəttlik üçün real misallar aşağıdakı kimidir:\n",
    "\n",
    "### 1. **Autocorrelation (Avtokorrelyasiya):**\n",
    "**Misal:** Hava istiliyi proqnozları.\n",
    "\n",
    "- **Real vəziyyət:** Bir şəhərin gündəlik hava istiliklərini müşahidə edirik. Deyək ki, dünən 30°C idi, bu gün isə 31°C. Sabahın proqnozu da dünən və bu günkü temperaturla bağlıdır. Beləliklə, bugünkü istilik sabahın istiliyini müəyyən dərəcədə təsir edə bilər. Bu ardıcıl müşahidələr arasındakı asılılıq avtokorrelyasiyadır.\n",
    "  \n",
    "- **Problem:** Hava proqnozu kimi zaman seriyasında ardıcıl dəyərlər bir-birinə çox bağlıdırsa, avtokorrelyasiya problemi yaranır və bunu nəzərə almayan modellər səhv nəticələr verə bilər.\n",
    "\n",
    "**Praktik təsir:** Bir iqtisadiyyatda aylıq satış rəqəmlərinin əvvəlki aylara təsir etməsi də avtokorrelyasiya ilə əlaqəli ola bilər. Məsələn, şirkət dekabr ayında çox satış edirsə, yanvar ayında həmin trend davam edə bilər.\n",
    "\n",
    "### 2. **Multicollinearity (Çoxxəttlik):**\n",
    "**Misal:** Ev qiymətləri ilə bağlı analiz.\n",
    "\n",
    "- **Real vəziyyət:** Bir şəhərdə ev qiymətləri modelini qurmaq istəyirsiniz. Modelinizdə dəyişənlər kimi evin sahəsi, otaq sayı, və hamam sayı kimi məlumatlar var. Evin sahəsi ilə otaq sayı arasında güclü bir əlaqə var, çünki daha böyük sahəsi olan evlər daha çox otağa malikdir.\n",
    "\n",
    "- **Problem:** Bu vəziyyətdə müstəqil dəyişənlər bir-biri ilə çox yüksək korrelyasiya göstərir. Bu, çoxxəttlik problemidir. Modelinizdə həm evin sahəsi, həm də otaq sayı varsa, bu iki dəyişən bir-birini təkrarlaya bilər və nəticədə modeldəki regresiya əmsalları qeyri-sabit olur.\n",
    "\n",
    "**Praktik təsir:** İqtisadi modeldə həm ümumi gəliri, həm də şəxsi gəliri müstəqil dəyişən kimi istifadə etmək çoxxəttliyə səbəb ola bilər, çünki şəxsi gəlir ümumi gəlirin bir hissəsidir. Bu da modelin proqnoz gücünü zəiflədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482c15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a162c2",
   "metadata": {},
   "source": [
    "## LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0eae8e",
   "metadata": {},
   "source": [
    "Bu kod, DataFrame-də obyekt (`object`) tipində olan sütunları `LabelEncoder` istifadə edərək ədədi dəyərlərə çevirir və hər sütun üçün istifadə olunan `LabelEncoder` obyektini saxlayır. Məsələn:\n",
    "\n",
    "```python\n",
    "label_encoders = {}  # Hər sütun üçün LabelEncoder obyektlərini saxlamaq üçün boş lüğət yaradılır.\n",
    "\n",
    "# Obyekt tipli sütunlar arasında dövr edilir.\n",
    "for column in df_final_2.select_dtypes(include=['object']).columns:  \n",
    "    le = LabelEncoder()  # Hər sütun üçün yeni LabelEncoder obyekti yaradılır.\n",
    "    # Həmin sütun ədədi dəyərlərə çevrilir və DataFrame-dəki həmin sütunu əvəz edir.\n",
    "    df_final_2[column] = le.fit_transform(df_final_2[column])  \n",
    "    label_encoders[column] = le  # LabelEncoder obyekti saxlanılır.\n",
    "```\n",
    "\n",
    "\n",
    "## Funksiya seklinde\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# DataFrame-in hər bir sütunu üçün label encoder tətbiq edən funksiyanı yazırıq\n",
    "def label_encode_all_columns(df):\n",
    "    labelencoder = LabelEncoder()\n",
    "    # Yalnız object və ya category tipində olan sütunları seçib kodlaşdırırıq\n",
    "    for column in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        df[column] = labelencoder.fit_transform(df[column])\n",
    "    return df\n",
    "\n",
    "# Məlumatları kodlaşdırırıq\n",
    "df = label_encode_all_columns(df)\n",
    "\n",
    "# Nəticəyə baxa bilərsiniz\n",
    "print(df.head())\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "1. İlk olaraq, boş bir `label_encoders` lüğəti yaradılır. Bu lüğətdə sütun adlarına uyğun olaraq `LabelEncoder` obyektləri saxlanacaq. Bu, sonradan lazım olarsa, kodlaşdırılmış dəyərləri orijinal vəziyyətinə geri çevirməyə imkan verir.\n",
    "\n",
    "2. `for column in df_final_2.select_dtypes(include=['object']).columns` döngəsi vasitəsilə DataFrame-dəki obyekt tipində olan bütün sütunlar tapılır və hər biri döngədə işlədirilir.\n",
    "\n",
    "3. Hər sütun üçün yeni bir `LabelEncoder` obyekti yaradılır (`le = LabelEncoder()`), daha sonra həmin sütunun dəyərləri `le.fit_transform(df_final_2[column])` vasitəsilə ədədi kodlara çevrilir. Bu çevrilmiş sütun DataFrame-dəki orijinal sütunu əvəz edir.\n",
    "\n",
    "4. Hər sütun üçün yaradılmış `LabelEncoder` obyekti `label_encoders[column] = le` ilə lüğətdə saxlanır ki, sonradan sütunların orijinal dəyərlərinə qayıda bilmək mümkün olsun.\n",
    "\n",
    "Bu şəkildə, mətn və ya obyekt tipli dəyərlər ədədi dəyərlərə çevrilir və bu sütunların `LabelEncoder` obyektləri də saxlanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2d9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c436bfd",
   "metadata": {},
   "source": [
    "## Frequence encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3481e33",
   "metadata": {},
   "source": [
    "Frequence encoder (frekans kodlayıcı), kateqoriyalı dəyişənlərin (categorical variables) dəyərlərini say (frekans) dəyərləri ilə əvəz etmək üçün istifadə edilən bir kodlama metodudur. Bu metod, məlumatların daha yaxşı modellənməsi və analizi üçün istifadə olunur, xüsusən də maşın öyrənməsi modellərində.\n",
    "\n",
    "### Frekans Kodlama Nədir?\n",
    "\n",
    "Frekans kodlama, hər bir unik dəyərin DataFrame-dəki sayını hesablamaq və sonra həmin sayları hər bir dəyərin əvəzinə istifadə etməkdir. Bu, hər bir kateqoriyalı dəyişənin unikal dəyərinin daha rəqəmsal formaya çevrilməsinə kömək edir.\n",
    "\n",
    "### Niyə Frekans Kodlama?\n",
    "\n",
    "1. **Model Performansı**: Bir çox maşın öyrənməsi alqoritmi, sayısal dəyərlərlə daha yaxşı işləyir. Frekans kodlama, kateqoriyalı dəyişənləri sayısal formaya çevirir, bu da modellərin daha yaxşı öyrənməsinə imkan tanıyır.\n",
    "\n",
    "2. **Bütün Dəyərləri İstifadə Etmək**: Bir çox kateqoriyalı kodlama metodları, məsələn, \"One-Hot Encoding\", yeni sütunlar yaradır ki, bu da məlumatın ölçüsünü artırır. Frekans kodlama isə yalnız bir sütun istifadə edir.\n",
    "\n",
    "3. **İnformasiya İtkisi Olmadan**: Kateqoriyalı dəyişənlərin dəyərlərinin sayını istifadə etdiyindən, frekans kodlama, müəyyən bir dəyərin önəmini itirmədən daha az informasiya itkisi ilə işləyir.\n",
    "\n",
    "### Nümunə\n",
    "\n",
    "Aşağıda frekans kodlamanın necə işlədiyini göstərəcək sadə bir nümunə verilib:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Nümunə DataFrame\n",
    "data = {\n",
    "    'color': ['red', 'blue', 'red', 'green', 'blue', 'blue', 'red']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Frekans kodlama\n",
    "frequency_encoding = df['color'].value_counts().to_dict()\n",
    "df['color_encoded'] = df['color'].map(frequency_encoding)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Bir nece sutun ucun\n",
    "\n",
    "```python\n",
    "def frequency_encoding_count(df):\n",
    "    # Yalnız 'object' və 'category' tipində olan sütunlara tətbiq edir\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        frequency_encoding = df[col].value_counts().to_dict()\n",
    "        df[col] = df[col].map(frequency_encoding)\n",
    "    return df\n",
    "\n",
    "# Məlumatları kodlayırıq\n",
    "df_cat_Fre_count = frequency_encoding_count(df)\n",
    "\n",
    "# Nəticəni çap edirik\n",
    "print(df_cat_Fre_count)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bu kod aşağıdakı nəticəni verəcəkdir:\n",
    "\n",
    "```\n",
    "   color  color_encoded\n",
    "0    red              3\n",
    "1   blue              3\n",
    "2    red              3\n",
    "3  green              1\n",
    "4   blue              3\n",
    "5   blue              3\n",
    "6    red              3\n",
    "```\n",
    "\n",
    "### Müsbət və Mənfi Cəhətləri\n",
    "\n",
    "**Müsbət Cəhətləri:**\n",
    "- Sayısal dəyərlərlə işləyir, buna görə də modellərdə daha effektivdir.\n",
    "- Məlumat ölçüsünü artırmır (bir sütun).\n",
    "\n",
    "**Mənfi Cəhətləri:**\n",
    "- Eyni dəyərin yüksək frekansı, modelin bu dəyərlərə üstünlük verməsinə səbəb ola bilər, bu da modelin yanlış öyrənməsinə yol açar.\n",
    "- Hər bir dəyərin dəyəri yalnız sayına görədir, bu da bəzən yanlış təhlil nəticələrinə səbəb ola bilər.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "Frekans kodlama, kateqoriyalı dəyişənləri effektiv şəkildə sayısal formaya çevirmək üçün istifadə olunan güclü bir metodur. Bu metoddan istifadə edərkən müsbət və mənfi cəhətləri nəzərə almaq vacibdir. Modelinizi qurarkən bu kodlama metodunu düşünmək, modelin effektivliyini artırmağa kömək edə bilər.\n",
    "\n",
    "---\n",
    "## Basqa usul\n",
    "Bütün sütunlara `Frequency Encoding` tətbiq etmək üçün aşağıdakı kodu istifadə edə bilərsiniz. `Frequency Encoding` kateqoriyaların hər birinin görülmə tezliyini (yəni neçə dəfə təkrarlanmasını) sütunun dəyərləri ilə əvəz edir.\n",
    "\n",
    "Aşağıdakı kod hər bir kateqoriya və ya obyekt sütununa `Frequency Encoding` tətbiq edir:\n",
    "\n",
    "```python\n",
    "def frequency_encode_all_columns(df):\n",
    "    for column in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        # Sütundakı hər bir dəyərin tezliklərini hesablayırıq\n",
    "        freq_encoding = df[column].value_counts() / len(df)\n",
    "        # Sütundakı dəyərləri onların tezlikləri ilə əvəz edirik\n",
    "        df[column] = df[column].map(freq_encoding)\n",
    "    return df\n",
    "\n",
    "# Məlumatlara Frequency Encoding tətbiq edirik\n",
    "df = frequency_encode_all_columns(df)\n",
    "\n",
    "# Nəticəyə baxa bilərsiniz\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Bu kod:\n",
    "1. Hər bir kateqoriyalı sütundakı dəyərlərin görülmə tezliyini hesablayır.\n",
    "2. Sütundakı orijinal dəyərləri bu tezliklərlə əvəz edir.\n",
    "\n",
    "Bu sayədə kateqoriyalı dəyərlər saylarına görə ədədi dəyərlərə çevrilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124461e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6454c2e8",
   "metadata": {},
   "source": [
    "## LabelEncoder ilə çevrilmiş dəyərləri orijinal vəziyyətinə geri çevirmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf1f4a",
   "metadata": {},
   "source": [
    "`LabelEncoder` ilə çevrilmiş dəyərləri orijinal vəziyyətinə geri çevirmək üçün saxlanmış `LabelEncoder` obyektlərindən istifadə edərək kodlaşdırılmış dəyərləri geri qaytarmaq mümkündür. Məsələn, əgər sütunu əvvəlcə `LabelEncoder` ilə çevirmisinizsə, həmin sütunu geri çevirmək üçün `inverse_transform()` metodunu istifadə edirsiniz.\n",
    "\n",
    "Kodun necə işlədiyini göstərim:\n",
    "\n",
    "```python\n",
    "# Sütunu geri çevirmək üçün əvvəl saxlanmış `LabelEncoder` obyektini istifadə edirik.\n",
    "for column, le in label_encoders.items():\n",
    "    df_final_2[column] = le.inverse_transform(df_final_2[column])\n",
    "```\n",
    "\n",
    "İzah:\n",
    "\n",
    "1. `label_encoders.items()` vasitəsilə saxladığınız bütün sütunlar və onların müvafiq `LabelEncoder` obyektləri arasında dövr edilir.\n",
    "2. `df_final_2[column] = le.inverse_transform(df_final_2[column])` xətti ilə əvvəl kodlaşdırılmış dəyərləri yenidən orijinal vəziyyətinə çevirirsiniz. Bu metod sütunda olan ədədi dəyərləri orijinal mətn və ya obyekt dəyərlərinə geri qaytarır.\n",
    "\n",
    "Beləliklə, DataFrame-dəki bütün çevrilmiş sütunlar orijinal mətn və obyekt dəyərlərinə qaytarılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fab710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55de2835",
   "metadata": {},
   "source": [
    "## VIF hesablama codu yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b1571",
   "metadata": {},
   "source": [
    "Əgər `df` DataFrame-inizdə multicollinearity-ni qiymətləndirmək üçün VIF (Variance Inflation Factor) hesablamaq istəyirsinizsə, aşağıdakı kodu istifadə edə bilərsiniz. Bu kod müstəqil dəyişənlər üçün VIF dəyərlərini hesablayacaq və nəticələri çap edəcəkdir:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# DataFrame-dən müstəqil dəyişənləri seçin (əgər target sütunu varsa onu çıxarın)\n",
    "# Burada target sütunu olmadığı təqdirdə bütün sütunları istifadə edirik\n",
    "X_VIF = df.copy()\n",
    "\n",
    "# Sabit sütununu əlavə edin\n",
    "X_VIF = add_constant(X_VIF)\n",
    "\n",
    "# VIF dəyərlərini hesablayın\n",
    "vif = pd.DataFrame()\n",
    "vif['Feature'] = X_VIF.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_VIF.values, i) for i in range(X_VIF.shape[1])]\n",
    "\n",
    "print(vif)\n",
    "```\n",
    "\n",
    "### İzahat\n",
    "\n",
    "1. **DataFrame-i Nümunə**:\n",
    "   - Bu kod, `df` adlı DataFrame-dəki bütün müstəqil dəyişənlər üzərində VIF hesablaması aparır. Əgər sizin `df` DataFrame-də target sütunu varsa, onu çıxarmaq lazımdır. Məsələn, target sütunu sonuncu sütun olarsa, `df.iloc[:, :-1]` istifadə edə bilərsiniz.\n",
    "\n",
    "2. **Sabit Sütununu Əlavə Etmək**:\n",
    "   - `add_constant` funksiyası ilə sabit (intercept) sütunu əlavə edilir. Bu, VIF hesablama prosesində istifadə olunur.\n",
    "\n",
    "3. **VIF Hesablama**:\n",
    "   - `variance_inflation_factor` funksiyasından istifadə edərək VIF dəyərləri hesablanır. Hər bir sütun üçün VIF dəyərləri hesablamaq üçün `X_VIF.values` və sütun indeksləri istifadə olunur.\n",
    "\n",
    "4. **Nəticələrin Çap Edilməsi**:\n",
    "   - VIF dəyərləri və müvafiq sütun adları `vif` adlı DataFrame-də toplanır və çap olunur.\n",
    "\n",
    "Bu kod müstəqil dəyişənlərin multicollinearity səviyyəsini qiymətləndirməkdə kömək edəcəkdir. VIF dəyərləri 10-dan böyük olan dəyişənlər multicollinearity problemi göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da0c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9062bb4",
   "metadata": {},
   "source": [
    "## iki sutunun deyerleri hamisi eyni olub olmaidigini nece yxolayim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9ee82",
   "metadata": {},
   "source": [
    "İki sütunun dəyərlərinin tamamilə eyni olub-olmadığını yoxlamaq üçün aşağıdakı yolları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`equals()` metodu:**\n",
    "Bu üsul tam uyğunluğu yoxlayır və `True` və ya `False` qaytarır.\n",
    "\n",
    "```python\n",
    "df['sutun1'].equals(df['sutun2'])\n",
    "```\n",
    "\n",
    "### 2. **`==` operatoru ilə:** \n",
    "Bu üsul iki sütunun elementlərini müqayisə edir və nəticəni boolean bir sütun kimi qaytarır. Hamısının eyni olub-olmadığını yoxlamaq üçün `all()` funksiyasını istifadə edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "(df['sutun1'] == df['sutun2']).all()\n",
    "```\n",
    "\n",
    "### 3. **`np.array_equal()` funksiyası ilə:** \n",
    "Bu da iki sütunu müqayisə edir və tam bərabərlik üçün `True` və ya `False` qaytarır.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "np.array_equal(df['sutun1'], df['sutun2'])\n",
    "```\n",
    "\n",
    "Bu üsulların hər biri iki sütunun eyni olub-olmadığını yoxlamaq üçün faydalıdır. Əgər hansısa bir fərq varsa, `False` qaytaracaqdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54cce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd31f0a1",
   "metadata": {},
   "source": [
    "## Klaster analizində modellərin düzgünlüyünü qiymətləndirmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84a4df",
   "metadata": {},
   "source": [
    "Klaster analizində modellərin düzgünlüyünü qiymətləndirmək üçün bir neçə daha yaxşı metod mövcuddur. Aşağıdakı metodlardan istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Adjusted Rand Index (ARI)**:\n",
    "   - ARI nümunələrin həqiqi etiketlərə uyğun olub-olmamasını yoxlayan bir metrikdir. Bu metod, təsadüfi uyğunluqları nəzərə alaraq klasterlərin keyfiyyətini ölçür. ARI dəyəri 0-dan 1-ə qədərdir. 1 ən yaxşı uyğunluğu, 0 isə təsadüfi uyğunluğu göstərir.\n",
    "   - **ARI üstünlüyü**: Klasterlərin tərkibini və düzgün bölüşdürməni təsadüfi uyğunluqları da nəzərə alaraq dəqiqliklə ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "   ari_kmeans = adjusted_rand_score(df_final['species'], df_final['k_means_cluster'])\n",
    "   ari_ac = adjusted_rand_score(df_final['species'], df_final['ac_cluster'])\n",
    "   ari_mini = adjusted_rand_score(df_final['species'], df_final['mini_cluster'])\n",
    "\n",
    "   print(\"ARI K-Means:\", ari_kmeans)\n",
    "   print(\"ARI Agglomerative Clustering:\", ari_ac)\n",
    "   print(\"ARI MiniBatchKMeans:\", ari_mini)\n",
    "   ```\n",
    "\n",
    "### 2. **Normalized Mutual Information (NMI)**:\n",
    "   - NMI klasterlər arasındakı qarşılıqlı əlaqəni və informasiyanın nə qədər itirildiyini ölçür. Dəyəri 0 ilə 1 arasında dəyişir; 1 dəyəri klasterlərin tam uyğun olduğunu göstərir.\n",
    "   - **NMI üstünlüyü**: Təkcə düzgün yerləşdirməni deyil, həm də klasterlərdə olan məlumatların keyfiyyətini ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "   nmi_kmeans = normalized_mutual_info_score(df_final['species'], df_final['k_means_cluster'])\n",
    "   nmi_ac = normalized_mutual_info_score(df_final['species'], df_final['ac_cluster'])\n",
    "   nmi_mini = normalized_mutual_info_score(df_final['species'], df_final['mini_cluster'])\n",
    "\n",
    "   print(\"NMI K-Means:\", nmi_kmeans)\n",
    "   print(\"NMI Agglomerative Clustering:\", nmi_ac)\n",
    "   print(\"NMI MiniBatchKMeans:\", nmi_mini)\n",
    "   ```\n",
    "\n",
    "### 3. **Silhouette Score**:\n",
    "   - Silhouette skoru klasterlərin içindəki nümunələrin nə qədər yaxın olduğunu və digər klasterlərdən nə qədər uzaq olduğunu ölçür. Bu metrik klasterlərin təbii olub-olmadığını qiymətləndirir.\n",
    "   - **Silhouette üstünlüyü**: Klasterlər arası fərqliliyi də ölçür və hər klasterin uyğun təyin olunduğunu göstərir.\n",
    "   - Ancaq burada həqiqi etiketlərə ehtiyac yoxdur, yalnız klasterləşdirmənin uyğunluğunu ölçür.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import silhouette_score\n",
    "\n",
    "   silhouette_kmeans = silhouette_score(df_final.drop(columns=['species']), df_final['k_means_cluster'])\n",
    "   silhouette_ac = silhouette_score(df_final.drop(columns=['species']), df_final['ac_cluster'])\n",
    "   silhouette_mini = silhouette_score(df_final.drop(columns=['species']), df_final['mini_cluster'])\n",
    "\n",
    "   print(\"Silhouette K-Means:\", silhouette_kmeans)\n",
    "   print(\"Silhouette Agglomerative Clustering:\", silhouette_ac)\n",
    "   print(\"Silhouette MiniBatchKMeans:\", silhouette_mini)\n",
    "   ```\n",
    "\n",
    "### 4. **Confusion Matrix (Karmaşıqlıq Matrisi)**:\n",
    "   - Klasterləşdirmənin performansını vizuallaşdırmaq üçün **confusion matrix** (çapraz matrisi) yarada bilərsiniz. Bu metod, hər klasterlə real etiketlər arasındakı uyğunluğu göstərir.\n",
    "   - **Üstünlüyü**: Hər bir klasterin nə qədər düzgün təyin olunduğunu dəqiq göstərə bilər.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import confusion_matrix\n",
    "   import seaborn as sns\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   cm_kmeans = confusion_matrix(df_final['species'], df_final['k_means_cluster'])\n",
    "   sns.heatmap(cm_kmeans, annot=True, fmt=\"d\")\n",
    "   plt.title('Confusion Matrix for K-Means')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "### Hansı daha yaxşıdır?\n",
    "\n",
    "- **ARI və NMI** real etiketlərlə klasterləşdirmənin nə qədər uyğun olduğunu daha dəqiq göstərir və təsadüfi uyğunluqları da nəzərə alır. Bu səbəbdən, ARI və NMI daha **dəqiq metriklər**dir.\n",
    "- **Silhouette Score** klasterlərin öz içindəki uyğunluğu göstərsə də, real etiketləri nəzərə almır, ancaq klasterlərin nə qədər təbii formalaşdığını göstərir.\n",
    "- **Confusion Matrix** isə klasterləşdirmə nəticələrini vizual olaraq başa düşmək üçün çox faydalıdır.\n",
    "\n",
    "### Tövsiyə:\n",
    "- Daha yaxşı bir ölçü əldə etmək üçün **Adjusted Rand Index (ARI)** və **Normalized Mutual Information (NMI)** istifadə edin.\n",
    "- Klasterlərin uyğunluğunu vizuallaşdırmaq üçün isə **Confusion Matrix** kömək edər.\n",
    "\n",
    "Bu metodlar nəticələr arasındakı uyğunsuzluqları daha dəqiq müəyyən etməyə kömək edəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8318c4e3",
   "metadata": {},
   "source": [
    "## Explained_Variance ne demekdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e3725",
   "metadata": {},
   "source": [
    "`explained_variance` (izah edilən dispersiya), xüsusən **PCA** (Principal Component Analysis) və digər statistik modellərdə istifadə edilən bir termindir və bir modelin məlumat dəstindəki variasiyanın nə qədərini izah etdiyini göstərir. \n",
    "\n",
    "### PCA kontekstində `explained_variance` nədir?\n",
    "\n",
    "PCA-da, məlumatların əsas komponentlərə parçalanması ilə hər bir komponent müəyyən miqdarda dispersiyanı izah edir. Bu komponentlər, məlumat dəstindəki əsas məlumat strukturlarını tutmağa çalışır. \n",
    "\n",
    "- **İzah edilən dispersiya** hər bir əsas komponentin məlumatlardakı ümumi variasiyanın nə qədərini izah etdiyini ölçür.\n",
    "- Daha çox dispersiya izah edən bir komponent, daha çox məlumat tutmuş olur.\n",
    "\n",
    "### Məsələn:\n",
    "Bir datasetdə PCA tətbiq etdiyiniz zaman, ilk əsas komponent məlumat dəstinin ən böyük dispersiyasını izah edir. İkinci komponent isə, qalan dispersiyanın maksimum hissəsini izah edir və s.\n",
    "\n",
    "### Formel tərif:\n",
    "Əgər bir məlumat dəstindəki ümumi dispersiya (total variance) 100% olsa, əsas komponentlərin hər biri bu dispersiyadan bir hissəni izah edir. Məsələn:\n",
    "- İlk komponent: 60% izah edir,\n",
    "- İkinci komponent: 25% izah edir,\n",
    "- Üçüncü komponent: 10% izah edir.\n",
    "\n",
    "Bu, məlumatın böyük bir hissəsinin ilk bir neçə komponentdə cəmləndiyini göstərir və bu da yüksək ölçülü datanı daha az ölçülü bir formaya salarkən məlumatın çoxunu itirməməyə kömək edir.\n",
    "\n",
    "### Python-da PCA ilə `explained_variance_ratio_` nümunəsi:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# NumPy ilə təsadüfi bir data yaradırıq\n",
    "X = np.random.rand(100, 5)\n",
    "\n",
    "# PCA modelini qururuq və tətbiq edirik\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# İzah edilən dispersiyanı çap edirik\n",
    "print(pca.explained_variance_ratio_)\n",
    "```\n",
    "\n",
    "Burada `explained_variance_ratio_` hər bir əsas komponentin izah etdiyi dispersiyanın nisbətini göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cb84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e3d497",
   "metadata": {},
   "source": [
    "## LabelEncoder ve get_dummies ferqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f242d00",
   "metadata": {},
   "source": [
    "Burada `LabelEncoder` istifadə edərək kateqorik dəyişənləri ədədi dəyərlərə çevirmişik, lakin bu həmişə ən yaxşı yanaşma olmaya bilər. **Niyə `get_dummies` istifadə etmədik** sualına cavab olaraq, bu iki metodun fərqli məqsədləri olduğunu başa düşmək vacibdir:\n",
    "\n",
    "### 1. **`LabelEncoder`** nədir?\n",
    "`LabelEncoder`, kateqorik dəyişənləri tək bir sütun şəklində ədədi dəyərlərə çevirir. Hər bir kateqorik dəyərə müəyyən bir tam ədəd təyin edir:\n",
    "- Məsələn, cinsiyyəti (`Sex`) təyin edərkən, `Male` = 1, `Female` = 0 kimi kodlaşdırılır.\n",
    "\n",
    "Bu, sıravi (ordinal) və ya sadəcə iki dəyəri olan (binary) dəyişənlər üçün uyğundur, çünki bu cür dəyişənlərdə dəyərlər arasındakı fərq sıralı mənaya malik ola bilər.\n",
    "\n",
    "Ancaq çoxlu kateqorik dəyəri olan dəyişənlər üçün (məsələn, `Embarked` — C, Q, S) bu metod həmişə uyğun olmur. Çünki `LabelEncoder` ilə təyin olunan dəyərlər arasında sıralı münasibət yarana bilər, halbuki bu dəyərlərin bir-birinə qarşı xüsusi bir sıralaması yoxdur (C = 0, Q = 1, S = 2). Bu, modelin performansına mənfi təsir göstərə bilər.\n",
    "\n",
    "### 2. **`get_dummies`** nədir?\n",
    "`get_dummies`, kateqorik dəyişənləri **one-hot encoding** ilə çoxlu ədədi sütuna çevirir. Hər bir kateqorik dəyər üçün ayrıca sütun yaradır və həmin dəyərin olduğu yerlərə 1, olmadığı yerlərə isə 0 təyin edir:\n",
    "- Məsələn, `Embarked` üçün üç sütun yaranır: `Embarked_C`, `Embarked_Q`, `Embarked_S`, və hər birində yalnız bir 1, digərlərində isə 0 olur.\n",
    "\n",
    "Bu metod daha uyğundur, çünki dəyərlər arasında süni sıralama yaranmır.\n",
    "\n",
    "### Bu nümunədə niyə `LabelEncoder` istifadə etdik?\n",
    "Bu sualın cavabı daha çox probleminizə və istifadə etdiyiniz modelə bağlıdır:\n",
    "- **Sadə model**: Əgər sadə bir model qurursunuzsa və `LabelEncoder` ilə kodlaşdırma nəticəsində süni sıralama problemi yaratmırsa, bu üsul sadəlik baxımından uyğundur.\n",
    "- **Daha mürəkkəb modellər** və ya çox dəyərli kateqorik dəyişənlər üçün isə **`get_dummies`** daha yaxşı nəticələr verəcək, çünki o, hər bir dəyərə uyğun ayrı sütunlar yaradır və aralarında sıralı münasibət yaratmır.\n",
    "\n",
    "Əgər `get_dummies` istifadə etmək istəsəniz, bunu aşağıdakı kimi edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "# 'Sex' və 'Embarked' dəyişənləri üçün get_dummies istifadə edirik\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Nəticədə 'Sex_male' və 'Embarked_S', 'Embarked_Q' sütunları yaranacaq\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Burada `drop_first=True` ilə birinci kateqoriyanı atırıq, çünki iki dəyərdən birini təyin etmək kifayət edir (dummy trap-dən qaçınmaq üçün)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc8495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a165b6c",
   "metadata": {},
   "source": [
    "## import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b7482",
   "metadata": {},
   "source": [
    "`plotly.express` interaktiv vizuallaşdırma üçün istifadə edilən bir Python kitabxanasıdır. Bu kitabxana, sadə və səmərəli şəkildə interaktiv qrafiklər yaratmağa imkan verir. Əsas xüsusiyyətləri aşağıdakılardır:\n",
    "\n",
    "### Xüsusiyyətlər:\n",
    "1. **İnteraktiv qrafiklər:** Qrafiklər zoom etmək, üzərində məlumatları göstərmək və filtr tətbiq etmək kimi funksionallıqlara malikdir.\n",
    "2. **Sadə sintaksis:** `plotly.express` ilə qrafiklər yaratmaq asandır. Çox az kodla mürəkkəb qrafiklər yarada bilərsiniz.\n",
    "3. **Çoxsaylı qrafik tipləri:** Scatter plots, line charts, bar charts, histograms, pie charts, box plots və s. kimi müxtəlif növ qrafiklər yarada bilərsiniz.\n",
    "4. **Dataların asan inteqrasiyası:** Pandas DataFrame-ləri və ya digər data strukturlarını birbaşa qrafiklərə daxil edə bilərsiniz.\n",
    "5. **İnteraktiv interfeys:** Qrafiklər veb-səhifələrdə, Jupyter notebook-larda və Dash tətbiqlərində interaktiv olaraq göstərilə bilər.\n",
    "\n",
    "### Məsələn, scatter plot yaratmaq üçün sadə nümunə:\n",
    "```python\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Məlumat yaradılması\n",
    "df = pd.DataFrame({\n",
    "    'X': [1, 2, 3, 4, 5],\n",
    "    'Y': [10, 11, 12, 13, 14]\n",
    "})\n",
    "\n",
    "# Scatter plot\n",
    "fig = px.scatter(df, x='X', y='Y', title='Simple Scatter Plot')\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "Bu nümunədə, `plotly.express` Pandas DataFrame-dəki verilənləri vizuallaşdırır və nəticə interaktiv bir scatter plot olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e98f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c723bff",
   "metadata": {},
   "source": [
    "## CRISP DM (Məlumat mədənçiliyi üçün sahələrarası standart proses) nedir?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439b83c",
   "metadata": {},
   "source": [
    "CRISP-DM (Cross-Industry Standard Process for Data Mining) data mining və data analitika layihələri üçün geniş istifadə olunan bir metodologiyadır. Bu proses, hər sahədə data analitik layihələrə uyğun və sistematik bir çərçivə təqdim edir. CRISP-DM aşağıdakı altı əsas mərhələdən ibarətdir:\n",
    "\n",
    "1. **Business Understanding (Biznes Anlayışı):** Layihənin məqsədlərini və biznes ehtiyaclarını başa düşmək. Hansı suallara cavab tapılmalı olduğu və data mining tətbiqinin biznesə necə dəyər qatacağı təyin edilir.\n",
    "\n",
    "2. **Data Understanding (Data Anlayışı):** Lazımi məlumatları toplamaq və ilkin olaraq analiz etmək. Bu mərhələdə datanın keyfiyyəti, strukturlaşdırılması və mövcud məlumatların uyğunluğu araşdırılır.\n",
    "\n",
    "3. **Data Preparation (Data Hazırlığı):** Model qurmaq üçün datanın təmizlənməsi və transformasiya edilməsi. Bu mərhələdə datanın düzgün formatda olması üçün dəyişikliklər aparılır, boş dəyərlər idarə edilir və lazım gəldikdə yeni dəyişənlər yaradılır.\n",
    "\n",
    "4. **Modeling (Model Qurulması):** Seçilmiş machine learning və ya data mining modelləri ilə analitik modellərin qurulması. Həmçinin, modellərin performansını qiymətləndirmək üçün fərqli parametrlər test olunur.\n",
    "\n",
    "5. **Evaluation (Dəyərləndirmə):** Qurulmuş modelin nəticələrinin biznes ehtiyaclarına cavab verib-vermədiyini yoxlamaq. Bu mərhələdə modelin düzgün işlədiyinə əmin olmaq üçün nəticələr təhlil edilir.\n",
    "\n",
    "6. **Deployment (İstifadəyə Verilməsi):** Modeldən əldə olunan nəticələrin real dünya problemlərini həll etmək üçün tətbiqi. Bu mərhələdə modelin inteqrasiyası, nəticələrin biznesə geri ötürülməsi və davamlı izlənməsi həyata keçirilir.\n",
    "\n",
    "CRISP-DM data mining və machine learning layihələrində ən çox qəbul edilmiş metodologiyalardan biri hesab edilir, çünki çevik, sektorlararası və strukturlu yanaşma təqdim edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c04f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59421d85",
   "metadata": {},
   "source": [
    "## API nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52785dff",
   "metadata": {},
   "source": [
    "API (Application Programming Interface) — tətbiqlərin bir-biri ilə ünsiyyət qurmasını təmin edən interfeysdir. Bu, proqramların və sistemlərin bir-birinə sorğu göndərməsi və məlumat alması üçün müəyyən edilmiş qaydalar və protokollar toplusudur.\n",
    "\n",
    "API-lər müxtəlif növ ola bilər:\n",
    "\n",
    "1. **Web API-lər** – Veb əsaslı sistemlərdə istifadə olunur və HTTP protokolu ilə işləyir. Məsələn, bir vebsaytın və ya mobil tətbiqin serverdən məlumatları API vasitəsilə sorğu edib alması.\n",
    "   \n",
    "2. **Operativ Sistem API-ləri** – Tətbiqlərin əməliyyat sisteminə sorğu göndərməsi və onun funksiyalarına (məsələn, fayl sisteminə giriş) çıxış əldə etməsi.\n",
    "\n",
    "3. **Kitabxana API-ləri** – Tətbiqin daxilində istifadə olunan xüsusi funksiyalar və metodlar toplusudur, məsələn, Python-da `pandas` kitabxanası.\n",
    "\n",
    "API istifadə etməyin əsas üstünlükləri:\n",
    "- **Modulların ayrılığı:** Tətbiqlər bir-biri ilə kodu paylaşmadan ünsiyyət qura bilər.\n",
    "- **Təkrar istifadə:** Bir API müxtəlif tətbiqlər tərəfindən istifadə edilə bilər.\n",
    "- **Sadəlik və təhlükəsizlik:** Məlumatlara yalnız lazımi funksiyalar vasitəsilə çıxış təmin olunur.\n",
    "\n",
    "Məsələn, hava proqnozunu göstərən bir tətbiq, hava məlumatını birbaşa serverdən çəkmək üçün API-dən istifadə edə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69e7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa79f1a6",
   "metadata": {},
   "source": [
    "## Objects nedir bes Pythonda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82918bd7",
   "metadata": {},
   "source": [
    "Python-da **object (obyekt)**, verilənləri və həmin verilənlər üzərində əməliyyatları bir arada saxlayan bir konstruksiyadır. Python-da hər şey bir obyekt hesab olunur — dəyişənlər, funksiyalar, siyahılar, hətta ən sadə verilənlər tipləri (nömrələr, mətnlər) belə obyektlərdir.\n",
    "\n",
    "Bir obyektin iki əsas komponenti var:\n",
    "1. **Attributes (xüsusiyyətlər)** — obyektin məlumatları (obyektin saxlaya biləcəyi verilənlər). Məsələn, bir obyektin rəngi və ya uzunluğu ola bilər.\n",
    "2. **Methods (metodlar)** — obyektin yerinə yetirə biləcəyi funksiyalar. Metodlar obyektin verilənləri üzərində əməliyyatlar aparır.\n",
    "\n",
    "### Python-da obyektlərin əsas xüsusiyyətləri:\n",
    "- **Class (sinif)**: Python obyektləri bir sinfə əsaslanır. Sinif obyektlərin sxemini təyin edir. Misal üçün, bir `Car` sinfi avtomobil obyektlərini təyin edə bilər, həmin sinfə əsaslanan obyektlər isə fərqli maşınlar ola bilər.\n",
    "  \n",
    "  ```python\n",
    "  class Car:\n",
    "      def __init__(self, brand, model):\n",
    "          self.brand = brand\n",
    "          self.model = model\n",
    "\n",
    "      def show_info(self):\n",
    "          print(f\"This car is a {self.brand} {self.model}\")\n",
    "  \n",
    "  # Car sinfinə əsaslanan obyekt yaratmaq\n",
    "  my_car = Car(\"Toyota\", \"Corolla\")\n",
    "  my_car.show_info()  # Output: This car is a Toyota Corolla\n",
    "  ```\n",
    "\n",
    "- **Instance (nüsxə)**: Bir sinifdən yaradılan fərdi obyektə instance deyilir. Məsələn, yuxarıdakı `my_car` bir instance-dır.\n",
    "  \n",
    "- **Inheritance (irsi əlaqə)**: Bir sinif digər sinifdən xüsusiyyətləri və metodları miras ala bilər. Bu, kodu təkrar istifadəni asanlaşdırır.\n",
    "\n",
    "Python-un obyekt yönümlü təbiəti proqramların daha modullu və idarəedilməsi asan olmasına kömək edir. Siniflərdən istifadə etməklə mürəkkəb tətbiqləri asanlıqla dizayn etmək və genişləndirmək mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17a092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65c393b6",
   "metadata": {},
   "source": [
    "## Merge və Join Pandas-da "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c5764",
   "metadata": {},
   "source": [
    "Pandas-da `merge` və `join` metodları DataFrame-ləri birləşdirmək üçün istifadə olunur, lakin onların bir neçə fərqli xüsusiyyəti var:\n",
    "\n",
    "### 1. **`merge` metodu:**\n",
    "   - SQL-dəki `JOIN` əməliyyatına çox bənzəyir.\n",
    "   - Hər iki DataFrame-də uyğun gələn sütunlar əsasında birləşmə aparır.\n",
    "   - Hansı sütunların birləşməyə əsaslanacağını açıq şəkildə təyin etmək mümkündür (`on`, `left_on`, `right_on` kimi parametrlər vasitəsilə).\n",
    "   - Dörd növ birləşmə tipi təklif edir: `inner`, `left`, `right`, `outer`.\n",
    "\n",
    "#### Sintaksis:\n",
    "```python\n",
    "df1.merge(df2, on='column_name', how='inner')\n",
    "```\n",
    "\n",
    "#### Misal:\n",
    "```python\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie']})\n",
    "df2 = pd.DataFrame({'ID': [1, 2, 4], 'Age': [25, 30, 35]})\n",
    "\n",
    "merged_df = df1.merge(df2, on='ID', how='inner')\n",
    "```\n",
    "\n",
    "Bu `df1` və `df2`-dəki `ID` sütunu üzərindən `inner` birləşmə aparacaq.\n",
    "\n",
    "### 2. **`join` metodu:**\n",
    "   - DataFrame-ləri indekslərə əsasən birləşdirir.\n",
    "   - Default olaraq sol DataFrame-in indeksinə əsaslanır.\n",
    "   - Əsasən birləşdiriləcək DataFrame-də indeksə sahib sütunlar olduqda istifadə olunur.\n",
    "   - Birinci DataFrame-də sütunları əsasən birləşmə edir.\n",
    "\n",
    "#### Sintaksis:\n",
    "```python\n",
    "df1.join(df2, how='left')\n",
    "```\n",
    "\n",
    "#### Misal:\n",
    "```python\n",
    "df1 = pd.DataFrame({'Name': ['Alice', 'Bob'], 'Age': [25, 30]}, index=[1, 2])\n",
    "df2 = pd.DataFrame({'Salary': [5000, 6000]}, index=[1, 2])\n",
    "\n",
    "joined_df = df1.join(df2)\n",
    "```\n",
    "\n",
    "Bu halda `df1` və `df2` indekslərə əsasən `left` join aparacaq.\n",
    "\n",
    "### Əsas fərqlər:\n",
    "- **İstifadə yeri:** `merge` sütunlar əsasında, `join` indekslər əsasında işləyir.\n",
    "- **Birləşdirmə metodları:** Hər ikisi `left`, `right`, `inner`, `outer` birləşmə metodlarını təklif edir, lakin `join` daha çox indeks əsaslıdır.\n",
    "- **Flexible Parametrlər:** `merge` daha çox flexible parametr təklif edir, məsələn, bir DataFrame-də fərqli sütun adlarına əsasən birləşdirmə aparmaq olur (`left_on`, `right_on`).\n",
    "\n",
    "Bu fərqlərdən asılı olaraq, hansı üsulu seçəcəyiniz birləşməni necə aparacağınıza bağlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cd9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360e832d",
   "metadata": {},
   "source": [
    "## Supervised ve Unsupervised arasinda ferq?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d37e3",
   "metadata": {},
   "source": [
    "Supervised (nəzarətli) və unsupervised (nəzarətsiz) öyrənmə, maşın öyrənməsi sahəsində istifadə olunan iki əsas yanaşmadır. Onlar arasında bir neçə mühüm fərq var:\n",
    "\n",
    "### Supervised Learning (Nəzarətli Öyrənmə)\n",
    "\n",
    "1. **Təyin Edilmiş Məlumat**: Supervised learning-də model, giriş verilənlərinin (x) və onların müvafiq çıxış etiketlərinin (y) olduğu bir dataset ilə öyrədilir. Bu etiketlər, modelin doğru proqnozlar etməsinə kömək edir.\n",
    "\n",
    "2. **Məqsəd**: Modelin məqsədi, verilən giriş verilənlərinə əsaslanaraq doğru çıxış etiketlərini proqnozlaşdırmaqdır. Məsələn, xəstələrin tibbi məlumatlarından istifadə edərək xəstəliyin olub-olmadığını proqnozlaşdırmaq.\n",
    "\n",
    "3. **Nümunələr**: Məsələn, sınıflandırma (classification) və regresiya (regression) tapşırıqları.\n",
    "\n",
    "4. **Tətbiq Sahələri**: E-poçt spam filtrasiya, müştəri davranışı proqnozu, maliyyə analizləri, tibbi diaqnozlar.\n",
    "\n",
    "5. **Örnek Algoritmlər**: Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), Neural Networks.\n",
    "\n",
    "### Unsupervised Learning (Nəzarətsiz Öyrənmə)\n",
    "\n",
    "1. **Təyin Edilməmiş Məlumat**: Unsupervised learning-də model, yalnız giriş verilənləri (x) olan bir dataset ilə öyrədilir, lakin çıxış etiketləri yoxdur. Yəni, model verilənlər arasında əlaqələri tapmağa çalışır.\n",
    "\n",
    "2. **Məqsəd**: Modelin məqsədi, verilənlər arasında gizli strukturları və ya klasterləri aşkar etməkdir. Məsələn, müştəri seqmentasiyası üçün müştəri məlumatlarını qruplaşdırmaq.\n",
    "\n",
    "3. **Nümunələr**: Məsələn, klasterləşdirmə (clustering) və əsas komponentlərin analizi (PCA).\n",
    "\n",
    "4. **Tətbiq Sahələri**: Müştəri seqmentasiyası, məlumatların analizi, anomaliya aşkar etmə, müstəqil öyrənmə.\n",
    "\n",
    "5. **Örnek Algoritmlər**: K-Means, Hierarchical Clustering, DBSCAN, Principal Component Analysis (PCA), t-SNE.\n",
    "\n",
    "### Qısaca Fərqlər\n",
    "\n",
    "- **Etiketlər**: Supervised learning-də etiketli verilənlər var, unsupervised learning-də isə yoxdur.\n",
    "- **Məqsəd**: Supervised learning-də modelin doğru proqnozlar etməsi üçün öyrədilməsi, unsupervised learning-də isə verilənlərdəki strukturları aşkar etmək.\n",
    "- **Nümunələr**: Supervised learning, sınıflandırma və regresiya, unsupervised learning, klasterləşdirmə və məlumat analizi.\n",
    "\n",
    "Bu fərqləri nəzərə alaraq, verilən tapşırığın təbiətinə uyğun öyrənmə metodunu seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df14f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6843d3bc",
   "metadata": {},
   "source": [
    "## Pythonda statistik testler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3d3fa",
   "metadata": {},
   "source": [
    "Python-da ən çox istifadə edilən statistik testlər aşağıdakı kimidir:\n",
    "\n",
    "1. **T-Test**:\n",
    "   - İki qrup arasındakı ortalamaların fərqlərini müqayisə etmək üçün istifadə olunur.\n",
    "   - `scipy.stats.ttest_ind()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "2. **ANOVA (Analysis of Variance)**:\n",
    "   - Bir neçə qrupun ortalamalarını müqayisə etmək üçün istifadə olunur.\n",
    "   - `scipy.stats.f_oneway()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "3. **Chi-Square Test (Chi-kvadrat testi)**:\n",
    "   - Kateqorial verilənlərin gözlənilən və faktiki dəyərləri arasındakı əlaqəni yoxlamaq üçün istifadə olunur.\n",
    "   - `scipy.stats.chi2_contingency()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "4. **Mann-Whitney U Test**:\n",
    "   - İki qrupun mediandakı fərqlərini yoxlamaq üçün istifadə olunur.\n",
    "   - `scipy.stats.mannwhitneyu()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "5. **Pearson Korrelyasiya Testi**:\n",
    "   - İki dəyişənin bir-biri ilə nə qədər əlaqəli olduğunu ölçmək üçün istifadə olunur.\n",
    "   - `scipy.stats.pearsonr()` funksiyası ilə həyata keçirilir.\n",
    "\n",
    "### Məşhur Dataset üzərində testlər tətbiqi\n",
    "**Titanic dataset**-i götürüb statistik testləri tətbiq edəcəyik. Əvvəlcə Titanic datasetini yükləyək və araşdırmaya başlayaq.\n",
    "\n",
    "### 1. Məlumatın Hazırlanması\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Titanic datasetini yükləyək\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# İlk beş sətri yoxlayaq\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "### 2. T-Test Indpendent (İki qrupun ortalamalarının müqayisəsi)\n",
    "**Sual**: 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarında fərq varmı?\n",
    "```python\n",
    "# 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarının fərqi\n",
    "first_class = df[df['Pclass'] == 1]['Age'].dropna()\n",
    "third_class = df[df['Pclass'] == 3]['Age'].dropna()\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(first_class, third_class)\n",
    "print(f\"T-Test nəticəsi: T-statistic = {t_stat}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- T-statistic = 7.934060672509778\n",
    "- P-value = 3.243665359138386e-15  \n",
    "Bu nəticə göstərir ki, 1-ci və 3-cü sinif sərnişinlərinin yaş ortalamaları arasında statistik olaraq əhəmiyyətli fərq var (P-value çox kiçikdir).\n",
    "\n",
    "---\n",
    "###  3. T-Test Paired (bir qrupun ortalamalarının müqayisəsi)\n",
    "Paired t-test (cüt t-testi) adətən eyni qrup üzvlərinin əvvəl və sonra nəticələrini müqayisə etmək üçün istifadə olunur. Bu test iki qrup arasında fərqi yoxlamır, əksinə eyni fərdlərin iki müxtəlif şəraitdə (məsələn, eyni sərnişinlərin əvvəl və sonra yaşları kimi) nəticələrini müqayisə edir. \n",
    "\n",
    "Sizin Titanic datasetinizdə 1-ci və 3-cü sinif sərnişinləri müstəqil qruplardır, buna görə də `paired t-test` məntiqi olaraq uyğun deyil, çünki eyni sərnişinlərin müxtəlif şəraitdə yaş ortalamalarını müqayisə etmirik. Ancaq əgər siz bunu sadəcə məşq üçün etmək istəyirsinizsə, `paired t-test` tətbiqini aşağıdakı kimi yaza bilərsiniz:\n",
    "\n",
    "```python\n",
    "# Əgər iki qrupda eyni sayda müşahidə yoxdursa, kəsilmə edə bilərik\n",
    "min_length = min(len(first_class), len(third_class))\n",
    "\n",
    "# Paired t-test üçün ilk və üçüncü siniflərdə bərabər uzunluqlu datanı götürürük\n",
    "first_class_sample = first_class[:min_length]\n",
    "third_class_sample = third_class[:min_length]\n",
    "\n",
    "# Paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(first_class_sample, third_class_sample)\n",
    "print(f\"Paired T-Test nəticəsi: T-statistic = {t_stat}, P-value = {p_value}\")\n",
    "```\n",
    "\n",
    "Burada:\n",
    "\n",
    "- `stats.ttest_rel()` funksiyası paired t-test üçün istifadə olunur.\n",
    "- `first_class` və `third_class` sərnişin qruplarının uzunluqları fərqli ola bilər. Buna görə eyni sayda müşahidəni əldə etmək üçün onları kəsirik (`min_length` ilə eyni uzunluqlu nümunələr götürürük).\n",
    "\n",
    "Lakin unutmayın ki, bu dataset müstəqil qruplarla işləyir və **paired t-test** məntiqi olaraq doğru seçim deyil, çünki sərnişinlər eyni fərdlər deyil. Bu sadəcə təcrübə üçün bir tətbiqdir.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Chi-Square Test (Kateqorial dəyişənlər arasında əlaqə)\n",
    "**Sual**: Cinsiyyət və sağ qalma arasında əlaqə varmı?\n",
    "```python\n",
    "# Cinsiyyət və sağ qalma arasındakı əlaqə\n",
    "contingency_table = pd.crosstab(df['Sex'], df['Survived'])\n",
    "chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f\"Chi-Square Test nəticəsi: Chi2-statistic = {chi2_stat}, P-value = {p_val}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- Chi2-statistic = 260.71702016732104\n",
    "- P-value = 1.1973570627755645e-58  \n",
    "Bu nəticə göstərir ki, cinsiyyət və sağ qalma arasında güclü əlaqə var (P-value çox kiçikdir).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ANOVA Testi (Bir neçə qrupun ortalamalarının müqayisəsi)\n",
    "**Sual**: 1-ci, 2-ci və 3-cü sinif sərnişinlərinin yaş ortalamalarında fərq varmı?\n",
    "```python\n",
    "# ANOVA testi: Siniflər üzrə yaş ortalamaları\n",
    "f_stat, p_value = stats.f_oneway(\n",
    "    df[df['Pclass'] == 1]['Age'].dropna(),\n",
    "    df[df['Pclass'] == 2]['Age'].dropna(),\n",
    "    df[df['Pclass'] == 3]['Age'].dropna()\n",
    ")\n",
    "print(f\"ANOVA nəticəsi: F-statistic = {f_stat}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- F-statistic = 36.16198155603057\n",
    "- P-value = 1.3983978326265877e-15  \n",
    "Bu nəticə göstərir ki, sərnişin siniflərinə görə yaş ortalamaları arasında statistik olaraq əhəmiyyətli fərq var.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Pearson Korrelyasiya Testi (İki dəyişən arasındakı əlaqə)\n",
    "**Sual**: Yaş və biletlərin qiyməti arasında əlaqə varmı?\n",
    "```python\n",
    "# Yaş və biletlərin qiyməti arasındakı korrelyasiya\n",
    "corr, p_value = stats.pearsonr(df['Age'].dropna(), df['Fare'].dropna())\n",
    "print(f\"Pearson Korrelyasiya Testi nəticəsi: Correlation = {corr}, P-value = {p_value}\")\n",
    "```\n",
    "**Nəticə**: \n",
    "- Correlation = 0.09606669176903893\n",
    "- P-value = 0.014676376252593772  \n",
    "Korrelyasiya əmsalı (0.096) çox zəif əlaqə olduğunu göstərir, lakin P-value 0.05-dən kiçik olduğu üçün bu zəif əlaqə statistik olaraq əhəmiyyətlidir.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.  Spearman Korrelyasiya Testi\n",
    "Hata, yaş və bilet qiyməti sütunlarının NaN dəyərlərini çıxardığımız zaman fərqli ölçülərə sahib olduqları üçün baş verdi. Hər iki sütundan eyni sayda verilənlərin olmasını təmin etmək üçün NaN dəyərlərini çıxardıb eyni indeksi olanları seçməliyik. Aşağıdakı kodu istifadə edərək Spearman korrelyasiya testini yenidən yerinə yetirəcəyik:\n",
    "\n",
    "###  Spearman Korrelyasiya Testi\n",
    "```python\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Titanic datasetini yükləyin\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# NaN dəyərləri olan sətrləri çıxarın\n",
    "filtered_df = titanic_df[['Age', 'Fare']].dropna()\n",
    "\n",
    "# Yaş və Bilet qiyməti məlumatlarını alın\n",
    "age = filtered_df['Age']\n",
    "fare = filtered_df['Fare']\n",
    "\n",
    "# Spearman korrelyasiya testi\n",
    "spearman_corr, p_value = stats.spearmanr(age, fare)\n",
    "\n",
    "# Nəticələri çap edin\n",
    "print(f\"Spearman Correlation Coefficient: {spearman_corr:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "İndi bunu yerinə yetirək.\n",
    "\n",
    "### Spearman Korrelyasiya Testinin Nəticələri\n",
    "- **Spearman Korrelasiya Koefisiyenti**: **0.1351**\n",
    "- **P-value**: **0.0003**\n",
    "\n",
    "### Nəticələrin İzahı\n",
    "1. **Spearman Korrelasiya Koefisiyenti**: 0.1351 dəyəri, yaş və bilet qiyməti arasında müsbət, lakin zəif bir əlaqənin olduğunu göstərir. Bu, yaş artdıqca bilet qiymətinin da artma ehtimalının olduğunu göstərir.\n",
    "\n",
    "2. **P-value**: 0.0003 dəyəri, 0.05-dən aşağı olduğu üçün, yaş və bilet qiyməti arasında statistik cəhətdən əhəmiyyətli bir əlaqənin olduğunu göstərir. Yəni, bu iki dəyişən arasında monotonic bir əlaqə vardır.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "Bu test, Titanic sərnişinlərinin yaş və bilet qiyməti arasında monotonic bir əlaqənin olduğunu ortaya qoyur. Hesablanmış koefisiyent, əlaqənin zəif olduğunu, amma statistik cəhətdən əhəmiyyətli olduğunu göstərir.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. One Sample T-test\n",
    "Titanic datasetindəki yaş ortalamasının 30 yaşdan fərqli olub olmadığını araşdırmaq üçün One Sample T-Test istifadə etdiniz. Aşağıda bu testin nəticələri, izahları və Python kodu ilə birlikdə təqdim olunur.\n",
    "\n",
    "### Titanic Datasetinin İlk Beş Sətri\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Titanic datasetini yükləyin\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# İlk beş sətri göstər\n",
    "print(titanic_df.head())\n",
    "```\n",
    "\n",
    "### One Sample T-Test Nəticələri\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Yaş məlumatlarını alın\n",
    "ages = titanic_df['Age'].dropna()  # NaN dəyərləri çıxar\n",
    "\n",
    "# Hipotez olunan ortalama\n",
    "hypothesized_mean = 30\n",
    "\n",
    "# One Sample T-Test\n",
    "t_statistic, p_value = stats.ttest_1samp(ages, hypothesized_mean)\n",
    "\n",
    "# Nəticələri çap edin\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "### One Sample T-Test Nəticələri\n",
    "- **T-statistic**: -0.5535\n",
    "- **P-value**: 0.5801\n",
    "\n",
    "### Nəticələrin İzahı\n",
    "1. **T-statistic**: Bu dəyər, nümunənin ortalamasının hipotez olunan ortalamadan nə qədər uzaq olduğunu göstərir. Negativ bir dəyər, nümunənin ortalamasının hipotez olunan ortalamadan (30 yaş) daha kiçik olduğunu göstərir.\n",
    "\n",
    "2. **P-value**: Bu dəyər, hipotezi qəbul etmək və ya rədd etmək üçün istifadə olunur. Ümumiyyətlə, p-value 0.05-dən aşağı olduqda, sıfır hipotezini (burada yaş ortalamasının 30-dan fərqli olduğu) rədd edirik. Burada p-value 0.5801 olduğu üçün, sıfır hipotezini rədd edə bilmirik. Yəni, sərnişinlərin yaş ortalamasının 30 yaşdan fərqli olduğuna dair statistik cəhətdən əhəmiyyətli bir dəlil yoxdur.\n",
    "\n",
    "### Nəticə\n",
    "Bu test, Titanic sərnişinlərinin yaş ortalamasının 30 yaşdan fərqli olub olmadığını araşdırmaq üçün istifadə edilmişdir. Lakin, nəticələr göstərir ki, yaş ortalaması 30 yaşdan əhəmiyyətli dərəcədə fərqlənmir.\n",
    "\n",
    "---\n",
    "\n",
    "Bu testlərin nəticələri Titanic datasetində fərqli dəyişənlər arasında statistik olaraq əhəmiyyətli əlaqələrin olub-olmamasını göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5984c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b7382f",
   "metadata": {},
   "source": [
    "## Moving Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58fe90",
   "metadata": {},
   "source": [
    "Bunu Pandas-da \"moving window\" yanaşması ilə edə bilərik, lakin burada fərqləndirici məqam odur ki, hər dəfə bir gün aşağı düşərək 7 günlük ortalama dəyər tapıb həmin ortalamaları yuxarıda dediyiniz kimi müəyyən tarixlərə qoyacağıq. Bunun üçün `rolling()` metodundan istifadə edərək, hər dəfə bir gün irəliləyərək 7 günlük ortalamaları hesablayıb onları uyğun günlərə yerləşdirə bilərik.\n",
    "\n",
    "Aşağıda bunu necə edə biləcəyinizi göstərirəm:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Zaman seriyası nümunə datası yaradaq\n",
    "data = {'date': pd.date_range(start='2023-01-01', periods=15, freq='D'),\n",
    "        'value': range(1, 16)}  # Nümunə olaraq 1-dən 15-ə qədər dəyərlər\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tarix sütununu indeks kimi qururuq\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# 7 günlük rolling window ortalamasını hesablayırıq və bir gün irəliləyərək dəyərləri uyğun tarixlərə yerləşdiririk\n",
    "df['rolling_mean_shifted'] = df['value'].rolling(window=7).mean().shift(-6)\n",
    "\n",
    "# Nəticəni göstərək\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Bu kodda:\n",
    "- `rolling(window=7)`: 7 günlük pəncərəni təyin edir.\n",
    "- `mean()`: Bu pəncərədə olan dəyərlərin ortalamasını tapır.\n",
    "- `shift(-6)`: Ortalamaları 8-ci günə (yəni pəncərənin sonuncu gününə) qoyur və hər dəfə bir gün irəliləyir.\n",
    "\n",
    "Bu yanaşma ilə:\n",
    "- İlk 7 günün ortalaması 8-ci günə yerləşdirilir.\n",
    "- Daha sonra, ikinci günün başlandığı nöqtədən etibarən növbəti 7 günün ortalaması 9-cu günə yerləşdirilir və bu proses davam edir.\n",
    "\n",
    "Nəticədə, yuxarıdakı dataframe-də hər 7 günlük ortalama 7 gün irəlidə yerləşdirilmiş olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3d582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee019a5a",
   "metadata": {},
   "source": [
    "## Pandas-da datanı bir vahid yuxarı və ya aşağı sürüşdürmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae2461",
   "metadata": {},
   "source": [
    "Pandas-da datanı bir vahid yuxarı və ya aşağı sürüşdürmək üçün `shift()` metodundan istifadə edə bilərsiniz. Bu, dəyərləri verilənlər bazasında bir və ya bir neçə sətir aşağı və ya yuxarı keçirməyə imkan verir, `Tableau`-da `LOOKUP` funksiyasına bənzəyir. \n",
    "\n",
    "Misal olaraq, bir sütundakı dəyərləri bir vahid yuxarı və ya aşağı necə sürüşdürəcəyinizi göstərirəm:\n",
    "\n",
    "### Aşağıya sürüşdürmək (bir vahid aşağıya):\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Nümunə data yaradaq\n",
    "data = {'date': pd.date_range(start='2023-01-01', periods=10, freq='D'),\n",
    "        'value': range(1, 11)}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tarix sütununu indeks kimi qururuq\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Datanı bir vahid aşağı sürüşdürürük\n",
    "df['shifted_down'] = df['value'].shift(1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### Yuxarıya sürüşdürmək (bir vahid yuxarı):\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Datanı bir vahid yuxarı sürüşdürürük\n",
    "df['shifted_up'] = df['value'].shift(-1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Bu kodlarda:\n",
    "- `shift(1)` dəyərləri bir vahid aşağı sürüşdürür (məsələn, birinci dəyər NaN olur, çünki aşağıya doğru sürüşdürülür).\n",
    "- `shift(-1)` dəyərləri bir vahid yuxarı sürüşdürür (məsələn, axırıncı dəyər NaN olur, çünki yuxarıya doğru sürüşdürülür).\n",
    "\n",
    "Bu əməliyyatlar sizə datanı Pandas DataFrame-də dinamik şəkildə hərəkət etdirməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad4b78a6",
   "metadata": {},
   "source": [
    "## Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca2eb",
   "metadata": {},
   "source": [
    "İmbalanced data problemini aradan qaldırmaq üçün az və ya çox nümayəndəli siniflərin mövcud olduğu vəziyyətlərdə istifadə edilə biləcək metodların kod nümunələrini təqdim edirəm.\n",
    "\n",
    "### 1. **Az Nümayəndəli Sinif (Minority Class) Olarsa**\n",
    "\n",
    "#### Over-sampling Metodları\n",
    "\n",
    "##### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "```python\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Nümunə dataset yaradılır\n",
    "data = {\n",
    "    'feature1': [1, 2, 1, 2, 1, 2, 1],\n",
    "    'feature2': [1, 1, 1, 1, 0, 0, 0],\n",
    "    'target': [0, 0, 0, 0, 1, 1, 1]  # 0: majority, 1: minority\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Özəllik və hədəf dəyişənlərini ayrırıq\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']\n",
    "\n",
    "# SMOTE tətbiq edirik\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"Original class distribution:\\n\", y.value_counts())\n",
    "print(\"Resampled class distribution:\\n\", y_resampled.value_counts())\n",
    "```\n",
    "\n",
    "##### ADASYN (Adaptive Synthetic Sampling)\n",
    "```python\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# ADASYN tətbiq edirik\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "\n",
    "print(\"ADASYN resampled class distribution:\\n\", y_adasyn.value_counts())\n",
    "```\n",
    "\n",
    "#### Sinif Cərimələmə (Class Weighting)\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Dataset bölmək\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest modeli sinif cəriməsi ilə\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnoz vermək\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Nəticələri qiymətləndirmək\n",
    "print(\"Classification report with class weighting:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Anomalya Dəyişdirilməsi\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Isolation Forest tətbiq etmək\n",
    "iso_model = IsolationForest(random_state=42)\n",
    "outliers = iso_model.fit_predict(X)\n",
    "\n",
    "print(\"Anomalies detected:\\n\", outliers)\n",
    "```\n",
    "\n",
    "#### Ensemble Metodlar\n",
    "```python\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "# EasyEnsembleClassifier tətbiq edirik\n",
    "ee_model = EasyEnsembleClassifier(random_state=42)\n",
    "ee_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnoz vermək\n",
    "y_pred_ee = ee_model.predict(X_test)\n",
    "\n",
    "# Nəticələri qiymətləndirmək\n",
    "print(\"Classification report from EasyEnsemble:\\n\", classification_report(y_test, y_pred_ee))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Çox Nümayəndəli Sinif (Majority Class) Olarsa**\n",
    "\n",
    "#### Under-sampling Metodları\n",
    "\n",
    "##### Random Under-sampling\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# RandomUnderSampler tətbiq edirik\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_under, y_under = under_sampler.fit_resample(X, y)\n",
    "\n",
    "print(\"Under-sampled class distribution:\\n\", y_under.value_counts())\n",
    "```\n",
    "\n",
    "##### Tomek Links\n",
    "```python\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Tomek Links tətbiq edirik\n",
    "tomek = TomekLinks()\n",
    "X_tomek, y_tomek = tomek.fit_resample(X, y)\n",
    "\n",
    "print(\"Tomek Links class distribution:\\n\", y_tomek.value_counts())\n",
    "```\n",
    "\n",
    "#### Sinif Cərimələmə (Class Weighting)\n",
    "```python\n",
    "# Random Forest modeli sinif cəriməsi ilə\n",
    "model_under = RandomForestClassifier(class_weight={0: 1, 1: 5}, random_state=42)  # 1 sinifə daha çox çəki\n",
    "model_under.fit(X_train, y_train)\n",
    "\n",
    "# Proqnoz vermək\n",
    "y_pred_under = model_under.predict(X_test)\n",
    "\n",
    "# Nəticələri qiymətləndirmək\n",
    "print(\"Classification report with class weighting for majority class:\\n\", classification_report(y_test, y_pred_under))\n",
    "```\n",
    "\n",
    "#### Model Seçimi\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting modeli\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnoz vermək\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Nəticələri qiymətləndirmək\n",
    "print(\"Classification report from Gradient Boosting:\\n\", classification_report(y_test, y_pred_gb))\n",
    "```\n",
    "\n",
    "Bu kod nümunələri, az və ya çox nümayəndəli siniflərin mövcud olduğu vəziyyətlərdə istifadə edilə biləcək metodları nümayiş etdirir. Hər bir metodun təsirini qiymətləndirmək üçün eksperimentlər aparmaq vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7591d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cbc76bb",
   "metadata": {},
   "source": [
    "## Label data ile Unlabel data arasindaki ferq nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cd262",
   "metadata": {},
   "source": [
    "Label data və unlabel data arasındakı fərq maşın öyrənməsi və data analitikası kontekstində mühüm bir mövzudur. Bu fərqi aydınlaşdırmaq üçün hər iki terminə nəzər salaq:\n",
    "\n",
    "### Label data (Etiketli data)\n",
    "- **Tərif**: Etiketli data, hər bir nümunənin (müxtəlif xüsusiyyətlərə və dəyərlərə malik olan verilənlər) yanında ona uyğun bir \"etiket\" (cavab) olan verilənlərdir. Bu etiket hədəf (target) və ya çıxış dəyəri kimi təyin edilir.\n",
    "- **Misal**: Bir avtomobil şəkilləri datasetində hər bir şəkilə uyğun bir \"avtomobil növü\" etiketi (məsələn, SUV, sedan) verilmiş ola bilər. Beləliklə, sistem hər bir görüntü ilə əlaqəli düzgün nəticəni öyrənir.\n",
    "- **İstifadə**: Etiketli data əsasən **supervised learning** (nəzarətli öyrənmə) modellərində istifadə olunur. Bu modellər mövcud etiketlər əsasında yeni verilənlər üçün düzgün etiketləri proqnozlaşdırmağa çalışır.\n",
    "\n",
    "### Unlabel data (Etiketsiz data)\n",
    "- **Tərif**: Etiketsiz data, sadəcə xüsusiyyətlərdən ibarət olan, lakin konkret etiketlər və ya cavab dəyərləri olmayan verilənlərdir. Bu verilənlərdə yalnız müşahidə olunan xüsusiyyətlər mövcuddur.\n",
    "- **Misal**: E-ticarət saytlarında istifadəçilərin davranışları ilə bağlı məlumatlar, məsələn, hansı məhsullara baxıldığı, hansı səhifələrdə çox vaxt keçirdiyi kimi məlumatlar ola bilər, lakin bu məlumatlar birbaşa nəticə ilə etiketlənməyib.\n",
    "- **İstifadə**: Etiketsiz data əsasən **unsupervised learning** (nəzarətsiz öyrənmə) modellərində istifadə olunur. Bu modellər etiketsiz verilənlərdə gizli strukturları (məsələn, klasterlər) tapmağa çalışır.\n",
    "\n",
    "### Əsas fərqlər\n",
    "- **Etiketin mövcudluğu**: Label data-da hədəf etiketi var, unlabel data-da isə bu yoxdur.\n",
    "- **Model növü**: Label data supervized modellərdə istifadə olunur, unlabel data isə unsupervised modellərdə tətbiq edilir.\n",
    "- **Məqsəd**: Label data-dan istifadə edərək model düzgün cavabları proqnozlaşdırmağı öyrənir, unlabel data-da isə verilənlərdəki strukturları və əlaqələri tapmaq hədəflənir.\n",
    "\n",
    "Bu fərqlər əsasən verilənlərin necə təhlil edildiyini və hansı növ modellərin istifadə olunacağını müəyyən edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a8590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd31f9c",
   "metadata": {},
   "source": [
    "## Regression və Classification arasinda ferq?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31677982",
   "metadata": {},
   "source": [
    "**Regression** və **classification**, hər ikisi də **nəzarətli öyrənmə** (supervised learning) metodlarıdır. Lakin onların arasında mühüm fərqlər var, çünki onlar fərqli növ proqnozlar və nəticələr üzərində işləyir. Hər ikisinin necə işlədiyini və əsas fərqlərini nəzərdən keçirək:\n",
    "\n",
    "### Regression (Regressiya)\n",
    "- **Tərif**: Regressiya, kəmiyyət bir dəyişən üçün proqnoz verməyi hədəfləyir. Bu tip modellər, davamlı (continuous) dəyişənləri proqnozlaşdırır, yəni nəticə rəqəm kimi olur.\n",
    "- **Misal**: Bir evin qiymətini proqnozlaşdırmaq. Verilənlər evin sahəsi, otaq sayı və digər xüsusiyyətlərə əsaslanır və nəticə evin təxmini qiyməti (rəqəmsal dəyər) olacaq.\n",
    "- **Məqsəd**: Davamlı bir nəticə proqnozlaşdırmaq (məsələn, bir şeyin qiyməti, temperaturu, çəkisi və s.).\n",
    "- **Nəticə növü**: **Davamlı (continuous)**, rəqəm şəklində nəticə. Məsələn, 0.0-dan 100.0-a qədər istənilən dəyər ola bilər.\n",
    "- **Məhsuldarlıq sahələri**:\n",
    "  - Satışların proqnozlaşdırılması.\n",
    "  - Bir insanın gəlirini təxmin etmək.\n",
    "  - Borsada qiymətlərin təxmin edilməsi.\n",
    "\n",
    "### Classification (Klassifikasiya)\n",
    "- **Tərif**: Klassifikasiya, fərqli kateqoriyalar və ya siniflər arasında qərar vermək məqsədi daşıyır. Nəticə diskret (diskret) olur, yəni bir neçə əvvəlcədən təyin olunmuş siniflərdən birinə aid olur.\n",
    "- **Misal**: E-poçtların \"spam\" və ya \"normal\" kimi təsnif edilməsi. Verilənlər e-poçt başlığı, göndərən və məzmununa əsaslanır və nəticədə e-poçt müəyyən bir kateqoriyaya (spam və ya normal) daxil edilir.\n",
    "- **Məqsəd**: Verilən məlumatlara əsaslanaraq müəyyən bir sinfə qərar vermək (diskret, fərqli siniflərə ayırmaq).\n",
    "- **Nəticə növü**: **Diskret (discrete)**, təyin olunmuş siniflərdən birinə aid edilən nəticə. Məsələn, siniflər 0 və 1 ola bilər, ya da daha çox siniflər (məsələn, 0, 1, 2, 3 və s.) ola bilər.\n",
    "- **Məhsuldarlıq sahələri**:\n",
    "  - Müştərinin kredit riskini müəyyənləşdirmək (məsələn, müştəri krediti ödəyə biləcək, ya da ödəyə bilməyəcək).\n",
    "  - Tibb sahəsində xəstəlik diaqnozu (məsələn, xəstə müəyyən bir xəstəliyə malikdir və ya deyil).\n",
    "  - İris data setində çiçəklərin növlərini (species) təsnif etmək.\n",
    "\n",
    "### Əsas fərqlər\n",
    "\n",
    "| Xüsusiyyət            | **Regression**                         | **Classification**                       |\n",
    "|-----------------------|----------------------------------------|------------------------------------------|\n",
    "| **Məqsəd**             | Davamlı bir dəyişəni proqnozlaşdırmaq  | Diskret siniflərə aid etmək              |\n",
    "| **Nəticə növü**        | Rəqəmsal, davamlı (continuous)         | Diskret, təyin olunmuş siniflər          |\n",
    "| **Məsələnin tipi**     | Nəticə real ədədi (rəhbərsiz dəyər)    | Nəticə siniflər şəklində (kategoriya)    |\n",
    "| **Misal**              | Ev qiymətinin təxmin edilməsi          | E-poçtun spam olub-olmaması              |\n",
    "| **Populyar alqoritmlər**| Linear Regression, Decision Tree Regressor | Logistic Regression, Random Forest Classifier, SVM |\n",
    "\n",
    "### Regression üçün bəzi alqoritmlər:\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "\n",
    "### Classification üçün bəzi alqoritmlər:\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Support Vector Machine (SVM)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Əsas fərq\n",
    "- **Regression** rəqəmsal nəticə proqnozlaşdırır, məsələn, qiymətlər, ölçülər və digər kəmiyyətlər.\n",
    "- **Classification** verilənləri müəyyən siniflərə ayırır, məsələn, bir obyektin hansı kateqoriyaya aid olduğunu müəyyən edir.\n",
    "\n",
    "### Nəticə\n",
    "Əgər məqsədiniz rəqəmsal dəyər (qiymət, məbləğ) proqnozlaşdırmaqdırsa, **regression** modellərindən istifadə etməlisiniz. Əgər verilənləri müəyyən siniflərə ayırmaq istəyirsinizsə, məsələn, müsbət və ya mənfi nəticələr arasında seçim etmək istəyirsinizsə, **classification** modelləri daha uyğun olacaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51d9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2381deb",
   "metadata": {},
   "source": [
    "## Classification novleri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff82475",
   "metadata": {},
   "source": [
    "Klassifikasiya, verilənlərin müəyyən siniflərə və ya kateqoriyalara bölünməsini təmin edən maşın öyrənməsi metodudur. Klassifikasiya problemlərində hədəf dəyişən diskret (diskret dəyərlər ala bilən) olur və müxtəlif klassifikasiya növləri mövcuddur. Klassifikasiya növlərini başa düşmək üçün aşağıdakı əsas kateqoriyaları nəzərdən keçirmək faydalıdır:\n",
    "\n",
    "### 1. **Binary Classification (İki Sinifli Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: İki sinif arasında seçim edir.\n",
    "   - **Misallar**:\n",
    "      - Xəstə/xəstə olmayan (məsələn, tibbi diaqnoz)\n",
    "      - Fırıldaq/fırıldaq olmayan (məsələn, bank fırıldaqçılığını aşkar etmək)\n",
    "   - **Ən çox istifadə olunan metodlar**:\n",
    "      - **Logistic Regression**: Xətti modelləşdirmə əsaslı və iki sinif arasında ehtimalı təxmin edir.\n",
    "      - **Support Vector Machines (SVM)**: Siniflər arasında ən yaxşı xətti sərhədi tapmağa çalışır.\n",
    "      - **Decision Trees**: Məlumatları təkrarən bölərək sinifləri təyin edir.\n",
    "\n",
    "### 2. **Multi-Class Classification (Çox Sinifli Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: Üç və ya daha çox sinif arasında seçim edir.\n",
    "   - **Misallar**:\n",
    "      - Şəkilləri \"it\", \"pişik\", \"quş\" və s. kimi müxtəlif kateqoriyalara ayırmaq\n",
    "      - Müştəriləri müxtəlif risk qruplarına ayırmaq\n",
    "   - **Ən çox istifadə olunan metodlar**:\n",
    "      - **Softmax Regression**: Logistic regression-ın genişləndirilmiş formasıdır, bir neçə sinif üçün ehtimalları çıxarmağa imkan verir.\n",
    "      - **Random Forest**: Çox sayda decision tree modelinin birləşməsi ilə yüksək dəqiqlik verir.\n",
    "      - **Naive Bayes**: Hər bir sinifin ehtimalını təxmin edərək ən uyğun sinfi seçir.\n",
    "      - **Neural Networks**: Sinir şəbəkələri sinifləndirmə üçün effektiv nəticələr verir, xüsusilə dərin öyrənmə modellərində.\n",
    "\n",
    "### 3. **Multi-Label Classification (Çox Etiketli Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: Hər bir nümunəyə birdən çox sinif etiketi verilir.\n",
    "   - **Misallar**:\n",
    "      - Şəkildə həm \"it\", həm \"avtomobil\", həm də \"ağac\" varsa, bütün bu etiketləri təyin etmək\n",
    "      - Mətnlərin birdən çox mövzu ilə əlaqələndirilməsi, məsələn, bir məqalə həm “İqtisadiyyat” həm də “Siyasət” kateqoriyalarında ola bilər.\n",
    "   - **Ən çox istifadə olunan metodlar**:\n",
    "      - **Binary Relevance**: Hər bir etiket üçün ayrı binary classifier qurur.\n",
    "      - **Classifier Chains**: Hər bir sinfi müəyyən etdikdən sonra nəticələri digər siniflərə ötürərək işləyir.\n",
    "      - **Adapted Algorithms**: Multi-label üçün xüsusi hazırlanmış algoritmlər, məsələn, ML-kNN (Multi-label KNN).\n",
    "\n",
    "### 4. **Imbalanced Classification (Balanssız Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: Verilənlərdə siniflərin bərabər sayda olmadığı halları əhatə edir.\n",
    "   - **Misallar**:\n",
    "      - Fırıldaqçılıq deteksiyasında \"fırıldaq\" halları \"normal\" hallara nisbətən çox az ola bilər.\n",
    "      - Səhiyyədə nadir xəstəliklərin diaqnozu üçün verilənlərdə xəstələr çox az, sağlamlar isə çox sayda ola bilər.\n",
    "   - **Həll üsulları**:\n",
    "      - **Class Weighting**: Modelin balanssızlığı aradan qaldırması üçün sinif çəkilərini tənzimləmək.\n",
    "      - **Resampling Techniques**: Azlıqda olan sinifləri artırmaq (oversampling) və ya çox olan sinifləri azaltmaq (undersampling).\n",
    "      - **Specialized Algorithms**: SMOTE (Synthetic Minority Over-sampling Technique) və ADASYN kimi balanssız verilənlər üçün uyğun metodlar.\n",
    "\n",
    "### 5. **Ordinal Classification (Sıralı Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: Siniflər arasında müəyyən bir sıralama olduğunda istifadə olunur.\n",
    "   - **Misallar**:\n",
    "      - Müştəri məmnuniyyəti sorğuları: “çox narazı”, “narazı”, “neytral”, “razı”, “çox razı”.\n",
    "      - Kredit reytinqləri: “çox riskli”, “riskli”, “normal”, “yüksək riskli” və s.\n",
    "   - **Metodlar**:\n",
    "      - **Ordinal Logistic Regression**: Sıralı klassifikasiya üçün logistic regression-ın genişləndirilmiş variantıdır.\n",
    "      - **Tree-Based Methods**: Decision tree və random forest metodları modifikasiyalarla sıralı klassifikasiya üçün istifadə edilə bilər.\n",
    "\n",
    "### 6. **Hierarchical Classification (Hierarşik Klassifikasiya)**\n",
    "   - **Ümumi prinsip**: Siniflər arasında hierarşik əlaqə olduqda istifadə olunur.\n",
    "   - **Misallar**:\n",
    "      - Canlılar aləmi təsnifatı: “Canlılar” → “Heyvanlar” → “Məməlilər” → “İtlər” və s.\n",
    "   - **Metodlar**:\n",
    "      - **Top-Down və Bottom-Up Approaches**: Məlumatları yuxarıdan aşağıya və ya əksinə, siniflər arasında ayırmaq.\n",
    "\n",
    "Bu klassifikasiya növləri tətbiq sahəsinə görə istifadə olunur və modellərin xüsusiyyətlərini nəzərə almaq vacibdir. Modelin düzgün seçilməsi və uyğun parametrlərin təyin edilməsi də nəticələrin dəqiqliyini artırır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbb960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118bdf9b",
   "metadata": {},
   "source": [
    "## Cluster nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34b97b",
   "metadata": {},
   "source": [
    "**Cluster** (klaster) verilənlərdəki müşahidələrin (data nöqtələrinin) oxşarlıqlarına əsaslanaraq qruplaşdırıldığı bir qrup və ya seqmentdir. **Clustering** isə bu qruplaşdırma prosesidir, hansı ki, verilənləri təbii olaraq bənzər xüsusiyyətlərə malik olan qruplara ayırmaq məqsədi daşıyır. Bu proses **nəzarətsiz öyrənmə** (unsupervised learning) texnikalarından biridir, yəni modelin təlimi üçün etiketli və ya hədəf dəyişəninə ehtiyac yoxdur.\n",
    "\n",
    "### Klasterləşdirmənin məqsədi nədir?\n",
    "Clustering-in əsas məqsədi verilənlərdəki təbii qrupları tapmaqdır. Klasterlər daxilindəki müşahidələr bir-birinə oxşar olmalı, fərqli klasterlərdə olan müşahidələr isə bir-birindən fərqlənməlidir. Bu metod, verilənlərdə gizli olan strukturları aşkar etmək və müşahidələri daha kiçik, bənzər qruplara bölmək üçün istifadə olunur.\n",
    "\n",
    "### Klaster (Cluster) xüsusiyyətləri:\n",
    "1. **Bənzərlik**: Klaster daxilindəki bütün məlumat nöqtələri bir-birinə mümkün qədər oxşar olmalıdır.\n",
    "2. **Fərqlilik**: Müxtəlif klasterlərdə olan məlumat nöqtələri mümkün qədər fərqli olmalıdır.\n",
    "3. **Köhnə etiketlər yoxdur**: Clustering təbii qrupları tapmaq üçün məlumatlardan istifadə edir və əvvəlcədən təyin edilmiş siniflər yoxdur.\n",
    "\n",
    "### Klasterləşdirmə üçün istifadə olunan əsas alqoritmlər:\n",
    "1. **K-Means Clustering**: Ən populyar klasterləşdirmə metodlarından biridir. K istifadəçi tərəfindən müəyyən edilir və alqoritm verilənləri \"K\" ədəd klasterə bölməyə çalışır.\n",
    "2. **Hierarchical Clustering**: Bu metod verilənləri iyerarxik şəkildə qruplaşdırır, yəni kiçik klasterlər daha böyük klasterlərə qoşularaq ağacvari struktur yaradır.\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Bu metod verilənlərin sıxlığına əsasən klasterləri müəyyən edir. Sıx verilənlər klaster kimi qəbul edilir, sıx olmayan nöqtələr isə \"şübhəli\" nöqtələr hesab olunur.\n",
    "4. **Mean Shift Clustering**: Verilənlərin sıxlığına əsaslanaraq qruplar tapmağa çalışır və klasterləri müəyyən edir.\n",
    "\n",
    "### Klasterləşdirmə harada istifadə olunur?\n",
    "- **Müştəri seqmentasiyası**: Müştəriləri davranışlarına, alış-veriş vərdişlərinə və digər göstəricilərə əsasən qruplara ayırmaq.\n",
    "- **Marketinq təhlili**: Müxtəlif məhsulların və ya bazar seqmentlərinin təhlil edilməsi.\n",
    "- **Tibbi tədqiqatlar**: Xəstələri xəstəlik növlərinə və ya oxşar simptomlara görə qruplaşdırmaq.\n",
    "- **Sosial şəbəkə analizləri**: İstifadəçilərin əlaqələrinə və davranışlarına əsasən sosial qrupların müəyyən edilməsi.\n",
    "- **Anomaliya aşkarlanması**: Normadan kənar olan verilənləri aşkar etmək (məsələn, fırıldaqçılıq, sistemdəki səhvlər).\n",
    "\n",
    "### Klasterlər ilə bağlı misal\n",
    "Təsəvvür edin ki, bir marketinq şirkətinin müştəri bazası var və bu müştəriləri onların satınalma vərdişlərinə əsasən qruplaşdırmaq istəyirsiniz. **Klasterləşdirmə** istifadə edərək, müştəriləri fərqli qruplara ayıra bilərsiniz:\n",
    "- **Klaster 1**: Daha çox lüks məhsullar alan müştərilər.\n",
    "- **Klaster 2**: Endirimli məhsulları almağa üstünlük verən müştərilər.\n",
    "- **Klaster 3**: Tez-tez alış-veriş edən, lakin kiçik məbləğdə alış-veriş edən müştərilər.\n",
    "\n",
    "Bu qruplar marketinq strategiyasını fərqli qruplar üçün optimallaşdırmaq üçün istifadə edilə bilər.\n",
    "\n",
    "### Nəticə\n",
    "Klaster, bənzər verilənlərin bir yerə toplandığı qrupdur. Klasterləşdirmə verilənləri təbii qruplara ayırmaq üçün istifadə olunan nəzarətsiz öyrənmə metodudur. Məqsəd, verilənlər arasında gizli qrupları tapmaq və onları oxşar xüsusiyyətlərə görə qruplaşdırmaqdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5efa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b81711",
   "metadata": {},
   "source": [
    "## Cluster metdolari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca257b2",
   "metadata": {},
   "source": [
    "Klaster analizində istifadə olunan metodlar, verilənləri qruplara ayırmaq və hər bir qrupa aid oxşar obyektləri təyin etmək üçün müxtəlif yanaşmaları əhatə edir. Əsas klaster metodları bunlardır:\n",
    "\n",
    "### 1. **K-Means Clustering**\n",
    "   - **Ümumi prinsip**: Verilənləri müəyyən sayda klasterə (qruplara) ayırır və hər klasterin mərkəzi olan \"centroid\" hesablanır.\n",
    "   - **Prosesi**: Verilən hər bir nöqtə ən yaxın centroid-ə təyin olunur, sonra centroid-lər yenidən hesablanır. Bu proses nöqtələr uyğun klasterlərə düşənə qədər təkrarlanır.\n",
    "   - **Əlavə məlumat**: K-means klasterləri təyin etmək üçün əvvəlcədən klaster sayını (`k`) təyin etməyi tələb edir.\n",
    "\n",
    "### 2. **Hierarchical Clustering**\n",
    "   - **Ümumi prinsip**: Verilənləri ardıcıl şəkildə birləşdirərək və ya bölərək klasterlər yaradır.\n",
    "   - **İki əsas növü**:\n",
    "      - **Agglomerative (Bottom-Up)**: Hər bir obyekt bir klaster olaraq başlayır, sonra klasterlər tədricən birləşir.\n",
    "      - **Divisive (Top-Down)**: Verilənlər əvvəlcə bir klasterdə olur, sonra təkrarlayan şəkildə kiçik klasterlərə bölünür.\n",
    "   - **Dendrogram**: Bütün verilənlərin qruplara necə bölündüyünü göstərən bir diaqram (ağac strukturu) yaradılır.\n",
    "\n",
    "### 3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "   - **Ümumi prinsip**: Verilənləri sıxlıq əsaslı qruplara ayırır.\n",
    "   - **Prosesi**: Verilən sıxlıq ətrafında toplanmış nöqtələri klaster olaraq təyin edir və \"noise\" (səs-küy) adlanan ayrılmış nöqtələri qruplaşdırmaz.\n",
    "   - **Əlavə məlumat**: DBSCAN klaster sayını əvvəlcədən təyin etməyə ehtiyac olmadan sıxlıq əsaslı klasterləri müəyyən etmək üçün effektivdir.\n",
    "\n",
    "### 4. **Mean-Shift Clustering**\n",
    "   - **Ümumi prinsip**: Verilən nöqtələrin sıx olduğu bölgələrə doğru nöqtələri tədricən hərəkət etdirir.\n",
    "   - **Prosesi**: Hər bir nöqtə sıxlığın yüksək olduğu sahəyə doğru iterativ şəkildə hərəkət edir.\n",
    "   - **Əlavə məlumat**: Mean-Shift, verilənlərdəki sıxlıq dəyişikliklərini analiz edərək klasterlərin təbii sayını tapmağa kömək edir.\n",
    "\n",
    "### 5. **Gaussian Mixture Models (GMM)**\n",
    "   - **Ümumi prinsip**: Verilənləri Gaussian (normal) paylanma ilə təmsil olunan alt qruplara ayırır.\n",
    "   - **Prosesi**: Hər bir klaster Gaussian paylanma olaraq modelləşdirilir və verilənlər bu modellərə əsasən bölünür.\n",
    "   - **Əlavə məlumat**: GMM, K-Means-in təkmilləşdirilmiş variantı kimi qəbul edilir, çünki GMM klasterlərin formasında daha elastikdir.\n",
    "\n",
    "### 6. **Spectral Clustering**\n",
    "   - **Ümumi prinsip**: Verilənləri qraf nəzəriyyəsi əsasında klasterlərə ayırır.\n",
    "   - **Prosesi**: Verilənlər bir qraf üzərində təmsil edilir və bu qrafın spektral xüsusiyyətlərindən istifadə edilərək klasterlər müəyyən edilir.\n",
    "   - **Əlavə məlumat**: Bu metod, xüsusilə qeyri-xətti strukturlara malik verilənlər üçün yaxşı nəticələr verir.\n",
    "\n",
    "### 7. **Agglomerative Clustering**\n",
    "   - **Ümumi prinsip**: Kiçik klasterləri iterativ olaraq birləşdirərək daha böyük klasterlər yaradır.\n",
    "   - **Prosesi**: Əvvəlcə hər bir nöqtə öz klasteri kimi qəbul edilir və hər dəfə ən yaxın iki klaster birləşdirilir.\n",
    "   - **Əlavə məlumat**: Bu metod dendrogram vasitəsilə vizuallaşdırıla bilər və klasterlərin sayını dendrogram əsasında müəyyən etmək mümkündür.\n",
    "\n",
    "### 8. **Birlikdə İstifadə Edilən Modellər və Klasterlər (Cluster Ensembles)**\n",
    "   - **Ümumi prinsip**: Müxtəlif klaster metodlarını birlikdə istifadə edərək daha dəqiq klasterlər əldə edir.\n",
    "   - **Əlavə məlumat**: Müxtəlif klasterlərin nəticələrini birləşdirərək, hər bir metodun çatışmazlıqlarını digərləri ilə kompensasiya edir.\n",
    "\n",
    "Hər bir metod fərqli verilən strukturlarına və məqsədlərə uyğun gəldiyi üçün, klaster analizində uyğun metodu seçmək üçün verilənlərin xüsusiyyətlərini və tətbiq sahəsini nəzərə almaq vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de398ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe5e1784",
   "metadata": {},
   "source": [
    "## Association Rule "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde650a",
   "metadata": {},
   "source": [
    "**Association rule** (assosiasiya qaydaları) verilənlər arasında gizli nümunələri və əlaqələri aşkar etmək üçün istifadə olunan bir **nəzarətsiz öyrənmə** metodudur. Bu metod, xüsusilə pərakəndə satışda və marketinq analizində geniş istifadə olunur. Məsələn, hansı məhsulların tez-tez birlikdə alındığını müəyyən etməyə imkan verir. \n",
    "\n",
    "### Association rule nədir?\n",
    "- **Assosiasiya qaydaları** müxtəlif obyektlər (məhsullar, hadisələr və s.) arasında əlaqələr quraraq onların birlikdə ortaya çıxma ehtimalını aşkar etməyə çalışır. Bu qaydalar əsasən **“Əgər-X Onda-Y”** şəklində yazılır. Yəni, **Əgər** bir hadisə baş veribsə, **Onda** başqa bir hadisənin baş vermə ehtimalı yüksəkdir.\n",
    "- **Misal**: Bir marketdə tez-tez birlikdə alınan məhsulları təhlil etmək. Məsələn, müştərilər tez-tez **çörək** aldıqda, **pendir** də alırlar. Bu zaman qayda belədir: \"Əgər çörək alınıbsa, onda pendir alma ehtimalı yüksəkdir.\"\n",
    "\n",
    "### Əsas terminlər\n",
    "Assosiasiya qaydalarının düzgün başa düşülməsi üçün bəzi əsas terminləri bilmək vacibdir:\n",
    "\n",
    "1. **Support (Dəstək)**:\n",
    "   - Bu, müəyyən bir məhsulun və ya məhsul qrupunun ümumi alış-verişdə nə qədər tez-tez baş verdiyini göstərir. Başqa sözlə, \"support\" həmin qaydanın nə qədər ümumi olduğu haqqında məlumat verir.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Support}(X) = \\frac{\\text{X obyektinin rast gəlmə sayı}}{\\text{ümumi alış-veriş sayı}}\n",
    "     \\]\n",
    "   - **Misal**: 100 alış-verişdə 20 dəfə çörək alınıbsa, çörəyin support dəyəri 0.2-dir (yəni, 20%).\n",
    "\n",
    "2. **Confidence (Əminlik)**:\n",
    "   - Bu, müəyyən bir məhsulun alışı zamanı digər bir məhsulun alınma ehtimalını göstərir. Yəni, \"Əgər X alınıbsa, onda Y də alınır\" ehtimalıdır.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Confidence}(X \\rightarrow Y) = \\frac{\\text{X və Y birlikdə alınma sayı}}{\\text{X alınma sayı}}\n",
    "     \\]\n",
    "   - **Misal**: 100 alış-verişdə 20 dəfə həm çörək, həm də pendir alınıbsa, amma çörək ümumilikdə 50 dəfə alınıbsa, \"çörək → pendir\" əminlik dəyəri 20/50 = 0.4 olacaq (yəni, 40%).\n",
    "\n",
    "3. **Lift**:\n",
    "   - Lift, **X və Y** məhsullarının birlikdə alınmasının təsadüfi olma ehtimalına nisbətən nə qədər daha çox olduğunu göstərir. Başqa sözlə, lift, bir qaydanın önəmli olub-olmamasını göstərir.\n",
    "   - **Formula**: \n",
    "     \\[\n",
    "     \\text{Lift}(X \\rightarrow Y) = \\frac{\\text{Confidence}(X \\rightarrow Y)}{\\text{Support}(Y)}\n",
    "     \\]\n",
    "   - **Misal**: Əgər lift dəyəri 1-dən böyükdürsə, deməli, X alındıqda Y alma ehtimalı yüksəkdir. 1-dən kiçikdirsə, o zaman birlikdə alınma ehtimalı aşağıdır.\n",
    "\n",
    "### Misal\n",
    "Bir supermarketdə alış-verişlə bağlı assosiasiya qaydalarını yaratmaq üçün belə bir nümunəyə baxaq:\n",
    "\n",
    "- Müştəri tez-tez **çörək** aldıqda **yağ** da alır. \n",
    "- **Support**: 20% (çörək və yağ birlikdə 100 alış-verişin 20-də alınıb).\n",
    "- **Confidence**: 50% (çörək alındıqda, onun yanında yağ alınması ehtimalı 50%).\n",
    "- **Lift**: 2 (çörək alındıqda yağ alma ehtimalı, yağın ümumi alınma ehtimalından iki dəfə çoxdur).\n",
    "\n",
    "Bu qaydanı istifadə edərək market, müştərilər çörək aldıqda yağın yanında endirim təklif edərək satışları artıra bilər.\n",
    "\n",
    "### Association rule-un istifadə sahələri:\n",
    "1. **Market Basket Analysis** (Alış-veriş səbəti təhlili): Müştərilərin hansı məhsulları birlikdə almağa meylli olduğunu müəyyən etmək.\n",
    "2. **Tövsiyə sistemləri**: Amazon və Netflix kimi platformalar istifadəçilərə hansı məhsul və ya filmi tövsiyə edəcəyini müəyyən etmək üçün assosiasiya qaydalarından istifadə edir.\n",
    "3. **Fırıldaqçılığın aşkarlanması**: Banklar və kredit kartı şirkətləri fırıldaqçı fəaliyyətlərin müəyyən bir nümunəsini aşkar etmək üçün assosiasiya qaydalarından istifadə edir.\n",
    "4. **Xəstəlik analizi**: Tibb sahəsində müəyyən simptomlar və xəstəliklərin birlikdə ortaya çıxma ehtimalını müəyyən etmək üçün assosiasiya qaydaları istifadə olunur.\n",
    "\n",
    "### Algoritmlər:\n",
    "- **Apriori**: Association rule-ları tapmaq üçün ən çox istifadə edilən alqoritmlərdən biridir. Bu metod verilənlərdəki tez-tez rast gəlinən məhsul dəstlərini aşkar edir və onların arasında əlaqələri müəyyən edir.\n",
    "- **FP-Growth**: Apriori-yə alternativ bir alqoritmdir və daha sürətli hesablamalarla tanınır. Verilənlərin sıx strukturlarını nəzərə alaraq tez-tez rast gəlinən obyekt qruplarını tapır.\n",
    "\n",
    "### Nəticə\n",
    "**Association rule learning** verilənlərdə obyektlərin birlikdə yaranma ehtimalını təhlil etmək və onların arasındakı əlaqələri aşkar etmək üçün istifadə olunur. Bu qaydalar marketinq, satış, fırıldaqçılıq aşkarlanması kimi bir çox sahədə biznes qərarlarını optimallaşdırmaq üçün istifadə oluna bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d1324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f05e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f0603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe0bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01881f03",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4be777",
   "metadata": {},
   "source": [
    "## Deep Learning ve Neural Networks  nedir  ve bunlar haqqinda genis melumat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed185d93",
   "metadata": {},
   "source": [
    "**Deep Learning** və **Neural Networks** müasir maşın öyrənməsi sahəsindəki iki əhəmiyyətli anlayışdır. Bu iki termin bəzən bir-biri ilə əvəz olunsa da, əslində biri digərinin bir alt kateqoriyasıdır. Gəlin, hər birini ayrı-ayrılıqda müzakirə edək.\n",
    "\n",
    "### Neural Networks (Neyron Şəbəkələri)\n",
    "\n",
    "**Neyron şəbəkələri**, insan beyninin işləmə prinsipindən ilhamlanaraq yaradılan hesablama modelləridir. Onlar, verilənlərin emalında istifadə olunan sadə sinir hüceyrələrindən (neyronlardan) ibarətdir. Neyron şəbəkələri, statistik öyrənmə və optimizasiya texnikalarından istifadə edərək verilənlərdən öz-özünə öyrənir.\n",
    "\n",
    "#### Struktur\n",
    "\n",
    "Neyron şəbəkələrinin əsas elementləri:\n",
    "\n",
    "1. **Neyronlar**: Hər bir neyron, girişləri qəbul edir, onları çəkilərlə (weights) çarparaq və bias (sabit dəyər) əlavə edərək aktivasiya funksiyası vasitəsilə çıxış verir.\n",
    "2. **Qatlar (Layers)**: Neyronlar bir neçə qat təşkil edir:\n",
    "   - **Giriş Qatı (Input Layer)**: Modelə daxil olan məlumatları qəbul edir.\n",
    "   - **Gizli Qatlar (Hidden Layers)**: Girişdən çıxışa qədərki emal mərhələlərini həyata keçirir. Bir neçə gizli qatın olması, şəbəkənin daha mürəkkəb funksiyaları öyrənməsinə imkan verir.\n",
    "   - **Çıxış Qatı (Output Layer)**: Modelin proqnozunu təqdim edir.\n",
    "\n",
    "#### Aktivasiya Funksiyaları\n",
    "\n",
    "Neyron şəbəkələrinin işini idarə edən aktivasiya funksiyaları vardır. Bu funksiyalar neyronların çıxışını hesablamaq üçün istifadə olunur. Məsələn:\n",
    "- **Sigmoid**: \\[ f(x) = \\frac{1}{1 + e^{-x}} \\]\n",
    "- **Tanh**: \\[ f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\]\n",
    "- **ReLU (Rectified Linear Unit)**: \\[ f(x) = \\max(0, x) \\]\n",
    "\n",
    "### Deep Learning (Dərin Öyrənmə)\n",
    "\n",
    "**Dərin öyrənmə**, neyron şəbəkələrinin daha dərin (çox sayda gizli qat) variantıdır. Bu, daha mürəkkəb verilənlərdən (məsələn, görüntülər, səslər, mətndən) daha yüksək səviyyədə xüsusiyyətləri çıxarmağa imkan tanıyır. Dərin öyrənmə, xüsusilə böyük verilənlərdə və yüksək hesablama gücündə mükəmməl nəticələr verir.\n",
    "\n",
    "#### Xüsusiyyətləri\n",
    "\n",
    "1. **Dərinlik**: Dərin öyrənmə modelləri, bir neçə gizli qatdan ibarətdir ki, bu da onlara daha mürəkkəb funksiyaları öyrənməyə imkan tanıyır.\n",
    "2. **Öz-özünə öyrənmə**: Dərin öyrənmə modelləri, verilənlərdəki mühüm xüsusiyyətləri avtomatik olaraq öyrənə bilər. Bu, xüsusiyyət mühəndisliyi prosesini minimuma endirir.\n",
    "3. **Verilənlərin Həcmi**: Dərin öyrənmə, böyük verilən dəstlərinə əsaslanır. Modelin daha yaxşı öyrənməsi üçün daha çox verilənə ehtiyacı var.\n",
    "\n",
    "#### Dərin Öyrənmə Tipləri\n",
    "\n",
    "1. **Konvolyusional Neyron Şəbəkələri (CNN)**: Xüsusilə görüntü işlənməsi üçün istifadə olunur. Konvolyusional qatlar görüntüdən mühüm xüsusiyyətləri çıxarmağa kömək edir.\n",
    "2. **Təkrarlanan Neyron Şəbəkələri (RNN)**: Zaman seriyası verilənləri, mətni və s. emal etmək üçün istifadə olunur. Zamanla bağlı məlumatları (sequential data) daha effektiv şəkildə idarə edir.\n",
    "3. **Transformerlər**: Mətni emal etmək üçün istifadə olunur və son dövrlərdə NLP (Natural Language Processing) sahəsində inqilab etmişdir.\n",
    "\n",
    "### Dərin Öyrənmə Prosesləri\n",
    "\n",
    "Dərin öyrənmə prosesləri aşağıdakı mərhələlərdən ibarətdir:\n",
    "\n",
    "1. **Veri Toplama**: Modelin öyrədilməsi üçün verilənlər toplanır.\n",
    "2. **Məlumatın Hazırlanması**: Verilənlər standartlaşdırılır, təmizlənir və mümkün olduqda bölünür.\n",
    "3. **Modelin Təyini**: Dərin neyron şəbəkəsi arxitekturası seçilir.\n",
    "4. **Təlim**: Model, verilənlərdəki nümunələrdən öyrənmək üçün təlim olunur.\n",
    "5. **Test**: Modelin performansı test edilir.\n",
    "6. **Tətbiq**: Model real verilənlərdə tətbiq edilir.\n",
    "\n",
    "### Tətbiq Sahələri\n",
    "\n",
    "Dərin öyrənmə və neyron şəbəkələri geniş tətbiq sahələrində istifadə olunur:\n",
    "- **Görüntü Tanıma**: Məsələn, şəkillərdə obyektlərin tanınması.\n",
    "- **Səs Tanıma**: Danışıqdan mətni çıxarmaq.\n",
    "- **Tərcümə**: Mətni bir dildən digərinə tərcümə etmək.\n",
    "- **Sosial Media Analizi**: Məlumatların təhlili və analitikası.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "Neyron şəbəkələri və dərin öyrənmə müasir maşın öyrənməsi sahəsində əsas metodlardan biridir. Onlar insan beyninin iş prinsipini təqlid edərək mürəkkəb verilənlərdən öz-özünə öyrənmə və analitik qərarlar qəbul etməyə imkan tanıyır. Dərin öyrənmə, xüsusilə böyük verilənlərdə daha mürəkkəb problemləri həll etməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aed454c",
   "metadata": {},
   "source": [
    "## Slope Deep Learning-de "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af7a731",
   "metadata": {},
   "source": [
    "Deep learning-də **slope** (meyl/əyrimə) termini ən çox gradient-based optimallaşdırma texnikaları ilə əlaqələndirilir. **Slope**, əsasən **gradient** anlayışı ilə eyni məna daşıyır və modelin optimallaşdırılmasında istifadə olunur.\n",
    "\n",
    "### Slope nədir?\n",
    "Deep learning-də **slope**, müəyyən bir nöqtədəki itkini (loss) dəyişdirərkən modelin necə davranacağını göstərən dəyərdir. Bu, əsasən neyron şəbəkəsinin təlimi zamanı istifadə olunur, çünki **slope** modelin parametrlərinin (çəkilərin və bias-ların) yenilənməsi üçün həlledici bir rol oynayır. \n",
    "\n",
    "### Gradient nədir və slope ilə necə bağlıdır?\n",
    "Gradient, itki funksiyasının dəyişmə istiqamətini göstərən vektordur. Əslində gradient, funksiyanın \"slope\"unu göstərir. Yəni, əgər modelin itki funksiyasının gradienti müsbətdirsə, bu o deməkdir ki, itki artır və parametrləri dəyişərək daha yaxşı nəticə əldə etmək olar. Əgər gradient mənfidirsə, bu o deməkdir ki, itki azalır və bu istiqamət yaxşılaşmaya doğru gedir.\n",
    "\n",
    "Deep learning-də istifadə olunan əsas optimallaşdırma texnikası **gradient descent** adlanır və burada əsas ideya modelin parametrlərini təkrarlayaraq slope-a (yəni gradientə) əsasən kiçik addımlarla optimallaşdırmaqdır.\n",
    "\n",
    "### Gradient Descent və slope\n",
    "**Gradient Descent** alqoritmi gradientin (slope-un) istiqamətindən istifadə edərək modelin çəkilərini yeniləyir. Məqsəd itki funksiyasını minimuma endirməkdir, bu da o deməkdir ki, gradienti (slope-u) izləyərək eniş istiqamətində hərəkət edirik.\n",
    "\n",
    "- **Mənfi slope**: İtki funksiyasının azalması deməkdir və gradient descent bu istiqamətdə davam edəcək.\n",
    "- **Müsbət slope**: İtki funksiyasının artması deməkdir və bu, tərs istiqamətdə hərəkət edilməli olduğunu göstərir.\n",
    "\n",
    "### Niyə slope bu qədər önəmlidir?\n",
    "Deep learning modellərində, xüsusilə çox qatlı neyron şəbəkələrində, hər qatın çəkiləri **backpropagation** üsulu ilə yenilənir. Bu yenilənmə prosesi gradientlərə, yəni slope-a əsaslanır. Əgər gradient çox kiçikdirsə, modelin təlim sürəti yavaş olar. Əgər gradient çox böyükdürsə, model tez optimallaşdırıla bilər, amma qlobal minimumdan keçərək yanlış yolda irəliləyə bilər.\n",
    "\n",
    "### Vanishing Gradient Problemi\n",
    "Çox qatlı neyron şəbəkələrdə (xüsusilə də, **Recurrent Neural Networks** və ya çox dərin şəbəkələrdə), gradientlərin çox kiçik olması problemi yaranır. Bu, **vanishing gradient problem** adlanır və təlim prosesini çox çətinləşdirir, çünki gradientlər (slope-lar) kiçik olduqda, şəbəkənin daha dərin qatları çox zəif öyrənir. Bunun həlli üçün **ReLU** kimi aktivasiya funksiyalarından və ya optimallaşdırma texnikalarından istifadə olunur.\n",
    "\n",
    "### Nəticə\n",
    "Deep learning-də **slope** itki funksiyasının gradienti ilə bağlıdır və modelin çəkilərinin yenilənməsi üçün vacib bir parametrdir. Gradient descent alqoritmi gradientə əsasən modelin çəkilərini optimallaşdırır, bu da təlim prosesinin əsas hissəsini təşkil edir. Gradient və slope modellərin öyrənmə effektivliyinə və nəticələrin dəqiqliyinə birbaşa təsir göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4291c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60938e1d",
   "metadata": {},
   "source": [
    "## New Weight Neural network-lərdə"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddfe98",
   "metadata": {},
   "source": [
    "Neural network-lərdə **new weight** anlayışı modelin çəkilərinin (weights) hər bir təlim iterasiyasında yenilənməsini ifadə edir. Neyron şəbəkəsinin təlim prosesi zamanı, model girişdən çıxışa gedən istiqamətdə məlumatı emal edir və bu emaldan sonra **backpropagation** alqoritmi vasitəsilə çəkilər yenilənir. Çəkilərin yenilənməsi modelin itki funksiyasını (loss function) minimuma endirmək üçün edilir.\n",
    "\n",
    "### Neural network-lərdə çəkilər nədir?\n",
    "\n",
    "Neyron şəbəkəsi girişdən çıxışa qədər qatlar (layers) və neyronlardan ibarətdir. Hər bir neyron bir və ya bir neçə giriş alır və bu girişlərə **çəki** tətbiq olunur. Çəkilər neyronun girdiyi məlumatın hansı əhəmiyyətə sahib olduğunu müəyyən edir. Çəkilərin ilkin dəyərləri təlimin əvvəlində təsadüfi olaraq təyin edilir və təlim prosesi zamanı bu çəkilər dəyişir. Məqsəd, şəbəkəni öyrədərək düzgün çəkilər tapmaqdır ki, model düzgün proqnozlar verə bilsin.\n",
    "\n",
    "### Çəkilərin yenilənməsi necə baş verir?\n",
    "\n",
    "Neyron şəbəkəsində çəkilərin yenilənməsi **backpropagation** (geriyə yayılma) və **gradient descent** alqoritmləri vasitəsilə həyata keçirilir.\n",
    "\n",
    "1. **Forward pass**: Şəbəkə girişdən çıxışa qədər məlumatı irəliyə doğru ötürür. Bu mərhələdə hər neyronun çıxışı hesablamaq üçün hər giriş verilən çəkilərlə vurulur və aktivasiya funksiyasından keçirilir.\n",
    "   $$\n",
    "   z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
    "  $$\n",
    "   Burada, \\(w\\) çəki, \\(x\\) giriş və \\(b\\) isə bias (sabit dəyər) hesab olunur.\n",
    "\n",
    "2. **Loss hesablanması**: Şəbəkənin çıxışı ilə real cavab arasındakı fərq **itki funksiyası** vasitəsilə ölçülür. İtki nə qədər az olarsa, model o qədər yaxşı öyrənir.\n",
    "\n",
    "3. **Backpropagation**: Bu mərhələdə, itki funksiyasının gradienti (meyli) hesablanır və hər bir çəkinin hansı istiqamətdə və nə qədər dəyişdirilməsi lazım olduğunu müəyyən etmək üçün geriyə doğru yayılır.\n",
    "\n",
    "4. **Çəkilərin yenilənməsi (New Weight hesablanması)**: Gradient descent alqoritmi çəkiləri yeniləmək üçün aşağıdakı formuldan istifadə edir:\n",
    "  $$\n",
    "   w_{\\text{new}} = w_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial w}\n",
    "   $$\n",
    "   Burada:\n",
    "   - $w_{\\text{new}}$ — yeni çəki,\n",
    "   - $w_{\\text{old}}$ — əvvəlki çəki,\n",
    "   - $\\eta$ — öyrənmə dərəcəsi (learning rate),\n",
    "   - $\\frac{\\partial L}{\\partial w}$ — itki funksiyasının çəkilərə görə törəməsi (gradienti).\n",
    "\n",
    "### Öyrənmə Dərəcəsi (Learning Rate)\n",
    "\n",
    "**Öyrənmə dərəcəsi** (\\(\\eta\\)) modelin çəkilərini yeniləyərkən addımın böyüklüyünü təyin edir. Çox kiçik öyrənmə dərəcəsi ilə modellər çox yavaş öyrənir və qlobal minimuma çatmaq uzun vaxt ala bilər. Çox böyük öyrənmə dərəcəsi isə modelin itki funksiyasının optimal nöqtəsindən kənara çıxmasına səbəb ola bilər. \n",
    "\n",
    "### Misal:\n",
    "Təsəvvür edin ki, modelin bir iterasiya sonunda çəkisi $w_{\\text{old}} = 0.5$ və gradient isə $\\frac{\\partial L}{\\partial w} = 0.1$-dir. Öyrənmə dərəcəsi isə $ \\eta = 0.01 $ olsun.\n",
    "\n",
    "Yeni çəki aşağıdakı kimi hesablanacaq:\n",
    "$$\n",
    "w_{\\text{new}} = 0.5 - 0.01 \\times 0.1 = 0.499\n",
    "$$\n",
    "Bu proses hər təlim iterasiyasında təkrarlanır və model itki funksiyasını azaldaraq daha doğru proqnozlar verməyə başlayır.\n",
    "\n",
    "### Neyron şəbəkəsinin çəkilərinin təyin edilməsi\n",
    "\n",
    "1. **İlkin çəkilərin təyin edilməsi**: Şəbəkənin çəkiləri təlimin əvvəlində ya təsadüfi olaraq, ya da müəyyən metodlarla təyin edilir. Məsələn, **Xavier Initialization** və **He Initialization** kimi üsullar daha optimal başlanğıc nöqtələri təmin etmək üçün istifadə edilir.\n",
    "   \n",
    "2. **Çəkilərin yenilənməsi**: Backpropagation və gradient descent ilə çəkilər hər iterasiya zamanı yenilənir və bu, modelin daha yaxşı işləməsi üçün itki funksiyasını minimallaşdırmağa kömək edir.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "**New weight**, neural network-lərdə modelin təlim prosesi zamanı çəkilərin yenilənmiş versiyasıdır. Bu yenilənmə, itki funksiyasını minimuma endirmək üçün gradient descent və backpropagation alqoritmləri vasitəsilə həyata keçirilir. Çəkilər düzgün öyrədildikdə, model girişə əsasən doğru nəticələr verə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e9ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "626e80b1",
   "metadata": {},
   "source": [
    "## new_weight = initial_weight − slope × learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e162ee",
   "metadata": {},
   "source": [
    "Bəli, bu ifadə çəkilərin yenilənməsi üçün əsas düstura uyğundur. Düzgün formulu aşağıdakı kimi yazaq:\n",
    "\n",
    "$$\n",
    "\\text{new_weight} = \\text{initial_weight} - \\text{slope} \\times \\text{learning_rate}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "\n",
    "- **initial_weight** — ilkin çəki dəyəri.\n",
    "- **slope** — gradient (itki funksiyasının çəkilərə görə törəməsi).\n",
    "- **learning_rate** — öyrənmə dərəcəsi (\\(\\eta\\)), hansı ki, hər iterasiya zamanı çəkilərin nə qədər dəyişdiriləcəyini təyin edir.\n",
    "\n",
    "Bu düstur **gradient descent** alqoritminin əsasıdır və hər iterasiyada şəbəkənin çəkilərini yeniləyərək modelin optimallaşdırılmasına kömək edir. \n",
    "\n",
    "Bir az daha açsaq:\n",
    "- **Slope (gradient)**, modelin itki funksiyasının meylini göstərir. Bu, itkinin azaldılması üçün hansı istiqamətdə hərəkət edilməli olduğunu göstərir.\n",
    "- **Learning rate**, təlimin sürətini idarə edir. Çox kiçik learning rate yavaş öyrənməyə, çox böyük learning rate isə qeyri-stabilliyə səbəb ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b77b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a809fec1",
   "metadata": {},
   "source": [
    "## Absolute Error (AE) , Mean Absolute Error (MAE) , Mean Square Error()MSE,  Root Mean Square Error(RMSE) ne vaxt hasni istifade olunur ve dusturlari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93f136",
   "metadata": {},
   "source": [
    "### 1. **Absolute Error (AE)**\n",
    "\n",
    "- **Formula**: \n",
    " $$\n",
    "  AE = |y - \\hat{y}|\n",
    "$$\n",
    "  - Burada:\n",
    "    - $ y $ — faktiki dəyər,\n",
    "    - $ \\hat{y} $ — proqnozlaşdırılmış dəyər.\n",
    "  \n",
    "- **Nə vaxt istifadə olunur**:\n",
    "  - Bir data nöqtəsinin real dəyəri ilə modelin proqnozu arasındakı fərqi qiymətləndirmək üçün istifadə olunur.\n",
    "  - Fərdi proqnoz səhvlərini görmək istədikdə.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mean Absolute Error (MAE)**\n",
    "\n",
    "- **Formula**:\n",
    " $$\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "  - Burada:\n",
    "    - $ y_i $ — faktiki dəyər,\n",
    "    - $ \\hat{y}_i $ — proqnozlaşdırılmış dəyər,\n",
    "    - $ n $ — nümunə sayı.\n",
    "  \n",
    "- **Nə vaxt istifadə olunur**:\n",
    "  - Proqnoz modelinin ümumi performansını qiymətləndirərkən.\n",
    "  - MAE **outlier-lərə həssas deyil**, ona görə də **sabit səhvlərin** orta dəyərini göstərir.\n",
    "  - Böyük səhvlərin proqnoz nəticələrinə çox təsir etməsini istəmədiyiniz hallarda istifadə olunur.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Mean Square Error (MSE)**\n",
    "\n",
    "- **Formula**:\n",
    "  $$\n",
    "  MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  \n",
    "- **Nə vaxt istifadə olunur**:\n",
    "  - **Outlier-lər proqnoz üçün əhəmiyyətli olduqda** və onların model nəticələrinə təsiri nəzərə alınmalıdırsa.\n",
    "  - MSE fərqləri kvadrat olaraq cəzalandırır, buna görə də daha dəqiq modelləri təhlil etmək və kiçik səhvləri də qiymətləndirmək üçün istifadə olunur.\n",
    "  - **Böyük səhvlərin** model performansına təsirini göstərmək üçün daha uyğundur.\n",
    "  \n",
    "- **Üstünlüklər**:\n",
    "  - Böyük səhvlərə qarşı həssasdır və böyük sapmaları daha çox cəzalandırır.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - **Outlier-lərə həddən artıq həssasdır** və çox böyük fərqlər MSE-ni şişirə bilər.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Root Mean Square Error (RMSE)**\n",
    "\n",
    "- **Formula**:\n",
    " $$\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "  \n",
    "- **Nə vaxt istifadə olunur**:\n",
    "  - **Outlier-lərin təsirini cəzalandırmaq** və ya kvadrat fərqlərdən çıxarılan ortalama ilə performans qiymətləndirmək üçün istifadə olunur.\n",
    "  - MSE kimi, RMSE də fərqləri kvadrat şəkildə qiymətləndirir, amma RMSE nəticəni faktiki dəyərə yaxınlaşdırmaq üçün kökünü alır.\n",
    "  - **Böyük səhvlər model performansına daha çox təsir edəndə** RMSE istifadə olunur.\n",
    "  \n",
    "- **Üstünlüklər**:\n",
    "  - Modelin səhvlərini daha həssas şəkildə cəzalandırır və böyük fərqlərə daha çox diqqət yetirir.\n",
    "  \n",
    "- **Çatışmazlıqlar**:\n",
    "  - **Outlier-lərə qarşı həssasdır**, böyük fərqlər RMSE-ni şişirdə bilər.\n",
    "\n",
    "---\n",
    "\n",
    "### **Nə vaxt hansı metrik istifadə olunur?**\n",
    "\n",
    "1. **AE (Absolute Error)**:\n",
    "   - Fərdi səhvləri qiymətləndirərkən.\n",
    "   \n",
    "2. **MAE (Mean Absolute Error)**:\n",
    "   - **Outlier-lərə həssas olmayan** və sabit bir səhv təhlili aparmaq istədikdə.\n",
    "   \n",
    "3. **MSE (Mean Square Error)**:\n",
    "   - **Outlier-lər əhəmiyyətli olduqda** və böyük səhvlərin təsirini daha çox qiymətləndirmək istədikdə.\n",
    "   \n",
    "4. **RMSE (Root Mean Square Error)**:\n",
    "   - **Outlier-lərin təsirini** daha ciddi cəzalandırmaq və böyük səhvlərə diqqət yetirmək istədikdə."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17792d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3115583",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb8693a",
   "metadata": {},
   "source": [
    "**Gradient Descent** optimallaşdırma alqoritmidir və əsasən maşın öyrənməsində və neyron şəbəkələrin öyrədilməsində istifadə olunur. Məqsəd modelin itki funksiyasını minimallaşdırmaq, yəni proqnoz səhvlərini mümkün qədər azaltmaqdır. \n",
    "\n",
    "### Əsas məntiq:\n",
    "Gradient Descent, modelin parametrlərini tədricən yeniləyərək itki funksiyasının ən aşağı nöqtəsini tapmağa çalışır. Bu proses parametrləri düzgün istiqamətə \"azaldaraq\" (gradient istiqamətində hərəkət) həyata keçirilir.\n",
    "\n",
    "### Təsəvvür etmək üçün:\n",
    "- **İtki funksiyası** (cost function): Bu, modelin performansını ölçmək üçün istifadə edilən riyazi bir funksiyadır. Məqsəd, modelin proqnozları ilə real dəyərlər arasındakı fərqi minimuma endirməkdir.\n",
    "- **Gradient**: İtki funksiyasının istiqaməti və sürəti haqqında məlumat verən törəmədir. Gradientin istiqaməti hansı tərəfə hərəkət etmək lazım olduğunu, böyüklüyü isə addımın nə qədər böyük olması lazım olduğunu göstərir.\n",
    "\n",
    "### Necə işləyir?\n",
    "\n",
    "1. **Parametrlərin ilkin dəyərlərinin seçilməsi**: Alqoritm ilkin təxminlərlə başlayır (məsələn, modelin çəkiləri və ya əmsalları).\n",
    "  \n",
    "2. **Gradientin hesablanması**: Gradient, itki funksiyasının hazırkı parametr dəyərləri ilə törəməsini (meşələnməsini) hesablayır. Gradientin dəyəri, dəyişənlərin hansı istiqamətə doğru hərəkət etməli olduğunu göstərir (bu, müsbət və ya mənfi ola bilər).\n",
    "\n",
    "3. **Parametrlərin yenilənməsi**:\n",
    "   $$\n",
    "   \\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)\n",
    "   $$\n",
    "   - Burada:\n",
    "     - $ \\theta $ — modelin parametrləri (məsələn, neyron şəbəkədə çəkilər),\n",
    "     - $ \\alpha $ — öyrənmə sürəti (learning rate), addımın nə qədər böyük olduğunu təyin edir,\n",
    "     - $ \\nabla J(\\theta) $ — itki funksiyasının gradienti.\n",
    "\n",
    "4. **Təkrarlama**: Alqoritm bu prosesi təkrarlayır və hər dəfə parametrləri yeniləyir. Əsas məqsəd itki funksiyasının minimum nöqtəsinə çatmaqdır.\n",
    "\n",
    "5. **Nəticə**: İtki funksiyasının minimum nöqtəsinə çatıldıqda, optimal parametrlər tapılmış olur.\n",
    "\n",
    "### Gradient Descent növləri:\n",
    "\n",
    "1. **Batch Gradient Descent**:\n",
    "   - Bütün training datası üzərində bir dəfəyə gradient hesablanır.\n",
    "   - Dəqiq, amma böyük datası olan sistemlər üçün yavaş ola bilər.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Hər bir nümunə üçün gradient hesablanır və parametrlər hər dəfə bir nümunəyə əsasən yenilənir.\n",
    "   - Daha sürətlidir, amma həmişə optimallaşdırmanın doğru istiqamətini göstərməyə bilər (sıçrayışlar olur).\n",
    "\n",
    "3. **Mini-batch Gradient Descent**:\n",
    "   - Gradient hesablaması üçün datasetin kiçik hissələri (mini-batch) istifadə olunur.\n",
    "   - Batch və SGD arasında balans yaradır, həm sürətli, həm də daha sabit optimallaşdırma təmin edir.\n",
    "\n",
    "### Gradient Descent'in üstünlükləri:\n",
    "- Böyük və mürəkkəb modellərin optimallaşdırılmasında geniş istifadə olunur.\n",
    "- Müxtəlif növ neyron şəbəkələrin və maşın öyrənməsi modellərinin öyrədilməsi üçün effektivdir.\n",
    "\n",
    "### Gradient Descent'in çətinlikləri:\n",
    "- **Öyrənmə sürəti** (\\( \\alpha \\)): Çox böyük seçilərsə, alqoritm həddindən artıq böyük addımlar ata və optimal həllə çatmaya bilər. Çox kiçik seçilərsə, çox vaxt alar və optimallaşdırma yavaş olar.\n",
    "- **Yerə baxma nöqtəsi (Local Minima)**: Alqoritm hər zaman qlobal minimuma çatmaya bilər, yerli minimumda ilişib qala bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4266f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9aace49",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d20ca2",
   "metadata": {},
   "source": [
    "## Trend, Seasonality, Stationary, Non-Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c8bd",
   "metadata": {},
   "source": [
    "Trend və seasonality, vaxt seriyalarının iki əsas komponentidir. Stasionar və qeyri-stasionar vaxt seriyaları arasında fərq onların statistiki xüsusiyyətlərində ortaya çıxır. Gəlin bu anlayışları daha ətraflı izah edək:\n",
    "\n",
    "### 1. **Trend**:\n",
    "   - **Trend** vaxt seriyasının ümumi istiqamətini göstərir, yəni uzun müddət ərzində artım və ya azalma meyilləri varsa, bu trenddir. Məsələn, əgər bir şirkətin gəlirləri illər ərzində davamlı olaraq artırsa, bu, müsbət bir trenddir.\n",
    "   \n",
    "### 2. **Seasonality (müəyyən dövrlər üzrə təkrarlanma)**:\n",
    "   - **Seasonality** (fəsillik) müəyyən dövrlərdə təkrarlanan nümunələri nəzərdə tutur. Məsələn, bir mağaza bayramlarda daha çox satış edirsə, bu, fəsililikdir. Bu təkrarlanmalar illik, aylıq, hətta həftəlik ola bilər və vaxt seriyasının əsas xüsusiyyətlərindən biridir.\n",
    "\n",
    "### 3. **Stasionar və qeyri-stasionar vaxt seriyaları**:\n",
    "   **Stasionar** və **qeyri-stasionar** vaxt seriyaları arasında fərqlər belədir:\n",
    "   \n",
    "   #### a) **Stasionar vaxt seriyası**:\n",
    "   - **Orta** və **varians** (dispersiya) zamanla sabitdir.\n",
    "   - Məhsulun statistik xüsusiyyətləri (məsələn, ortalama və dispersiya) dəyişmir.\n",
    "   - Stasionar vaxt seriyasında uzunmüddətli trend və ya seasonality yoxdur.\n",
    "   - Məsələn, qiymətlərin zamanla təsadüfi şəkildə ətrafında dəyişdiyi bir aktivin qiyməti stasionar bir seriya ola bilər.\n",
    "   \n",
    "   #### b) **Qeyri-stasionar vaxt seriyası**:\n",
    "   - **Trend** və ya **seasonality** kimi komponentlər mövcuddur.\n",
    "   - Orta və dispersiya zamanla dəyişir.\n",
    "   - Uzunmüddətli olaraq artım, azalma və ya dövri təkrarlanmalar (seasonality) qeyri-stasionar seriyalara aiddir.\n",
    "   - Məsələn, ev qiymətlərinin illər ərzində artması qeyri-stasionar bir seriyadır, çünki burada bir artım trendi var.\n",
    "\n",
    "### 4. **Trend, seasonality və stasionarlıq arasındakı fərq**:\n",
    "   - **Trend** və **seasonality** qeyri-stasionarlığın səbəbləridir. Əgər bir vaxt seriyasında trend və ya seasonality mövcuddursa, o, qeyri-stasionardır.\n",
    "   - **Stasionar vaxt seriyasında** isə trend və seasonality komponentləri yoxdur. Əgər vaxt seriyası stasionar deyilsə, onu stasionar etmək üçün trend və seasonality aradan qaldırılmalıdır.\n",
    "\n",
    "**Nəticə:** Trend və seasonality varsa, vaxt seriyası çox güman ki, qeyri-stasionardır. Stasionarlıq isə statistik olaraq sabit olan seriyaları ifadə edir, yəni seriyanın uzun müddətli trend və fəsililik təsirinə məruz qalmadığını göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42506e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2468d3d7",
   "metadata": {},
   "source": [
    "## Window Horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07a2c4",
   "metadata": {},
   "source": [
    "**Time Series-də \"Window Horizon\" nədir?**\n",
    "\n",
    "\"Window Horizon\" zaman sıralarında verilənlərin müəyyən bir zaman aralığında necə analiz edildiyini göstərir. Xüsusilə zaman sırası proqnozlaşdırılmasında, keçmiş müşahidələrə əsaslanaraq gələcək dəyərləri təxmin etmək üçün istifadə edilir.\n",
    "\n",
    "### Window Horizon növləri:\n",
    "1. **Sabit Horizon**: Bu yanaşma sabit sayda keçmiş müşahidələri təhlil edir. Məsələn, əgər siz həmişə son 10 günün məlumatlarını istifadə edirsinizsə, bu sabit horizondur.\n",
    "  \n",
    "2. **Genişlənən Horizon**: Burada zaman irəlilədikcə istifadə edilən müşahidələrin sayı artır. Başlanğıcda, 10 dəyərdən istifadə edilir, daha sonra bu say 11-ə, 12-yə çatır.\n",
    "\n",
    "3. **Rolling (Sürüşən) Pəncərə**: Sabit ölçülü bir pəncərə zaman seriyası üzərində hərəkət edir, hər yeni müşahidə ilə pəncərə bir addım irəli gedir, ən köhnə müşahidə atılır və ən yenisi əlavə olunur.\n",
    "\n",
    "**Window Horizon** balans yaratmağa kömək edir: çox keçmiş məlumatdan istifadə edildikdə köhnə məlumatlar çox ola bilər, az məlumat istifadə edildikdə isə vacib trendlər itə bilər.\n",
    "\n",
    "---\n",
    "\n",
    "### Məsələn kod nümunəsi:\n",
    "\n",
    "Aşağıdakı kodda biz sürüşən pəncərə yanaşmasından istifadə edərək zaman sırasının növbəti zaman nöqtəsini proqnozlaşdırmaq üçün \"Linear Regression\" modelindən istifadə edirik.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Sintetik zaman sırası məlumatları yaradırıq\n",
    "# (KOD ŞƏRHİ: Trend və səs-küy ilə sintetik zaman seriyası məlumatları yaradılır)\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "time = np.arange(n)\n",
    "data = 0.5 * time + np.sin(time) + np.random.normal(scale=0.5, size=n)\n",
    "\n",
    "# 2. Pəncərə horizontunu təyin edirik (sürüşən pəncərə)\n",
    "# (KOD ŞƏRHİ: Keçmiş müşahidələr üçün ölçüsü 10 olan bir sürüşən pəncərə təyin edilir)\n",
    "window_size = 10\n",
    "X = np.array([data[i:i + window_size] for i in range(len(data) - window_size)])\n",
    "y = np.array([data[i + window_size] for i in range(len(data) - window_size)])\n",
    "\n",
    "# 3. Linear Regression modelini öyrədirik\n",
    "# (KOD ŞƏRHİ: Zaman sırası proqnozlaşdırılması üçün \"Linear Regression\" modelini təyin edirik)\n",
    "model = LinearRegression()\n",
    "\n",
    "# (KOD ŞƏRHİ: Sürüşən pəncərədən keçmiş məlumatlarla modeli öyrədirik)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 4. Təlim edilmiş modeldən istifadə edərək proqnozlar hazırlayırıq\n",
    "# (KOD ŞƏRHİ: Sürüşən pəncərə istifadə edilərək növbəti zaman nöqtəsini təxmin edirik)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# 5. Xətanı hesablayır və nəticələri vizuallaşdırırıq\n",
    "# (KOD ŞƏRHİ: Modelin proqnozlarının orta kvadrat xətasını (MSE) hesablayırıq)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(f\"Orta Kvadrat Xəta (MSE): {mse:.3f}\")\n",
    "\n",
    "# (KOD ŞƏRHİ: Zaman seriyasının həqiqi və proqnozlaşdırılmış dəyərlərini cızırıq)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time, data, label=\"Həqiqi Məlumat\")\n",
    "plt.plot(time[window_size:], predictions, label=\"Proqnozlar\", color='orange')\n",
    "plt.title('Window Horizon ilə Proqnozlaşdırma')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Koda Aydınlıq Gətirək:\n",
    "1. **Sintetik Məlumat Yaradılması**: Kod, xətti trend və səs-küy ilə sintetik zaman seriyası yaradır.\n",
    "2. **Pəncərə Horizon**: Keçmişdəki 10 müşahidədən istifadə edərək proqnozlaşdırma edirik.\n",
    "3. **Linear Regression**: Keçmiş müşahidələr və gələcək dəyər arasındakı əlaqəni xətti model vasitəsilə qururuq.\n",
    "4. **Proqnozlaşdırma**: Model təlim edilir və keçmiş pəncərədən istifadə edərək gələcək dəyərləri təxmin edirik.\n",
    "5. **Vizuallaşdırma**: Həqiqi məlumatları və proqnozları müqayisə etmək üçün grafiklər çəkirik.\n",
    "\n",
    "Bu metod ARIMA və ya LSTM kimi daha mürəkkəb modellərlə genişləndirilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842107eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0c776d",
   "metadata": {},
   "source": [
    "## model = sm.tsa.arima.ARIMA(train, order=(3, 2, 1))\n",
    "- ## p, d, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52b77e",
   "metadata": {},
   "source": [
    "Bu sətirdə ARIMA modeli qurularkən `order=(3, 2, 1)` parametri ARIMA modelinin tənzimləmələrini təyin edir. Bu parametrlər ARIMA modelinin üç əsas komponentini göstərir:\n",
    "\n",
    "1. **p (Autoregressive part)** - Burada **p=3**:\n",
    "   - **p** modeli qurmaq üçün istifadə olunan **avtoreqressiv (AR)** komponentin dərəcəsini göstərir. Bu, əvvəlki müşahidələrin modeldə neçə keçmiş dəyərinə əsaslandığını təyin edir.\n",
    "   - **p=3**: Model keçmişin 3 dəyərinə (lag-3) qədər nəzərə alaraq indiki dəyəri proqnozlaşdırır. Məsələn, $X_t$ dəyəri keçmişdəki $X_{t-1}$, $X_{t-2}$ və $X_{t-3}$ dəyərləri əsasında proqnozlaşdırıla bilər.\n",
    "\n",
    "2. **d (Differencing part)** - Burada **d=2**:\n",
    "   - **d** modeli qurarkən seriyanı **stasionar** etmək üçün neçə dəfə differencing (fərq almanın) tətbiq edildiyini göstərir.\n",
    "   - **d=2**: Model, verilən zaman seriyasındakı qeyri-stasionarlığı aradan qaldırmaq üçün 2 dəfə fərq alır. Bu, seriyadakı trendi və mövsümi təsirləri çıxarmaq üçündür.\n",
    "\n",
    "3. **q (Moving Average part)** - Burada **q=1**:\n",
    "   - **q** modelin **hərəkət edən orta (MA)** hissəsinin dərəcəsini göstərir. Bu, modelin proqnozlaşdırmada keçmişdəki qalıqlardan neçə lag (gecikmə) əsasında istifadə etdiyini təyin edir.\n",
    "   - **q=1**: Model keçmişdəki qalıqlardan (error terms) birinə qədər (lag-1) nəzərə alır. Yəni model əvvəlki qalıq (error) dəyərlərini indiki proqnoza daxil edir.\n",
    "\n",
    "### Qısa izah:\n",
    "- **p=3**: Model keçmiş 3 dəyərə (lag-3) qədər istifadə edir.\n",
    "- **d=2**: Model 2 dəfə differencing (fərq alma) tətbiq edir ki, seriya stasionar olsun.\n",
    "- **q=1**: Model keçmiş qalıqların 1 lag-lığını (hərəkət edən orta hissəsi) nəzərə alır.\n",
    "\n",
    "Bu parametrlər ARIMA modelini müəyyən edir və onu zaman seriyasına uyğunlaşdırır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce53c75",
   "metadata": {},
   "source": [
    "**q = 1**, ARIMA modelinin **hərəkət edən orta (Moving Average, MA)** hissəsinin dərəcəsini təyin edir. Bu, keçmiş qalıqların (error terms) modelə daxil edilməsini göstərir.\n",
    "\n",
    "İndi bunu daha detallı izah edək:\n",
    "\n",
    "### **Moving Average (MA) modeli nədir?**\n",
    "\n",
    "MA modeli zaman seriyasındakı **qalıqların** proqnoza təsirini təhlil edir. Proqnozlaşdırdığımız dəyər əvvəlki qalıqlara (keçmiş error-lara) əsaslanır. Bu qalıqlar, proqnozlaşdırdığımız qiymət ilə faktiki qiymət arasındakı fərqlərdir.\n",
    "\n",
    "- **q** parametri, **neçə keçmiş qalıq** (error) dəyərini nəzərə alacağımızı göstərir.\n",
    "\n",
    "### **q = 1 nə deməkdir?**\n",
    "\n",
    "Bu o deməkdir ki, model sonuncu müşahidədəki qalıqları (lag-1 error) nəzərə alır. Yəni proqnozlaşdırdığımız dəyər keçmişdəki **yalnız bir qalıq dəyəri** ilə düzəldilir.\n",
    "\n",
    "**Misal**:\n",
    "- **q = 1** ilə, model sonuncu müşahidədəki qalıq dəyərini götürür və yeni proqnoza təsir edir. Yəni proqnozunuzu düzəldərkən sonuncu dəfə nə qədər səhv etdiyinizi nəzərə alırsınız. Əgər əvvəlki müşahidədə proqnozunuz çox aşağı idisə, bu dəfə daha yüksək bir qiymət proqnozlaşdırırsınız.\n",
    "\n",
    "### Formula baxımından:\n",
    "Proqnoz $ \\hat{X}_t $, keçmiş müşahidənin qalıq dəyəri $ \\epsilon_{t-1} $ ilə düzəldilir:\n",
    "$$\n",
    "\\hat{X}_t = \\mu + \\theta_1 \\epsilon_{t-1}\n",
    "$$\n",
    "Burada:\n",
    "- $ \\mu $ sabitdir,\n",
    "- $ \\theta_1 $ qalıqların çəkisini təyin edir (model tərəfindən öyrədilir),\n",
    "- $ \\epsilon_{t-1} $ keçmiş qalıqdır (keçmişdəki proqnoz və faktiki dəyər arasındakı fərq).\n",
    "\n",
    "### **Nəticə**:\n",
    "- **q = 1** model son 1 qalıq (error) dəyərini nəzərə alır.\n",
    "- **q nə qədər böyükdürsə**, model o qədər çox keçmiş qalıq dəyərini nəzərə alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6843367e",
   "metadata": {},
   "source": [
    "## p, d, q -ni  neye esasen teyin edim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f76c8",
   "metadata": {},
   "source": [
    "ACF (Autocorrelation Function) və PACF (Partial Autocorrelation Function) qrafiklərini təhlil edərək ARIMA modelinin `p`, `d`, və `q` parametrlərini təyin etmək vacibdir. Gəlin hər birinin necə təyin edildiyini addım-addım izah edək:\n",
    "\n",
    "### ARIMA Parametrləri:\n",
    "1. **p** — AR (Autoregressive) dərəcəsi: Zaman seriyasının öz keçmiş dəyərləri ilə nə qədər əlaqəli olduğunu təyin edir. Bu, **PACF** qrafikinə əsaslanaraq seçilir.\n",
    "2. **d** — Differencing dərəcəsi: Zaman seriyasını stasionar hala gətirmək üçün nə qədər differencing tətbiq olunmalı olduğunu göstərir. **ADF testi** və vizual olaraq zaman seriyasının trendi əsasındadır.\n",
    "3. **q** — MA (Moving Average) dərəcəsi: Zaman seriyasının qalıqları (errors) ilə əlaqəni təyin edir. Bu, **ACF** qrafikinə əsaslanaraq seçilir.\n",
    "\n",
    "### Addım-addım ACF və PACF təhlili\n",
    "\n",
    "#### 1. **Differencing dərəcəsi (d)** — Zaman seriyasını stasionar etmək:\n",
    "Əvvəlcə seriyanızın stasionar olub-olmadığını yoxlayın:\n",
    "- Əgər ADF testinin p-value dəyəri 0.05-dən böyükdürsə, zaman seriyası stasionar deyil.\n",
    "- **Seriya stasionar deyilsə**, fərqləndirmə (differencing) tətbiq etməliyik.\n",
    "- `d` dəyərini təyin etmək üçün fərqləndirməni birinci, ikinci, və ya daha çox dəfə tətbiq edə bilərsiniz. Əksər hallarda **`d = 1`** kifayətdir.\n",
    "\n",
    "#### 2. **Autoregressive dərəcəsi (p)** — PACF qrafikinə əsaslanaraq:\n",
    "- **PACF qrafiki**: Bu qrafikdə x oxu lags-i (gecikmələri), y oxu isə hər bir gecikmənin qismən korrelyasiyasını göstərir.\n",
    "  - **p dəyəri** PACF qrafikindəki kəskin \"cut-off\" nöqtəsinə (ilk kəskin azalma) əsaslanır.\n",
    "  - Misal üçün, əgər PACF qrafikində `lag 1` və `lag 2` əhəmiyyətli dərəcədə yüksək, lakin `lag 3` və sonrakı lags sıfıra yaxındırsa, **p = 2** kimi götürülə bilər.\n",
    "\n",
    "#### 3. **Moving Average dərəcəsi (q)** — ACF qrafikinə əsaslanaraq:\n",
    "- **ACF qrafiki**: Bu qrafik hər bir gecikmənin ümumi korrelyasiyasını göstərir.\n",
    "  - **q dəyəri** ACF qrafikindəki kəskin \"cut-off\" nöqtəsinə əsaslanır. ACF qrafiki bir neçə lag üçün əhəmiyyətli korrelyasiya göstərir, sonra kəskin azalırsa, **q** həmin nöqtədəki lag-a uyğun təyin olunur.\n",
    "  - Misal üçün, əgər ACF qrafikində `lag 1` və `lag 2` əhəmiyyətli korrelyasiyaya malikdirsə, lakin `lag 3`-dən sonra sıfıra yaxındırsa, **q = 2** götürülə bilər.\n",
    "\n",
    "### Praktik Misal:\n",
    "\n",
    "Tutaq ki, zaman seriyanızda ADF testi göstərir ki, `d = 1` fərqləndirmə lazımdır. Sonra ACF və PACF qrafiklərinə baxaraq bu nəticələri əldə edirsiniz:\n",
    "\n",
    "- **PACF** qrafikində, `lag 1` və `lag 2` yüksəkdir, lakin `lag 3` çox aşağıdır. Buna görə **p = 2**.\n",
    "- **ACF** qrafikində isə `lag 1` və `lag 2` əhəmiyyətli korrelyasiya göstərir, lakin `lag 3`-dən sonra dəyərlər sıfıra yaxın olur. Buna görə **q = 2**.\n",
    "\n",
    "Bu halda, ARIMA modelinizin təxmini parametrləri belə olacaq:\n",
    "```python\n",
    "model = ARIMA(df['value'], order=(2, 1, 2))  # p = 2, d = 1, q = 2\n",
    "```\n",
    "\n",
    "### Qrafiklərin Təhlili:\n",
    "\n",
    "#### PACF Qrafiki:\n",
    "- Kəskin cut-off varsa, bu AR (p) modelini göstərir.\n",
    "- **Misal**: İlk 2 lags yüksək korrelyasiya göstərir, lakin 3-cü lag sıfıra yaxındırsa, p = 2 götürülür.\n",
    "\n",
    "#### ACF Qrafiki:\n",
    "- Kəskin cut-off varsa, bu MA (q) modelini göstərir.\n",
    "- **Misal**: İlk 2 lags yüksək korrelyasiya göstərir, sonra isə azalma müşahidə olunursa, q = 2 götürülür.\n",
    "\n",
    "Əgər cut-off yoxdursa, bu ARIMA modelinin düzgün parametrlərinin seçilməsi üçün fərqli dəyərləri sınamağa ehtiyac ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5efe88d",
   "metadata": {},
   "source": [
    "## Confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f722c",
   "metadata": {},
   "source": [
    "**Confidence interval** (etibarlılıq intervalı) statistik təhlillərdə bir nəticənin müəyyən bir dərəcədə dəqiqliklə harada yerləşəcəyini göstərən bir intervaldır. Bu, nəticələrin müəyyən bir ehtimalla həmin interval daxilində olacağını ifadə edir.\n",
    "\n",
    "### Etibarlılıq intervalı nəyi ifadə edir?\n",
    "Bir nümunədən əldə edilən nəticənin (məsələn, ortalama, proqnoz dəyəri) ətrafında müəyyən bir interval verilir ki, bu intervalda həmin nəticənin müəyyən bir ehtimalla (adətən 95% və ya 99%) yerləşəcəyini deyə bilərik.\n",
    "\n",
    "Məsələn, bir modelin proqnozu 50-dirsə və etibarlılıq intervalı [45, 55] arasındadırsa, bu o deməkdir ki, 95% ehtimalla real dəyər həmin aralıqda olacaq.\n",
    "\n",
    "### Əsas terminlər:\n",
    "1. **Point estimate (nöqtə qiymətləndirməsi)**: Hədəf dəyərin ən yaxşı təxminidir (məsələn, proqnoz dəyəri).\n",
    "2. **Confidence level (etibarlılıq səviyyəsi)**: Etibarlılıq intervalının doğru olacağı ehtimalı, adətən 95% və ya 99% istifadə olunur. Yəni, 95% etibarlılıq intervalı varsa, hər 100 təhlildən 95-də real dəyər həmin intervalda olacaq.\n",
    "3. **Alpha (\\(\\alpha\\))**: Etibarlılıq səviyyəsinin tamamlayıcısıdır. Məsələn, 95% etibarlılıq üçün \\(\\alpha = 1 - 0.95 = 0.05\\).\n",
    "\n",
    "### Bir neçə nümunə:\n",
    "- **95% etibarlılıq intervalı**: Real dəyərin 95% ehtimalla həmin intervalda olduğunu ifadə edir.\n",
    "- **99% etibarlılıq intervalı**: Daha geniş interval olacaq və real dəyərin 99% ehtimalla həmin intervalda olduğunu bildirir.\n",
    "\n",
    "### Məsələn, ARIMA modelində:\n",
    "Proqnozlaşdırma zamanı `alpha=0.05` olaraq istifadə edildikdə, bu **95% etibarlılıq intervalı** yaradır. Bu o deməkdir ki, model 7 dəyəri proqnozlaşdırarkən hər dəyər üçün 95% ehtimalla real dəyər həmin interval daxilində olacaq.\n",
    "\n",
    "### Qısa izah:\n",
    "- Etibarlılıq intervalı, modelin nəticələrinin və ya statistik nəticələrin hansı aralıqda yerləşəcəyini müəyyən dərəcədə dəqiqliklə ifadə edir.\n",
    "- **95% etibarlılıq intervalı** deyir ki, 100 analizdən 95-də real nəticə bu aralıqda olacaq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa5eaba",
   "metadata": {},
   "source": [
    "### Etibarlılıq intervalının (confidence interval) formulu ümumi halda belədir:\n",
    "\n",
    "$$\n",
    "CI = \\hat{x} \\pm Z_{\\alpha/2} \\cdot \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "\n",
    "- **CI**: Etibarlılıq intervalı (Confidence Interval).\n",
    "- **$\\hat{x}$**: Nümunənin ortalaması (point estimate).\n",
    "- **$Z_{\\alpha/2}$**: Standart normal paylanmada $ \\alpha/2 $ səviyyəsi üçün z-skoru. Məsələn, 95% etibarlılıq üçün \n",
    "$ \\alpha = 0.05 $ olduğunda, $ Z_{\\alpha/2} \\approx 1.96 $ olur.\n",
    "- **$\\sigma$**: Nümunənin standart sapması.\n",
    "- **n**: Nümunənin ölçüsü.\n",
    "\n",
    "### Açıqlama:\n",
    "1. **$\\hat{x}$**: Nümunənin ortalamasıdır, yəni siz bir müşahidələr toplusundan bir nəticə əldə edirsiniz və bu ortalama sizin proqnoz və ya nəticənizi göstərir.\n",
    "2. **Z**: Etibarlılıq səviyyəsinə əsasən təyin olunan z-skorudur. Bu, normal paylanma əyrisinin altında qalan sahəni müəyyən edir. Məsələn:\n",
    "   - 95% etibarlılıq səviyyəsi üçün **Z ≈ 1.96**,\n",
    "   - 99% etibarlılıq səviyyəsi üçün **Z ≈ 2.576**.\n",
    "3. **$\\sigma/\\sqrt{n}$**: Standart sapmanın nümunə ölçüsünə bölünməsi ilə əldə olunan **standart error** adlanan dəyərdir. Nümunə nə qədər böyükdürsə, standart error bir o qədər kiçik olacaq və etibarlılıq intervalı daha dar olacaq.\n",
    "\n",
    "### Məsələn, 95% etibarlılıq intervalı:\n",
    "Əgər nümunə ortalaması $\\hat{x} = 50$, standart sapma $\\sigma = 10$, nümunə ölçüsü $n = 100$ olsa və 95% etibarlılıq səviyyəsi üçün Z ≈ 1.96 götürsək:\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 1.96 \\cdot \\left(\\frac{10}{\\sqrt{100}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 1.96 \\cdot (1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 1.96\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = [48.04, 51.96]\n",
    "$$\n",
    "\n",
    "Bu nəticə bizə deyir ki, real dəyər 95% ehtimalla 48.04 ilə 51.96 aralığında olacaq.\n",
    "\n",
    "### Qeydlər:\n",
    "- **Nümunənin ölçüsü (n)** böyüdükcə etibarlılıq intervalı daralır, çünki daha böyük nümunə dəqiqliyi artırır.\n",
    "- **Standart sapma ($\\sigma$)** böyük olarsa, etibarlılıq intervalı da geniş olacaq, çünki məlumatlar daha dəyişkəndir.\n",
    "\n",
    "Bu formul verilən müşahidələrdəki orta dəyəri və onun nə qədər etibarlı olduğunu təxmin etmək üçün istifadə olunur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a07fbd",
   "metadata": {},
   "source": [
    "\n",
    "## t paylanması ilə Confidence interval\n",
    "\n",
    "**t paylanması** kiçik nümunələr üçün və ya standart sapma məlum olmadıqda istifadə edilir. Etibarlılıq intervalı qurularkən **z-skoru** əvəzinə **t-skoru** istifadə olunur. t paylanması, normal paylanmaya çox yaxındır, amma kiçik nümunələrdə (n < 30) daha geniş quyruqlara malikdir, bu da daha geniş etibarlılıq intervalı ilə nəticələnir.\n",
    "\n",
    "### t paylanması ilə etibarlılıq intervalının formulu:\n",
    "\n",
    "$$\n",
    "CI = \\hat{x} \\pm t_{\\alpha/2, n-1} \\cdot \\left(\\frac{s}{\\sqrt{n}}\\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "\n",
    "- **CI**: Etibarlılıq intervalı (Confidence Interval).\n",
    "- **$\\hat{x}$**: Nümunənin ortalaması (point estimate).\n",
    "- **$t_{\\alpha/2, n-1}$**: t paylanmasından alınan t-skoru, müəyyən edilmiş etibarlılıq səviyyəsi üçün istifadə olunur. $ n-1 $ azadlıq dərəcəsi ilə təyin olunur.\n",
    "- **$s$**: Nümunənin standart sapması.\n",
    "- **$n$**: Nümunənin ölçüsü.\n",
    "\n",
    "### Açıqlama:\n",
    "\n",
    "1. **$t_{\\alpha/2, n-1}$**: t paylanmasından alınan kritik t-skorudur. Bu dəyər nümunənin ölçüsünə və etibarlılıq səviyyəsinə əsaslanır. t paylanmasında z-skoru ilə müqayisədə dəyərlər daha böyük olur, çünki kiçik nümunə ölçüsündə qeyri-müəyyənlik daha çoxdur.\n",
    "2. **$s/\\sqrt{n}$**: Standart error, nümunənin standart sapmasının nümunə ölçüsünün kvadrat kökünə bölünməsidir.\n",
    "3. **n-1**: Azadlıq dərəcəsidir. Bu, kiçik nümunələr üçün t paylanmasının qeyri-müəyyənliyini artırmaq üçün istifadə olunur.\n",
    "\n",
    "### Misal:\n",
    "Əgər nümunənin ortalaması $\\hat{x} = 50$, nümunənin standart sapması $s = 10$, nümunə ölçüsü $n = 15$ olsa və 95% etibarlılıq səviyyəsi üçün t-skoru (t(14, 0.025)) ≈ 2.145 götürsək:\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 2.145 \\cdot \\left(\\frac{10}{\\sqrt{15}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 2.145 \\cdot 2.582\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = 50 \\pm 5.54\n",
    "$$\n",
    "\n",
    "$$\n",
    "CI = [44.46, 55.54]\n",
    "$$\n",
    "\n",
    "Bu o deməkdir ki, 95% ehtimalla real dəyər 44.46 ilə 55.54 aralığında yerləşəcək.\n",
    "\n",
    "### t paylanmasının istifadəsi:\n",
    "- Kiçik nümunələrdə (n < 30) və ya əhali standart sapması bilinmədikdə.\n",
    "- **Azadlıq dərəcəsi (df = n-1)** artdıqca, t paylanması z paylanmasına daha yaxınlaşır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c3afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbfe4a59",
   "metadata": {},
   "source": [
    "## Standart Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e972c",
   "metadata": {},
   "source": [
    "Nümunə sayına bölməyin səbəbi, statistik analizlərdə **standart error** (SE) adlı bir anlayışdan istifadə edilməsidir. Standart error, bir nümunənin standart sapmasının nümunə ölçüsünə əsaslanaraq tənzimlənmiş dəyəridir. Bu, nümunədən çıxarılan məlumatların nə qədər dəqiq olduğunu və əhali parametri haqqında nə qədər inamlı olduğumuzu göstərir.\n",
    "\n",
    "### Detallı izah:\n",
    "1. **Standart sapma ($\\sigma$)**:\n",
    "   Standart sapma, nümunə daxilində fərdi müşahidələrin ortalamadan nə qədər uzaqlaşdığını göstərir. Lakin, bir nümunənin standart sapması, nümunənin əhali parametrlərinə nə qədər dəqiq yaxın olduğunu tam göstərmir, çünki nümunə ölçüsü nəzərə alınmalıdır.\n",
    "\n",
    "2. **Standart error (SE)**:\n",
    "   Standart error, nümunənin standart sapmasının nümunə ölçüsünün kvadrat kökünə bölünməsi ilə hesablanır:\n",
    "\n",
    "   $$\n",
    "   SE = \\frac{\\sigma}{\\sqrt{n}}\n",
    "   $$\n",
    "\n",
    "   - **$n$**: Nümunənin ölçüsüdür.\n",
    "   - **$\\sigma$**: Standart sapmadır.\n",
    "\n",
    "   Standart error nümunə ölçüsü artdıqca azalır, yəni daha böyük nümunələrdən əldə olunan nəticələr daha dəqiq olur.\n",
    "\n",
    "### Niyə nümunə ölçüsünə bölürük?\n",
    "1. **Nümunə ölçüsü böyüdükcə dəqiqlik artır**: Daha çox müşahidə olunduqda, ortalamaya daha yaxın dəyərlər əldə edirik və nəticədə ölçmələrin dəqiqliyi artır. Buna görə də, nümunə ölçüsü artdıqca standart error kiçilir və etibarlılıq intervalı daralır.\n",
    "\n",
    "2. **Kiçik nümunələrdə qeyri-müəyyənlik daha çoxdur**: Kiçik nümunə ölçüsündə daha az məlumat var və bu, ortalama ətrafında daha çox dəyişkənlik yaradır. Standart error bu qeyri-müəyyənliyi nəzərə alaraq, nümunənin nə qədər dəyişkən olduğunu göstərir.\n",
    "\n",
    "3. **Nümunənin təmsilçilik qabiliyyəti**: Daha böyük nümunələr əhalini daha yaxşı təmsil edir, çünki nümunənin təsadüfi sapmaları daha kiçik olur. Standart error bu təsadüfi sapmaların təxmini dərəcəsini göstərir.\n",
    "\n",
    "### Misal:\n",
    "Əgər 100 dəyərdən ibarət bir nümunəniz varsa və onun standart sapması \\( \\sigma = 15 \\) olsa:\n",
    "\n",
    "- **Standart sapma**: 15. Bu, dəyərlərin orta dəyərdən orta sapmasını göstərir.\n",
    "- **Standart error**: $ SE = \\frac{15}{\\sqrt{100}} = \\frac{15}{10} = 1.5 $.\n",
    "\n",
    "Burada standart error 1.5 olduğu üçün ortalamanın əhali ortalamasından çox uzaq olmama ehtimalı yüksəkdir.\n",
    "\n",
    "Əgər nümunə ölçüsünü $ n = 25 $ etsək, standart error belə olar:\n",
    "\n",
    "- $ SE = \\frac{15}{\\sqrt{25}} = \\frac{15}{5} = 3 $.\n",
    "\n",
    "Bu, ortalamanın əhali ortalamasından daha uzaq olma ehtimalını göstərir, çünki kiçik nümunələrdə daha çox qeyri-müəyyənlik olur.\n",
    "\n",
    "### Nəticə:\n",
    "Nümunə sayına bölmək **standart erroru** verir və bu da nümunənin təxminlərinin nə qədər dəqiq olduğunu göstərir. Daha böyük nümunə ölçüləri dəqiqliyi artırır və nəticələrə olan inamı yüksəldir. Buna görə də nümunə ölçüsü hər zaman nəzərə alınır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cdf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7175cda7",
   "metadata": {},
   "source": [
    "## QQ plot nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3011a0",
   "metadata": {},
   "source": [
    "QQ plot (Quantile-Quantile plot), iki paylanmanın bir-biri ilə necə uyğunlaşdığını yoxlamaq üçün istifadə olunan qrafik metodudur. Əsasən verilənlərin nəzəri olaraq normal paylanmaya nə qədər uyğun olduğunu qiymətləndirmək üçün istifadə olunur.\n",
    "\n",
    "### QQ Plot necə işləyir?\n",
    "1. **X oxu**: Nəzəri olaraq seçilən paylanmanın kvantillərini göstərir. Çox vaxt normal paylanmanın kvantilləri götürülür.\n",
    "2. **Y oxu**: Real (empirik) verilənlərin kvantillərini göstərir.\n",
    "\n",
    "Verilənlərin kvantilləri nəzəri paylanmanın kvantilləri ilə müqayisə edilir. Əgər verilənlər nəzəri paylanmaya uyğun gəlirsə, nöqtələr düz bir xətt boyunca sıralanmalıdır.\n",
    "\n",
    "### QQ Plot nə göstərir?\n",
    "- **Düz xəttə yaxınlıq**: Verilənlər nəzəri paylanmaya uyğundursa, nöqtələr təxminən düz bir xətt üzərində olacaq.\n",
    "- **Səpələnmə və əyri xətt**: Əgər nöqtələr düz xəttdən kənara çıxırsa və ya əyri bir xətt boyunca sıralanırsa, bu, verilənlərin normal paylanmadığına işarədir.\n",
    "\n",
    "### QQ Plotdan nə üçün istifadə olunur?\n",
    "QQ plot, qalıqların normal paylanıb-paylanmadığını yoxlamaq üçün ARIMA kimi statistik modellərdə və digər çoxsaylı modellərdə istifadə olunur. Əgər qalıqlar normal paylanmırsa, bu, modelin düzgün olmadığını və daha da təkmilləşdirilməli olduğunu göstərə bilər.\n",
    "\n",
    "### Misal:\n",
    "- **Düz xətt**: Normal paylanma ilə uyğun gələn verilənlər.\n",
    "- **Düz xəttdən kənarda**: Normal paylanmadan uzaqlaşan verilənlər. Bu halda model uyğun olmaya bilər.\n",
    "\n",
    "QQ plot modeli qiymətləndirmək üçün əhəmiyyətli bir vasitədir, xüsusən qalıqların normallığını yoxlayarkən."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0a3e4",
   "metadata": {},
   "source": [
    "Python-da QQ plot yaratmaq üçün `scipy.stats` və `matplotlib` kitabxanalarından istifadə edə bilərsiniz. Aşağıda nümunə bir kod verilmişdir:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Random normal data (siz öz datasınızı istifadə edə bilərsiniz)\n",
    "data = np.random.normal(0, 1, 1000)\n",
    "\n",
    "# QQ plot yaratmaq\n",
    "stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "\n",
    "# Qrafiki göstərmək\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Kodu izah edək:\n",
    "1. **`np.random.normal(0, 1, 1000)`**: 1000 dəyərdən ibarət normal paylanma yaradır.\n",
    "2. **`stats.probplot(data, dist=\"norm\", plot=plt)`**: QQ plot yaradır. Burada `dist=\"norm\"` normal paylanmanı nəzərdə tutur. Siz başqa paylanmaları da sınaya bilərsiniz, məsələn, `dist=\"expon\"` eksponensial paylanmanı yoxlamaq üçün.\n",
    "3. **`plt.show()`**: QQ plotu göstərir.\n",
    "\n",
    "Bu kodda, verdiyiniz datanın normal paylanmaya uyğun olub-olmadığını qiymətləndirmək üçün QQ plot yaradırsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad3662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "917a3997",
   "metadata": {},
   "source": [
    "## Regression Model Assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72539e5",
   "metadata": {},
   "source": [
    "Regressiya analizində (xüsusilə lineer regresiya) müəyyən asumptiyalar vardır ki, bunlar analiz nəticələrinin düzgünlüyünü təmin edir. Bu asumptiyaların başlıcaları aşağıdakılardır:\n",
    "\n",
    "1. **Homoscedasticity Linearlıq:**\n",
    "   - Modeldəki müstəqil dəyişənlər ilə asılı dəyişən arasında xətti bir əlaqə olmalıdır. Yəni, asılı dəyişənin qiyməti müstəqil dəyişənlərin qiymətləri ilə düz xətt üzrə dəyişməlidir.\n",
    "\n",
    "2. **Autocorrelation residuals - Müstəqillik (Independence):**\n",
    "   - Qalıqlar (residuals) bir-birindən müstəqil olmalıdır. Yəni, bir müşahidənin qalığı digər müşahidənin qalığını təsir etməməlidir.\n",
    "\n",
    "   **Autocorrelation residuals:**\n",
    "   - Qalıqlar arasında autokorrelyasiya olmamalıdır. Bu, xüsusilə zaman seriyası analizi üçün vacibdir. Durumda, qalıqların hər biri öz-özünə müstəqil olmalıdır.\n",
    "\n",
    "3. **Normal paylanma (Normality):**\n",
    "   - Qalıqların dəyişkənliyi (variance) bütün müstəqil dəyişənlərin qiymətləri üçün eyni olmalıdır. Yəni, qalıqların yayılması eyni dərəcədə olmalıdır.\n",
    "\n",
    "   **Normal paylanma (Normality):**\n",
    "   - Qalıqlar normal paylanmaya malik olmalıdır. Bu, modelin daha dəqiq nəticələr verməsi üçün vacibdir. Normal paylanma, qalıqların histogramında və ya QQ plot-da göstərilə bilər.\n",
    "\n",
    "4. **Multikolinearlıq:**\n",
    "   - Müstəqil dəyişənlər arasında çox güclü bir əlaqə olmamalıdır. Multikolinearlıq, modelin parametrlərinin düzgün təxmin edilməsinə mane ola bilər. Bunu yoxlamaq üçün Variance Inflation Factor (VIF) istifadə edilə bilər.\n",
    "\n",
    "5. **Qalıqların Cəminin Sıfıra Bərabər Olması**\n",
    "   - Bəli, doğrudur. Regressiya analizində qalıqların (residuals) cəmi sıfıra bərabər olmalıdır. Bu, lineer regresiya modelinin bir assumptiyasıdır və aşağıdakı səbəblərlə əhəmiyyətlidir:\n",
    "\n",
    "\n",
    "   **Model Düzgünlüyü:**\n",
    "   - Qalıqların cəmi sıfır olduqda, bu, modelin müstəqil dəyişənlərin təsirini doğru şəkildə əks etdirdiyini göstərir. Yəni, modeldəki xətalar, yəni qalıqlar, pozitiv və negativ dəyərlərin tarazlığında olmalıdır.\n",
    "\n",
    "   **Bərabər Paylanma:**\n",
    "   - Qalıqların cəminin sıfır olması, onların normal paylanması və düzgün yayılması üçün də vacibdir. Bu, modelin düzgün işləməsini təmin edir.\n",
    "\n",
    "   **Qalıq Analizi:**\n",
    "   - Qalıqların cəminin sıfır olması, modelin tətbiqi zamanı daha dərindən analiz etmək üçün imkan yaradır. Eyni zamanda, modelin düzəldilməsi və ya təkmilləşdirilməsi üçün potensial sahələri aşkar etməkdə kömək edə bilər.\n",
    "\n",
    "Bu asumptiyaların doğruluğunu yoxlamaq üçün müxtəlif qrafiklər və statistik testlər istifadə edilə bilər. Asumptiyalardan hər hansı biri pozulsa, modelin nəticələri etibarsız ola bilər və bu, daha çox dəyişən əlavə etməyi, transformasiya etməyi və ya fərqli modellər istifadə etməyi tələb edə bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea81aa7",
   "metadata": {},
   "source": [
    "**Kitabxanalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a37be4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634b771",
   "metadata": {},
   "source": [
    "**Data ve Model qurmaq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "582d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing datasetini yükləyin\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target)\n",
    "\n",
    "# Modeli qurun\n",
    "X = sm.add_constant(X)  # Sabit əlavə edin\n",
    "model = sm.OLS(y, X).fit()  # OLS modeli qurun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4d902",
   "metadata": {},
   "source": [
    "1. **Homoscedasticity Linearlıq:**\n",
    "- Linearlığı yoxlamaq üçün qalıqları və X-in hər bir dəyişənini plota gətirək:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6225cf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Qalıqları al\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "\n",
    "# Qalıqların və əks olunmuş dəyərlərin plota gətirilməsi\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(fitted, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Linearity Homoscedasticity Check')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97cca9",
   "metadata": {},
   "source": [
    "2. **Autocorrelation resuduals - Müstəqillik (Independence):**\n",
    "\n",
    "- Qalıqların autokorrelyasiyasını yoxlamaq üçün Durbin-Watson testini istifadə edin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6070774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durbin-Watson testi: 0.8851659750498142\n"
     ]
    }
   ],
   "source": [
    "# Durbin-Watson testi\n",
    "dw = sm.stats.durbin_watson(model.resid)\n",
    "print(f'Durbin-Watson testi: {dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6945c",
   "metadata": {},
   "source": [
    "   **Autocorrelation resuduals (No Autocorrelation):**\n",
    "- ACF plot ilə qalıqları yoxlayaq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "481b291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGxCAYAAABvIsx7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABAjklEQVR4nO3deXQUVeL28afJ0glbCwSyaAiRH3sAWQQSRVAgLOKKEESDMBBFQAejZzRuLCoRHBUVQXFQQBHiiAz6imhANoeAgCCKDKLCJEAiBiEBhGzU+weTNp3uhARShC6+n3PqSN+6dftWt0093LpVZTMMwxAAAICF1KjuDgAAAFQ1Ag4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg4AALAcAg5QyquvviqbzaaoqKjzbmv58uWaNGnS+XfKC+zbt082m03z5s2r9LYHDx7UpEmTtH37drd1kyZNks1mO/8Omig/P19jxoxRaGiofHx8dNVVV5VZd8SIEbLZbM7F399fTZs21SOPPKLc3FzT+tizZ0/17NnzrPXO53s0oz/AufKt7g4AF5u3335bkrRz505t2rRJXbt2Pee2li9frtdff/2SCTnn6uDBg5o8ebKaNGniFg5Gjx6tfv36VU/HKmj27Nl688039dprr6lTp06qXbt2ufUDAwP15ZdfSpKOHj2qDz/8UC+++KJ27NihL774wpQ+zpo1y5R2gYsVAQcoYcuWLfr2229144036tNPP9XcuXPPK+B4gz/++EM1a9Z0KzcMQ6dOnVJgYGA19OpPV1xxha644opq7cPZfP/99woMDNT48eMrVL9GjRrq1q2b83W/fv30yy+/KDU1VXv37lVkZGSV97F169ZV3iZwMeMUFVDC3LlzJUnPP/+8YmJitHjxYv3xxx8uddasWSObzaY1a9a4lJce2h8xYoRef/11SXI5JbFv3z5J0qlTp5SUlKTIyEj5+/vr8ssv17hx43T06FG3fr3//vuKjo5W7dq1Vbt2bV111VXOvhZ7++231b59ewUEBKh+/fq67bbbtGvXLpc6I0aMUO3atfXdd98pNjZWderUUa9evZx9HD9+vN544w21atVKdrtd8+fPlyTt2bNHw4YNU6NGjWS329WqVSvnvpXnp59+0siRI9WsWTPVrFlTl19+uW666SZ99913Lp/n1VdfLUkaOXKk83MqHvXydIrq9OnTmj59ulq2bCm73a5GjRpp+PDh2r9/v0u9nj17KioqSps3b1b37t1Vs2ZNXXnllXr++ed1+vTps/a/It+RzWbTP/7xD508edLZ93M5vdO5c2dJ0q+//upSnpKSoujoaNWqVUu1a9dW3759tW3bNpc6v/zyi4YOHaqwsDDZ7XYFBwerV69eLqf8PJ0SOnjwoIYMGaI6derI4XAoLi5OWVlZbn0r63TSiBEj1KRJE5eyyZMnq2vXrqpfv77q1q2rjh07au7cuarIc51nz56t9u3bq3bt2qpTp45atmypxx9//KzbAZ4wggP8z8mTJ7Vo0SJdffXVioqK0l/+8heNHj1a//znP3XPPfdUur2nnnpKJ06c0Icffqi0tDRneWhoqAzD0K233qpVq1YpKSlJ3bt3144dOzRx4kSlpaUpLS1NdrtdkvT000/rmWee0e23366HH35YDodD33//vf773/8620xOTtbjjz+uO++8U8nJyTp8+LAmTZqk6Ohobd68Wc2aNXPWzc/P180336z77rtPjz32mAoLC53r/vWvf2n9+vV6+umnFRISokaNGumHH35QTEyMGjdurBdffFEhISH6/PPP9eCDDyo7O1sTJ04s8zM4ePCgGjRooOeff14NGzbU77//rvnz56tr167atm2bWrRooY4dO+qdd97RyJEj9eSTT+rGG2+UpHJHbe6//37NmTNH48eP18CBA7Vv3z499dRTWrNmjb755hsFBQU562ZlZemuu+7Sww8/rIkTJ2rp0qVKSkpSWFiYhg8fXuZ7VPQ7SktL0zPPPKPVq1c7Tzs1bdq0zHbLsnfvXvn6+urKK690lk2dOlVPPvmk87PJz8/XCy+8oO7du+vrr792jsoMGDBARUVFmj59uho3bqzs7Gxt2LDBY1gudvLkSfXu3VsHDx5UcnKymjdvrk8//VRxcXGV7ntJ+/bt03333afGjRtLkjZu3KgHHnhABw4c0NNPP13mdosXL9bYsWP1wAMP6O9//7tq1Kihn376ST/88MN59QeXMAOAYRiGsWDBAkOS8cYbbxiGYRjHjh0zateubXTv3t2l3urVqw1JxurVq13K9+7da0gy3nnnHWfZuHHjDE8/sxUrVhiSjOnTp7uUp6SkGJKMOXPmGIZhGL/88ovh4+Nj3HXXXWX2+8iRI0ZgYKAxYMAAl/L09HTDbrcbw4YNc5bdc889hiTj7bffdmtHkuFwOIzff//dpbxv377GFVdcYeTk5LiUjx8/3ggICHDW97T/pRUWFhr5+flGs2bNjIceeshZvnnz5jK3nThxostnuGvXLkOSMXbsWJd6mzZtMiQZjz/+uLOsR48ehiRj06ZNLnVbt25t9O3bt8x+GkbFvyPDOPO51qpVq9z2StctKCgwCgoKjOzsbGP27NlGjRo1XPqenp5u+Pr6Gg888IDL9seOHTNCQkKMIUOGGIZhGNnZ2YYkY8aMGeW+b48ePYwePXo4X8+ePduQZCxbtsylXkJCgtt3UXrbkvsSERFR5nsWFRUZBQUFxpQpU4wGDRoYp0+fLrPN8ePHG5dddlm5+wBUBqeogP+ZO3euAgMDNXToUElS7dq1NXjwYK1fv1579uyp0vcq/pf+iBEjXMoHDx6sWrVqadWqVZKk1NRUFRUVady4cWW2lZaWppMnT7q1FR4erhtuuMHZVkmDBg3y2NYNN9ygevXqOV+fOnVKq1at0m233aaaNWuqsLDQuQwYMECnTp3Sxo0by+xbYWGhpk6dqtatW8vf31++vr7y9/fXnj173E6fVdTq1asluX92Xbp0UatWrdz2NyQkRF26dHEpa9euncsImCcV/Y7OxYkTJ+Tn5yc/Pz8FBQXp/vvvV1xcnJ577jlnnc8//1yFhYUaPny4y+ceEBCgHj16OE+R1q9fX02bNtULL7ygl156Sdu2bavQ6bfVq1erTp06uvnmm13Khw0bds77JZ353Hr37i2HwyEfHx/5+fnp6aef1uHDh3Xo0KEyt+vSpYuOHj2qO++8U8uWLVN2dvZ59QMg4AA6M1dk3bp1uvHGG2UYho4ePaqjR4/qjjvukPTnlVVV5fDhw/L19VXDhg1dym02m0JCQnT48GFJ0m+//Sap/NM1xXVDQ0Pd1oWFhTnXF6tZs6bq1q3rsa3SbRw+fFiFhYV67bXXnAfk4mXAgAGSVO6BKDExUU899ZRuvfVWffLJJ9q0aZM2b96s9u3b6+TJk2VuV57K7m+DBg3c6tnt9rO+f0W/o3MRGBiozZs3a/Pmzfrkk0/Us2dPLVq0SM8//7yzTvFcnKuvvtrts09JSXF+7jabTatWrVLfvn01ffp0dezYUQ0bNtSDDz6oY8eOlbt/wcHBbuUhISHnvF9ff/21YmNjJUlvvfWW/v3vf2vz5s164oknJKnczzw+Pl5vv/22/vvf/2rQoEFq1KiRunbtqtTU1HPuDy5tzMEBdCbAGIahDz/8UB9++KHb+vnz5+vZZ5+Vj4+PAgICJEl5eXkudSrzL84GDRqosLBQv/32m8sB1DAMZWVlOSfdFq/bv3+/wsPDy2xLkjIzM93WHTx40GU+iqRy7ylTel29evXk4+Oj+Pj4MkeRyrvi57333tPw4cM1depUl/Ls7GxddtllZW5XnpL7Wzr4edrfc1XR7+hc1KhRwzmpWJL69OmjTp06afLkybrrrrsUHh7u3I8PP/xQERER5bYXERHhnHT+448/6oMPPtCkSZOUn5+vN954o8z9+/rrr93KPU0yDggIUE5Ojlt56f/nFy9eLD8/P/2///f/nL8T6czcrooYOXKkRo4cqRMnTmjdunWaOHGiBg4cqB9//PGsnwFQGiM4uOQVFRVp/vz5atq0qVavXu22PPzww8rMzNRnn30mSc6rRnbs2OHSzscff+zWdvFE4dL/ci2+cum9995zKV+yZIlOnDjhXB8bGysfHx/Nnj27zP5HR0crMDDQra39+/fryy+/dLZ1LmrWrKnrr79e27ZtU7t27dS5c2e3xdMISTGbzeb8DIp9+umnOnDggEtZWZ+TJzfccIMk989u8+bN2rVr13ntb0kV/Y6qgt1u1+uvv65Tp07p2WeflST17dtXvr6++vnnnz1+7iUDUknNmzfXk08+qbZt2+qbb74p8z2vv/56HTt2zO3/2/fff9+tbpMmTfTjjz+6hPrDhw9rw4YNLvVsNpt8fX3l4+PjLDt58qTefffds38IJdSqVUv9+/fXE088ofz8fO3cubNS2wMSIziAPvvsMx08eFDTpk3zeClsVFSUZs6cqblz52rgwIEKCQlR7969lZycrHr16ikiIkKrVq3SRx995LZt27ZtJUnTpk1T//795ePjo3bt2qlPnz7q27evHn30UeXm5uqaa65xXqHToUMHxcfHSzpzYHn88cf1zDPP6OTJk7rzzjvlcDj0ww8/KDs7W5MnT9Zll12mp556So8//riGDx+uO++8U4cPH9bkyZMVEBBQ7lVOFfHKK6/o2muvVffu3XX//ferSZMmOnbsmH766Sd98sknzrkqngwcOFDz5s1Ty5Yt1a5dO23dulUvvPCC28hL06ZNFRgYqIULF6pVq1aqXbu2wsLCFBYW5tZmixYtdO+99+q1115TjRo11L9/f+dVVOHh4XrooYfOa3+LVfQ7qio9evTQgAED9M477+ixxx5TZGSkpkyZoieeeEK//PKL+vXrp3r16unXX3/V119/rVq1amny5MnasWOHxo8fr8GDB6tZs2by9/fXl19+qR07duixxx4r8/2GDx+ul19+WcOHD9dzzz2nZs2aafny5fr888/d6sbHx+vNN9/U3XffrYSEBB0+fFjTp093O9V544036qWXXtKwYcN077336vDhw/r73//uFnI9SUhIUGBgoK655hqFhoYqKytLycnJcjgc5zVahktY9c5xBqrfrbfeavj7+xuHDh0qs87QoUMNX19fIysryzAMw8jMzDTuuOMOo379+obD4TDuvvtuY8uWLW5Xn+Tl5RmjR482GjZsaNhsNkOSsXfvXsMwDOPkyZPGo48+akRERBh+fn5GaGiocf/99xtHjhxxe/8FCxYYV199tREQEGDUrl3b6NChg9sVR//4xz+Mdu3aGf7+/obD4TBuueUWY+fOnS51yrvaR5Ixbtw4j+v27t1r/OUvfzEuv/xyw8/Pz2jYsKERExNjPPvssy51Su//kSNHjFGjRhmNGjUyatasaVx77bXG+vXrPV6Vs2jRIqNly5aGn5+fIcmYOHGiYRjuV1EZxpmrc6ZNm2Y0b97c8PPzM4KCgoy7777byMjIcKnXo0cPo02bNm77c7arf4pV9Ds6l6uoPPnuu++MGjVqGCNHjnSW/etf/zKuv/56o27duobdbjciIiKMO+64w1i5cqVhGIbx66+/GiNGjDBatmxp1KpVy6hdu7bRrl074+WXXzYKCwud7Xj6zPfv328MGjTIqF27tlGnTh1j0KBBxoYNGzxe0TZ//nyjVatWRkBAgNG6dWsjJSXF4+f49ttvGy1atDDsdrtx5ZVXGsnJycbcuXNd/t/31J/58+cb119/vREcHGz4+/sbYWFhxpAhQ4wdO3ZU6HMFSrMZRgXuvgQAAOBFmIMDAAAsh4ADAAAsh4ADAAAsx9SAs27dOt10000KCwuTzWar0L0Q1q5dq06dOikgIEBXXnmlx3s4LFmyRK1bt5bdblfr1q21dOlSE3oPAAC8lakB58SJE2rfvr1mzpxZofp79+7VgAED1L17d23btk2PP/64HnzwQS1ZssRZJy0tTXFxcYqPj9e3336r+Ph4DRkyRJs2bTJrNwAAgJe5YFdR2Ww2LV26VLfeemuZdR599FF9/PHHLs+oGTNmjL799lvn05jj4uKUm5vrvOmaJOf9IRYtWmRa/wEAgPe4qG70l5aW5nyOSbG+fftq7ty5KigokJ+fn9LS0txu5NW3b1/NmDGjzHbz8vJc7sB5+vRp/f7772rQoEG5t60HAAAXD8MwdOzYMYWFhalGjfJPQl1UAScrK8vt4W/BwcEqLCxUdna28+6Wnup4en5KseTkZE2ePNmUPgMAgAsrIyOj3IcQSxdZwJHcH/ZXfAatZLmnOuWNxCQlJSkxMdH5OicnR40bN1ZGRkaZT1WujJdTf9S8DftUdNr9bJ9PDZtGxDTRQ32an/f7AABwKcvNzVV4eLjq1Klz1roXVcAJCQlxG4k5dOiQfH19nQ/0K6tO6VGdkux2u8dnodStW7dKAs7wHq00f8uvquFhNpPNJt3To5Xq1q113u8DAADcBzo8uajugxMdHa3U1FSXsi+++EKdO3eWn59fuXViYmIuWD9LiwyqpWmD2qlGic/bx2ZTDZs0bVA7NQki3AAAcCGZOoJz/Phx/fTTT87Xe/fu1fbt21W/fn01btxYSUlJOnDggBYsWCDpzBVTM2fOVGJiohISEpSWlqa5c+e6XB3117/+Vdddd52mTZumW265RcuWLdPKlSv11VdfmbkrZzW4c7iiLq+r/q+c6cfIa5vo7q4RhBsAAKqBqSM4W7ZsUYcOHdShQwdJUmJiojp06KCnn35akpSZman09HRn/cjISC1fvlxr1qzRVVddpWeeeUavvvqqBg0a5KwTExOjxYsX65133lG7du00b948paSkqGvXrmbuSoVENPgzzCT2aU64AQCgmlySTxPPzc2Vw+FQTk5OlczBKfZHfqFaP/25JOmHKX1V0/+imuIEAIBXq8zx+6KagwMAAFAVCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByLkjAmTVrliIjIxUQEKBOnTpp/fr1ZdYdMWKEbDab29KmTRtnnXnz5nmsc+rUqQuxOwAA4CJnesBJSUnRhAkT9MQTT2jbtm3q3r27+vfvr/T0dI/1X3nlFWVmZjqXjIwM1a9fX4MHD3apV7duXZd6mZmZCggIMHt3AACAFzA94Lz00ksaNWqURo8erVatWmnGjBkKDw/X7NmzPdZ3OBwKCQlxLlu2bNGRI0c0cuRIl3o2m82lXkhIiNm7AgAAvISpASc/P19bt25VbGysS3lsbKw2bNhQoTbmzp2r3r17KyIiwqX8+PHjioiI0BVXXKGBAwdq27ZtZbaRl5en3NxclwUAAFiXqQEnOztbRUVFCg4OdikPDg5WVlbWWbfPzMzUZ599ptGjR7uUt2zZUvPmzdPHH3+sRYsWKSAgQNdcc4327NnjsZ3k5GQ5HA7nEh4efu47BQAALnoXZJKxzWZzeW0YhluZJ/PmzdNll12mW2+91aW8W7duuvvuu9W+fXt1795dH3zwgZo3b67XXnvNYztJSUnKyclxLhkZGee8LwAA4OLna2bjQUFB8vHxcRutOXTokNuoTmmGYejtt99WfHy8/P39y61bo0YNXX311WWO4Njtdtnt9sp1HgAAeC1TR3D8/f3VqVMnpaamupSnpqYqJiam3G3Xrl2rn376SaNGjTrr+xiGoe3btys0NPS8+gsAAKzB1BEcSUpMTFR8fLw6d+6s6OhozZkzR+np6RozZoykM6ePDhw4oAULFrhsN3fuXHXt2lVRUVFubU6ePFndunVTs2bNlJubq1dffVXbt2/X66+/bvbuAAAAL2B6wImLi9Phw4c1ZcoUZWZmKioqSsuXL3deFZWZmel2T5ycnBwtWbJEr7zyisc2jx49qnvvvVdZWVlyOBzq0KGD1q1bpy5dupi9OwAAwAvYDMMwqrsTF1pubq4cDodycnJUt27dKmv3j/xCtX76c0nSD1P6qqa/6fkRAIBLRmWO3zyLCgAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWA4BBwAAWI5vdXcAZ7c3+4Q+2JKh/UdO6op6gRrSOVyRQbWqu1sAAFy0CDgXuQ+2ZOixJTtks9lkGIZsNpveXPuzpg1qp8Gdw6u7ewAAXJQ4RXUR25t9Qo8t2aHThlR02nD576NLdmhf9onq7iIAABclAs5F7IMtGbLZbB7X2Ww2pWzJuMA9AgDAOxBwLmL7j5yUYRge1xmGof1HTl7gHgEA4B0IOBexK+oFljuCc0W9wAvcIwAAvAMB5yI2pHN4uSM4cUwyBgDAIwLORSwyqJamDWqnGiUGcXxsNtWwSdMGtVMTLhUHAMAjLhO/yA3uHK6oy+uq/ytfSZJGXttEd3eNINwAAFAOAo4XiGjwZ5hJ7NNcNf352gAAKA+nqAAAgOVckIAza9YsRUZGKiAgQJ06ddL69evLrLtmzRrZbDa35T//+Y9LvSVLlqh169ay2+1q3bq1li5davZuWM7e7BOatuI/emDRNk1b8R/t5caBAACLMD3gpKSkaMKECXriiSe0bds2de/eXf3791d6enq52+3evVuZmZnOpVmzZs51aWlpiouLU3x8vL799lvFx8dryJAh2rRpk9m7YxkfbMlQrxfXaM66X/TpjoOas+4X9Xpxjf7JzQMBABZgesB56aWXNGrUKI0ePVqtWrXSjBkzFB4ertmzZ5e7XaNGjRQSEuJcfHx8nOtmzJihPn36KCkpSS1btlRSUpJ69eqlGTNmmLw31sAjIAAAVmdqwMnPz9fWrVsVGxvrUh4bG6sNGzaUu22HDh0UGhqqXr16afXq1S7r0tLS3Nrs27dvmW3m5eUpNzfXZbmU8QgIAIDVmRpwsrOzVVRUpODgYJfy4OBgZWVledwmNDRUc+bM0ZIlS/TRRx+pRYsW6tWrl9atW+esk5WVVak2k5OT5XA4nEt4+KV9gzweAQEAsLoLcr1x6dECwzDKHEFo0aKFWrRo4XwdHR2tjIwM/f3vf9d11113Tm0mJSUpMTHR+To3N/eSDjnOR0B4CDlV8QiIvdkn9MGWDO0/clJX1AvUkM7hiuS+PQCAC8jUgBMUFCQfHx+3kZVDhw65jcCUp1u3bnrvvfecr0NCQirVpt1ul91ur0TPrW1I53C9ufZnj+vO9xEQH2zJ0GNLdshmszlD55trf9a0Qe00+DwfLUFwAgBUlKmnqPz9/dWpUyelpqa6lKempiomJqbC7Wzbtk2hoaHO19HR0W5tfvHFF5Vq81Jm1iMgzJy8zFVfAIDKMP0UVWJiouLj49W5c2dFR0drzpw5Sk9P15gxYySdOX104MABLViwQNKZK6SaNGmiNm3aKD8/X++9956WLFmiJUuWONv861//quuuu07Tpk3TLbfcomXLlmnlypX66quvzN4dyzDjERDOyctlnPpK2ZKhR/u1rHS7JYOTs+3//ffRJTt0dZP6PLoCAODC9IATFxenw4cPa8qUKcrMzFRUVJSWL1+uiIgISVJmZqbLPXHy8/P1yCOP6MCBAwoMDFSbNm306aefasCAAc46MTExWrx4sZ588kk99dRTatq0qVJSUtS1a1ezd8dSqvoREGZNXjYrOBXj1BcAWM8FmWQ8duxYjR071uO6efPmubz+29/+pr/97W9nbfOOO+7QHXfcURXdQxUxa/KymVd9mTlnCABQfXgWFarMkM7h5QaRc5287AxOHpxPcOKGhwBgXQQcVBmzJi+bFZy44SEAWNcFOUWFS4cZk5eLg9OjxRONdSY4GTLOKziZfcND5vYAQPUh4KDKVfXkZcmc4GTmDQ+Z2wMA1YuAA69R1cHJrBsemn1ZOyNDAHB2zMHBJcusOUNmzu3hhocAUDEEHFzSBncO16cPXut8PfLaJvry4Z7ndRrJrLk9XPUFABXHKSpc8qr61JdZc3u89YaHnFIDUB0IOEAVM2tujzfe8JDJ1gCqC6eogCpm1tweb7vhIafUAFQnAg5gAjPm9njbDQ+5kSKA6sQpKsAkVT23x9tueMiNFAFUJwIO4EW86YaH3EgRQHXiFBXgZUqPDJ1PuJHMO/VlVrtmz+3Zm31C01b8Rw8s2qZpK/6jvcwVArwSAQe4xJk1KZobKQKoTpyiAmDKqS+z2r0QN1LkERve11+gNAIOAEnmPCTVjHa98UaKZs4ZMiOIMMcJVkDAAeBVvO1GimaODJkRRBjJglUQcAB4FbMul/e2kSGzgggjWRemXZiPgAPA65gxt8fbRobMCiKMZJnfLi4MrqIC4JWq+nJ5b3vEhllBxKz+mnX1G48aQVkIOADwP970iA2zgohZ/TV9JMuDS/FRI2bdx8kb7w/FKSoAKMFbHrFh1ik1b5vjxKNG/sSpOleM4ACAycwYGTLrlJpZ/fW2kSyz2pXMuaEkp+rcEXAA4AKo6jlDkjlBpJi3zHHiUSNncKrOHQEHALyYGcHJLN40kuVtjxrx1lN1ZmIODgDggjHjjtk8asS8OU5mtXshMIIDAPB6Zo1kVXW73nb1m1ntXggEHAAALhCzAoO3naq7EAg4AABcIN529ZuZ7ZqNOTgAAFxAZs0ZksyZ42Rmu2a6+HsIAIDFeGNg8DacogIAAJZDwAEAAJZDwAEAAJZzQQLOrFmzFBkZqYCAAHXq1Enr168vs+5HH32kPn36qGHDhqpbt66io6P1+eefu9SZN2+ebDab23Lq1CmzdwUAAHgB0wNOSkqKJkyYoCeeeELbtm1T9+7d1b9/f6Wnp3usv27dOvXp00fLly/X1q1bdf311+umm27Stm3bXOrVrVtXmZmZLktAQIDZuwMAALyA6dO2X3rpJY0aNUqjR4+WJM2YMUOff/65Zs+ereTkZLf6M2bMcHk9depULVu2TJ988ok6dOjgLLfZbAoJCTG17wAAwDuZOoKTn5+vrVu3KjY21qU8NjZWGzZsqFAbp0+f1rFjx1S/fn2X8uPHjysiIkJXXHGFBg4c6DbCU1JeXp5yc3NdFgAAYF2mBpzs7GwVFRUpODjYpTw4OFhZWVkVauPFF1/UiRMnNGTIEGdZy5YtNW/ePH388cdatGiRAgICdM0112jPnj0e20hOTpbD4XAu4eEX990XAQDA+bkgk4xLP1jMMIwyHzZW0qJFizRp0iSlpKSoUaNGzvJu3brp7rvvVvv27dW9e3d98MEHat68uV577TWP7SQlJSknJ8e5ZGSc2+PoAQCAdzB1Dk5QUJB8fHzcRmsOHTrkNqpTWkpKikaNGqV//vOf6t27d7l1a9SooauvvrrMERy73S673V65zgMAAK9l6giOv7+/OnXqpNTUVJfy1NRUxcTElLndokWLNGLECL3//vu68cYbz/o+hmFo+/btCg0NPe8+AwAA72f6VVSJiYmKj49X586dFR0drTlz5ig9PV1jxoyRdOb00YEDB7RgwQJJZ8LN8OHD9corr6hbt27O0Z/AwEA5HA5J0uTJk9WtWzc1a9ZMubm5evXVV7V9+3a9/vrrZu8OAADwAqYHnLi4OB0+fFhTpkxRZmamoqKitHz5ckVEREiSMjMzXe6J8+abb6qwsFDjxo3TuHHjnOX33HOP5s2bJ0k6evSo7r33XmVlZcnhcKhDhw5at26dunTpYvbuAAAAL3BBHl86duxYjR071uO64tBSbM2aNWdt7+WXX9bLL79cBT0DAABWxLOoAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5RBwAACA5fhWdwcAAPAGhmH8bylRVmq95/KS9c+8yCsscpadKihSDZut1HuVeu8SLbqv+9OJvALnn4+dKlDRaUOlqntsw62hUu97Mv/P/h49UaC8gtPlbupTwyZHoJ+HN7lwCDgAvEbxAcQwpNOn//wrtbDotAqLTjv/ki3+y7v4L2e3A4KHA05x2Yn8Que6Y6cKVHjaw4GlnPb/7IPh8rrkAeK3Y3kK9C8s1ZZ7/yrSfsl29/9+UoH+Ph7bcC0r+V6GhzLpZMGf7f7823EF+Pp46EPJfnl4Hw/rT5Vo94eDuQrw8ymz7tn6XLK8ZLvb0o8qwK+Gh3Zd97G89orrlGz3671HnP09XyXb/TYjx5R2fzh4zJR2d/969nZr233V9gpHlbz3uSLgABVU/C83o/jPzvIzfyGX/kv9j1IHyuJ/SZU8SLpsJ9eDTXG7f9b9X7t5f/5Fk3n0lAKcB7RS/7IscYArfZAo3X/JcDmg7Tn05wGt5EG23AOvh3WGXP9i3J5xVAF+Ps73dN2uVDulPovSSra79b9HveoA8ctvJ0xp98DRk6a0m30s35R2j50qVEGRp6GEyis5opBfeNptRASXHgIOzslp58Ha0OkSB2rDcD2w554qOPMXWMk6+vMAa5Ta1q1OidBQst292SdcDpR/buO5PZVeV6IPJf8F/M1/j8j+v3bdQ0DlXIgDZfrvf5jS7u/HzTmg5RWclk0ceACY74IEnFmzZumFF15QZmam2rRpoxkzZqh79+5l1l+7dq0SExO1c+dOhYWF6W9/+5vGjBnjUmfJkiV66qmn9PPPP6tp06Z67rnndNttt5m9K1WueFjdMKTTJYffjT8DxPG8Pw/sR04U6GR+kTNUyJBLwDhdIhQU/9lQyfL/BRJDOlnwZ7s/HMyV3dfHtR3J5UB/uvQIRBlKHtB2mXRgP5SbV2Xt5hf++S+/giJDPjWq5l+UAIDqY3rASUlJ0YQJEzRr1ixdc801evPNN9W/f3/98MMPaty4sVv9vXv3asCAAUpISNB7772nf//73xo7dqwaNmyoQYMGSZLS0tIUFxenZ555RrfddpuWLl2qIUOG6KuvvlLXrl0r3Lc/8gvlW2JU4HyVHGH4owLt5hUWaXt6ztnrlTiwf7f/qOxVdGDPcxmCzjOl3ZJ/pl3apV3apd1Lo13fGrYKHQcrqzJt2gzjXAbfK65r167q2LGjZs+e7Sxr1aqVbr31ViUnJ7vVf/TRR/Xxxx9r165dzrIxY8bo22+/VVpamiQpLi5Oubm5+uyzz5x1+vXrp3r16mnRokVubebl5SkvL8/5Ojc3V+Hh4Qqf8IFq2GtWyX4CAABznc77QxkzhignJ0d169Ytt66p98HJz8/X1q1bFRsb61IeGxurDRs2eNwmLS3NrX7fvn21ZcsWFRQUlFunrDaTk5PlcDicS3h4+LnuEgAA8AKmnqLKzs5WUVGRgoODXcqDg4OVlZXlcZusrCyP9QsLC5Wdna3Q0NAy65TVZlJSkhITE52vi0dwvn6i11kToJkqeooKAABvUtvuqzaXV/3xNTc3V6EzKlb3gkwytrndwMhwKztb/dLllWnTbrfLbre7ldf091VN/+q7kMynhq3KJsoCAHCxCPDzMeX4WliJNk09ugcFBcnHx8dtZOXQoUNuIzDFQkJCPNb39fVVgwYNyq1TVpsXK7uvj7pdWd/tSqfSlzmfLnElk3Fa5V7pVPoKrDLb8XCFVWWuwCpZFwCAi42pAcff31+dOnVSamqqyyXcqampuuWWWzxuEx0drU8++cSl7IsvvlDnzp3l5+fnrJOamqqHHnrIpU5MTIwJe2Eum82m4oEnHy+8P4jzPjjFl5KXEZJKBjBP98Qpedm72/1s9GcgU1nrits+j3vinNnavR1nfZf3AQBczEw/P5OYmKj4+Hh17txZ0dHRmjNnjtLT0533tUlKStKBAwe0YMECSWeumJo5c6YSExOVkJCgtLQ0zZ071+XqqL/+9a+67rrrNG3aNN1yyy1atmyZVq5cqa+++srs3UEpNptNPjZJXhjOzkfJRwa4hyz3MOVWVmIbZ51S9f5c537XZLm0U3b7Jd//bH0p3V7xmxhlvJfLOsO9rOR7utUvsW8VeV9nOyXqe3ovAChmesCJi4vT4cOHNWXKFGVmZioqKkrLly9XRESEJCkzM1Pp6enO+pGRkVq+fLkeeughvf766woLC9Orr77qvAeOJMXExGjx4sV68skn9dRTT6lp06ZKSUmp1D1wgPNRPN/rz2lfl1bAu9iUDnOlA5RrWfHrsgNZyfXltWt42E6V2N5Tv1y2LbVdRfvn0scyQmBl2ymrr6UDaaX662mfy+hzReqerd9l9b0i/a94m7hYmH4fnItRbm6uHA5Hha6jBwCgolxCUCXDl/s25Ye+s21T3raemvMUBzzX81To+rJGDalOQNU/Tbwyx2+eRQUAQBVxvdq3zFoXpC+XOlNv9AcAAFAdCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByCDgAAMByTA04R44cUXx8vBwOhxwOh+Lj43X06NEy6xcUFOjRRx9V27ZtVatWLYWFhWn48OE6ePCgS72ePXvKZrO5LEOHDjVzVwAAgBcxNeAMGzZM27dv14oVK7RixQpt375d8fHxZdb/448/9M033+ipp57SN998o48++kg//vijbr75Zre6CQkJyszMdC5vvvmmmbsCAAC8iK9ZDe/atUsrVqzQxo0b1bVrV0nSW2+9pejoaO3evVstWrRw28bhcCg1NdWl7LXXXlOXLl2Unp6uxo0bO8tr1qypkJAQs7oPAAC8mGkjOGlpaXI4HM5wI0ndunWTw+HQhg0bKtxOTk6ObDabLrvsMpfyhQsXKigoSG3atNEjjzyiY8eOldlGXl6ecnNzXRYAAGBdpo3gZGVlqVGjRm7ljRo1UlZWVoXaOHXqlB577DENGzZMdevWdZbfddddioyMVEhIiL7//nslJSXp22+/dRv9KZacnKzJkyef244AAACvU+kRnEmTJrlN8C29bNmyRZJks9nctjcMw2N5aQUFBRo6dKhOnz6tWbNmuaxLSEhQ7969FRUVpaFDh+rDDz/UypUr9c0333hsKykpSTk5Oc4lIyOjsrsNAAC8SKVHcMaPH3/WK5aaNGmiHTt26Ndff3Vb99tvvyk4OLjc7QsKCjRkyBDt3btXX375pcvojScdO3aUn5+f9uzZo44dO7qtt9vtstvt5bYBAACso9IBJygoSEFBQWetFx0drZycHH399dfq0qWLJGnTpk3KyclRTExMmdsVh5s9e/Zo9erVatCgwVnfa+fOnSooKFBoaGjFdwQAAFiWaZOMW7VqpX79+ikhIUEbN27Uxo0blZCQoIEDB7pcQdWyZUstXbpUklRYWKg77rhDW7Zs0cKFC1VUVKSsrCxlZWUpPz9fkvTzzz9rypQp2rJli/bt26fly5dr8ODB6tChg6655hqzdgcAAHgRU++Ds3DhQrVt21axsbGKjY1Vu3bt9O6777rU2b17t3JyciRJ+/fv18cff6z9+/frqquuUmhoqHMpvvLK399fq1atUt++fdWiRQs9+OCDio2N1cqVK+Xj42Pm7gAAAC9hMwzDqO5OXGi5ublyOBzKyck56/weAABwcajM8ZtnUQEAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsxNeAcOXJE8fHxcjgccjgcio+P19GjR8vdZsSIEbLZbC5Lt27dXOrk5eXpgQceUFBQkGrVqqWbb75Z+/fvN3FPAACANzE14AwbNkzbt2/XihUrtGLFCm3fvl3x8fFn3a5fv37KzMx0LsuXL3dZP2HCBC1dulSLFy/WV199pePHj2vgwIEqKioya1cAAIAX8TWr4V27dmnFihXauHGjunbtKkl66623FB0drd27d6tFixZlbmu32xUSEuJxXU5OjubOnat3331XvXv3liS99957Cg8P18qVK9W3b9+q3xkAAOBVTBvBSUtLk8PhcIYbSerWrZscDoc2bNhQ7rZr1qxRo0aN1Lx5cyUkJOjQoUPOdVu3blVBQYFiY2OdZWFhYYqKiiqz3by8POXm5rosAADAukwLOFlZWWrUqJFbeaNGjZSVlVXmdv3799fChQv15Zdf6sUXX9TmzZt1ww03KC8vz9muv7+/6tWr57JdcHBwme0mJyc75wE5HA6Fh4efx54BAICLXaUDzqRJk9wmAZdetmzZIkmy2Wxu2xuG4bG8WFxcnG688UZFRUXppptu0meffaYff/xRn376abn9Kq/dpKQk5eTkOJeMjIxK7DEAAPA2lZ6DM378eA0dOrTcOk2aNNGOHTv066+/uq377bffFBwcXOH3Cw0NVUREhPbs2SNJCgkJUX5+vo4cOeIyinPo0CHFxMR4bMNut8tut1f4PQEAgHerdMAJCgpSUFDQWetFR0crJydHX3/9tbp06SJJ2rRpk3JycsoMIp4cPnxYGRkZCg0NlSR16tRJfn5+Sk1N1ZAhQyRJmZmZ+v777zV9+vTK7g4AALAg0+bgtGrVSv369VNCQoI2btyojRs3KiEhQQMHDnS5gqply5ZaunSpJOn48eN65JFHlJaWpn379mnNmjW66aabFBQUpNtuu02S5HA4NGrUKD388MNatWqVtm3bprvvvltt27Z1XlUFAAAubaZdJi5JCxcu1IMPPui84unmm2/WzJkzXers3r1bOTk5kiQfHx999913WrBggY4eParQ0FBdf/31SklJUZ06dZzbvPzyy/L19dWQIUN08uRJ9erVS/PmzZOPj4+ZuwMAALyEzTAMo7o7caHl5ubK4XAoJydHdevWre7uAACACqjM8ZtnUQEAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsh4AAAAMsxNeAcOXJE8fHxcjgccjgcio+P19GjR8vdxmazeVxeeOEFZ52ePXu6rR86dKiZuwIAALyIr5mNDxs2TPv379eKFSskSffee6/i4+P1ySeflLlNZmamy+vPPvtMo0aN0qBBg1zKExISNGXKFOfrwMDAKuw5AADwZqYFnF27dmnFihXauHGjunbtKkl66623FB0drd27d6tFixYetwsJCXF5vWzZMl1//fW68sorXcpr1qzpVhcAAEAy8RRVWlqaHA6HM9xIUrdu3eRwOLRhw4YKtfHrr7/q008/1ahRo9zWLVy4UEFBQWrTpo0eeeQRHTt2rMx28vLylJub67IAAADrMm0EJysrS40aNXIrb9SokbKysirUxvz581WnTh3dfvvtLuV33XWXIiMjFRISou+//15JSUn69ttvlZqa6rGd5ORkTZ48ufI7AQAAvFKlR3AmTZpU5kTg4mXLli2SzkwYLs0wDI/lnrz99tu66667FBAQ4FKekJCg3r17KyoqSkOHDtWHH36olStX6ptvvvHYTlJSknJycpxLRkZGJfcaAAB4k0qP4IwfP/6sVyw1adJEO3bs0K+//uq27rffflNwcPBZ32f9+vXavXu3UlJSzlq3Y8eO8vPz0549e9SxY0e39Xa7XXa7/aztAAAAa6h0wAkKClJQUNBZ60VHRysnJ0dff/21unTpIknatGmTcnJyFBMTc9bt586dq06dOql9+/Znrbtz504VFBQoNDT07DsAAAAsz7RJxq1atVK/fv2UkJCgjRs3auPGjUpISNDAgQNdrqBq2bKlli5d6rJtbm6u/vnPf2r06NFu7f7888+aMmWKtmzZon379mn58uUaPHiwOnTooGuuucas3QEAAF7E1Bv9LVy4UG3btlVsbKxiY2PVrl07vfvuuy51du/erZycHJeyxYsXyzAM3XnnnW5t+vv7a9WqVerbt69atGihBx98ULGxsVq5cqV8fHzM3B0AAOAlbIZhGNXdiQstNzdXDodDOTk5qlu3bnV3BwAAVEBljt88iwoAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFgOAQcAAFiOqQHnueeeU0xMjGrWrKnLLrusQtsYhqFJkyYpLCxMgYGB6tmzp3bu3OlSJy8vTw888ICCgoJUq1Yt3Xzzzdq/f78JewAAALyRqQEnPz9fgwcP1v3331/hbaZPn66XXnpJM2fO1ObNmxUSEqI+ffro2LFjzjoTJkzQ0qVLtXjxYn311Vc6fvy4Bg4cqKKiIjN2AwAAeBmbYRiG2W8yb948TZgwQUePHi23nmEYCgsL04QJE/Too49KOjNaExwcrGnTpum+++5TTk6OGjZsqHfffVdxcXGSpIMHDyo8PFzLly9X3759z9qf3NxcORwO5eTkqG7duue9fwAAwHyVOX77XqA+VcjevXuVlZWl2NhYZ5ndblePHj20YcMG3Xfffdq6dasKCgpc6oSFhSkqKkobNmzwGHDy8vKUl5fnfJ2TkyPpzAcFAAC8Q/FxuyJjMxdVwMnKypIkBQcHu5QHBwfrv//9r7OOv7+/6tWr51anePvSkpOTNXnyZLfy8PDwqug2AAC4gI4dOyaHw1FunUoHnEmTJnkMCyVt3rxZnTt3rmzTTjabzeW1YRhuZaWVVycpKUmJiYnO16dPn9bvv/+uBg0anLXdysrNzVV4eLgyMjI4/eVF+N68E9+bd+J7804Xw/dmGIaOHTumsLCws9atdMAZP368hg4dWm6dJk2aVLZZSVJISIikM6M0oaGhzvJDhw45R3VCQkKUn5+vI0eOuIziHDp0SDExMR7btdvtstvtLmUVvarrXNWtW5cfrhfie/NOfG/eie/NO1X393a2kZtilQ44QUFBCgoKqnSHKiIyMlIhISFKTU1Vhw4dJJ25Emvt2rWaNm2aJKlTp07y8/NTamqqhgwZIknKzMzU999/r+nTp5vSLwAA4F1MnYOTnp6u33//Xenp6SoqKtL27dslSf/3f/+n2rVrS5Jatmyp5ORk3XbbbbLZbJowYYKmTp2qZs2aqVmzZpo6dapq1qypYcOGSTqT3EaNGqWHH35YDRo0UP369fXII4+obdu26t27t5m7AwAAvISpAefpp5/W/Pnzna+LR2VWr16tnj17SpJ2797tvKpJkv72t7/p5MmTGjt2rI4cOaKuXbvqiy++UJ06dZx1Xn75Zfn6+mrIkCE6efKkevXqpXnz5snHx8fM3akQu92uiRMnup0Sw8WN78078b15J7437+Rt39sFuQ8OAADAhcSzqAAAgOUQcAAAgOUQcAAAgOUQcAAAgOUQcAAAgOUQcKrQrFmzFBkZqYCAAHXq1Enr16+v7i6hHJMmTZLNZnNZiu+mjYvLunXrdNNNNyksLEw2m03/+te/XNYbhqFJkyYpLCxMgYGB6tmzp3bu3Fk9nYXT2b63ESNGuP0Gu3XrVj2dhaQzz268+uqrVadOHTVq1Ei33nqrdu/e7VLHW35vBJwqkpKSogkTJuiJJ57Qtm3b1L17d/Xv31/p6enV3TWUo02bNsrMzHQu3333XXV3CR6cOHFC7du318yZMz2unz59ul566SXNnDlTmzdvVkhIiPr06aNjx45d4J6ipLN9b5LUr18/l9/g8uXLL2APUdratWs1btw4bdy4UampqSosLFRsbKxOnDjhrOM1vzcDVaJLly7GmDFjXMpatmxpPPbYY9XUI5zNxIkTjfbt21d3N1BJkoylS5c6X58+fdoICQkxnn/+eWfZqVOnDIfDYbzxxhvV0EN4Uvp7MwzDuOeee4xbbrmlWvqDijl06JAhyVi7dq1hGN71e2MEpwrk5+dr69atio2NdSmPjY3Vhg0bqqlXqIg9e/YoLCxMkZGRGjp0qH755Zfq7hIqae/evcrKynL5/dntdvXo0YPfnxdYs2aNGjVqpObNmyshIUGHDh2q7i6hhOInDdSvX1+Sd/3eCDhVIDs7W0VFRc4nnhcLDg5WVlZWNfUKZ9O1a1ctWLBAn3/+ud566y1lZWUpJiZGhw8fru6uoRKKf2P8/rxP//79tXDhQn355Zd68cUXtXnzZt1www3Ky8ur7q5BZ+baJCYm6tprr1VUVJQk7/q9mfosqkuNzWZzeW0YhlsZLh79+/d3/rlt27aKjo5W06ZNNX/+fCUmJlZjz3Au+P15n7i4OOefo6Ki1LlzZ0VEROjTTz/V7bffXo09gySNHz9eO3bs0FdffeW2zht+b4zgVIGgoCD5+Pi4pddDhw65pVxcvGrVqqW2bdtqz5491d0VVELxlW/8/rxfaGioIiIi+A1eBB544AF9/PHHWr16ta644gpnuTf93gg4VcDf31+dOnVSamqqS3lqaqpiYmKqqVeorLy8PO3atUuhoaHV3RVUQmRkpEJCQlx+f/n5+Vq7di2/Py9z+PBhZWRk8BusRoZhaPz48froo4/05ZdfKjIy0mW9N/3eOEVVRRITExUfH6/OnTsrOjpac+bMUXp6usaMGVPdXUMZHnnkEd10001q3LixDh06pGeffVa5ubm65557qrtrKOX48eP66aefnK/37t2r7du3q379+mrcuLEmTJigqVOnqlmzZmrWrJmmTp2qmjVratiwYdXYa5T3vdWvX1+TJk3SoEGDFBoaqn379unxxx9XUFCQbrvttmrs9aVt3Lhxev/997Vs2TLVqVPHOVLjcDgUGBgom83mPb+3ar2Gy2Jef/11IyIiwvD39zc6duzovKwOF6e4uDgjNDTU8PPzM8LCwozbb7/d2LlzZ3V3Cx6sXr3akOS23HPPPYZhnLl0deLEiUZISIhht9uN6667zvjuu++qt9Mo93v7448/jNjYWKNhw4aGn5+f0bhxY+Oee+4x0tPTq7vblzRP35ck45133nHW8Zbfm80wDOPCxyoAAADzMAcHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYDgEHAABYzv8Hm+L0vOMYJ1wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# ACF plot\n",
    "plot_acf(residuals, lags=20)\n",
    "plt.title('Autocorrelation of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514829e7",
   "metadata": {},
   "source": [
    "3. **Normal paylanma (Normality):**\n",
    "- Qalıqların yayılmasını yoxlamaq üçün qalıqların histogramını və ya Q-Q plotunu çəkək:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfdac5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Qalıqların histogramı\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Residuals Histogram')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd49a9",
   "metadata": {},
   "source": [
    "   **Normal paylanma (Normality):**\n",
    "- Shapiro-Wilk testi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbbbc2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk testi: ShapiroResult(statistic=0.9231338500976562, pvalue=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Shapiro-Wilk testi\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "print(f'Shapiro-Wilk testi: {shapiro_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f44710",
   "metadata": {},
   "source": [
    "4. **Multikolinearlıq:**\n",
    "- Müstəqil dəyişənlər arasında multikolinearlığı yoxlamaq üçün VIF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a384414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Variable           VIF\n",
      "0       const  17082.623698\n",
      "1      MedInc      2.501295\n",
      "2    HouseAge      1.241254\n",
      "3    AveRooms      8.342786\n",
      "4   AveBedrms      6.994995\n",
      "5  Population      1.138125\n",
      "6    AveOccup      1.008324\n",
      "7    Latitude      9.297624\n",
      "8   Longitude      8.962263\n"
     ]
    }
   ],
   "source": [
    "# VIF hesablamaq\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Variable'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ace54",
   "metadata": {},
   "source": [
    "5. **Qalıqların Sum ve Mean Sıfıra Bərabər Olması**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c012851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qalıqların cəmi: 3.5635139283840545e-09\n",
      "Qalıqların ortalaması: 1.7265086862325845e-13\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Modeli yaratdıqdan sonra qalıqları hesablayın\n",
    "residuals = model.resid\n",
    "\n",
    "# Qalıqların cəmini və ortalamasını hesablayın\n",
    "sum_residuals = residuals.sum()\n",
    "mean_residuals = residuals.mean()\n",
    "\n",
    "print(f\"Qalıqların cəmi: {sum_residuals}\")\n",
    "print(f\"Qalıqların ortalaması: {mean_residuals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fade38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8380c56",
   "metadata": {},
   "source": [
    "## Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65051778",
   "metadata": {},
   "source": [
    "Homoskedasticity (həmçinin homoscedasticity kimi yazılır) statistik modeldə **error termlərin** (qalıqların) və ya **residualların** sabit bir varyansa malik olduğunu bildirən bir anlayışdır. Yəni, modelin **proqnozlaşdırılan dəyərlərinə** qarşı qalıqların yayılması sabitdirsə, bu vəziyyət **homoskedasticity** adlanır.\n",
    "\n",
    "### Əsas nöqtələr:\n",
    "- **Homoskedasticity**: Residualların varyansı proqnozlar və ya müstəqil dəyişənlər üçün sabitdirsə, homoskedastikdir. Yəni qrafik üzərində qalıqlar sabit bir \"bulud\" şəklində görünür.\n",
    "- **Heteroskedasticity**: Əgər varyans proqnoz dəyərlərinə görə dəyişirsə (bəzən az, bəzən çox yayılmış qalıqlar varsa), buna **heteroskedasticity** deyilir. Bu, model üçün ciddi problemlər yarada bilər, çünki dəyişən varyans nəticələri qeyri-dəqiq edər və statistik testlərin nəticələrini etibarsızlaşdıra bilər.\n",
    "\n",
    "### Homoskedasticity ilə əlaqəli vizuallaşdırma:\n",
    "Modelinizin qalıqlarını (residualları) nəzərdən keçirmək üçün ən yaxşı yollardan biri **Residual vs Fitted** qrafikidir. Əgər qalıqlar sabit bir yayılmaya malikdirsə, homoskedastiklik mövcuddur və qalıqlar proqnoz dəyərlərinə qarşı təsadüfi yayılmış olmalıdır.\n",
    "\n",
    "### Homoskedasticity nümunəsi:\n",
    "Əgər qalıqlar proqnoz dəyərlərinə qarşı sabit bir varyansa malikdirsə, bu vəziyyətdə model homoskedastik sayılır. Məsələn, xətti regresiya modelində qalıqların yayılması hər bir proqnoz dəyəri üçün təxminən eyni olarsa, homoskedastiklik mövcuddur.\n",
    "\n",
    "### Heteroskedasticity nümunəsi:\n",
    "Əgər qalıqların yayılması müstəqil dəyişənlər artdıqca və ya azaldıqca dəyişirsə (məsələn, daha böyük dəyərlərdə qalıqların yayılması genişlənir və ya daralırsa), bu, heteroskedastiklikdir.\n",
    "\n",
    "### Heteroskedasticity problemini həll etmə yolları:\n",
    "- **Log və ya kvadrat kök transformasiyaları**: Bu cür transformasiyalar heteroskedastikliyi azaltmağa kömək edə bilər.\n",
    "- **Robust standart səhvlər**: Bu üsul heteroskedastiklik olsa belə, etibarlı statistik testlər aparmaq üçün istifadə olunur.\n",
    "\n",
    "Homoskedasticity modelin doğruluğunu qorumaq üçün əhəmiyyətlidir, çünki heteroskedastiklik model nəticələrinin düzgün interpretasiyasına mane ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f6919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0dc961",
   "metadata": {},
   "source": [
    "## Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a20808",
   "metadata": {},
   "source": [
    "**Temporal Fusion Transformer (TFT)**, zaman sıralı (time-series) verilənlər üzərində proqnozlaşdırma məqsədilə istifadə olunan dərin öyrənmə modelidir. TFT, müxtəlif zaman aralıqlarında müşahidələr və dəyişənlər arasındakı mürəkkəb əlaqələri öyrənmək üçün xüsusi olaraq dizayn edilmişdir. Bu model, seqmentlənmiş məlumatlarla (çoxlu verilənlər toplusu, dəyişənlər və dəyişən növləri) uğurla işləyə bilir və fərqli növ xüsusiyyətlər üzərində transformasiya apara bilir.\n",
    "\n",
    "### TFT Modelinin Əsas Xüsusiyyətləri:\n",
    "\n",
    "1. **Çoxlu vaxt nöqtələrini və xüsusiyyətləri birləşdirmək:**\n",
    "   - TFT həm \"static\" (dəyişməyən) xüsusiyyətləri, həm də \"time-varying\" (dəyişən) xüsusiyyətləri birgə işləyə bilir. Bu, modelin həm zaman sıralı verilənləri, həm də verilənlərin zamanla necə dəyişdiyini öyrənməsinə imkan verir.\n",
    "   \n",
    "2. **Gələcək dəyişənlər və keçmiş müşahidələri inteqrasiya etmək:**\n",
    "   - TFT modeli gələcəkdə olan dəyişənləri də analiz edərək proqnozlar yaradır. Bu, klassik zaman seriyası modellərindən fərqli olaraq, gələcəkdə gözlənilən dəyişiklikləri də nəzərə almaq qabiliyyəti verir.\n",
    "   \n",
    "3. **Temporal Pattern Attention Layer (Müvəqqəti Nümunə Diqqət Qatı):**\n",
    "   - Modeldə xüsusi diqqət qatları (attention layers) vasitəsilə müəyyən zaman nöqtələrindəki məlumatlar arasında əlaqələr gücləndirilir. Bu, modelin uzunmüddətli və qısamüddətli nümunələri effektiv şəkildə öyrənməsinə imkan yaradır.\n",
    "\n",
    "4. **Güclü proqnozlaşdırma imkanları:**\n",
    "   - TFT, əsasən çoxluq içərisində olan zaman seriyalarını təhlil etmək üçün dizayn edilib, bu səbəbdən həm qısa müddətli, həm də uzun müddətli proqnozlaşdırmalarda güclü nəticələr əldə edir.\n",
    "\n",
    "5. **Dynamic Local Variable (Dinamik Yerli Dəyişənlər):**\n",
    "   - TFT həm qlobal (bütün məlumat üçün sabit qalan) dəyişənləri, həm də yerli (fərdi zaman seriyaları üçün dəyişən) dəyişənləri nəzərə alır. Bu, müxtəlif zaman nöqtələri üçün daha dəqiq proqnozlar yaratmağa imkan verir.\n",
    "\n",
    "### TFT Modelinin Tətbiq Sahələri:\n",
    "- **İstehlakçı tələb proqnozlaşdırması** (ticarət və logistika sahələrində),\n",
    "- **Maliyyə proqnozları**, \n",
    "- **Enerji bazarlarında tələbat təxminləri**,\n",
    "- **İqlim və hava proqnozlaşdırması**,\n",
    "- **Medikal sahədə zaman sıralı verilənlər analizi**.\n",
    "\n",
    "### TFT Modeli Necə Qurulur?\n",
    "TFT modelini qurmaq üçün ümumiyyətlə `PyTorch` və ya `TensorFlow` kimi dərin öyrənmə kitabxanaları istifadə olunur. Bunun üçün daha rahat yanaşma isə `pytorch-forecasting` adlı paketdən istifadə etməkdir. Bu paket TFT və digər vaxt seriyası modelləri üçün hazır implementasiyalar təmin edir.\n",
    "\n",
    "### Sadə bir TFT model kod nümunəsi:\n",
    "\n",
    "```python\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# Məlumat hazırlığı\n",
    "data = TimeSeriesDataSet(\n",
    "    df,  # məlumat DataFrame-də olmalıdır\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",  # proqnozlaşdırılacaq dəyişən\n",
    "    group_ids=[\"group_id\"],\n",
    "    time_varying_known_reals=[\"time_varying_known\"],  # bilinəən zaman dəyişənləri\n",
    "    time_varying_unknown_reals=[\"time_varying_unknown\"],  # bilinməyən zaman dəyişənləri\n",
    "    max_encoder_length=60,  # keçmişdən götürüləcək zaman nöqtələri\n",
    "    max_prediction_length=30,  # gələcəkdə proqnozlaşdırılacaq zaman nöqtələri\n",
    ")\n",
    "\n",
    "# Modelin qurulması\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    data, \n",
    "    learning_rate=0.03, \n",
    "    hidden_size=16, \n",
    "    attention_head_size=4, \n",
    "    dropout=0.1, \n",
    "    loss=QuantileLoss(), \n",
    "    log_interval=10, \n",
    "    reduce_on_plateau_patience=4\n",
    ")\n",
    "\n",
    "# Modelin öyrədilməsi\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=30)\n",
    "trainer.fit(tft, train_dataloader)\n",
    "```\n",
    "\n",
    "Bu nümunədə:\n",
    "- **TimeSeriesDataSet**: Məlumatları TFT modelinə uyğun formatda hazırlayır.\n",
    "- **TemporalFusionTransformer**: TFT modelini qurur.\n",
    "- **QuantileLoss**: Proqnozlar üçün istifadə olunan xüsusi bir itki funksiyasıdır.\n",
    "\n",
    "### Nəticə:\n",
    "TFT modeli çox qatlı və geniş bir zaman seriyası proqnozlaşdırma modelidir və xüsusilə də çox dəyişkənli zaman seriyaları üçün uyğun hesab olunur. Bu model mürəkkəb zaman əlaqələrini və dəyişənlərin birgə təsirini effektiv şəkildə öyrənmək üçün güclüdür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c182b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ac8b566",
   "metadata": {},
   "source": [
    "## Fourier Transform ve Fast Fourier Transform (FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c91ea",
   "metadata": {},
   "source": [
    "**Fourier Transform** və onun daha spesifik forması olan **Fast Fourier Transform (FFT)**, zaman seriyası analizində və siqnal emalında istifadə edilən güclü riyazi alətlərdən biridir. Bu alətlər funksiyaların zaman məkanından tezlik məkanına çevrilməsinə imkan verir. Fourier Transform, zaman seriyalarının və ya siqnalların tezlik komponentlərini müəyyən etməyə kömək edir.\n",
    "\n",
    "### Fourier Transform nədir?\n",
    "\n",
    "Fourier Transform bir funksiyanı fərqli sinusoidal komponentlərin (tezliklərin) cəmi şəklində yenidən təqdim etməyə imkan verir. Başqa sözlə, zaman domenində bir siqnalı alıb, həmin siqnalın hansı tezliklərdən ibarət olduğunu təhlil etməyə kömək edir.\n",
    "\n",
    "Fourier Transform funksiyası bu şəkildə ifadə edilir:\n",
    "\n",
    "\\[\n",
    "X(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-j 2 \\pi f t} dt\n",
    "\\]\n",
    "\n",
    "Burada:\n",
    "- \\( x(t) \\) — zaman domenindəki siqnal,\n",
    "- \\( X(f) \\) — tezlik domenindəki siqnal,\n",
    "- \\( f \\) — tezlik,\n",
    "- \\( j \\) — xəyali vahid.\n",
    "\n",
    "### Fast Fourier Transform (FFT)\n",
    "\n",
    "**Fast Fourier Transform (FFT)**, Fourier Transform-un sürətləndirilməsini təmin edən bir alqoritmdir. FFT, zaman seriyasını və ya siqnalı çox daha səmərəli şəkildə tezlik domeninə çevirmək üçün istifadə olunur. FFT-nin əsas üstünlüyü onun hesablanma sürətidir. Klassik Fourier Transform O(n²) vaxt mürəkkəbliyinə malik olduğu halda, FFT bunu O(n log n) ilə yerinə yetirir, bu da böyük ölçülü verilənlər üçün nəzərəçarpan fərq yaradır.\n",
    "\n",
    "### Fourier Transformun Zaman Seriyalarında İstifadəsi\n",
    "\n",
    "Fourier Transform zaman seriyası verilənlərində iki əsas məqsəd üçün istifadə olunur:\n",
    "1. **Tezlik komponentlərinin müəyyənləşdirilməsi**: Zaman seriyasında hansı tezliklərin daha güclü olduğunu müəyyən etmək.\n",
    "2. **Səs-küyün filtrasiyası**: Zaman seriyasından arzuolunmaz yüksək və ya aşağı tezlikli komponentləri çıxarmaq üçün istifadə edilə bilər.\n",
    "\n",
    "### Fourier Transformun tətbiqi\n",
    "\n",
    "Zaman seriyasında dövrləri (periodik dəyişiklikləri) tapmaq və ya siqnalda əsas tezlikləri müəyyən etmək üçün Fourier Transform tətbiq olunur. FFT-nin tətbiqi Python kimi proqramlaşdırma dillərində olduqca asandır və aşağıdakı kimi sadə bir şəkildə həyata keçirilə bilər.\n",
    "\n",
    "### FFT tətbiqi - Python nümunəsi\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Saxta zaman seriyası yaradırıq\n",
    "time = np.linspace(0, 2, 1000, endpoint=False)\n",
    "signal = np.sin(5 * 2 * np.pi * time) + 0.5 * np.sin(80 * 2 * np.pi * time)\n",
    "\n",
    "# FFT tətbiq edirik\n",
    "fft_result = np.fft.fft(signal)\n",
    "\n",
    "# FFT nəticəsini vizuallaşdırırıq\n",
    "freqs = np.fft.fftfreq(len(signal), d=(time[1] - time[0]))\n",
    "plt.plot(freqs, np.abs(fft_result))\n",
    "plt.title('FFT nəticəsi')\n",
    "plt.xlabel('Tezlik (Hz)')\n",
    "plt.ylabel('Amplituda')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Bu kod nümunəsində:\n",
    "- Zaman seriyasında iki tezlikdən ibarət bir siqnal yaratdıq: biri 5 Hz, digəri isə 80 Hz.\n",
    "- `np.fft.fft()` funksiyasını istifadə edərək FFT-ni tətbiq etdik.\n",
    "- FFT nəticələrini tezlik domenində vizuallaşdırdıq, bu da siqnalın tərkibində olan tezlikləri göstərir.\n",
    "\n",
    "### Fourier Transformun Xüsusiyyətləri və Üstünlükləri\n",
    "- **Tezlik analizində güclü vasitədir**: Periodik strukturları və siqnalda dominant tezlikləri aşkar etmək üçün istifadə olunur.\n",
    "- **Siqnalın filtrasiyası**: FFT istifadə edərək səs-küyü çıxarıb, siqnalı təmizləyə bilərsiniz.\n",
    "- **Tətbiq sahələri**: Fourier Transform siqnal emalı, səs-küy filtrasiyası, audio siqnalları, görüntü emalı və hətta maliyyə zaman seriyalarında tezlik analizləri üçün geniş istifadə olunur.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "**Fourier Transform** və onun daha sürətli versiyası olan **Fast Fourier Transform (FFT)**, siqnalın və zaman seriyalarının tezlik domenində təhlil edilməsinə imkan verir. Bu metod zaman seriyalarında dövrlərin aşkarlanması, təmizlənməsi və əsas tezlik komponentlərinin aşkar edilməsi üçün əvəzsiz bir alətdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d525537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea64f92",
   "metadata": {},
   "source": [
    "## Optuna ilə Hiperparametr Optimizasiyası"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e98fc6",
   "metadata": {},
   "source": [
    "### Optuna ilə Hiperparametr Optimizasiyası\n",
    "\n",
    "**Optuna**, açıq mənbə kodlu bir kitabxana olub, maşın öyrənməsi modellərində hiperparametrlərin avtomatik optimizasiyasını təmin edir. Modelin performansını yaxşılaşdırmaq üçün optimal hiperparametrləri tapmaq vacibdir və bu proses çox vaxt əl ilə həyata keçirildikdə vaxt aparıcı olur. **Optuna** bu prosesi avtomatlaşdırır və daha səmərəli edir.\n",
    "\n",
    "Optuna-nın əsas xüsusiyyəti onun **Bayesian Optimization** texnikasına əsaslanan səmərəli hiperparametr axtarışıdır. Bu texnika random axtarışdan fərqli olaraq daha ağıllı şəkildə axtarış sahəsini daraldır və daha sürətli nəticələrə çatmağa kömək edir.\n",
    "\n",
    "#### Optuna-nın iş prinsipi:\n",
    "1. **Objective Funksiyası:** Hər bir optimizasiya prosesində bir objective funksiyası yazılır. Bu funksiya hansı hiperparametrlərin sınaqdan keçirildiyini və modelin performansının necə qiymətləndirildiyini müəyyən edir.\n",
    "2. **Study:** Optimizasiya prosesi bir **study** daxilində həyata keçirilir. Bu study müxtəlif **trial**-lardan ibarətdir və hər bir trial hiperparametrlərin fərqli bir konfiqurasiyası ilə modelin təlimini və testini həyata keçirir.\n",
    "3. **Trial:** Hər bir trial üçün müxtəlif hiperparametr dəyərləri təyin edilir və objective funksiyasına ötürülür.\n",
    "\n",
    "### Optuna ilə Hiperparametr Optimizasiyası Kodu\n",
    "\n",
    "Aşağıda Optuna ilə hiperparametr optimizasiyasının sadə bir nümunəsini təqdim edirəm. Bu nümunədə **NeuralProphet** modeli üçün **learning rate** hiperparametrini optimizasiya edirik və hər trial üçün learning rate dəyərini çap edirik:\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "# Objective funksiyası\n",
    "def objective(trial):\n",
    "    # Hər bir trial üçün learning rate dəyəri təyin olunur\n",
    "    lr = trial.suggest_loguniform('learning_rate', 0.001, 0.9)\n",
    "    \n",
    "    # NeuralProphet modelini qururuq\n",
    "    model = NeuralProphet(\n",
    "        growth=\"linear\",\n",
    "        changepoints=None,\n",
    "        n_changepoints=5,\n",
    "        changepoints_range=0.8,\n",
    "        trend_reg=0,\n",
    "        trend_reg_threshold=False,\n",
    "        yearly_seasonality=\"auto\",\n",
    "        weekly_seasonality=\"auto\",\n",
    "        daily_seasonality=\"auto\",\n",
    "        seasonality_mode=\"additive\",\n",
    "        seasonality_reg=0,\n",
    "        n_forecasts=1,\n",
    "        n_lags=0,\n",
    "        loss_func=\"Huber\",\n",
    "        normalize=\"auto\",\n",
    "        impute_missing=True\n",
    "    )\n",
    "    \n",
    "    # Hər trial-da seçilən learning rate dəyərini çap edirik\n",
    "    print(f\"Current trial learning rate: {lr}\")\n",
    "    \n",
    "    # Modeli fit edirik\n",
    "    metrics = model.fit(y, learning_rate=lr, epochs=40, freq=\"MS\")\n",
    "    \n",
    "    # Zərərləri qiymətləndiririk\n",
    "    if len(metrics['Loss']) > 0:\n",
    "        return metrics['Loss'].iloc[-1]\n",
    "    else:\n",
    "        raise ValueError(\"Loss seriyası boşdur.\")\n",
    "\n",
    "# Study prosesini başlatmaq\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Ən yaxşı learning rate dəyərini çap edirik\n",
    "print(\"Best learning rate: \", study.best_params)\n",
    "```\n",
    "\n",
    "### İzah:\n",
    "1. **Objective Funksiyası:** Hər bir trial zamanı `objective()` funksiyası çağırılır. Bu funksiya modelin qurulmasını və `learning_rate` kimi hiperparametrlərin sınanmasını təmin edir.\n",
    "2. **Hiperparametr Axtarışı:** `trial.suggest_loguniform('learning_rate', 0.001, 0.9)` ilə hər trial üçün fərqli `learning_rate` dəyəri seçilir. `loguniform` funksiyası log miqyasda rastgele bir dəyər seçir, 0.001 və 0.9 aralığında.\n",
    "3. **Learning Rate Çapı:** `print(f\"Current trial learning rate: {lr}\")` sətiri hər bir trial-da seçilən `learning_rate` dəyərini konsola çap edir.\n",
    "4. **Fit Prosesi:** Model təlim edilir (`fit` funksiyası ilə) və sonra əldə olunan `Loss` metric-i optimizasiya məqsədi ilə geri qaytarılır.\n",
    "5. **Optuna Study:** `optuna.create_study(direction='minimize')` ilə optimizasiya prosesi yaradılır. `study.optimize()` isə müəyyən sayda trial icra edir (bu nümunədə 10).\n",
    "\n",
    "### Digər Modellərə Uyğunlaşdırma\n",
    "\n",
    "Əgər başqa bir model, məsələn **XGBoost** və ya **RandomForest** ilə işləyirsinizsə, eyni metodologiya tətbiq edilə bilər, sadəcə model konfiqurasiyası və `fit` prosesi fərqli olacaq. Aşağıda **XGBoost** ilə eyni optimizasiya prosesini göstərirəm:\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Objective funksiyası\n",
    "def objective(trial):\n",
    "    # Hər bir trial üçün learning rate dəyəri təyin edilir\n",
    "    lr = trial.suggest_loguniform('learning_rate', 0.001, 0.9)\n",
    "    \n",
    "    # XGBoost modelini qururuq\n",
    "    model = XGBRegressor(learning_rate=lr, n_estimators=trial.suggest_int('n_estimators', 50, 300))\n",
    "    \n",
    "    # Hər trial-da seçilən learning rate dəyərini çap edirik\n",
    "    print(f\"Current trial learning rate: {lr}\")\n",
    "    \n",
    "    # Modeli fit edirik\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Proqnozları əldə edib RMSE-ni hesablayırıq\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    \n",
    "    # RMSE geri qaytarılır\n",
    "    return rmse\n",
    "\n",
    "# Study prosesini başlatmaq\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Ən yaxşı parametrləri çap edirik\n",
    "print(\"Best parameters: \", study.best_params)\n",
    "```\n",
    "\n",
    "Bu nümunədə **XGBoost** modelini istifadə edərək learning rate və n_estimators kimi hiperparametrləri optimizasiya edirik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1aa061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab5ab797",
   "metadata": {},
   "source": [
    "### interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e331813",
   "metadata": {},
   "source": [
    "Pandas-da `interpolate()` metodu itkin dəyərləri doldurmaq üçün müxtəlif interpolasiya üsulları təklif edir. Hansı interpolasiya metodunun istifadə ediləcəyi datanın növünə və ehtiyacınıza bağlıdır. Aşağıda ən çox istifadə olunan interpolasiya metodlarını və hansı hallarda istifadə edilə biləcəyinizi izah edirəm:\n",
    "\n",
    "### 1. **`method='linear'` (Xətti interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     İtkin dəyərləri qonşu nöqtələr arasında xətti bir əlaqə ilə interpolasiya etmək üçün uyğundur. Əgər məlumatlarınız zamana bağlıdırsa və davamlı dəyişir, xətti interpolasiya faydalı ola bilər.\n",
    "   - **Nümunə:**\n",
    "     Temperatur, maliyyə göstəriciləri və ya zaman seriyalarında istifadə edilə bilər.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='linear')\n",
    "   ```\n",
    "\n",
    "### 2. **`method='time'` (Zamana əsaslanan interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Vaxt seriyası məlumatlarında, indeksdəki zaman məlumatlarına əsasən interpolasiya edilir.\n",
    "   - **Nümunə:**\n",
    "     Zaman fərqlərinə uyğun interpolasiya etmək üçün istifadə olunur.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='time')\n",
    "   ```\n",
    "\n",
    "### 3. **`method='polynomial'` (Polinomial interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Dəyərlər arasında qeyri-xətti bir əlaqə varsa, polinomial interpolasiya daha uyğun ola bilər. Bu metod çoxlu dərəcəli əyri interpolasiyası yaradır.\n",
    "   - **Nümunə:**\n",
    "     Məsələn, ekonometrik modellərdə və ya fiziologiya məlumatlarında yüksək dərəcəli interpolasiya üçün istifadə edilə bilər.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='polynomial', order=2)  # 2-ci dərəcəli polinom\n",
    "   ```\n",
    "\n",
    "### 4. **`method='spline'` (Spline interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Daha hamar əyri interpolasiyası yaratmaq üçün spline interpolasiyası istifadə olunur. `polynomial` metoduna bənzəyir, lakin daha düzgün əyri yaratmaq üçün uyğundur.\n",
    "   - **Nümunə:**\n",
    "     Qeyri-xətti və mürəkkəb dəyişikliklər olan datasetlərdə.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='spline', order=2)\n",
    "   ```\n",
    "\n",
    "### 5. **`method='pad'` və `method='ffill'` (Əvvəlki dəyərlə doldurma):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     İtkin dəyərləri əvvəlki dəyərlə doldurur. Verilənlər ardıcıl olaraq monoton şəkildə dəyişirsə, bu metod faydalıdır.\n",
    "   - **Nümunə:**\n",
    "     Müştəri məlumatları və ya ardıcıl zaman seriyaları.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='pad')\n",
    "   ```\n",
    "\n",
    "### 6. **`method='backfill'` və ya `method='bfill'` (Sonrakı dəyərlə doldurma):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     İtkin dəyərləri sonrakı dəyərlə doldurur. Gələcək dəyərlərə əsaslanan interpolasiya üçün uyğundur.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='bfill')\n",
    "   ```\n",
    "\n",
    "### 7. **`method='nearest'` (Ən yaxın dəyərlə interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     İtkin dəyəri ona ən yaxın olan mövcud dəyərlə doldurur. Diskret məlumatlarda istifadə üçün idealdır.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='nearest')\n",
    "   ```\n",
    "\n",
    "### 8. **`method='zero'` (Addım funksiyası interpolasiyası):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Dəyərləri dəyişməz saxlayır və addım-addım interpolasiya edir. Xüsusilə addım dəyişikliklərin olduğu proseslər üçün uyğundur.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='zero')\n",
    "   ```\n",
    "\n",
    "### 9. **`method='slinear'` (Sadə xətti interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Xətti interpolasiya ilə sadə bir şəkildə iki nöqtə arasında interpolasiya edir.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='slinear')\n",
    "   ```\n",
    "\n",
    "### 10. **`method='quadratic'` (Kvadratik interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     İkinci dərəcəli interpolasiya ilə əyri interpolasiyası üçün istifadə olunur.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='quadratic')\n",
    "   ```\n",
    "\n",
    "### 11. **`method='cubic'` (Kub interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Üçüncü dərəcəli interpolasiya tətbiq edir və daha hamar interpolasiya yaradır.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='cubic')\n",
    "   ```\n",
    "\n",
    "### 12. **`method='barycentric'` (Barycentrik interpolasiya):**\n",
    "   - **Nə vaxt istifadə etməli:** \n",
    "     Kompleks çoxnöqtəli interpolasiya üçün uyğundur.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='barycentric')\n",
    "   ```\n",
    "\n",
    "### **Digər Parametrlər:**\n",
    "- **`limit`**: Doldurulacaq maksimum itkin dəyər sayını məhdudlaşdırmaq üçün istifadə edilir.\n",
    "   ```python\n",
    "   df['column'].interpolate(method='linear', limit=2)\n",
    "   ```\n",
    "\n",
    "### Nəticə:\n",
    "- **Diskret dəyərlər** üçün: `nearest`, `zero`\n",
    "- **Xətti dəyişikliklər** üçün: `linear`, `slinear`\n",
    "- **Qeyri-xətti dəyişikliklər** üçün: `quadratic`, `cubic`, `polynomial`, `spline`\n",
    "- **Sadə interpolasiya** üçün: `pad`, `backfill`\n",
    "\n",
    "Bu interpolasiya metodlarını datasınızdakı məlumatların təbiətinə və ehtiyacınıza uyğun seçə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0c139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb03a0e",
   "metadata": {},
   "source": [
    "## adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3a541",
   "metadata": {},
   "source": [
    "Aşağıda ADF (Augmented Dickey-Fuller) testinin yerinə yetirildiyi kodun hər bir sətrinin nə etdiyini izah edən şərhləri Azərbaycan dilində əlavə etdim. Həmçinin, nəticələrin izahını da verdim:\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller  # ADF testini yerinə yetirmək üçün lazımlı funksiyanı yükləyir\n",
    "\n",
    "# ADF Test\n",
    "result = adfuller(df.value.values, autolag='AIC')  # 'value' sütununu istifadə edərək ADF testini yerinə yetirir\n",
    "print(f'ADF Statistic: {result[0]}')  # ADF statistik dəyərini çap edir\n",
    "print(f'p-value: {result[1]}')  # p-dəyərini çap edir\n",
    "for key, value in result[4].items():  # Tənqidi dəyərləri çap etmək üçün döngü\n",
    "    print('Critial Values:')  # 'Critial Values' başlığını çap edir\n",
    "    print(f'   {key}, {value}')  # Tənqidi dəyəri və onun müvafiq dəyərini çap edir\n",
    "```\n",
    "\n",
    "### Nəticələrin İzahtı:\n",
    "\n",
    "1. **ADF Statistic: 3.14518568930675**:\n",
    "   - Bu dəyər, test edilən zaman seriyasının stasionar olub olmadığını göstərir. ADF statistikası daha yüksək olduqda (müsbət), zaman seriyası stasionar olmaya bilər. Bu dəyər, sıfır hipotezini (stasionar olmayan) yoxlamaq üçün istifadə olunur.\n",
    "\n",
    "2. **p-value: 1.0**:\n",
    "   - P-dəyəri, sıfır hipotezinin (zaman seriyasının stasionar olmaması) qəbul edilməsi üçün ehtimalı göstərir. 1.0 dəyəri, sıfır hipotezinin qəbul edilməsinin çox yüksək olduğunu göstərir, yəni zaman seriyası stasionar deyil.\n",
    "\n",
    "3. **Critical Values**:\n",
    "   - **1%: -3.465620397124192**: Bu dəyər, ADF testinin 1% əhəmiyyət səviyyəsində stasionarlıq üçün tənqidi dəyəridir. ADF statistikası bu dəyərdən aşağı olmalıdır ki, sıfır hipotezini (stasionar olmama) rədd edək.\n",
    "   - **5%: -2.8770397560752436**: 5% əhəmiyyət səviyyəsində tənqidi dəyərdir. ADF statistikası bu dəyərdən aşağı olmalıdır ki, stasionarlıq təsdiqlənsin.\n",
    "   - **10%: -2.5750324547306476**: 10% əhəmiyyət səviyyəsində tənqidi dəyərdir.\n",
    "\n",
    "### Ümumi Yekun:\n",
    "ADF testinin nəticələri, test olunan zaman seriyasının stasionar olmadığını göstərir. Çünki ADF statistik dəyəri (3.145) bütün tənqidi dəyərlərdən (1%, 5%, 10%) yuxarıdır və p-dəyəri (1.0) sıfır hipotezini qəbul etməyə işarə edir. Bu, zaman seriyasının qeyri-stasionar olduğunu bildirir və bu halda zaman seriyasının transformasiyası (məsələn, fərqləndirmə) lazım ola bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d006",
   "metadata": {},
   "source": [
    "---\n",
    "Tənqidi dəyərlər (critical values) ADF (Augmented Dickey-Fuller) testinin nəticələrini qiymətləndirmək üçün istifadə olunan əhəmiyyət səviyyələridir. Bu dəyərlər, testin sıfır hipotezini (zaman seriyasının stasionar olmaması) rədd etmək üçün nə qədər əhəmiyyətli olduğunu müəyyən edir. Gəlin daha ətraflı izah edək:\n",
    "\n",
    "### Tənqidi Dəyərlərin Anlayışı\n",
    "\n",
    "- **Tənqidi Dəyər**: ADF testinin nəticəsi olan ADF statistik dəyəri ilə müqayisə edilən dəyərdir. Bu dəyər, zaman seriyasının stasionar olub olmadığını müəyyən etmək üçün istifadə olunur.\n",
    "\n",
    "- **Əhəmiyyət Səviyyələri**:\n",
    "  - **1% Əhəmiyyət Səviyyəsi**: Bu, tənqidi dəyərin ADF testinin 1% əhəmiyyət səviyyəsindəki qiymətidir. Əgər ADF statistik dəyəri bu dəyərdən aşağıdırsa, sıfır hipotezini (stasionar olmama) 1% risk səviyyəsi ilə rədd edə bilərsiniz. Yəni, nəticəniz 1% əhəmiyyət səviyyəsində güvənlidir.\n",
    "  - **5% Əhəmiyyət Səviyyəsi**: Bu, 5% risk səviyyəsində tənqidi dəyərdir. ADF statistik dəyəri bu dəyərdən aşağıdırsa, sıfır hipotezini 5% risk səviyyəsi ilə rədd edə bilərsiniz.\n",
    "  - **10% Əhəmiyyət Səviyyəsi**: 10% risk səviyyəsində tənqidi dəyərdir. ADF statistik dəyəri bu dəyərdən aşağı olarsa, sıfır hipotezini 10% risk səviyyəsi ilə rədd edə bilərsiniz.\n",
    "\n",
    "### Nəticələrin İzahtı\n",
    "\n",
    "- **ADF Statistik Dəyəri**: Sizin test nəticənizdə ADF statistik dəyəri 3.145-dir.\n",
    "- **Tənqidi Dəyərlər**:\n",
    "  - 1%: -3.4656\n",
    "  - 5%: -2.8770\n",
    "  - 10%: -2.5750\n",
    "\n",
    "### Qiymətləndirmə\n",
    "\n",
    "- **Stasionarlıq Rəddi**: Əgər ADF statistik dəyəri bu tənqidi dəyərlərdən aşağıdırsa (məsələn, 1%, 5% və ya 10% tənqidi dəyər), zaman seriyası stasionar deyil və sıfır hipotezini rədd edirsiniz.\n",
    "- **Stasionarlığın Qəbul Edilməsi**: Əgər ADF statistik dəyəri tənqidi dəyərlərdən yuxarıdırsa, sıfır hipotezini qəbul edirsiniz (yəni, zaman seriyası stasionardır).\n",
    "\n",
    "### Sizin Nəticələriniz\n",
    "Sizin ADF statistik dəyəriniz (3.145) bütün tənqidi dəyərlərdən (1%, 5%, 10%) yuxarıdır, bu da zaman seriyanızın stasionar olmadığını göstərir. Yəni, sıfır hipotezini qəbul edirsiniz. \n",
    "\n",
    "Ümid edirəm ki, bu izahat sizə tənqidi dəyərləri daha yaxşı başa düşməyə kömək edəcək! Hər hansı sualınız varsa, soruşmaqdan çəkinməyin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddbfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9ea172f",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ad9a0",
   "metadata": {},
   "source": [
    "Aktivasiya funksiyaları, neyron şəbəkələrində hər bir neyronun digərinə olan təsirini tənzimləyən əsas elementlərdir. Bu funksiyalar, bir neyronun çıxışını müəyyənləşdirərək onun növbəti qatlarda ötürülməsinə və şəbəkənin qeyri-xətti problemləri öyrənməsinə imkan yaradır. Aktivasiya funksiyalarını seçmək neyron şəbəkəsinin performansına və doğruluğuna əhəmiyyətli dərəcədə təsir edir. Gəlin geniş yayılmış aktivasiya funksiyalarını təfərrüatı ilə nəzərdən keçirək.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1. Linear Aktivasiya Funksiyası\n",
    "\n",
    "Linear aktivasiya funksiyası neyron şəbəkələrdə istifadə olunur, xüsusilə də regresiya problemlərində. Linear aktivasiya funksiyası giriş dəyərini olduğu kimi çıxışa verir, yəni burada hər hansı bir qeyri-xətti çevrilmə baş vermir. \n",
    "\n",
    "**Tənlik**: \\( f(x) = x \\)\n",
    "\n",
    "Linear aktivasiya funksiyası əsasən son çıxış qatında (output layer) istifadə olunur və modelin giriş-çıxış arasında sadə xətti əlaqə qurmasına imkan yaradır. Regressiya problemlərində modelin davamlı ədədi dəyərlər verməsi lazım olduğu hallarda linear funksiyadan istifadə edilir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Sadədir və çıxışı heç bir dəyişiklik etmədən olduğu kimi verir.\n",
    "  - Regressiya kimi problemlərdə giriş və çıxış arasındakı xətti əlaqəni öyrənmək üçün idealdır.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Qeyri-xətti çevrilmə təmin etmədiyi üçün yalnız xətti problemlər üçün uyğundur.\n",
    "  - Şəbəkəyə mürəkkəb, qeyri-xətti strukturları öyrənməyə imkan vermir, bu da onun dərin qatlarda tətbiqini məhdudlaşdırır.\n",
    "  - Bir neçə linear qat üst-üstə əlavə edildikdə, nəticə yenə də xətti olur. Yəni, bir neçə linear funksiya qatını birləşdirmək kompleks modellər üçün faydalı olmur.\n",
    "\n",
    "### Linear Funksiyanın İstifadəsi\n",
    "Çox zaman, dərin neyron şəbəkələrdə linear aktivasiya funksiyası yalnız çıxış qatında tətbiq edilir və ara qatlarda (hidden layers) qeyri-xətti funksiyalar istifadə olunur. Bu, şəbəkənin kompleks əlaqələri öyrənməsi üçün vacibdir. Lakin çıxışın davamlı ədədi dəyərləri verməsi lazım olduqda linear funksiya optimal seçimdir. \n",
    "\n",
    "Linear funksiyanı istifadə etmək, modelin xətti əlaqəni daha yaxşı öyrənməsinə və nisbətən sadə strukturlu problemləri həll etməsinə imkan yaradır.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. Sigmoid Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "\n",
    "Sigmoid funksiyası giriş dəyərlərini 0 ilə 1 arasında normallaşdırır. Bu, xüsusilə ehtimal çıxışlarına ehtiyac olan model tətbiqlərində istifadə olunur.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Sadədir və çıxışları 0-1 arasında normallaşdırır.\n",
    "  - Klassifikasiya problemlərində ehtimal kimi şərh edilə bilən nəticələr verir.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Doyma problemindən əziyyət çəkir; yəni böyük müsbət və mənfi girişlərdə funksiya demək olar ki, sabit olur.\n",
    "  - Gradientlər çox kiçik olur, buna görə də **vanishing gradient problem** (itən gradient problemi) yaranır.\n",
    "  - Çıxış aralığı sıfırdan fərqli olduğundan şəbəkə oyanışları qeyri-mərkəzləşmiş olur.\n",
    "\n",
    "### 3. Tanh (Hyperbolic Tangent) Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "\n",
    "Tanh funksiyası girişləri -1 ilə 1 arasında xəritələndirir. Bu funksiya da sigmoidə bənzərdir, amma çıxış aralığı -1 ilə 1 arasında olduğundan şəbəkəni mərkəzləşdirir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Mərkəzləşmiş çıxış verdiyinə görə optimizasiya prosesini daha da asanlaşdırır.\n",
    "  - Geniş tətbiq sahəsinə malikdir və bir çox hallarda sigmoidə üstünlük verilir.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Yenə də itən gradient problemi ilə qarşılaşır.\n",
    "  - Çıxışlar doyana qədər doymaya meyillidir.\n",
    "\n",
    "### 4. ReLU (Rectified Linear Unit) Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = \\max(0, x) \\)\n",
    "\n",
    "ReLU funksiyası son illərdə neyron şəbəkələrdə ən geniş istifadə edilən aktivasiya funksiyasıdır. ReLU yalnız müsbət girişlər üçün açılır və mənfi dəyərləri sıfıra çevirir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Sadeliyi və sürətliliyi ilə seçilir.\n",
    "  - **Vanishing gradient** problemindən əziyyət çəkmir, buna görə də daha dərinə gedən şəbəkələr üçün uyğun gəlir.\n",
    "  - Modelin öyrənməsini sürətləndirir.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - **Dying ReLU** problemi var; bəzi neyronlar sıfır dəyərinə \"ilişib\" qalır və daha öyrənmir.\n",
    "  - Daha böyük dəyərlərdə daha yüksək aktivasiya yarada biləcəyindən qeyri-sabit çıxışlar yarada bilər.\n",
    "\n",
    "### 5. Leaky ReLU Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = x \\) (x > 0),  \\( f(x) = 0.01x \\) (x ≤ 0)\n",
    "\n",
    "Leaky ReLU, ReLU funksiyasının dəyişmiş versiyasıdır. Bu funksiya, mənfi giriş dəyərlərinə kiçik bir əmsalla çıxış verir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - **Dying ReLU** problemini aradan qaldırmağa kömək edir.\n",
    "  - Mənfi girişlər üçün kiçik dəyərlər təyin edərək öyrənmə qabiliyyətini artırır.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Mənfi girişlər üçün təsadüfi bir əmsal seçilməsi modelin konvergensiyasına təsir göstərə bilər.\n",
    "  \n",
    "### 6. Softmax Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}} \\)\n",
    "\n",
    "Softmax çox sinifli klassifikasiya problemlərində çıxışları ehtimal dəyərlərinə çevirmək üçün istifadə olunur. Bu funksiya, bir neçə sinifə bölünmə hallarını 1-ə bərabər bir ehtimala çevirərək sinif ehtimallarını verir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Çox sinifli problemlər üçün uyğun gəlir.\n",
    "  - Sinif ehtimallarını verir, bununla da hansı sinifin daha çox ehtimalla təyin olunduğunu göstərir.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - İtən gradient problemi yarana bilər.\n",
    "  - Siniflər arasında asılılıq olduğunda daha mürəkkəb analizlər tələb olunur.\n",
    "\n",
    "### 7. Swish Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = x \\cdot \\text{sigmoid}(x) \\)\n",
    "\n",
    "Swish funksiyası Google tərəfindən təqdim olunub və ReLU ilə bənzərliklər göstərir, lakin daha axıcıdır. Funksiya daha yüksək dəqiqlik və konvergensiya sürəti verir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Gradientlərin davamlı olduğu üçün daha dərin şəbəkələrdə daha yaxşı performans göstərir.\n",
    "  - Modelin doğruluğunu artırır və itən gradient problemini qismən aradan qaldırır.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Hesablama baxımından daha ağırdır.\n",
    "  \n",
    "### 8. GELU (Gaussian Error Linear Unit) Aktivasiya Funksiyası\n",
    "**Tənlik**: \\( f(x) = x \\cdot \\Phi(x) \\), burada \\( \\Phi(x) \\) normal paylanmış yığılmış ehtimaldır.\n",
    "\n",
    "GELU, Transformer modellərində istifadə olunan məşhur bir funksiyadır. Girişləri ehtimal əsaslı keçid ilə çıxışa çevirməklə təkmilləşdirilmiş öyrənmə xüsusiyyətləri təmin edir.\n",
    "\n",
    "- **Üstünlüklər**:\n",
    "  - Dərin modellərdə yüksək performans göstərir.\n",
    "  - Gradientlər daha yaxşı idarə olunur və modelin konvergensiya sürətini artırır.\n",
    "\n",
    "- **Çatışmazlıqlar**:\n",
    "  - Daha mürəkkəbdir və hesablamalar daha çox vaxt tələb edir.\n",
    "\n",
    "### Aktivasiya Funksiyalarının Seçilməsi\n",
    "Şəbəkənin təbiətinə görə aktivasiya funksiyasını seçmək performansa böyük təsir göstərir. Sadə klassifikasiya üçün **sigmoid** və **softmax**, dərin şəbəkələr üçün **ReLU**, **Leaky ReLU** və **GELU** daha uyğundur. Çox sinifli problemlərdə softmax tətbiq olunur, regressiya və digər mürəkkəb tətbiqlərdə isə tanh, ReLU və onun müxtəlif versiyalarından istifadə edilir. \n",
    "\n",
    "Bu funksiyaların məqsədi şəbəkəni qeyri-xətti hala gətirərək daha mürəkkəb və çox ölçülü strukturların öyrənilməsini təmin etməkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d203f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "722dca60",
   "metadata": {},
   "source": [
    "## Linear və Non-Linear Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a1c3f",
   "metadata": {},
   "source": [
    "Neyron şəbəkələrdə aktivasiya funksiyaları xətti (linear) və qeyri-xətti (non-linear) olaraq iki əsas kateqoriyaya bölünür. Hər iki növün tətbiqi müxtəlifdir və şəbəkənin öyrənmə imkanlarını fərqli şəkildə təsir edir.\n",
    "\n",
    "### Xətti (Linear) Aktivasiya Funksiyaları\n",
    "Xətti aktivasiya funksiyaları giriş və çıxış arasında xətti əlaqə saxlayır, yəni girişin sadəcə bir əmsalla vurulması və ya sabit bir ədədin əlavə olunması ilə çıxış alınır. Bu funksiyalar yalnız xətti əlaqələri modelləşdirə bilər və mürəkkəb, qeyri-xətti nümunələri öyrənmək üçün əlverişli deyillər.\n",
    "\n",
    "1. **Linear Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x) = x \\)\n",
    "   - Giriş olduğu kimi çıxışa ötürülür. Çox vaxt son çıxış qatında, xüsusən də regresiya problemlərində istifadə olunur.\n",
    "  \n",
    "2. **Identity Funksiyası**\n",
    "   - Eyni zamanda linear funksiyanın bir növü olaraq qəbul edilir.\n",
    "   - **Tənlik**: \\( f(x) = x \\)\n",
    "\n",
    "### Qeyri-Xətti (Non-Linear) Aktivasiya Funksiyaları\n",
    "Qeyri-xətti aktivasiya funksiyaları şəbəkənin mürəkkəb, qeyri-xətti nümunələri öyrənməsinə imkan verir və hər qat arasındakı əlaqənin xətti olmamasını təmin edir. Bu funksiyalar dərin neyron şəbəkələrdə istifadə olunur və kompleks problemləri həll etmək üçün vacibdir.\n",
    "\n",
    "1. **Sigmoid Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "   - Çıxış dəyərləri 0 ilə 1 arasında olur. Əsasən, klassifikasiya problemlərində ehtimallar kimi istifadə olunur.\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent) Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "   - Çıxış -1 ilə 1 arasında dəyişir və mərkəzləşdirilmiş nəticələr verir.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit) Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x) = \\max(0, x) \\)\n",
    "   - Mənfi dəyərləri sıfıra çevirir, müsbətləri olduğu kimi ötürür. Dərin şəbəkələrdə geniş istifadə edilir.\n",
    "\n",
    "4. **Leaky ReLU**\n",
    "   - **Tənlik**: \\( f(x) = x \\) (x > 0), \\( f(x) = 0.01x \\) (x ≤ 0)\n",
    "   - Mənfi dəyərlər üçün kiçik bir əmsalla nəticə verir, bu da **Dying ReLU** problemini azaltmağa kömək edir.\n",
    "\n",
    "5. **Softmax Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}} \\)\n",
    "   - Çox sinifli klassifikasiya üçün istifadə edilir və sinif ehtimallarını 0 ilə 1 arasında təmin edir.\n",
    "\n",
    "6. **Swish Aktivasiya Funksiyası**\n",
    "   - **Tənlik**: \\( f(x) = x \\cdot \\text{sigmoid}(x) \\)\n",
    "   - Səlis keçid təmin edərək dərin şəbəkələrdə performansı artırır.\n",
    "\n",
    "7. **GELU (Gaussian Error Linear Unit)**\n",
    "   - **Tənlik**: \\( f(x) = x \\cdot \\Phi(x) \\), burada \\( \\Phi(x) \\) normal paylanmış yığılmış ehtimaldır.\n",
    "   - Transformer modellərində istifadə olunur və davamlı, yaxşı idarə olunan gradientlər təmin edir.\n",
    "\n",
    "### Xətti və Qeyri-Xətti Funksiyaların Seçimi\n",
    "- **Xətti funksiyalar** yalnız giriş və çıxış arasında xətti əlaqə yaratmaq üçün istifadə olunur. Buna görə də əsasən regresiya problemlərində və modelin yalnız bir xətti çevrilmə ehtiyac duyduğu hallarda tətbiq edilir.\n",
    "- **Qeyri-xətti funksiyalar** isə şəbəkəyə mürəkkəb strukturları öyrənməyə imkan verir. Dərin öyrənmə modellərində qat-qat mürəkkəb əlaqələr yaratmaq üçün əsasən qeyri-xətti funksiyalara üstünlük verilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eed9175a",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34e8f2",
   "metadata": {},
   "source": [
    "**Convolution layer** (konvolusiya qatı), konvolusiya neyron şəbəkələrində (CNN) istifadə olunan əsas qatdır və şəkillər kimi strukturlaşdırılmış verilənlərdən xüsusiyyətləri çıxarmaq üçün nəzərdə tutulub. Bu qat, kiçik bir **kernel** və ya **filtr** matrisini giriş məlumatının üzərində sürüşdürərək (sliding) xüsusiyyətləri aşkar edir, məsələn, kənarlar, toxumalar və mürəkkəb naxışlar. \n",
    "\n",
    "### Convolution Layer necə işləyir?\n",
    "1. **Kernelin sürüşdürülməsi**:\n",
    "   - Kernel matrisləri (məsələn, 3x3 ölçüsündə) giriş matrisinin üzərində müəyyən addımlarla (stride) sürüşdürülür.\n",
    "   \n",
    "2. **Konvolusiya əməliyyatı**:\n",
    "   - Hər bir sürüşmə addımında kernel ilə girişin uyğun hissəsi arasında çarpma və toplama aparılır və bu nəticə çıxış matrisinin müvafiq hüceyrəsinə yerləşdirilir.\n",
    "   \n",
    "3. **Çıxış xüsusiyyət xəritəsi**:\n",
    "   - Nəticədə alınan çıxış matrisi **xüsusiyyət xəritəsi** (feature map) adlanır. Bu xəritə, girişin müxtəlif xüsusiyyətlərini (kənarları, rəng dəyişmələri və formaları) təmsil edir.\n",
    "\n",
    "### Convolution Layer-in əsas komponentləri:\n",
    "- **Kernel (Filtr)**: Kiçik ölçülü matrislərdir və hər biri fərqli xüsusiyyətləri çıxarmağa kömək edir.\n",
    "- **Stride (Addım ölçüsü)**: Kernelin giriş matrisi üzərində hərəkət edərkən addım ölçüsüdür.\n",
    "- **Padding**: Giriş matrisinin kənarına sıfır əlavə edilərək məlumat itkisini minimuma endirmək üçün istifadə olunur.\n",
    "- **Aktivasiya funksiyası**: Adətən, `ReLU` (Rectified Linear Unit) kimi qeyri-xətti funksiyalar xüsusiyyət xəritəsinə tətbiq edilir ki, mürəkkəb nümunələr tanınsın.\n",
    "\n",
    "### Convolution Layer nə üçün vacibdir?\n",
    "Bu qat, böyük ölçülü şəkil məlumatlarını daha kiçik və idarə olunan xüsusiyyət xəritələrinə çevirməyə kömək edir. CNN-lərdə ilk qatlar sadə xüsusiyyətləri (kənarları, rəngləri) öyrənir, daha dərin qatlar isə mürəkkəb obyektlərə aid xüsusiyyətləri tanıyır. Bu xüsusiyyətlər obyektin müxtəlif bucaqlardan belə tanınmasına imkan yaradır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32b8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867da0ab",
   "metadata": {},
   "source": [
    "## Convolutional Layer çıxış ölçüsünü hesablamaq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab75884",
   "metadata": {},
   "source": [
    "Bu düstur, çox güman ki, konvolyusiya layının çıxış ölçüsünü hesablamaq üçün istifadə olunur. Konvolyusiya təbəqələrində çıxış ölçüsünü tapmaq üçün bu düsturdan istifadə edilir:\n",
    "\n",
    "$$ \n",
    "{Output size} = \\frac{(S - F + 2P)}{\\text{stride}} + 1\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- \\( S \\) — giriş ölçüsü (en və ya hündürlük),\n",
    "- \\( F \\) — filter ölçüsü (kernel ölçüsü),\n",
    "- \\( P \\) — padding (məhdudlaşdırma qatı),\n",
    "- \\(stride\\) — addım ölçüsüdür.\n",
    "\n",
    "Bu düstur ilə konvolyusiya sonrası çıxış ölçüsünü hesablaya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a8b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f1f9a4",
   "metadata": {},
   "source": [
    "## Kernel nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5b21a",
   "metadata": {},
   "source": [
    "**Kernel** (və ya filtr), konvolusiya neyron şəbəkələrində (CNN) giriş məlumatlarından xüsusiyyətləri çıxarmaq üçün istifadə edilən kiçik matrisdir. Kernel, giriş matrisinin (məsələn, bir şəkil) üzərində sürüşdürülərək (slide) hər addımda elementar konvolusiya əməliyyatını yerinə yetirir. Bu əməliyyat nəticəsində xüsusiyyət xəritəsi (feature map) adlanan bir çıxış matrisi yaranır.\n",
    "\n",
    "### Kernel necə işləyir?\n",
    "1. **Sürüşdürmə (Sliding)**:\n",
    "   - Kernel, giriş matrisinin üzərində sürüşdürülərək hər nöqtədə kiçik bir sahəni tutur.\n",
    "   \n",
    "2. **Çarpma və toplama**:\n",
    "   - Kernel ilə həmin sahənin elementləri arasında çarpma və toplama əməliyyatı aparılır. Hər bir sürüşmə nəticəsində bir skaler dəyər əldə edilir və bu dəyər çıxış matrisinə yerləşdirilir.\n",
    "\n",
    "3. **Xüsusiyyətləri çıxarmaq**:\n",
    "   - Fərqli kerneellər fərqli xüsusiyyətləri (kənarlar, toxuma, detalları) çıxara bilər. Məsələn, bir kernel kənarları aşkar edə bilər, digəri isə şəkildəki naxışları tanıya bilər.\n",
    "\n",
    "### Kernel ölçüsü\n",
    "- Ən çox istifadə edilən kernel ölçüləri 3x3, 5x5 kimi kiçik matrislərdir. Kernel ölçüsü və sürüşdürmə addımları (stride) çıxış matrisinin ölçüsünü müəyyən edir.\n",
    "\n",
    "**Məsələn:** 3x3 kernel, 5x5 bir şəkil matrisində sürüşdürülərsə, çıxış matrisinin ölçüsü 3x3 olacaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8dcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d7b2a63",
   "metadata": {},
   "source": [
    "## Padding nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41304388",
   "metadata": {},
   "source": [
    "**Padding**, neyron şəbəkələrdə, xüsusən konvolusiya laylarında (convolutional layers) istifadə edilən bir texnikadır. Konvolusiya əməliyyatı zamanı giriş matrisinin (şəkil və ya digər məlumatlar) ətrafına əlavə sıfırlar və ya başqa dəyərlər əlavə edilir. Bunun əsas məqsədi girişin ölçüsünü saxlamaq və ya müəyyən xüsusiyyətləri qoruyaraq məlumat itkisini minimuma endirməkdir.\n",
    "\n",
    "### Padding növləri:\n",
    "1. **Valid (Sıfır padding olmadan)**:\n",
    "   - Heç bir padding tətbiq edilmir, nəticədə çıxış matrisinin ölçüsü girişdən kiçik olur.\n",
    "   \n",
    "2. **Same (Sıfır padding ilə)**:\n",
    "   - Giriş matrisinə sıfırlar əlavə edilir ki, çıxış matrisinin ölçüsü girişlə eyni olsun.\n",
    "   \n",
    "3. **Full padding**:\n",
    "   - Daha çox sıfır əlavə edilir və nəticədə çıxış ölçüsü girişdən böyük olur.\n",
    "\n",
    "**Nümunə**:\n",
    "\n",
    "Əgər giriş matrisi 3x3-dür və ətrafına sıfır əlavə edilərsə, 5x5 ölçüsündə yeni bir matrisa əldə olunur. Bu, xüsusən şəkillərdə kənar məlumatların itirilməməsi üçün faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da8faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1496e2d2",
   "metadata": {},
   "source": [
    "## stride-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb5afb",
   "metadata": {},
   "source": [
    "Stride dəyəri, xüsusən konvolyusiyalı sinir şəbəkələrində (CNN) istifadə olunur və hər bir konvolyusiya əməliyyatında filtrin nə qədər \"hərəkət\" edəcəyini göstərir. Bu dəyər iki ölçüdə - eninə və uzununa - təyin edilə bilər.\n",
    "\n",
    "Bir stride dəyəri təyin olunanda:\n",
    "\n",
    "- **Stride = 1**: Filtr hər addımda yalnız bir vahid hərəkət edir. Bu halda konvolyusiya nəticəsində yaranan çıxış matrisi çox detallı olur.\n",
    "- **Stride > 1**: Filtr bir neçə vahid hərəkət edir, bu da çıxış matrisinin ölçüsünü azaldır. Məsələn, stride = 2 olarsa, hər addımda filtr iki vahid hərəkət edər və çıxış matrisi daha kiçik, lakin daha az detalla olacaqdır.\n",
    "\n",
    "Yəni, stride dəyərlərini artırsanız, model daha az detal saxlayaraq daha kiçik çıxış matrisi yaradacaqdır. Bu, xüsusən də görüntü tanıma prosesində detalları sadələşdirmək üçün istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b7ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc65779",
   "metadata": {},
   "source": [
    "## Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68631e61",
   "metadata": {},
   "source": [
    "**Max pooling** (maksimum havuzlama), konvolusiya neyron şəbəkələrində (CNN) xüsusiyyət xəritəsinin ölçüsünü kiçiltmək və əsas xüsusiyyətləri vurğulamaq üçün istifadə edilən bir əməliyyatdır. Bu, konvolusiya qatından sonra tətbiq olunan bir aşağıya nümunələmə (downsampling) metodudur və xüsusiyyət xəritəsini daha kompakt hala gətirir, modelin hesablama yükünü azaldır.\n",
    "\n",
    "### Max Pooling necə işləyir?\n",
    "1. **Pəncərənin sürüşdürülməsi**:\n",
    "   - Max pooling əməliyyatında əvvəlcə bir pəncərə (məsələn, 2x2) xüsusiyyət xəritəsinin üzərində müəyyən bir addımla (stride) sürüşdürülür.\n",
    "   \n",
    "2. **Maksimum dəyərin seçilməsi**:\n",
    "   - Hər pəncərə sahəsindəki dəyərlər arasından maksimum dəyər seçilir və yeni, aşağı ölçülü çıxış matrisinə yazılır.\n",
    "\n",
    "3. **Ölçünü kiçiltmək**:\n",
    "   - Bu əməliyyat, xüsusiyyət xəritəsinin ölçüsünü (genişlik və hündürlüyünü) azaltmaqla yanaşı, şəkilin əsas xüsusiyyətlərini qoruyur və daha kiçik ölçülü məlumat yaradır.\n",
    "\n",
    "### Max Pooling-in əsas üstünlükləri:\n",
    "- **Ölçünün azaldılması**: Xüsusiyyət xəritəsinin ölçüsünü kiçildərək yaddaş istifadəsini və hesablama xərclərini azaldır.\n",
    "- **Yerli xüsusiyyətlərin vurğulanması**: Hər pəncərədə ən güclü siqnalı (maksimum dəyəri) saxlamaqla modelin daha vacib xüsusiyyətləri öyrənməsinə kömək edir.\n",
    "- **Overfitting-in azaldılması**: Məlumatları kiçildərək daha ümumi xüsusiyyətləri çıxarmağa kömək edir, beləliklə, modelin overfitting etmə ehtimalını azaldır.\n",
    "\n",
    "### Nümunə\n",
    "Bir 4x4 xüsusiyyət xəritəsinə 2x2 max pooling tətbiq etsək və addım ölçüsünü də (stride) 2 təyin etsək, çıxışda 2x2 ölçüsündə bir matrisa alınar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42e02df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6009502",
   "metadata": {},
   "source": [
    "## Dropout nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea48662",
   "metadata": {},
   "source": [
    "Dropout, neyron şəbəkələrində *overfitting*-in qarşısını almaq üçün istifadə olunan bir texnikadır. Bu texnika, hər təlim dövründə neyronların bir qismini təsadüfi olaraq deaktiv edir (məsələn, aktiv neyronların 20%-50%-ni). Bu, şəbəkənin məlumatdakı nümunələrə həddən artıq uyğunlaşmasının qarşısını alır və modelin daha ümumiləşdirilmiş, yəni yeni verilənlərə daha yaxşı uyğunlaşmasını təmin edir.\n",
    "\n",
    "Dropout-un iş prinsipi belədir:\n",
    "\n",
    "1. Təlim zamanı şəbəkənin müəyyən neyronları təsadüfi olaraq deaktiv edilir, yəni həmin neyronların çıxışı sıfıra bərabər olur.\n",
    "2. Deaktiv olunan neyronlar, digər neyronlarla birlikdə məlumatı ötürmür və çıxışda heç bir təsir göstərmir.\n",
    "3. Test və ya proqnozlaşdırma mərhələsində isə Dropout tətbiq edilmir, bütün neyronlar aktiv olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13047458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c849de",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a4a55",
   "metadata": {},
   "source": [
    "`Learning rate scheduler` təlim zamanı öyrənmə sürətini tənzimləmək üçün istifadə olunur. Bu, modelin performansını artırmaq və optimallaşdırmanı daha stabilləşdirmək üçün faydalıdır. Təlimin əvvəlində öyrənmə sürəti daha yüksək, sonrakı mərhələlərdə isə daha aşağı olur. \n",
    "\n",
    "PyTorch-da bir neçə məşhur `scheduler` növü var:\n",
    "\n",
    "1. **StepLR** - Hər müəyyən sayda dövrədən sonra öyrənmə sürətini azaldır.\n",
    "2. **ExponentialLR** - Öyrənmə sürətini eksponensial olaraq azaldır.\n",
    "3. **ReduceLROnPlateau** - Əgər modelin performansı müəyyən bir dövrdən sonra yaxşılaşmazsa, öyrənmə sürətini azaldır.\n",
    "\n",
    "Bir `StepLR` scheduler nümunəsi belədir:\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Optimallaşdırıcıyı müəyyənləşdiririk\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1) # Hər 10 dövrədən sonra öyrənmə sürətini 0.1 faktoruna vuraraq azaldır\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(...) # Təlim funksiyanız\n",
    "    validate(...) # Yoxlama funksiyanız\n",
    "    scheduler.step() # Hər epochdan sonra scheduler-i yeniləyir\n",
    "    \n",
    "    print(f\"Epoch: {i} | Loss: {epoch_loss / len(train_loader)} | Accuracy: {epoch_acc / len(train_loader)}% | Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "```\n",
    "\n",
    "Burada `step_size=10`, hər 10 epoch-dan bir öyrənmə sürətini azaldır. `gamma=0.1` isə öyrənmə sürətini 10 dəfə azaldır (0.1 faktor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279cd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e12ddc1",
   "metadata": {},
   "source": [
    "## Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399367d4",
   "metadata": {},
   "source": [
    "Öyrənmə sürəti cədvəlini (`Learning Rate Schedule`) tətbiq etmək üçün TensorFlow və ya PyTorch kimi kitabxanalardan istifadə edə bilərik. Aşağıda TensorFlow/Keras və PyTorch ilə ən çox istifadə olunan öyrənmə sürəti cədvəli metodlarının kod nümunələri verilmişdir.\n",
    "\n",
    "### 1. TensorFlow/Keras ilə Öyrənmə Sürəti Cədvəli\n",
    "TensorFlow/Keras-da `LearningRateScheduler` funksiyasından istifadə edərək `Step Decay`, `Exponential Decay`, `Cosine Annealing` kimi metodları təyin edə bilərsiniz.\n",
    "\n",
    "#### Step Decay\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step Decay funksiyası\n",
    "def step_decay(epoch):\n",
    "    initial_lr = 0.1\n",
    "    drop_factor = 0.5\n",
    "    epochs_drop = 10\n",
    "    lr = initial_lr * (drop_factor ** (epoch // epochs_drop))\n",
    "    return lr\n",
    "\n",
    "# LearningRateScheduler geri çağırma\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "\n",
    "# Modeli təlim edərkən geri çağırmanı əlavə edin\n",
    "model = tf.keras.models.Sequential([...])  # Modelinizi yaradın\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, callbacks=[lr_scheduler])  # Təlim\n",
    "```\n",
    "\n",
    "#### Exponential Decay\n",
    "\n",
    "```python\n",
    "initial_lr = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=1000,       # neçə addımdan bir learning rate-i azaldır\n",
    "    decay_rate=0.96,        # hər azalma addımında learning rate-ə vurulan faktor\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "#### Cosine Annealing\n",
    "TensorFlow-da Cosine Annealing tətbiqi üçün `tf.keras.experimental.CosineDecay` istifadə olunur.\n",
    "\n",
    "```python\n",
    "initial_lr = 0.1\n",
    "lr_schedule = tf.keras.experimental.CosineDecay(initial_learning_rate=initial_lr, decay_steps=1000, alpha=0.0)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "### 2. PyTorch ilə Öyrənmə Sürəti Cədvəli\n",
    "\n",
    "#### Step Decay\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Optimizer\n",
    "model = YourModel()\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# StepLR öyrənmə sürəti cədvəli\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Təlim dövrü\n",
    "for epoch in range(50):\n",
    "    train(...)  # Təlim funksiyanızı buraya əlavə edin\n",
    "    scheduler.step()  # Hər epox sonunda öyrənmə sürətini yeniləyir\n",
    "    \n",
    "    print(f\"Epoch: {i} | Loss: {epoch_loss / len(train_loader)} | Accuracy: {epoch_acc / len(train_loader)}% | Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "```\n",
    "\n",
    "#### Exponential Decay\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.96)\n",
    "\n",
    "for epoch in range(50):\n",
    "    train(...)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch: {i} | Loss: {epoch_loss / len(train_loader)} | Accuracy: {epoch_acc / len(train_loader)}% | Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "```\n",
    "\n",
    "#### Cosine Annealing\n",
    "\n",
    "```python\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50)  # T_max epox sayını göstərir\n",
    "\n",
    "for epoch in range(50):\n",
    "    train(...)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch: {i} | Loss: {epoch_loss / len(train_loader)} | Accuracy: {epoch_acc / len(train_loader)}% | Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "```\n",
    "\n",
    "Bu üsullarla learning rate-in hər epox və ya addımda necə tənzimlənəcəyini təyin edə bilərsiniz. Hər bir metod müəyyən bir konvergensiya sürətini təmin edir və modelin öyrənmə performansını artırmağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fafe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b981381",
   "metadata": {},
   "source": [
    "## Overfitting və Underfitting nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc78dba",
   "metadata": {},
   "source": [
    "**Overfitting** və **underfitting** maşın öyrənməsində modelin verilənləri düzgün öyrənməsi və ümumiləşdirmə qabiliyyətinə təsir edən əsas problemlərdəndir.\n",
    "\n",
    "### 1. Overfitting\n",
    "Overfitting, modelin təlim verilənlərinə çox yaxşı uyğunlaşması və demək olar ki, hər bir nüansı öyrənməsi nəticəsində yaranır. Bu, modelin təlim verilənləri üzərində yüksək dəqiqliyə nail olması deməkdir, lakin test verilənləri və ya yeni verilənlər üzərində performansının zəif olması ilə nəticələnir.\n",
    "\n",
    "- **Əlamətləri:**\n",
    "  - Təlim dəsti üzərində yüksək dəqiqlik, amma test dəsti üzərində aşağı dəqiqlik.\n",
    "  - Model təlim verilənlərinin təsadüfi səhvlərini və səs-küyünü öyrənir.\n",
    "\n",
    "- **Çözüm yolları:**\n",
    "  - **Daha çox məlumat toplamaq:** Modelin ümumiləşdirmə qabiliyyətini artırmaq üçün daha çox məlumat toplamaq.\n",
    "  - **Regularization:** L2 və ya L1 kimi regularization üsulları ilə parametrləri cəzalandıraraq modeli daha az mürəkkəb hala gətirmək.\n",
    "  - **Dropout:** Neyron şəbəkələrində bəzi neyronları təsadüfi olaraq deaktiv edərək modelin sadələşdirilməsini təmin edir.\n",
    "  - **Erkən dayandırma (Early Stopping):** Model təlimdə müəyyən mərhələdən sonra daha yaxşı performans göstərmirsə, təlimi dayandırmaq.\n",
    "\n",
    "### 2. Underfitting\n",
    "Underfitting, modelin həm təlim, həm də test verilənləri üzərində aşağı performans göstərməsi ilə xarakterizə olunur. Bu vəziyyətdə model məlumatdakı mürəkkəblikləri öyrənə bilmir və ümumiləşdirmə qabiliyyəti çox aşağı olur.\n",
    "\n",
    "- **Əlamətləri:**\n",
    "  - Təlim və test dəstləri üzərində aşağı dəqiqlik.\n",
    "  - Model verilənlərdəki nümunələri kifayət qədər yaxşı öyrənmir.\n",
    "\n",
    "- **Çözüm yolları:**\n",
    "  - **Daha mürəkkəb model seçmək:** Daha çox qat və ya parametrə malik model istifadə edərək verilənlərdəki daha mürəkkəb strukturları öyrənmək.\n",
    "  - **Daha çox epoch ilə təlim keçirmək:** Təlim müddətini artırmaqla modelin verilənləri daha yaxşı öyrənməsini təmin etmək.\n",
    "  - **Daha uyğun xüsusiyyətlər yaratmaq:** Əsaslı xüsusiyyət mühəndisliyi tətbiq edərək verilənlərdən daha çox məlumat çıxarmaq.\n",
    "\n",
    "### Əsas fərqlər:\n",
    "- **Overfitting**: Təlimdə yüksək performans, testdə aşağı performans.\n",
    "- **Underfitting**: Həm təlim, həm də test verilənləri üzərində aşağı performans.\n",
    "\n",
    "Bu problemləri həll etməklə, modelin verilənlərə daha yaxşı uyğunlaşmasını və daha yaxşı ümumiləşdirmə qabiliyyətinə malik olmasını təmin edə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435994f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09528b9d",
   "metadata": {},
   "source": [
    "## Model-de 0-lar onemlidirse bu halda recall yoxsa precision-na baxilmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfb57b",
   "metadata": {},
   "source": [
    "Modeldə sıfırların önəmli olduğu hallarda **recall** daha vacib göstərici olur. Əsasən, sıfır sinifi əhəmiyyətlidirsə və səhvən sıfır sinifi bir kimi təsnif etməmək vacibdirsə, recall dəyəri yüksək saxlanmalıdır. Bu halda modelin sıfır sinifinə aid nümunələri düzgün təsnif etmə qabiliyyəti diqqət mərkəzindədir. \n",
    "\n",
    "### Niyə Recall?\n",
    "- **Recall**, həqiqi müsbət nümunələrdən neçə faizinin model tərəfindən düzgün müəyyən edildiyini göstərir. \n",
    "- Sıfır sinifi daha vacibdirsə, bu sinif üçün recall-u artırmaq, modelin bu sinifdəki bütün əhəmiyyətli nümunələri qaçırmamasını təmin etməyə çalışır.\n",
    "- **False negative** (yəni, sıfır olan bir nümunənin bir olaraq təsnif edilməsi) minimuma endirilir.\n",
    "\n",
    "### Əgər Precision və Recall Əhəmiyyətli Olarsa\n",
    "Ancaq həm sıfır sinifində **yanlış pozitivləri** (sıfır olmayanı sıfır kimi təsnif etmək) həm də **yanlış neqativləri** (sıfır olanı bir kimi təsnif etmək) minimuma endirmək istəyirsinizsə, həm precision, həm də recall arasında tarazlıq yaratmaq lazım ola bilər. Bu halda **F1 score** göstəricisi istifadə edilə bilər, çünki F1 score həm recall, həm də precision-u birləşdirir və ortalama balans təmin edir.\n",
    "\n",
    "Beləliklə, sıfır sinifi model üçün əsasdırsa, **recall** üstünlük verilməli, lakin hər iki metrikanı balansda saxlamaq vacibdirsə, **F1 score** nəzərə alınmalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ac7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28498a36",
   "metadata": {},
   "source": [
    "## Model-de 1-ler onemlidirse bu halda recall yoxsa precision-na baxilmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e78c2d",
   "metadata": {},
   "source": [
    "Əgər modeldə **birlər önəmlidirsə**, yəni bir sinfinə aid olan dəyərlər daha vacibdirsə, bu halda **precision** daha önəmlidir. Bir sinifinə aid olan nümunələrin düzgün olaraq bir kimi təsnif edilməsi, yanlış pozitivləri (false positives) azaltmaqla həyata keçirilir və bu da modelin bu sinif üçün doğruluğunu artırır.\n",
    "\n",
    "### Niyə Precision?\n",
    "- **Precision**, modelin bir sinifinə aid olduğunu iddia etdiyi nümunələrdən neçəsinin həqiqətən də bir olduğunu göstərir. \n",
    "- Əgər modeldə \"bir\" sinfi əsaslıdırsa, modelin bir kimi təsnif etdiyi nümunələrin əksəriyyətinin həqiqətən də bu sinifə aid olması vacibdir.\n",
    "- Beləliklə, **yanlış pozitivlərin** (bir sinfinə aid olmayan bir dəyərin səhvən bir kimi təsnif edilməsi) sayını minimuma endirmək istəyirsinizsə, **precision** vacib rol oynayır.\n",
    "\n",
    "### Nümunə:\n",
    "Məsələn, xəstəliyin varlığını aşkar edən bir model qurulursa, bir sinfi \"xəstə\" vəziyyətini göstərirsə, burada precision önəmlidir. Çünki modelin \"xəstə\" dediyi nəticələrin həqiqətən xəstə olması vacibdir. Bu halda yanlış müsbət hallar xəstə olmayan şəxslərin xəstə kimi diaqnoz edilməsinə gətirib çıxarır ki, bu da ehtiyac olmayan əlavə testlər və narahatlığa səbəb ola bilər.\n",
    "\n",
    "### Əgər Recall və Precision hər ikisi əhəmiyyətlidirsə\n",
    "Həm birlərin doğru müəyyən edilməsi, həm də bir sinfinin yanlış neqativlərini azaltmaq vacibdirsə, **F1 score** istifadə edilə bilər. F1 score hər iki göstərici arasında balans təmin edir və bir sinfinin həm recall, həm də precision dəyərlərini birgə nəzərə alır.\n",
    "\n",
    "Bu səbəbdən, modeldə bir sinfi əhəmiyyətlidirsə, **precision** üstünlük verilməli, lakin hər iki metrikanı balanslaşdırmaq lazım gəldikdə **F1 score** seçilməlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74260137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce14825f",
   "metadata": {},
   "source": [
    "## Robust scale nece olur ki outlier-lere hessas olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6012a5a",
   "metadata": {},
   "source": [
    "`RobustScaler` məlumatları çevirmək üçün median və interquartile range (IQR) istifadə edir, bu da onu **outlier-lərə qarşı həssas olmayan** hala gətirir. Outlier-lərin təsirini azaltmaq üçün bu metod, **medianı 0, IQR-ni isə 1**-ə gətirir. Detallı şəkildə bu belə işləyir:\n",
    "\n",
    "1. **Medianı çıxarmaq**: Verilənlərin mərkəzini median ilə təyin edir, yəni hər bir dəyərdən medianı çıxarır. Median, dəyərlərin mərkəzini tapmaq üçün daha dayanıqlı bir göstəricidir və outlier-lərin təsirini azaltmağa kömək edir.\n",
    "\n",
    "2. **IQR ilə bölmək**: Dəyərləri 1-ci və 3-cü kvartillər (25%-lik və 75%-lik dəyərlər) arasındakı fərqlə (IQR) bölür. IQR, orta nöqtədən kənarda olan dəyərlərə qarşı dayanıqlıdır və outlier-ləri normallaşdırma prosesinə daxil etmir.\n",
    "\n",
    "Nəticədə, `RobustScaler` ilə transformasiya edilmiş verilənlər daha kiçik və daha konsistent aralığa sahib olur, lakin outlier-lər prosesdəki paylanmanı ciddi şəkildə təsirləndirmir. Bu, xüsusilə outlier-ləri çox olan dataset-lərdə effektivdir. \n",
    "\n",
    "Məsələn, `sklearn.preprocessing` kitabxanasında `RobustScaler`-dən belə istifadə edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "Bu metod, hər bir xüsusiyyət sütununu medianı 0, IQR-ni isə 1 şəklinə gətirərək çevrilmiş dəyərlər alır və outlier-lərin təsirini minimuma endirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ba24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411d3e00",
   "metadata": {},
   "source": [
    "## Min-Max scaling nece olur ki outlier-lere hessas olur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9acbfc",
   "metadata": {},
   "source": [
    "Linear skaling (min-max skaling) verilənlərdə outlier-lərə qarşı həssasdır, çünki bu metod maksimum və minimum dəyərlərdən asılıdır. Outlier-lər, yəni çox böyük və ya çox kiçik dəyərlər bu min-max metodunda məhdudları təyin edir, buna görə də bütün digər dəyərlərin aralığını dəyişdirir.\n",
    "\n",
    "**Necə işləyir:**\n",
    "\n",
    "Min-max skaling hər bir dəyəri müəyyən aralıqda yenidən şkalaya salır (məsələn, 0 ilə 1 arasında) və bu proses aşağıdakı kimi aparılır:\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$\n",
    "\n",
    "Bu düsturda:\n",
    "- $ X_{\\text{scaled}} $: Çevrilmiş dəyər\n",
    "- $ X $: Əsas dəyər\n",
    "- $ X_{\\min} $: Bütün verilənlərdəki minimum dəyər\n",
    "- $ X_{\\max} $: Bütün verilənlərdəki maksimum dəyər\n",
    "\n",
    "**Niyə outlier-lərə həssasdır:**\n",
    "\n",
    "1. **Ekstremal dəyərlərə asılılıq**: Min-max skalingdə bütün verilənlər maksimum və minimum dəyərlər əsasında çevrilir. Əgər outlier-lər varsa, onların çox böyük və ya kiçik dəyərləri bütün verilənlər intervalını genişləndirəcək.\n",
    "  \n",
    "2. **Dəyərlərin sıxlaşması**: Outlier-lər səbəbindən min-max skaling normal dəyərləri mərkəzə sıxlaşdırır, yəni əksər dəyərlər 0.4-0.6 kimi dar aralıqda sıxışa bilər. Bu, model üçün ayrıntılı məlumatı azaltmış olur və verilənlərdə təhrif yaradır.\n",
    "\n",
    "Outlier-ləri az təsir edən metodlara üstünlük vermək üçün alternativlərdən istifadə edilə bilər, məsələn, `RobustScaler`, log-transformasiyalar və ya z-skaling (`StandardScaler`), çünki z-skaling də outlier-lərə müəyyən həddə qədər dayanıqlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ae398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083c78cc",
   "metadata": {},
   "source": [
    "## StandardScaler nece olur ki outlier-lere hessas olur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bbae3",
   "metadata": {},
   "source": [
    "StandardScaler outlier-lərə müəyyən dərəcədə həssasdır, çünki bu metod verilənlərin ortalamasını və standart sapmasını hesablayaraq dəyərləri çevrilmiş forma gətirir.\n",
    "\n",
    "**Necə işləyir:**\n",
    "\n",
    "Hər bir dəyəri onun ortalama və standart sapmasına görə transformasiya edir:\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- $ X_{\\text{scaled}} $: Çevrilmiş dəyər\n",
    "- $ X $: Əsas dəyər\n",
    "- $ \\mu $: Orta dəyər (mean) — verilənlərin mərkəzini təyin edir\n",
    "- $ \\sigma $: Standart sapma (standard deviation) — verilənlərin yayılma dərəcəsini göstərir\n",
    "\n",
    "Bu çevrilmə ilə verilənlər **ortalama 0 və standart sapma 1** olacaq şəkildə şkalanır.\n",
    "\n",
    "**Outlier-lərə qarşı həssaslığı:**\n",
    "\n",
    "- **Müəyyən dərəcədə həssasdır**: Standartlaşdırmada da outlier-lər ortalama və standart sapmaya təsir edir. Əgər dataset-də çox böyük və ya kiçik dəyərlər (outlier-lər) varsa, onlar ortalamadan uzaqda yerləşdiyi üçün həm ortalamaya, həm də standart sapmaya təsir göstərəcək.\n",
    "  \n",
    "- **Verilənlərin mərkəzini və yayılmasını yenidən təyin edir**: Standartlaşdırma bütün dəyərləri ortalamadan fərqinə əsasən şkala edir, yəni outlier-lər mərkəzi bir qədər dəyişdirsə də, min-max skalingdə olduğu kimi bütün intervala hədsiz təsir etmir. \n",
    "\n",
    "**Məsələn, `StandardScaler` istifadə etmək üçün Python-da belə yaza bilərsiniz:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "Bu metodda, outlier-lərin təsiri qismən azaldılsa da, yüksək ekstremal dəyərlər hələ də standartlaşdırma prosesinə təsir edə bilər. Bu səbəbdən, çox sayda outlier varsa, **RobustScaler** daha uyğun ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f629b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177eccd1",
   "metadata": {},
   "source": [
    "## Sekli 3x64x64 ölçüsünə endirmək və sonunda 1x32x32 ölçüsünə çevirmək"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6185d3",
   "metadata": {},
   "source": [
    "Aşağıda `image.png` adlı bir şəkili yükləmək, onu `3x64x64` ölçüsünə endirmək və sonunda `1x32x32` ölçüsünə çevirmək üçün istifadə edə biləcəyiniz kod nümunəsi təqdim olunub:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Şəkili yükləyirik\n",
    "image = Image.open('image.png')\n",
    "\n",
    "# Şəkili 64x64 ölçüsünə endirmək üçün transformasiya tətbiq edirik\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Şəkili 64x64-ə endir\n",
    "    transforms.ToTensor()          # Tensor formatına çevir\n",
    "])\n",
    "\n",
    "# Transformasiyanı tətbiq edirik\n",
    "image_tensor = transform(image)  # Şəkil (3, 64, 64) ölçüsündə tensor olacaq\n",
    "\n",
    "# Hündürlüyü və eni 32-yə endirmək üçün interpolasiya\n",
    "image_resized = F.interpolate(image_tensor.unsqueeze(0), size=(32, 32), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Kanal ölçüsünü 1-ə çevirmək (orta dəyəri alaraq)\n",
    "image_final = image_resized.mean(dim=1, keepdim=True)  # 3 kanaldan 1 kanal yaradılır\n",
    "\n",
    "print(image_final.shape)  # Çıxış: torch.Size([1, 32, 32])\n",
    "```\n",
    "\n",
    "### Kodun İstifadəsi:\n",
    "\n",
    "1. **Şəkilin Yüklənməsi**: `Image.open('image.png')` ilə şəkili yükləyirik.\n",
    "2. **Transformasiya**: `transforms.Resize((64, 64))` ilə şəkili 64x64 ölçüsünə endiririk, daha sonra `transforms.ToTensor()` ilə onu tensor formatına çeviririk. Nəticədə, `image_tensor` dəyişəni `(3, 64, 64)` ölçüsündə tensor olur.\n",
    "3. **İnterpolasiya**: `F.interpolate` funksiyası ilə şəkilin ölçüsünü `(32, 32)`-ə endiririk. `unsqueeze(0)` metodu ilə tensorun ölçüsünü `(1, 3, 64, 64)`-ə çeviririk, beləliklə interpolasiyadan sonra `(1, 3, 32, 32)` ölçüsündə olacaq.\n",
    "4. **Kanalın Dəyişdirilməsi**: `mean(dim=1, keepdim=True)` metodu ilə 3 kanaldan ortalama dəyəri alaraq 1 kanal yaradırıq. Nəticə `(1, 32, 32)` ölçüsündə olacaq.\n",
    "\n",
    "Bu kod parçası ilə `image.png` şəkilini yükləyərək, onu 32x32 ölçüsündə 1 kanala çevirirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8111b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982a12e8",
   "metadata": {},
   "source": [
    "## Cost nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e308207",
   "metadata": {},
   "source": [
    "**Cost** (itki dəyəri) və ya **Loss** (zərər) maşın öyrənməsində və dərin öyrənmədə bir modelin nə qədər uğurlu öyrəndiyini ölçmək üçün istifadə edilən əsas dəyərdir. Bu dəyər, modelin proqnozlaşdırdığı nəticə ilə həqiqi nəticə arasındakı fərqi ifadə edir. Cost funksiyasının məqsədi, modelin səhv proqnozları minimuma endirməkdir. Təlim prosesi zamanı cost funksiyası optimizasiya olunur, beləliklə modelin daha dəqiq nəticələr verməsi təmin edilir.\n",
    "\n",
    "### Cost funksiyası necə işləyir?\n",
    "1. **Giriş verilərindən proqnoz**: Model təlim verilərini (məsələn, `X`) qəbul edir və bu verilərə əsasən proqnoz (`ŷ`) yaradır.\n",
    "2. **Proqnozun düzgünlüyünü ölçmək**: Cost funksiyası modelin proqnozu (`ŷ`) ilə həqiqi nəticə (`y`) arasındakı fərqi ölçür. Bu fərq nə qədər böyükdürsə, cost dəyəri o qədər yüksək olur, yəni modelin səhvləri çoxdur.\n",
    "3. **Parametrləri optimizasiya etmək**: Cost dəyərini azaltmaq üçün modelin parametrləri (çəki və qərəz dəyərləri) optimizator (məsələn, `SGD`, `Adam`) vasitəsilə yenilənir.\n",
    "\n",
    "### Cost funksiyalarına misallar\n",
    "- **Mean Squared Error (MSE)**: Davamlı dəyişənlər üçün istifadə olunur və proqnoz ilə həqiqi dəyər arasındakı fərqlərin kvadratının ortalamasını hesablayır. \n",
    "  $$\n",
    "  \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "- **Cross-Entropy Loss**: Klassifikasiya problemlərində geniş istifadə olunur. Hədəf və proqnoz ehtimalları arasındakı məsafəni ölçür.\n",
    "  \n",
    "Cost funksiyasının minimuma endirilməsi modelin daha dəqiq nəticələr verməsi deməkdir. Beləliklə, cost maşın öyrənməsində modelin performansını yaxşılaşdırmaq üçün əsas vasitədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1312ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce24057",
   "metadata": {},
   "source": [
    "## CNN-də w və b necə"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36356bd1",
   "metadata": {},
   "source": [
    "CNN (Convolutional Neural Network) modellərində istifadə olunan **w** və **b**, neyronların öyrənmə prosesində əsas rollardan birini oynayan parametrlərdir. Bunlar, modelin girişi ilə çıxışı arasında əlaqə qurmaq üçün optimallaşdırılan dəyərlərdir:\n",
    "\n",
    "### **1. w - Weights (Ağırlıqlar):**\n",
    "- **Weights**, bir qatdakı neyronun bir növbəti qatdakı neyronlara necə təsir edəcəyini müəyyən edən parametrlərdir.\n",
    "- Hər bir giriş xüsusiyyəti (məsələn, bir piksel dəyəri) ağırlıqla çarpılır və sonra növbəti neyrona ötürülür.\n",
    "- CNN-lərdə **konvolusiya qatları** daxilində, ağırlıqlar **filtrə** (kernel) dəyərləridir. Bu filtr, şəkillərdən xüsusiyyətləri çıxarmaq üçün giriş matrisinin üzərində sürüşdürülür.\n",
    "- Öyrənmə prosesi zamanı, `backpropagation` vasitəsilə bu ağırlıqlar optimallaşdırılır və daha yaxşı xüsusiyyət çıxarma bacarığı əldə edilir.\n",
    "\n",
    "### **2. b - Bias (Ofset):**\n",
    "- **Bias**, ağırlıqlarla gələn girişlərin cəminə əlavə edilən bir sabitdir. Neyronun çıxışını tənzimləməyə kömək edir.\n",
    "- Məqsəd, neyronun yalnız girişlərdən və ağırlıqlardan asılı qalmadan daha elastik işləməsini təmin etməkdir.\n",
    "- Riyazi ifadə:\n",
    "  $$\n",
    "  z = w \\cdot x + b\n",
    "  $$\n",
    "  Burada:\n",
    "  - $ x $: giriş dəyəri,\n",
    "  - $ w $: ağırlıq,\n",
    "  - $ b $: ofset (bias).\n",
    "\n",
    "Bias, neyronun qərar sərhədini tənzimləməyə kömək edir. Məsələn, yalnız ağırlıqdan asılı olsaydı, neyron yalnız orijindən keçən qərar sərhədləri yarada bilərdi. Bias əlavə etməklə daha çevik sərhədlər mümkündür.\n",
    "\n",
    "---\n",
    "\n",
    "### **CNN-də w və b necə öyrənilir?**\n",
    "1. **İrəli yayılma (Forward Propagation):** Giriş məlumatları ağırlıqlarla (\\(w\\)) çarpılır, bias (\\(b\\)) əlavə edilir və aktivasiya funksiyasına ötürülür.\n",
    "2. **Zərərlər hesablanır (Loss Function):** Modelin çıxışı ilə hədəf arasında fərq hesablanır.\n",
    "3. **Geri yayılma (Backpropagation):** Ağırlıq (\\(w\\)) və bias (\\(b\\)) gradient enmə (gradient descent) metodu ilə yenilənir. Məqsəd zərər funksiyasını minimuma endirməkdir.\n",
    "\n",
    "### Sadə Dildə:\n",
    "- **w**: Hansı xüsusiyyətlərin daha vacib olduğunu \"öyrənir\".\n",
    "- **b**: Modelin sərhədlərini daha elastik etməyə kömək edir.\n",
    "\n",
    "Hər iterasiyada bu parametrlər optimallaşdırılaraq modelin dəqiqliyi artırılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca01bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5017919",
   "metadata": {},
   "source": [
    "## Confusion Matrix nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f6061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772fabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
