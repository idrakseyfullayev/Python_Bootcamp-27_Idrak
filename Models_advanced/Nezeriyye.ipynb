{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9c6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba40b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0667bac8",
   "metadata": {},
   "source": [
    "## Supervised Learning Regression Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31acd3f",
   "metadata": {},
   "source": [
    "### R-squared: This measures the variation of a regression model. R-squared either increases or remains the same when new predictors are added to the model. ( R-kvadrat: Bu, reqressiya modelinin dəyişməsini ölçür. Modelə yeni proqnozlaşdırıcılar əlavə edildikdə R-kvadrat ya artır, ya da eyni qalır.)\n",
    "### Adjusted R-squared: This measures the variation for a multiple regression model, and helps you determine goodness of fit. Unlike R-squared, adjusted R-squared only adds new predictors to its model if it improves the model’s predicting power. (Düzəliş edilmiş R-kvadrat: Bu, çoxlu reqressiya modeli üçün variasiyanı ölçür və uyğunluğun yaxşılığını müəyyən etməyə kömək edir. R-kvadratdan fərqli olaraq, düzəliş edilmiş R-kvadrat modeli yalnız modelin proqnozlaşdırma gücünü yaxşılaşdırdığı halda modelinə yeni proqnozlaşdırıcılar əlavə edir.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76550179",
   "metadata": {},
   "source": [
    "#### https://images.app.goo.gl/Qgeqz1LPsGkEAfoD9\n",
    "#### https://images.app.goo.gl/xRogYdJjCpqKPc937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07874c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0b212c",
   "metadata": {},
   "source": [
    "## SST, SSR, SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d2871",
   "metadata": {},
   "source": [
    "**SST, SSR, və SSE** statistika və regresiya analizində istifadə olunan əsas ölçülərdir. Bu ölçülər modelin nə dərəcədə yaxşı uyğunlaşdığını və modeldəki variasiyaların nə qədərinin izah edildiyini qiymətləndirmək üçün istifadə olunur.\n",
    "\n",
    "**1. SST (Total Sum of Squares):** Ümumi kvadratlar cəmi (Total Sum of Squares) \n",
    "- Bu, `y` dəyişəninin ümumi dəyişkənliyini ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\bar{y})$ isə müşahidə olunan dəyərlərin orta qiymətidir.\n",
    "\n",
    "**2. SSR (Regression Sum of Squares):** Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- Bu, modelin nə dərəcədə uyğun gəldiyini ölçür, yəni modelin izah etdiyi variasiyadır.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSR} = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\n",
    "  $$\n",
    "  Burada $( \\hat{y}_i )$ model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**3. SSE (Error Sum of Squares):** Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- Bu, modelin izah edə bilmədiyi, qalıq variasiyanı ölçür.\n",
    "- Formula:\n",
    "  $$\n",
    "  \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "  Burada \\( y_i \\) həqiqi müşahidə olunan dəyərlər, $( \\hat{y}_i )$ isə model tərəfindən proqnozlaşdırılan dəyərlərdir.\n",
    "\n",
    "**Əlaqə**:\n",
    "Bu üç ölçü arasında belə bir əlaqə var:\n",
    "$$\n",
    "\\text{SST} = \\text{SSR} + \\text{SSE}\n",
    "$$\n",
    "Bu formula, ümumi variasiyanın izah edilən variasiyaya (SSR) və izah edilməmiş qalıq variasiyaya (SSE) bölünməsini göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345e34bc",
   "metadata": {},
   "source": [
    "## R2 and Adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2619e63b",
   "metadata": {},
   "source": [
    "**R-squared (R²)** və **Adjusted R-squared** regresiya modelinin uyğunluğunu qiymətləndirmək üçün istifadə olunan iki əsas ölçüdür. \n",
    "\n",
    "### 1. **R-squared (R²)**\n",
    "R², modelin nə qədər yaxşı izah etdiyini, yəni müstəqil dəyişənlərin asılı dəyişən üzərindəki variasiyanı nə qədər izah etdiyini göstərir. \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **SSR** - Regresiya kvadratlar cəmi (Regression Sum of Squares)\n",
    "- **SSE** - Qalıq kvadratlar cəmi (Error Sum of Squares)\n",
    "- **SST** - Ümumi kvadratlar cəmi (Total Sum of Squares)\n",
    "\n",
    "R² dəyəri həmişə 0 ilə 1 arasında olur. 1-ə yaxın olduqda modelin yüksək uyğunluğunu, 0-a yaxın olduqda isə zəif uyğunluğu göstərir.\n",
    "\n",
    "### 2. **Adjusted R-squared**\n",
    "**Adjusted R²** (Düzəldilmiş R²) modelə əlavə edilən dəyişənlərin sayını nəzərə alır və modelin sadəcə dəyişənlərin sayına görə şişirdilməməsini təmin edir. Əgər modelə faydalı olmayan bir dəyişən əlavə edilərsə, Adjusted R² dəyəri azalacaq. Bu xüsusiyyət Adjusted R²-i daha etibarlı edir.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2) \\cdot (n - 1)}{n - p - 1} \\right)\n",
    "$$\n",
    "\n",
    "Burada:\n",
    "- **R²** - R-squared dəyəri\n",
    "- **n** - Nümunə ölçüsü (müşahidələrin sayı)\n",
    "- **p** - Müstəqil dəyişənlərin sayı\n",
    "\n",
    "**Adjusted R²** dəyəri hər zaman **R²**-dən kiçik və ya ona bərabərdir. Bu, modelin sadəcə əlavə dəyişənlərin sayı ilə deyil, onların əhəmiyyəti ilə də əlaqəli olduğunu göstərir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f44d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c21ed24b",
   "metadata": {},
   "source": [
    "### R2 = SSregression / SST = 1 - SSresiduals / SST\n",
    "### R2 adjusted = 1 - (1 - R2) * (n - 1) / ( n - p - 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5052549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.7111220768354529\n",
      "Adjusted R^2: 0.7110611578301462\n",
      "MAE: 2.15\n",
      "MSE: 7.17\n",
      "RMSE: 2.68\n",
      "Pearson Korelyasiya Əmsalı: 0.8434059453729525\n",
      "Pearson Korelyasiya Əmsalı (scipy ilə): 0.8434059453729523\n",
      "Intercept (kəsik): 7.603902211896262\n",
      "Slope (meyl əmsalı): 0.9982101225691038\n",
      "Xətti tənlik: y = 0.9982101225691038 * x + 7.603902211896262\n",
      "Yeni feature x_1 = 23\n",
      "Predict: y = 30.56273503098565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week4\\day1\\Weather.csv\")\n",
    "dataset = data.copy()\n",
    "\n",
    "# Data\n",
    "x = dataset['MinTemp'].values.reshape(-1,1)\n",
    "y = dataset['MaxTemp'].values.reshape(-1,1)\n",
    "\n",
    "# Train-test bölünməsi\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True)\n",
    "\n",
    "# Model təlimi\n",
    "regressor = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Metriklərin hesablamaq\n",
    "R2 = r2_score(y_test, y_pred)\n",
    "print('R^2:', R2)\n",
    "\n",
    "n = x_test.shape[0]\n",
    "p = x_test.shape[1]\n",
    "adjusted_R2 = 1 - (1-R2)*(n-1)/(n-p-1)\n",
    "print('Adjusted R^2:', adjusted_R2)\n",
    "\n",
    "MAE = round(mean_absolute_error(y_test, y_pred), 2)\n",
    "print(\"MAE:\", MAE)\n",
    "\n",
    "MSE = round(mean_squared_error(y_test, y_pred), 2)\n",
    "print(\"MSE:\", MSE)\n",
    "\n",
    "RMSE = round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)\n",
    "print(\"RMSE:\", RMSE)\n",
    "\n",
    "# Pearson korelyasiya əmsalını hesablamaq\n",
    "correlation_matrix = np.corrcoef(y_test.flatten(), y_pred.flatten())\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print('Pearson Korelyasiya Əmsalı:', pearson_corr)\n",
    "\n",
    "# Alternativ olaraq, scipy istifadə etməklə\n",
    "pearson_corr, _ = pearsonr(y_test.flatten(), y_pred.flatten())\n",
    "print('Pearson Korelyasiya Əmsalı (scipy ilə):', pearson_corr)\n",
    "\n",
    "# Intercept və Slope tapmaq\n",
    "intercept = regressor.intercept_[0]\n",
    "slope = regressor.coef_[0][0]\n",
    "print(\"Intercept (kəsik):\", intercept)\n",
    "print(\"Slope (meyl əmsalı):\", slope)\n",
    "\n",
    "# Xətti tənliyi qurmaq\n",
    "print(f\"Xətti tənlik: y = {slope} * x + {intercept}\")\n",
    "\n",
    "# Yeni feature\n",
    "x_1 = 23\n",
    "print(f'Yeni feature x_1 = {x_1}')\n",
    "\n",
    "# Predict\n",
    "print(f\"Predict: y = {slope * x_1 + intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae2d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "094b065a",
   "metadata": {},
   "source": [
    "## Supervised Learning Classification Models Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2b083",
   "metadata": {},
   "source": [
    "#### Accuracy:\n",
    "#### Accuracy, modelin düzgün təsnif etdiyi nümunələrin ümumi nümunələrə olan nisbətini göstərir.\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394276f",
   "metadata": {},
   "source": [
    "#### AUC (Area Under the Curve):\n",
    "#### AUC, ROC (Receiver Operating Characteristic) əyrisinin altında qalan sahəni ölçür. ROC əyrisi modelin True Positive Rate (TPR) və False Positive Rate (FPR) göstəricilərini müxtəlif qərar hədlərinə (threshold) görə təyin edir. AUC dəyəri 0 ilə 1 arasında dəyişir və modelin təsnifat qabiliyyətini qiymətləndirir. Yüksək AUC dəyəri modelin müsbət və mənfi sinifləri yaxşı ayırdığını göstərir.\n",
    "#### TPR = TP / (TP+FN)\n",
    "#### FPR = FP / (FP+TN)\n",
    "\n",
    "#### AUC dəyərinin formulu birbaşa verilmir, amma AUC əsasən ROC əyrisinin altında qalan sahədir və inteqrasiya vasitəsilə hesablanır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1310546",
   "metadata": {},
   "source": [
    "#### https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html\n",
    "#### Accurence = (TP + TN ) / (TP + FP + TN + FN)\n",
    "#### PRECISON = TP / TP + FP\n",
    "#### RECALL = TP / TP + FN\n",
    "### F1 = 2 * ( (precision *  recall) / (precision+  recall) )\n",
    "### F0.5 = 1.25 * ( (precision * recall) / (0.25 * precision + recall) )\n",
    "### F2 = 5 * ( (precision * recall) / (4 * precision + recall) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e35012",
   "metadata": {},
   "source": [
    "### F1 score, modelin həm dəqiqlik, həm də recall göstəricilərində yaxşı performans göstərib-göstərmədiyini dəyərləndirir və balanslı bir metrikdir. Bir modelin F1 score-u yüksəkdirsə, demək olar ki, model həm yanlış müsbət, həm də yanlış mənfi nəticələri minimallaşdırmağa nail olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf5b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                     Predicted\n",
    "#                     *******************************************     \n",
    "#                     *  Positive        *     Negative         *\n",
    "#         ***************************************************************************************\n",
    "#         * Positive  *   True Positive  *    False Negative    *    Sensitivity                *\n",
    "#         *           *     (TP)         *  (FN)Type II error   *  TP / (TP + FN)               *  \n",
    "# Actual  ***************************************************************************************   \n",
    "#         * Negative  *  False Positive  *   True Negative      *    Specificity                *  \n",
    "#         *           * (FP)Type I error *      (TN)            *  TN / (TN + FP)               * \n",
    "#         *************************************************************************************** \n",
    "#         *           *  Precision       *  Negative Predictive *    Accuracy                   *   \n",
    "#         *           * TP / (TP + FP)   *    TN / (TN + FN)    * TP + TN / (TP + TN + FP + FN) *\n",
    "#         *************************************************************************************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c98215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487ad4d7",
   "metadata": {},
   "source": [
    "## True Positive Rate and  False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518be03",
   "metadata": {},
   "source": [
    "#### True Positive Rate (TPR) və False Positive Rate (FPR) təsnifat modellərinin performansını qiymətləndirmək üçün istifadə edilən iki mühüm göstəricidir. Onlar modelin müsbət və mənfi təsnifatlarla necə başa çıxdığını izah edirlər.\n",
    "\n",
    "#### 1. True Positive Rate (TPR)\n",
    "#### True Positive Rate (TPR), digər adı ilə Recall və ya Sensitivity (Sensitivlik), həqiqətən müsbət olan nümunələrin neçə faizinin model tərəfindən düzgün olaraq müsbət kimi təsnif edildiyini göstərir.\n",
    "\n",
    "#### TPR formulu:\n",
    "\n",
    "#### TPR = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n",
    "\n",
    " \n",
    "#### True Positives (TP): Modelin müsbət olaraq təsnif etdiyi və həqiqətən müsbət olan nümunələr.\n",
    "#### False Negatives (FN): Modelin mənfi olaraq təsnif etdiyi, lakin əslində müsbət olan nümunələr.\n",
    "#### TPR yüksəkdirsə, bu o deməkdir ki, model həqiqətən müsbət olanları yaxşı tanıyır və az sayda yanlış mənfi nəticə verir.\n",
    "\n",
    "#### 2. False Positive Rate (FPR)\n",
    "#### False Positive Rate (FPR), həqiqətən mənfi olan nümunələrin neçə faizinin model tərəfindən səhvən müsbət kimi təsnif edildiyini göstərir. FPR, modelin yanlış müsbət nəticələr vermə tendensiyasını göstərir.\n",
    "\n",
    "#### FPR formulu:\n",
    "#### FPR = 1 - Specificity = 1 - TN / (TN + FP)\n",
    "#### FPR = False Positives (FP) /  (False Positives (FP) + True Negatives (TN))\n",
    " \n",
    "#### False Positives (FP): Modelin müsbət olaraq təsnif etdiyi, lakin əslində mənfi olan nümunələr.\n",
    "#### True Negatives (TN): Modelin mənfi olaraq təsnif etdiyi və həqiqətən mənfi olan nümunələr.\n",
    "#### FPR yüksəkdirsə, bu, modelin çox sayda yanlış müsbət nəticə verdiyini göstərir.\n",
    "\n",
    "#### Nəticə:\n",
    "#### TPR yüksəkdirsə, bu, modelin həqiqətən müsbət olanları yaxşı tanıdığını göstərir.\n",
    "#### FPR aşağıdırsa, bu, modelin yanlış müsbət nəticələri azaldığını göstərir.\n",
    "#### Bu iki göstərici adətən ROC (Receiver Operating Characteristic) əyrisi ilə birlikdə istifadə edilir və AUC (Area Under the Curve) dəyəri ilə modelin performansı qiymətləndirilir. TPR və FPR dəyərlərinin tarazlaşdırılması modelin ümumi effektivliyini təmin edir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abfc11",
   "metadata": {},
   "source": [
    "## Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f40ab1",
   "metadata": {},
   "source": [
    "#### https://www.saedsayad.com/images/MLR_r2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7a43a",
   "metadata": {},
   "source": [
    " Ridge :  (yi − ŷi)^2 + λ ∗ Slope^2  \n",
    " Lasso: (yi − ŷi)^2 + λ ∗ |Slope|\n",
    "\n",
    "Bu ifadə, ümumiyyətlə, cərimə terminləri (penalization terms) ilə birlikdə istifadə olunan optimallaşdırma məqsəd funksiyalarından biridir. Buradakı simvollar və ifadələrin mənası belədir:\n",
    "\n",
    "- `(yi − ŷi)^2`: Bu, müşahidə olunan dəyərlər (yi) ilə proqnozlaşdırılan dəyərlər (ŷi) arasındakı fərqlərin kvadratıdır. Bu fərq, adətən xəta və ya qalıq (residual) adlanır. Kvadrat halında ifadə edilməsinin səbəbi fərqlərin müsbət və mənfi olmasının fərqini aradan qaldırmaq və daha böyük fərqləri cəza olaraq daha çox nəzərə almaqdır.\n",
    "  \n",
    "- `λ ∗ Slope`: Bu, cərimə terminidir. Burada `λ` (lambda) cərimə əmsalıdır. Bu, modelin sadəliyinə nə qədər əhəmiyyət verildiyini göstərir. `Slope` isə modelin meyli və ya meyl əmsalının (məsələn, xətti reqressiyada meyl əmsalı beta) qiymətidir. Cərimə termini modelin çox mürəkkəb olmasının qarşısını almaq üçün əlavə olunur.\n",
    "\n",
    "Bu cür cərimə terminləri Ridge və ya Lasso kimi reqressiya modellərində istifadə olunur. Məqsəd, sadəcə modelin dəqiqliyini artırmaq deyil, həm də modelin sadəliyini və ümumiləşdirmə qabiliyyətini (overfitting-i aradan qaldırmaq) təmin etməkdir. Lambda (`λ`) çox böyük olarsa, model həddindən artıq sadə ola bilər; əksinə, çox kiçik olarsa, model mürəkkəb və ya overfit ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d74db131",
   "metadata": {},
   "source": [
    "## Ridge an Lasso haqqinda bilmek isteyirem. ne is gorurler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16efde",
   "metadata": {},
   "source": [
    "Ridge və Lasso reqressiyası, xətti reqressiya modellərində dəyişənlərin seçilməsi və modelin sadələşdirilməsi məqsədilə istifadə olunan iki məşhur cərimələndirmə (penalization) metodudur. Hər ikisi overfitting-in qarşısını almaq üçün reqressiya əmsallarına cərimə tətbiq edir, lakin bunu fərqli yollarla edirlər.\n",
    "\n",
    "### 1. **Ridge Regression**\n",
    "   - **Nədir?**\n",
    "     Ridge reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların kvadratlarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ (βj^2)` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Ridge reqressiyası, əmsalların böyük olmasının qarşısını almaq üçün onları kiçikləşdirir. Bu metod xüsusilə yüksək kollinear (bir-birinə bağlı) dəyişənlər olduğu hallarda effektivdir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L2 norması (kvadrat cərimə).\n",
    "     - **Əsas üstünlüyü:** Overfitting-in qarşısını alır və modelin stabilliyini artırır.\n",
    "     - **Dəyişənlər:** Bütün dəyişənlər saxlanılır, lakin əmsalları kiçikləşdirilir.\n",
    "\n",
    "### 2. **Lasso Regression**\n",
    "   - **Nədir?**\n",
    "     Lasso reqressiyası, dəyişənlərin seçilməsi üçün cərimə olaraq əmsalların modullarının cəmini minimallaşdırır. Burada cərimə termini `λ * Σ |βj|` şəklindədir.\n",
    "   - **Nə iş görür?**\n",
    "     Lasso reqressiyası, bəzi əmsalların tamamilə sıfıra bərabər olmasına səbəb ola bilər. Bu, modeldə əhəmiyyətsiz olan dəyişənlərin tamamilə çıxarılmasına imkan verir.\n",
    "   - **Xüsusiyyətlər:**\n",
    "     - **Cərimə:** L1 norması (modul cərimə).\n",
    "     - **Əsas üstünlüyü:** Dəyişənlərin seçilməsi və modelin sadələşdirilməsi.\n",
    "     - **Dəyişənlər:** Bəzi dəyişənlər sıfırlanır və modeldən çıxarılır.\n",
    "\n",
    "### **Ridge və Lasso Arasındakı Fərqlər:**\n",
    "   - **Cərimə növü:** Ridge L2 cəriməsi istifadə edərək əmsalları kiçikləşdirir, Lasso isə L1 cəriməsi ilə bəzi əmsalları sıfıra çevirir.\n",
    "   - **Dəyişənlərin seçilməsi:** Ridge bütün dəyişənləri saxlayır, lakin Lasso bəzi dəyişənləri tamamilə çıxarır.\n",
    "   - **Yüksək korrelyasiya:** Ridge yüksək korrelyasiyalı dəyişənlər üçün daha uyğundur, çünki o, bu dəyişənləri bir arada saxlayır, Lasso isə yüksək korrelyasiyalı dəyişənlərin bir qismini sıfırlaya bilər.\n",
    "\n",
    "### **Ridge və Lasso'nun İstifadəsi:**\n",
    "   - Ridge reqressiyası daha çox dəyişən olduğu və onların əhəmiyyətinin hamısının qorunması lazım olduğu hallarda istifadə olunur.\n",
    "   - Lasso reqressiyası isə dəyişənlərin seçilməsi və daha sadə modellər qurmaq lazım olduğu hallarda istifadə olunur.\n",
    "\n",
    "Bu metodlar, xüsusilə böyük verilənlər dəstləri və yüksək miqdarda dəyişən olan problemlər üçün faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fa618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5db5fba0",
   "metadata": {},
   "source": [
    "## bes λ nece tapilir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c67a8",
   "metadata": {},
   "source": [
    "Lambda (`λ`) dəyəri, modelin cərimə əmsalıdır və onun optimal dəyəri modelin performansını yaxşılaşdırmaq üçün seçilir. Lambda-nı təyin etmək üçün bir neçə üsul mövcuddur:\n",
    "\n",
    "### 1. **Cross-Validation (Çapraz Doğrulama)**\n",
    "   - **Addım 1:** Verilənlər toplusunu bir neçə hissəyə bölmək (məsələn, 5 və ya 10 fold cross-validation).\n",
    "   - **Addım 2:** Müxtəlif lambda dəyərləri üçün modelin performansını hər bir hissədə qiymətləndirmək.\n",
    "   - **Addım 3:** Hər bir fold üçün modelin performansını ölçmək və ortalama performansı hesablayaraq ən yaxşı nəticə verən lambda dəyərini seçmək.\n",
    "\n",
    "### 2. **Grid Search**\n",
    "   - Müəyyən bir aralıqda lambda dəyərləri təyin olunur (məsələn, 0.001-dən 10-a qədər).\n",
    "   - Hər bir lambda dəyəri üçün model qurulur və performans ölçülür.\n",
    "   - Ən yaxşı performansı verən lambda seçilir.\n",
    "\n",
    "### 3. **Regularization Paths**\n",
    "   - Bu üsulda, müxtəlif lambda dəyərləri üçün reqressiya əmsallarının dəyişməsi analiz edilir.\n",
    "   - Bu qrafik üzərində, reqressiya əmsallarının stabilləşdiyi nöqtədəki lambda dəyəri seçilir.\n",
    "\n",
    "### 4. **Information Criteria (AIC, BIC)**\n",
    "   - Akaike Information Criterion (AIC) və ya Bayesian Information Criterion (BIC) kimi informasiya meyarları ilə lambda seçilə bilər. Bu meyarlar modeli uyğunlaşdırma keyfiyyətini və onun mürəkkəbliyini qiymətləndirir.\n",
    "\n",
    "Bu üsulların hər biri fərqli vəziyyətlər üçün uyğun ola bilər. Ən çox istifadə olunan metod isə Cross-Validation üsuludur, çünki bu metod həm modelin performansını yaxşı ölçməyə, həm də overfitting-in qarşısını almağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2dba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d8eba0b",
   "metadata": {},
   "source": [
    "## penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0 bu paramertler haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d4199",
   "metadata": {},
   "source": [
    "Bu parametrlər, `sklearn.linear_model.LogisticRegression` sinfində ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə olunur. Aşağıda hər bir parametrin detallı izahı verilmişdir:\n",
    "\n",
    "### 1. **`penalty='elasticnet'`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsini tətbiq edir. ElasticNet cərimələndirməsi həm L1 (Lasso) həm də L2 (Ridge) normlarını birləşdirir. Yəni, ElasticNet modeli həm bəzi əmsalları sıfıra yaxınlaşdırır (Lasso kimi), həm də digər əmsalları kiçikləşdirir (Ridge kimi).\n",
    "   - **Necə işləyir?**: Bu parametr təkbaşına işləməz, onu aktivləşdirmək üçün `l1_ratio` parametri və uyğun `solver` lazımdır.\n",
    "\n",
    "### 2. **`solver='saga'`**\n",
    "   - **İşlevi:** `solver` parametri Logistic Regression modelinin təlimi üçün istifadə edilən optimizasiya alqoritmini təyin edir. `saga`, həm L1, həm də ElasticNet cərimələndirmələrini dəstəkləyən bir solverdir. Bu alqoritm böyük verilənlər dəstlərində və sparse (sıx olmayan) xüsusiyyətlərə malik olan modellərdə çox effektivdir.\n",
    "   - **Nə üçün vacibdir?**: ElasticNet cərimələndirməsi üçün `saga` solverini seçmək məcburidir, çünki yalnız `saga` (və `liblinear`) L1 normunu dəstəkləyir, ElasticNet isə həm L1, həm də L2 normlarını birləşdirdiyindən, onun üçün də uyğundur.\n",
    "\n",
    "### 3. **`l1_ratio=0.5`**\n",
    "   - **İşlevi:** Bu parametr ElasticNet cərimələndirməsi daxilində L1 və L2 cərimələri arasındakı balansı təyin edir. \n",
    "     - **`l1_ratio=0`** olduqda, ElasticNet yalnız L2 (yəni, Ridge) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=1`** olduqda, ElasticNet yalnız L1 (yəni, Lasso) cərimələndirməsini həyata keçirir.\n",
    "     - **`l1_ratio=0.5`** dəyəri həm L1, həm də L2 cərimələndirmələrinin qarışığını tətbiq edir, yəni hər ikisini bərabər şəkildə qarışdırır.\n",
    "   - **İstifadə məqsədi:** Bu parametr L1 və L2 normları arasında uyğun bir balans yaratmaq üçündür. Verilən problemə uyğun optimal balansı tapmaq üçün `l1_ratio` dəyərini dəyişdirərək təcrübə etmək mümkündür.\n",
    "\n",
    "### 4. **`C=1.0`**\n",
    "   - **İşlevi:** `C` parametri modelin cərimələndirmə əmsalını təyin edir. `C` dəyəri cərimələndirmənin tərs əmsalıdır (yəni, `C` nə qədər böyükdürsə, cərimə bir o qədər kiçikdir və əksinə).\n",
    "   - **Nə üçün vacibdir?**: \n",
    "     - **Kiçik `C` dəyəri:** Daha böyük cərimələndirməyə səbəb olur, yəni modelin sadələşdirilməsinə və əmsalların kiçikləşdirilməsinə (və ya sıfıra yaxınlaşmasına) gətirib çıxarır.\n",
    "     - **Böyük `C` dəyəri:** Daha az cərimələndirməyə səbəb olur və model daha çox uyğunlaşır (overfitting riskini artırır).\n",
    "   - **Standart dəyər:** `C=1.0` balanslı bir dəyərdir, lakin onu artırıb azaltmaqla modelin performansını dəyişdirə bilərsiniz.\n",
    "\n",
    "### **Yekun:**\n",
    "Bu parametrlər birlikdə ElasticNet cərimələndirməsini həyata keçirmək üçün istifadə edilir. `penalty='elasticnet'` ElasticNet cərimələndirməsini tətbiq edir, `solver='saga'` bu cərimələndirməni dəstəkləyən optimizasiya alqoritmidir, `l1_ratio=0.5` L1 və L2 normları arasında balansı təyin edir, və `C=1.0` isə cərimələndirmənin dərəcəsini müəyyən edir. Bu parametrlər sizin modelinizin kompleksliyini idarə etmək və overfitting riskini azaldaraq daha dəqiq proqnozlar vermək üçün istifadə edilə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab96bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe20b15",
   "metadata": {},
   "source": [
    "## solver nedir umumiyyetle ve imkan varsa formulasini yaz ve ya adi dilde izah ele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd31be",
   "metadata": {},
   "source": [
    "`Solver`, statistik modellərin və optimizasiya problemlərinin həlli üçün istifadə olunan bir alqoritmdir. Xüsusən, `sklearn.linear_model.LogisticRegression` sinifində olduğu kimi, xətti modellərdə (xətti reqressiya, logistika reqressiyası və s.) əmsalları tapmaq üçün bu alqoritmlərdən istifadə olunur. \n",
    "\n",
    "### **Solver Nədir?**\n",
    "\n",
    "- **Nə iş görür?**: Solver, optimizasiya problemini həll edərək modelin əmsallarını tapır. Bu prosesdə məqsəd, itki funksiyasını (loss function) minimallaşdırmaq və ən uyğun model parametrlərini tapmaqdır.\n",
    "- **İş prinsipi:** Solver modelin parametrlərini iterativ şəkildə dəyişdirir və itki funksiyasını tənzimləyərək ən yaxşı nəticəyə çatmağa çalışır. Hər bir iterasiyada modelin parametrləri yenilənir və daha az itki ilə nəticə əldə etməyə çalışılır.\n",
    "\n",
    "### **Populyar Solver-lər və İş Prinsipləri**\n",
    "\n",
    "1. **`liblinear`**\n",
    "   - **Xüsusiyyətləri:** Çoxlu miqdarda xüsusiyyətə malik olan (feature) kiçik və orta miqdarda verilənlər üçün yaxşıdır. L1 və L2 cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Koordinat Descent alqoritmindən istifadə edir. L1 və L2 cərimələndirməsi ilə səmərəlidir.\n",
    "\n",
    "2. **`newton-cg`**\n",
    "   - **Xüsusiyyətləri:** Daha böyük verilənlər dəstləri üçün uyğundur. L2 cərimələndirməsini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Newton-Raphson metodundan istifadə edir. Hər iterasiyada ikinci dərəcəli diferensial məlumatlardan istifadə edərək əmsalları yeniləyir.\n",
    "\n",
    "3. **`saga`**\n",
    "   - **Xüsusiyyətləri:** Böyük verilənlər dəstləri və sıx olmayan (sparse) verilənlər üçün yaxşıdır. L1, L2 və ElasticNet cərimələndirmələrini dəstəkləyir.\n",
    "   - **İş Prinsipi:** Stochastic Average Gradient Descent (SAG) metodunun inkişaf etdirilmiş formasıdır. Bu, gradient descend metodu ilə əmsalları iterativ şəkildə yeniləyir.\n",
    "\n",
    "4. **`lbfgs`**\n",
    "   - **Xüsusiyyətləri:** L2 cərimələndirməsini dəstəkləyir və böyük verilənlər üçün yaxşı nəticələr verir.\n",
    "   - **İş Prinsipi:** Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) alqoritmindən istifadə edir. İkinci dərəcəli optimizasiya üsulu olaraq parametrlərin optimallaşdırılmasına kömək edir.\n",
    "\n",
    "### **Solver-lərin İstifadəsi**\n",
    "\n",
    "Solver-lər müxtəlif optimizasiya alqoritmləri ilə modelin parametrlərini optimallaşdırmağa kömək edir. Onların əsas məqsədi itki funksiyasını minimallaşdırmaqdır.\n",
    "\n",
    "### **Formulalar**\n",
    "\n",
    "Solver-lərin istifadə etdiyi əsas formulalar ümumiyyətlə optimizasiya və gradient descend metodlarına əsaslanır. Burada ən çox istifadə olunan metodlardan biri olan gradient descent formulunu təqdim edirəm:\n",
    "\n",
    "**Gradient Descent Formula:**\n",
    "\n",
    "$$ \\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} $$\n",
    "\n",
    "- **$\\theta_{j}$**: Modelin parametridir.\n",
    "- **$\\alpha$**: Öğrenmə sürətidir (learning rate).\n",
    "- **$J(\\theta)$**: İtki funksiyasıdır (loss function).\n",
    "- **$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}$**: İtki funksiyasının $\\theta_{j}$ parametrinə görə gradientidir.\n",
    "\n",
    "### **Adi Dildə İzahetmə**\n",
    "\n",
    "Solver-lər modelin \"öyrənmə\" mərhələsində əmsalları tapmağa kömək edir. Məsələn, Logistic Regression modelini qurduğunuz zaman, solver modelin düzgün parametrlərini tapmaq üçün müxtəlif üsullardan istifadə edir. Hər iterasiyada solver əmsalları dəyişdirərək itki funksiyasını (modelin səhvini) azaldır və nəticədə ən yaxşı parametrləri tapmağa çalışır. \n",
    "\n",
    "Solver-lərin seçimləri, verilənlərin ölçüsünə, cərimələndirmə növünə və modelin tələblərinə görə fərqlənə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82674380",
   "metadata": {},
   "source": [
    "## itki funksiayasi nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d078b70",
   "metadata": {},
   "source": [
    "İtki funksiyası (loss function), statistik modelləşdirmədə modelin nə qədər düzgün proqnoz verdiyini ölçmək üçün istifadə olunan bir funksiyadır. Bu funksiya modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqi kvantlaşdırır. İtki funksiyasının məqsədi modelin performansını qiymətləndirmək və modelin parametrlərini optimallaşdırmaq üçün istifadə olunur.\n",
    "\n",
    "İtki funksiyası müxtəlif modellərdə müxtəlif formullarla təyin olunur. Burada iki əsas növ itki funksiyasını təqdim edirəm: **Xətti Reqressiya** və **Logistik Reqressiya** üçün.\n",
    "\n",
    "### **1. Xətti Reqressiya üçün İtki Funksiyası (Mean Squared Error - MSE)**\n",
    "\n",
    "Xətti reqressiya modelində itki funksiyası adətən **Mean Squared Error (MSE)** ilə təyin olunur. MSE, modelin proqnozları ilə həqiqi dəyərlər arasındakı fərqin kvadratlarının ortalamasını ölçür. \n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı (datanın ölçüsü).\n",
    "- **$y_i$**: Həqiqi dəyər (ground truth).\n",
    "- **$\\hat{y_i}$**: Modelin proqnozu (predicted value).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- MSE, modelin nə qədər yanıldığını ölçür. Kiçik MSE dəyərləri modelin daha yaxşı nəticə verdiyini göstərir.\n",
    "- Kvadratlı fərqlərin ortalaması olduğu üçün, böyük səhvləri daha çox vurğulayır.\n",
    "\n",
    "### **2. Logistik Reqressiya üçün İtki Funksiyası (Log Loss / Binary Cross-Entropy Loss)**\n",
    "\n",
    "Logistik reqressiya modelində itki funksiyası adətən **Log Loss** (və ya Binary Cross-Entropy Loss) ilə təyin olunur. Bu itki funksiyası modelin proqnozlarının doğru etiketlərlə nə qədər uyğun olduğunu ölçür.\n",
    "\n",
    "**Formul:**\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]\n",
    "$$\n",
    "\n",
    "- **$N$**: Nümunələrin sayı.\n",
    "- **$y_i$**: Həqiqi etiket (1 və ya 0).\n",
    "- **$\\hat{y_i}$**: Modelin ehtimal proqnozu (0 ilə 1 arasında).\n",
    "\n",
    "**İzahat:**\n",
    "\n",
    "- Log Loss, proqnozların həqiqi etiketlərlə nə qədər uyumlu olduğunu qiymətləndirir.\n",
    "- Əgər modelin proqnozu həqiqi etiketə yaxınsa, Log Loss dəyəri kiçik olacaqdır.\n",
    "- Log Loss, ehtimal dəyərləri ilə işləyir və aşağı ehtimallar üçün yüksək cəzalar verir.\n",
    "\n",
    "### **Əlavə Nümunələr:**\n",
    "\n",
    "- **Hinge Loss**: SVM (Support Vector Machine) modellərində istifadə olunur. Formulu belədir: \n",
    "  $$\n",
    "  \\text{Hinge Loss} = \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y_i})\n",
    "  $$\n",
    "  burada **$y_i$** doğru etiketlərdir (1 və ya -1), **$\\hat{y_i}$** isə proqnoz edilən etiketlərdir.\n",
    "\n",
    "İtki funksiyası modelin öyrənmə prosesində əsas rol oynayır, çünki modelin parametrləri itki funksiyasını minimuma endirmək üçün optimallaşdırılır. Hər bir model və problem üçün itki funksiyası fərqli ola bilər və bu, modelin məqsədinə və xüsusiyyətlərinə əsaslanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bcb53f6",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LogisticRegression daxilinde ridge lasso elasticnet tetbiqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987371e",
   "metadata": {},
   "source": [
    "`sklearn.linear_model.LogisticRegression` sinfi daxilində Ridge, Lasso və ElasticNet cərimələndirmələrini tətbiq etmək mümkündür. Bu cərimələndirmələr `penalty` parametri vasitəsilə seçilir. Aşağıda hər bir cərimələndirmə üsulunun necə tətbiq edildiyini göstərən nümunələr təqdim edirəm.\n",
    "\n",
    "### 1. **Ridge Regression (L2 Penalty)**\n",
    "Ridge cərimələndirməsi `penalty='l2'` parametri ilə həyata keçirilir. Bu, `LogisticRegression` sinfinin standart cərimələndirmə üsuludur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Məlumat dəstini bölmək\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression modeli L2 cəriməsi ilə\n",
    "ridge_model = LogisticRegression(penalty='l2', C=1.0)  # C parametri lambda-nın tərsidir\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ridge Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed563de",
   "metadata": {},
   "source": [
    "### 2. **Lasso Regression (L1 Penalty)**\n",
    "Lasso cərimələndirməsi `penalty='l1'` parametri ilə həyata keçirilir. Bunun üçün `solver` parametrini də uyğun olaraq `liblinear` və ya `saga` seçmək lazımdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LogisticRegression(penalty='l1', solver='saga', C=1.0)  # SAGA və ya liblinear solver\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Lasso Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4951",
   "metadata": {},
   "source": [
    "### 3. **ElasticNet Regression**\n",
    "ElasticNet cərimələndirməsi üçün həm `l1_ratio`, həm də `penalty='elasticnet'` parametrləri istifadə olunur. Burada da `solver` olaraq `saga` istifadə edilməlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d461ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)\n",
    "elasticnet_model.fit(X_train, y_train)\n",
    "\n",
    "# Proqnozlaşdırmaq və performansı ölçmək\n",
    "y_pred = elasticnet_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ElasticNet Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb3bb",
   "metadata": {},
   "source": [
    "### Parametrlərin İzahı:\n",
    "- **`penalty`**: Cərimələndirmə növünü təyin edir (`l1`, `l2`, `elasticnet`).\n",
    "- **`C`**: Cərimə əmsalını təyin edir. Daha böyük `C` daha az cərimələndirmə deməkdir (`C` dəyəri lambda-nın tərsidir).\n",
    "- **`l1_ratio`**: ElasticNet üçün L1 və L2 arasında balans yaradır. `l1_ratio=0` yalnız Ridge, `l1_ratio=1` isə yalnız Lasso cəriməsi deməkdir.\n",
    "- **`solver`**: Problemi həll etmək üçün istifadə edilən alqoritmdir. `liblinear` və `saga` solverləri L1 və ElasticNet cərimələndirməsini dəstəkləyir.\n",
    "\n",
    "Bu nümunələrdə Logistic Regression daxilində Ridge, Lasso və ElasticNet cərimələndirmə üsullarını tətbiq edə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "555e2d88",
   "metadata": {},
   "source": [
    "## Ridge, Lasso ve Elasticnet  svm, KNN, Decision tree-de de isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfa227",
   "metadata": {},
   "source": [
    "Ridge, Lasso və ElasticNet kimi cərimələndirmə üsulları xətti modellərdə, xüsusilə xətti reqressiya və logistika reqressiya modellərində tətbiq olunan texnikalardır. Bu cərimələndirmə metodları, xətti modellərin daha yaxşı ümumiləşdirilməsini təmin etmək üçün nəzərdə tutulmuşdur. Ancaq digər maşın öyrənməsi alqoritmlərində (məsələn, SVM, KNN, Decision Tree) bu cərimələndirmə üsullarının eyni formada işlədilməsi mümkün deyil, çünki bu alqoritmlər fərqli optimizasiya və iş prinsiplərinə malikdir.\n",
    "\n",
    "### **Ridge, Lasso və ElasticNet Cərimələndirmələri**\n",
    "- **Ridge (L2 Penalty):** Əmsalların böyüklüyünü cərimələyir və onların kiçik olmasına səbəb olur. Overfitting riskini azaldır.\n",
    "- **Lasso (L1 Penalty):** Bəzi əmsalları tam sıfıra bərabər edir, yəni xüsusiyyət seçimi (feature selection) rolunu oynayır.\n",
    "- **ElasticNet:** Həm L1, həm də L2 cərimələndirməsini birləşdirir və xüsusiyyətlər arasındakı əlaqələri idarə edir.\n",
    "\n",
    "### **Bu Texnikalar Aşağıdakı Alqoritmlərdə Tətbiq Edilə Bilərmi?**\n",
    "\n",
    "1. **Support Vector Machines (SVM)**\n",
    "   - **SVM Cərimələndirməsi:** SVM-də **cərimə parametri (C)** Ridge Regression-a bənzər şəkildə işləyir. Bu parametr səhv təsnifatları cərimələndirərək marjanı böyütməyə çalışır. Yəni, SVM-də L1 və L2 cəriməsi istifadə edilməsə də, itki funksiyası və cərimələndirmə prinsipləri cərimə parametri `C` ilə idarə olunur.\n",
    "   - **L1 və L2 Cərimələndirməsi:** SVM-də **LinearSVC** sinfi daxilində `penalty='l1'` və `penalty='l2'` parametrləri ilə L1 və L2 cərimələri tətbiq oluna bilər.\n",
    "\n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "   - **KNN-də Cərimələndirmə:** KNN alqoritmində cərimələndirmə tətbiq olunmur, çünki bu alqoritm xüsusiyyət əmsalları ilə deyil, məsafə metrikləri ilə işləyir. KNN sadəcə verilənlərdəki ən yaxın nöqtələrə əsaslanaraq qərar verir və əmsalların böyüklüyü kimi anlayışlar burada mövcud deyil.\n",
    "   \n",
    "3. **Decision Tree**\n",
    "   - **Decision Tree Cərimələndirməsi:** Decision Tree alqoritmlərində də Ridge və Lasso kimi cərimələndirmələr tətbiq edilmir, çünki bu alqoritm əmsallarla işləmək əvəzinə, verilənləri ağac strukturu şəklində bölür.\n",
    "   - **Overfitting-i İdarə Etmək:** Overfitting riskini azaltmaq üçün Decision Tree alqoritmində **ağacın dərinliyini məhdudlaşdırmaq** və ya **minimum nümunə sayını** təyin etmək kimi texnikalardan istifadə olunur.\n",
    "\n",
    "### **Nəticə**\n",
    "- **Ridge, Lasso və ElasticNet** xətti modellərə aid texnikalardır və onlar SVM-də müəyyən formada tətbiq oluna bilər.\n",
    "- **KNN** və **Decision Tree** kimi qeyri-xətti modellərdə isə bu texnikalar tətbiq edilmir, çünki bu alqoritmlər əmsallarla deyil, fərqli prinsiplərlə işləyirlər.\n",
    "\n",
    "Bu cərimələndirmə metodlarının işlədiyi modelləri və digər modellərdə alternativ üsulları başa düşmək maşın öyrənməsində doğru texnikaları seçmək üçün vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209e5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3321d294",
   "metadata": {},
   "source": [
    "## Bes Random Forest-de Ridge, Lasso ve Elasticnet isleyir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e03f2f",
   "metadata": {},
   "source": [
    "**Random Forest** alqoritmində **Ridge**, **Lasso** və **ElasticNet** kimi cərimələndirmə üsulları birbaşa tətbiq olunmur. Bunun səbəbi Random Forest-in təməlində **Decision Tree** alqoritmi dayanmasıdır və Decision Tree əmsallarla işləmək əvəzinə, verilənləri qaydalara görə bölən bir alqoritmdir.\n",
    "\n",
    "### **Niyə Random Forest-də Ridge, Lasso və ElasticNet İşləmir?**\n",
    "\n",
    "1. **Decision Tree ilə İşləmə Prinsipi:**\n",
    "   - Random Forest bir çox decision tree-dən ibarət olan ansambl metodudur. Hər bir decision tree verilənlər dəstini qaydalara görə bölərək təsnifat və ya reqressiya işləri görür.\n",
    "   - Decision Tree-lər verilənləri əmsallarla təhlil etmədiyi üçün, Ridge, Lasso və ElasticNet cərimələri ilə işləmirlər. Bu cərimələr xətti modellərdə əmsalları tənzimləmək üçün istifadə olunur, lakin Decision Tree-lərdə belə bir əmsal yoxdur.\n",
    "\n",
    "2. **Overfitting-lə Mübarizə:**\n",
    "   - Random Forest alqoritmi decision tree-lərin təbii overfitting meylini azaldır. Bir çox ağacın nəticələrini birləşdirərək (və fərqli verilənlər dəstləri və xüsusiyyətlər üzərində hər bir ağacı təlim etdirərək), Random Forest daha sabit və ümumiləşdirilmiş nəticələr verir.\n",
    "   - Random Forest modelində overfitting-in qarşısını almaq üçün ağacın dərinliyini məhdudlaşdırmaq, minimum nümunə sayını təyin etmək və ağaclar arasında müxtəliflik yaratmaq kimi metodlardan istifadə olunur. Bu üsullar Ridge, Lasso və ElasticNet cərimələndirmələrinin funksiyasını əvəz edir.\n",
    "\n",
    "### **Random Forest üçün Alternativlər:**\n",
    "Ridge, Lasso və ElasticNet kimi cərimələndirmələr xətti modellərdə xüsusiyyətlərin seçimi və modelin sadələşdirilməsi üçün istifadə olunur. Random Forest-də isə xüsusiyyətlərin seçimi avtomatik şəkildə edilir, çünki hər ağac yalnız bir hissə xüsusiyyəti istifadə edir. \n",
    "\n",
    "Əgər xüsusiyyət seçimi və ya əmsallar üzərində müəyyən bir cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərdən istifadə etməlisiniz, məsələn:\n",
    "\n",
    "- **Linear Regression** üçün Ridge, Lasso və ElasticNet.\n",
    "- **Logistic Regression** üçün L1, L2 və ElasticNet.\n",
    "\n",
    "Random Forest-də belə bir ehtiyac yoxdur, çünki o, ansambl metodlarının təbiəti ilə artıq overfitting-ə qarşı dayanır və xüsusiyyət seçimi prosesini təbii olaraq həyata keçirir.\n",
    "\n",
    "### **Nəticə:**\n",
    "Random Forest-də Ridge, Lasso və ElasticNet kimi cərimələndirmələr işləməz, çünki bu cərimələndirmələr xətti modellər üçün nəzərdə tutulmuşdur və Random Forest-in iş prinsipinə uyğun gəlmir. Əgər modeldə xüsusiyyət seçimi və cərimələndirmə tətbiq etmək istəyirsinizsə, xətti modellərə baxmaq daha uyğun olar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a0e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a911b4c3",
   "metadata": {},
   "source": [
    "## Types of Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3c35f",
   "metadata": {},
   "source": [
    "Ensemble learning metodları, bir neçə zəif modelin birləşdirilərək daha güclü və sabit nəticələr əldə edilməsinə imkan verir. Bu metodlar fərqli modelləri və ya eyni modelin fərqli versiyalarını istifadə edərək nəticələrin ortalamasını alır və ya səs çoxluğunu təyin edir. Aşağıda Ensemble learning-in əsas növləri haqqında geniş məlumat verilir:\n",
    "\n",
    "### 1. **Voting**\n",
    "Voting, müxtəlif modellərin proqnozlarının birləşdirilməsi üsuludur. Burada əsas məqsəd bir neçə fərqli modelin proqnozlarını birləşdirərək daha dəqiq nəticə əldə etməkdir.\n",
    "\n",
    "- **Hard Voting**: Hər modelin proqnozları nəzərə alınır və ən çox səs verilən sinif seçilir. Yəni, hansı sinif daha çox model tərəfindən seçilirsə, o sinif yekun proqnoz kimi təyin edilir.\n",
    "- **Soft Voting**: Hər modelin ehtimal proqnozları nəzərə alınır və ortalama ehtimallar əsasında yekun proqnoz verilir.\n",
    "\n",
    "### 2. **Bagging (Bootstrap Aggregating)**\n",
    "Bagging, Bootstrap metodunu istifadə edərək təkrarlı olaraq verilənlərdən müxtəlif alt dəstələr seçir və hər bir alt dəstədə fərqli modellər qurur. Bu metod əsasən yüksək dəyişkənliyə malik modellərin stabilləşdirilməsi üçün istifadə edilir.\n",
    "\n",
    "- **Random Forest**: Bagging-in ən məşhur tətbiqi olan Random Forest, müxtəlif decision tree-lərdən ibarət bir ansambl yaradır. Hər bir decision tree fərqli verilənlər dəstəsində qurulur və nəticələr səs çoxluğuna əsasən təyin edilir.\n",
    "\n",
    "### 3. **Boosting**\n",
    "Boosting, ardıcıl olaraq zəif modellər qurur və hər yeni model əvvəlki modelin düzgün proqnoz verə bilmədiyi nümunələri düzəltməyə çalışır. Bu metod zəif modelləri güclü modelə çevirmək üçün nəzərdə tutulub.\n",
    "\n",
    "- **AdaBoost**: AdaBoost, hər modelin düzgün və ya səhv proqnozlarına görə ağırlıqlarını dəyişdirərək zəif modelləri ardıcıl qurur. Səhv proqnozlar daha çox ağırlıq alır, doğru proqnozlar isə daha az.\n",
    "- **Gradient Boosting**: Gradient Boosting, qalıqların üzərində işləyən modellər yaradır. Hər yeni model əvvəlki modelin səhvlərini düzəltməyə çalışır.\n",
    "- **XGBoost**: Gradient Boosting-in optimallaşdırılmış versiyasıdır. Hesablama performansını artırmaq və overfitting-i azaltmaq üçün müxtəlif optimallaşdırmalar təqdim edir.\n",
    "- **CatBoost**: KatBoost, əsasən kateqorik verilənlərlə işləmək üçün optimallaşdırılmış bir Boosting metodudur. Bu metod xüsusilə kateqorik dəyişənlərin avtomatik olaraq kodlaşdırılmasını və optimal performans təmin etməsini asanlaşdırır.\n",
    "- **LightGBM**: LightGBM, Gradient Boosting-in daha yüngül və sürətli bir variantıdır. Bu metod böyük verilənlər dəstələri üzərində effektiv şəkildə işləyir və hesablama müddətini əhəmiyyətli dərəcədə azaldır.\n",
    "\n",
    "### 4. **Model Stacking**\n",
    "Model Stacking, fərqli modellərin proqnozlarını birləşdirərək ikinci səviyyəli bir model qurur. Bu ikinci səviyyəli model ilkin modellərin proqnozlarına əsaslanaraq yekun proqnoz verir.\n",
    "\n",
    "- **Base Models**: İlk səviyyədə bir neçə müxtəlif model istifadə olunur və hər biri verilənlər dəstəsində təlim olunur.\n",
    "- **Meta-Model**: İkinci səviyyədə, ilkin modellərin proqnozlarını input olaraq istifadə edən bir meta-model qurulur. Bu meta-model yekun proqnozu təyin edir.\n",
    "\n",
    "### Nəticə\n",
    "Bu metodlar Ensemble learning-in müxtəlif tətbiq üsullarıdır və hər biri fərqli üstünlüklərə malikdir. Bagging yüksək dəyişkənlikli modelləri stabilləşdirmək üçün istifadə edilir, Boosting isə zəif modelləri güclü modellərə çevirmək üçün. Voting sadə birləşdirmə üsuludur, Stacking isə daha kompleks və güclü ansambl modellər yaratmağa imkan verir.\n",
    "\n",
    "CatBoost və LightGBM kimi spesifik alqoritmlər isə Boosting texnikalarının optimallaşdırılmış formalarını təklif edirlər və müxtəlif problem sahələrində yüksək performans təmin edə bilirlər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ab1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e37dec7",
   "metadata": {},
   "source": [
    "## 3. Boosting modellerin daha etrafli izahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869bd18",
   "metadata": {},
   "source": [
    "Boosting metodları maşın öyrənməsində güclü və effektiv modellər qurmaq üçün geniş istifadə olunur. AdaBoost, Gradient Boosting, XGBoost, CatBoost və LightGBM kimi müxtəlif Boosting alqoritmləri mövcuddur. Hər birinin özünəməxsus xüsusiyyətləri, üstünlükləri və məhdudiyyətləri vardır. Aşağıda bu modellərin detallı fərqlərini və hansı vəziyyətdə hansı modelin daha uyğun olacağını izah edirəm.\n",
    "\n",
    "## 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni zəif modelin əvvəlki modellərin səhvlərinə fokuslanaraq qurulmasını təmin edir. Yəni, səhv proqnoz verilən nümunələrə daha çox ağırlıq verilir.\n",
    "- **Ağırlıq Yeniləmə:** Hər iterasiyada nümunələrin ağırlıqları yenilənir. Səhv proqnoz verilən nümunələrin ağırlığı artırılır, doğru proqnoz verilənlərin isə azaldılır.\n",
    "- **Zəif Modellər:** Adətən, sadə qərar ağacları (decision stumps) kimi zəif modellər istifadə olunur.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sadə və asan tətbiq edilə biləndir.\n",
    "- Overfitting riskini azaldır.\n",
    "- Sürətli təlim prosesinə malikdir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Məsələn, çoxlu sayda outliers olan verilənlərdə performansı zəif ola bilər.\n",
    "- Kompleks verilənlər üçün optimal olmaya bilər.\n",
    "\n",
    "## 2. Gradient Boosting\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **İş Prinsipi:** Gradient Boosting, hər yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışdığı iterativ bir prosesdir.\n",
    "- **Qalıq Əsasında Təkmilləşdirmə:** Hər iterasiyada əvvəlki modellərin səhvləri üzərində işləyərək ümumi dəqiqliyi artırır.\n",
    "- **Loss Funksiyaları:** Müxtəlif itki funksiyalarını dəstəkləyir, bu da onu müxtəlif problemlərə uyğunlaşdırır.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Yüksək dəqiqlik və performans.\n",
    "- Çeşidli problemlərə tətbiq oluna bilir (regressiya, təsnifat və s.).\n",
    "- Parametrlərin geniş konfiqurasiyası ilə optimallaşdırma imkanı.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Hesablama baxımından bahalı ola bilər.\n",
    "- Overfitting riski yüksəkdir, xüsusilə çox sayda iterasiya ilə.\n",
    "\n",
    "## 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Optimallaşdırılmış Gradient Boosting:** Hesablama sürətini və yaddaş istifadəsini optimallaşdırır.\n",
    "- **Regularizasiya:** L1 və L2 regularizasiyası vasitəsilə overfitting-i azaldır.\n",
    "- **Paralel Təlim:** Paralel hesablama imkanları ilə təlim prosesini sürətləndirir.\n",
    "- **Tree Pruning:** Daha effektiv ağac kəsimi alqoritmləri istifadə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Sürətli və effektiv, böyük verilənlər üzərində yaxşı işləyir.\n",
    "- Yüksək performans və dəqiqlik.\n",
    "- Çoxlu sayda parametr tuning imkanları.\n",
    "- Kaggle kimi yarışmalarda geniş istifadə olunur.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Parametr tuning mürəkkəb ola bilər.\n",
    "- Daha çox resurs tələb edir.\n",
    "\n",
    "## 4. CatBoost\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Kateqorik Dəyişənlərin Avtomatik Emalı:** Kateqorik verilənləri avtomatik olaraq kodlaşdırır, əlavə ön emal tələb etmir.\n",
    "- **Order Boosting:** Overfitting-i azaltmaq üçün xüsusi təlim strategiyası istifadə edir.\n",
    "- **Default Parametrlər:** Yaxşı performans üçün az parametr tuning tələb edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Kateqorik verilənlərlə asan işləyir.\n",
    "- Overfitting riski aşağıdır.\n",
    "- Performansı yüksək, xüsusən kateqorik xüsusiyyətlər çox olan verilənlər üçün.\n",
    "- Minimal ön emal tələb edir.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Bəzi hallarda digər modellərə nisbətən daha yavaş ola bilər.\n",
    "- Bəzi platformalarda performans fərqləri ola bilər.\n",
    "\n",
    "## 5. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Əsas Xüsusiyyətləri:\n",
    "- **Sürətli Təlim:** Gradient Boosting metodunu daha sürətli və yüngül etmək üçün optimallaşdırılmışdır.\n",
    "- **Histogram-based:** Histogram əsasında təlim alqoritmi daha az yaddaş istifadə edir və sürətli təlim təmin edir.\n",
    "- **Leaf-wise Tree Growth:** Dərin ağaclar quraraq daha yaxşı dəqiqlik əldə edir.\n",
    "\n",
    "### Üstünlükləri:\n",
    "- Böyük verilənlər dəstləri üçün idealdır.\n",
    "- Yüksək sürət və effektivlik.\n",
    "- Daha az yaddaş tələb edir.\n",
    "- Yaxşı performans və dəqiqlik.\n",
    "\n",
    "### Məhdudiyyətləri:\n",
    "- Overfitting riski, xüsusilə dərin ağaclar istifadə edildikdə.\n",
    "- Kateqorik xüsusiyyətlər üçün əlavə ön emal tələb edə bilər (CatBoost-dan fərqli olaraq).\n",
    "\n",
    "## Hansını Necə Tətbiq Etmək?\n",
    "\n",
    "### **AdaBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Sadə problemlər və kiçik verilənlər üçün idealdır.\n",
    "- **Misal:** Klassik təsnifat problemləri, məsələn, spam filtrləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riski azaldılmışdır, lakin çoxlu outliers olan verilənlər üçün uyğun deyil.\n",
    "\n",
    "### **Gradient Boosting:**\n",
    "- **Uyğun Vəziyyətlər:** Yüksək dəqiqlik tələb edən müxtəlif problemlər üçün geniş istifadə olunur.\n",
    "- **Misal:** Proqnozlaşdırma modelləri, satış təhlili.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Optimal parametr tuning ilə yüksək performans əldə etmək mümkündür, amma hesablama baxımından bahalı ola bilər.\n",
    "\n",
    "### **XGBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Böyük verilənlər və yüksək performans tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Kaggle yarışmaları, böyük biznes məlumat analizi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Parametr tuning mürəkkəb ola bilər, amma yüksək performans təmin edir.\n",
    "\n",
    "### **CatBoost:**\n",
    "- **Uyğun Vəziyyətlər:** Kateqorik xüsusiyyətlər çox olan verilənlər üçün ən yaxşı seçimdir.\n",
    "- **Misal:** Marketinq analizi, müştəri seqmentləşdirməsi.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Minimal ön emal tələb edir və kateqorik verilənlərlə asan işləyir.\n",
    "\n",
    "### **LightGBM:**\n",
    "- **Uyğun Vəziyyətlər:** Çox böyük verilənlər və yüksək sürət tələb edən tətbiqlər üçün idealdır.\n",
    "- **Misal:** Real-time təhlil sistemləri, böyük verilənlər dəstləri ilə işləyən biznes tətbiqləri.\n",
    "- **Nəzərə Alınmalı Faktorlar:** Overfitting riskinə diqqət edilməli, xüsusən dərin ağaclar istifadə edildikdə.\n",
    "\n",
    "## Nəticə\n",
    "\n",
    "Hansı Boosting metodunu seçməyiniz layihənizin tələblərinə və verilənlərinizin xüsusiyyətlərinə bağlıdır. Aşağıdakı tövsiyələrdən istifadə edə bilərsiniz:\n",
    "\n",
    "- **Kateqorik verilənlər çoxdursa:** **CatBoost** ən yaxşı seçimdir.\n",
    "- **Böyük verilənlər üzərində sürətli və effektiv performans istəyirsinizsə:** **LightGBM** uyğun olar.\n",
    "- **Optimal performans və geniş optimallaşdırma imkanları üçün:** **XGBoost** istifadə edilə bilər.\n",
    "- **Sadə və kiçik verilənlər üçün:** **AdaBoost** və ya **Gradient Boosting** istifadə edilə bilər.\n",
    "\n",
    "Əlavə olaraq, müxtəlif modelləri sınayıb onların performansını müqayisə etmək, layihəniz üçün ən uyğun olanını müəyyənləşdirməyə kömək edəcəkdir. Modellərin hər biri müxtəlif vəziyyətlərdə üstünlük göstərə bilər, buna görə də verilənlərinizin xüsusiyyətlərini və layihənizin tələblərini diqqətlə qiymətləndirmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b062ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd571f2",
   "metadata": {},
   "source": [
    "## Voting de regressor tetbiqi haqqinda yaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6587a",
   "metadata": {},
   "source": [
    "**Voting Regressor** də mövcuddur və bu metod, müxtəlif regresiya modellərinin nəticələrini birləşdirərək yekun proqnoz çıxarmaq üçün istifadə olunur. Voting Regressor, modellərin birləşdirilməsində fərqli strategiyalar istifadə edərək, bir neçə fərqli regressoru birləşdirir. \n",
    "\n",
    "### **Voting Regressor**\n",
    "Voting Regressor, bir neçə fərqli regressoru birləşdirərək yekun proqnoz əldə edir. Burada əsas ideya, müxtəlif modellərin proqnozlarını birləşdirərək ümumi nəticəni yaxşılaşdırmaqdır. Voting Regressor iki növ ola bilər:\n",
    "\n",
    "1. **Averaging (Orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır və onların ortalaması çıxarılır.\n",
    "   - Məsələn, üç fərqli model (Model 1, Model 2, Model 3) bir verilənlər dəstəsi üzərində tətbiq edildikdə, hər bir modelin proqnozu alınır və bu proqnozların arifmetik ortalaması nəticə kimi qəbul edilir.\n",
    "\n",
    "2. **Weighted Averaging (Çəkili orta hesabla birləşdirmə):**\n",
    "   - Bütün modellərin proqnozları alınır, lakin hər bir modelin proqnozuna müəyyən bir çəki verilir. Daha güclü performansa sahib modellərə daha çox çəki verilir.\n",
    "   - Bu metod, daha yaxşı performans göstərən modellərin nəticələrə daha çox təsir etməsini təmin edir.\n",
    "\n",
    "### **Voting Regressor-un üstünlükləri**\n",
    "- **Model müxtəlifliyi**: Fərqli modellərin birləşdirilməsi ilə, hər bir modelin zəif cəhətləri digər modellər tərəfindən kompensasiya edilə bilər.\n",
    "- **Təkmilləşdirilmiş performans**: Model müxtəlifliyi nəticəsində Voting Regressor daha sabit və dəqiq nəticələr təmin edə bilər.\n",
    "\n",
    "### **Nümunə: Voting Regressor**\n",
    "Tutaq ki, siz Linear Regression, Decision Tree Regressor və Random Forest Regressor modellərindən istifadə edirsiniz. Bu modellərin proqnozlarını Voting Regressor ilə birləşdirə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Ayrı-ayrı modelləri təyin edək\n",
    "model1 = LinearRegression()\n",
    "model2 = DecisionTreeRegressor()\n",
    "model3 = RandomForestRegressor()\n",
    "\n",
    "# Voting Regressor\n",
    "voting_regressor = VotingRegressor(estimators=[\n",
    "    ('lr', model1), \n",
    "    ('dt', model2), \n",
    "    ('rf', model3)\n",
    "])\n",
    "\n",
    "# Modeli təlim etdirin və proqnoz verin\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "predictions = voting_regressor.predict(X_test)\n",
    "```\n",
    "\n",
    "Bu yanaşma, modellərin hər birinin güclü tərəflərindən faydalanmağa imkan verir və beləliklə, yekun proqnoz daha dəqiq ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec88396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e038e07b",
   "metadata": {},
   "source": [
    "## bagging de regressor modelleri tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d129672",
   "metadata": {},
   "source": [
    "Bəli, Bagging yalnız klassifikatorlar deyil, həm də regressor modelləri üçün tətbiq edilə bilər. Bagging metodunun əsas məqsədi, modelin təlim edildiyi verilənlər dəstələrini müxtəlif alt dəstələrə bölərək hər bir alt dəstədə fərqli modellər qurmaqdır. Bu metod, həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Bagging Regressor**\n",
    "- **Bagging Regressor**: Əsas model olaraq hər hansı bir regresor seçilə bilər, məsələn, Decision Tree Regressor, Linear Regression, və s. Bagging Regressor, müxtəlif verilənlər alt dəstələri üzərində bu əsas modelləri qurur və nəticələrin ortalamasını alaraq yekun proqnoz verir.\n",
    "  \n",
    "- **Random Forest Regressor**: Bagging-in spesifik bir tətbiqi olan Random Forest, Decision Tree Regressor-larının bir ansamblından ibarət olan bir regresor modeldir. Random Forest Regressor, verilənlər dəstəsindən təkrarən seçilmiş alt dəstələr üzərində bir neçə decision tree qurur və nəticədə bütün ağacların proqnozlarının ortalamasını alaraq yekun proqnoz verir.\n",
    "\n",
    "Bagging metodunun regressorlar üzərində istifadəsi, modelin variansını azaldır və daha stabil proqnozlar əldə etməyə kömək edir. Bu, xüsusilə decision tree kimi yüksək dəyişkənlikli modellərdə faydalıdır, çünki Bagging bu dəyişkənliyi stabilləşdirir və overfitting-in qarşısını alır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49b554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe3b13a",
   "metadata": {},
   "source": [
    "## bes boosting de regressor tetbiq edile biler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845b0ce",
   "metadata": {},
   "source": [
    "Bəli, Boosting metodları da regressor modelləri üçün tətbiq oluna bilər. Boosting, əsas modelin ardıcıl olaraq qurulması ilə işləyir, burada hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu metod da həm klassifikasiya, həm də regresiya problemlərində istifadə oluna bilər.\n",
    "\n",
    "### **Boosting Regressor**\n",
    "- **AdaBoost Regressor**: AdaBoost metodunun regresiya üçün tətbiqi. Burada hər bir ardıcıl model əvvəlki modelin proqnoz səhvlərinə daha çox diqqət yetirir. Nəticədə, modelin performansı ardıcıl olaraq yaxşılaşır.\n",
    "\n",
    "- **Gradient Boosting Regressor**: Gradient Boosting, hər yeni modelin qalıqları (yəni əvvəlki modellərin proqnoz səhvləri) üzərində qurulması ilə işləyir. Bu metod çox populyardır və yüksək dəqiqlik təmin edir. Scikit-learn kitabxanasında Gradient Boosting Regressor olaraq tətbiq edilir.\n",
    "\n",
    "- **XGBoost, LightGBM, və CatBoost**: Bunlar Boosting-in daha inkişaf etmiş versiyalarıdır və böyük verilənlər üzərində sürətli və effektiv təlim etmə qabiliyyəti ilə tanınır. Hər üç metod da regresiya problemləri üçün tətbiq oluna bilər və xüsusilə böyük ölçülü və mürəkkəb verilənlər dəstələri üzərində çox yaxşı nəticələr verir.\n",
    "\n",
    "Boosting metodları regressor modellərində də çox uğurlu ola bilər, xüsusən də verilənlər dəstəsində qeyri-xətti əlaqələr və mürəkkəb strukturlar olduqda. Bu metodlar modelin performansını artırmaq üçün ardıcıl olaraq səhvləri azaldır və nəticədə daha yaxşı proqnozlar təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8580f2f1",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96744d1",
   "metadata": {},
   "source": [
    "Aşağıdakı cədvəldə Voting, Bagging və Boosting metodlarının əsas fərqləri göstərilib:\n",
    "\n",
    "| **Xüsusiyyət**                | **Voting**                                      | **Bagging**                                   | **Boosting**                                      |\n",
    "|-------------------------------|-------------------------------------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **İş prinsipi**               | Fərqli modellərin nəticələrini birləşdirir      | Eyni modelin bir neçə varyantını təlim edir   | Modellər ardıcıl olaraq təlim edilir, səhvləri düzəldir |\n",
    "| **Modellərin birləşdirilməsi**| Bir neçə fərqli model (ən çox istifadə edilənlər) | Eyni modelin fərqli alt nümunələr üzərində təlimi | Hər bir növbəti model əvvəlkilərin səhvlərini düzəldir |\n",
    "| **İşləmə tərzi**              | Paralel                                         | Paralel                                        | Sequential (ardıcıl)                               |\n",
    "| **Overfitting riski**         | Aşağı                                           | Aşağı                                          | Daha yüksək, amma düzgün parametrlərlə idarə edilə bilər |\n",
    "| **Çeviklik**                  | Çox müxtəlif modelləri birləşdirə bilir         | Əsasən eyni modelin təkrarlanması               | Modellər daha çox özünü tənzimləyir, çətin nümunələrə fokuslanır |\n",
    "| **Hesablama mürəkkəbliyi**    | Orta                                            | Aşağı                                          | Yüksək, çünki ardıcıl təlim tələb olunur          |\n",
    "| **Məşhur alqoritmlər**        | VotingClassifier, VotingRegressor               | Random Forest, BaggingClassifier               | AdaBoost, XGBoost, Gradient Boosting, LightGBM, CatBoost |\n",
    "| **Paralel İşləmə**            | Bəli                                            | Bəli                                           | Xeyr                                              |\n",
    "| **Stabillik**                 | Artırır, modelləri birləşdirir                  | Artırır, varyansı azaldır                      | Riskli, ancaq yaxşı idarə olunarsa dəqiqlik artır  |\n",
    "| **Tətbiq sahələri**           | Hər iki klassifikasiya və reqressiya problemləri | Hər iki klassifikasiya və reqressiya problemləri | Əsasən klassifikasiya, bəzən reqressiya            |\n",
    "\n",
    "Bu cədvəl Voting, Bagging və Boosting metodlarının əsas xüsusiyyətlərini və fərqlərini ümumi şəkildə göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167de2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a60b3719",
   "metadata": {},
   "source": [
    "## voting bagging boosting feqrlerini cedvel seklinde cixart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d1071",
   "metadata": {},
   "source": [
    "| **Xüsusiyyət**                      | **Voting**                                      | **Bagging**                                      | **Boosting**                                  |\n",
    "|-------------------------------------|-------------------------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **Əsas İdeya**                      | Müxtəlif modellərin nəticələrinin birləşdirilməsi | Eyni modelin müxtəlif nümunələr və xüsusiyyətlər üzərində təlim edilməsi | Modellərin ardıcıl şəkildə təlim edilməsi və səhvlərin düzəldilməsi |\n",
    "| **Model Birləşdirmə**               | Fərqli modellər (məs. SVM, DT, LR)               | Eyni modelin fərqli versiyaları (məs. bir neçə decision tree) | Hər bir model əvvəlki modelin səhvlərini düzəldir |\n",
    "| **Nəticə Birləşdirmə**              | Səs çoxluğu və ya ortalama ilə                   | Orta/median (regressiya) və ya səs çoxluğu (klassifikasiya) | Ardıcıl modellərin nəticələri birləşdirilir |\n",
    "| **Paralel/Sequential İşləmə**       | Paralel                                         | Paralel                                          | Sequential, yəni ardıcıl |\n",
    "| **Overfitting Risk**                | Orta səviyyədə                                  | Aşağı                                            | Yüksək (əgər düzgün təlim edilməzsə)        |\n",
    "| **Əsas Məqsəd**                     | Modellərin ümumi dəqiqliyini artırmaq            | Varyansı azaltmaq və stabil nəticələr əldə etmək | Səhvləri azaltmaq və dəqiqliyi artırmaq     |\n",
    "| **İstifadə Edilən Məşhur Algoritmlər** | VotingClassifier                                | Random Forest, BaggingClassifier                 | AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost |\n",
    "| **Parametr Sayı**                   | Əsasən modellərin sayına bağlıdır               | Estimator, n_estimators, max_samples, max_features, bootstrap | Learning rate, n_estimators, base_estimator |\n",
    "| **Hesablama Çətinliyi**             | Aşağı və ya orta                                 | Orta                                             | Yüksək                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b5d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5443ad3",
   "metadata": {},
   "source": [
    "### X_train,  X_test,  y_train,  y_test  =  train_test_split(X,  y,  test_size= 0.3, stratify = y, random_state=42) \n",
    "* stratify nedir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d473ff",
   "metadata": {},
   "source": [
    "`stratify` parametri, `train_test_split` funksiyasında verilən dataset-i təlim və test dəstlərinə bölərkən, target dəyişəninin (`y`) siniflərinin nisbətini qorumaq üçün istifadə edilir. Bu, dataset-dəki siniflərin balanslı qalmasını təmin edir.\n",
    "\n",
    "### Nümunə:\n",
    "Tutaq ki, dataset-də iki sinif var: `0` və `1`. Əgər dataset-inizdə `0` sinfi 70% və `1` sinfi 30% təşkil edirsə, `stratify=y` olaraq `train_test_split` funksiyasına verildikdə, həm təlim, həm də test dəstlərində bu nisbətlər təxminən qorunacaq. Bu, modelin hər iki sinifi daha yaxşı öyrənməsinə kömək edir, xüsusilə də sinif balansı qeyri-bərabər olduqda.\n",
    "\n",
    "Əgər `stratify` parametrini verməsəniz və dataset sinifləri qeyri-bərabərdirsə, təlim və test dəstlərində siniflərin nisbəti fərqli ola bilər, bu da modelin performansına mənfi təsir göstərə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bb581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7202b8ed",
   "metadata": {},
   "source": [
    "## confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad768cf",
   "metadata": {},
   "source": [
    "`confusion_matrix` funksiyasının Sklearn-də qaytardığı qarışıqlıq matrisi aşağıdakı formada olur:\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | True Negative (TN)         | False Positive (FP)        |\n",
    "| **Əsl Sinif 1** | False Negative (FN)        | True Positive (TP)         |\n",
    "\n",
    "### Açıklama:\n",
    "- **True Negative (TN):** Əslində 0 olan nümunələr və düzgün 0 olaraq proqnozlaşdırılanlar.\n",
    "- **False Positive (FP):** Əslində 0 olan nümunələr, amma səhvən 1 olaraq proqnozlaşdırılanlar.\n",
    "- **False Negative (FN):** Əslində 1 olan nümunələr, amma səhvən 0 olaraq proqnozlaşdırılanlar.\n",
    "- **True Positive (TP):** Əslində 1 olan nümunələr və düzgün 1 olaraq proqnozlaşdırılanlar.\n",
    "\n",
    "### Confusion Matrix-dəki Hüceyrələr\n",
    "\n",
    "|                 | **Proqnoz Edilən Sinif 0** | **Proqnoz Edilən Sinif 1** |\n",
    "|-----------------|---------------------------|---------------------------|\n",
    "| **Əsl Sinif 0** | TN (True Negative)         | FP (False Positive)        |\n",
    "| **Əsl Sinif 1** | FN (False Negative)        | TP (True Positive)         |\n",
    "\n",
    "Bu cədvəl confusion matrix-dəki hüceyrələrin yerləşməsini göstərir. Sklearn-də `confusion_matrix` funksiyasını istifadə edərək əldə etdiyiniz matrisi bu qaydada oxuya bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b4016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1d0a",
   "metadata": {},
   "source": [
    "## xgbt.score(X_train, y_train)\n",
    "* bu neyin score dur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52720c",
   "metadata": {},
   "source": [
    "`xgbt.score(X_train, y_train)` metodu, `xgbt` modelinin (`XGBClassifier` obyektinin) təlim məlumat dəstində (`X_train`, `y_train`) əldə etdiyi dəqiqliyi (`accuracy`) qaytarır.\n",
    "\n",
    "### Ətraflı:\n",
    "- **`X_train`:** Təlim üçün istifadə olunan xüsusiyyətlər (features).\n",
    "- **`y_train`:** Təlim üçün istifadə olunan hədəf dəyişəni (labels).\n",
    "- **`xgbt.score`:** Dəqiqlik hesabını qaytarır, yəni təlim dəstində proqnozların neçə faizinin düzgün olduğunu göstərir.\n",
    "\n",
    "Əgər modelin proqnozları `y_train` ilə uyğun gəlirsə, `score` metodu təlim dəstindəki ümumi düzgün proqnozların nisbətini verir. Dəqiqlik hesabı 0 ilə 1 arasında dəyişir, burada 1 tam dəqiqliyi (bütün proqnozlar doğrudur) göstərir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce0fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7da861e",
   "metadata": {},
   "source": [
    " xgbt = XGBClassifier( max_depth = 2,\n",
    "*             learning_rate = 0.2,\n",
    "*             objective  = \"multi:softmax\",\n",
    "*             num_class = 2,\n",
    "*             booster = \"gbtree\",\n",
    "*             n_estimators = 10,\n",
    "*             random_state = 123)\n",
    "\n",
    "### Parametrler haqqinda melumet ver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fef5f",
   "metadata": {},
   "source": [
    "`XGBClassifier` üçün istifadə olunan parametrler aşağıdakılardır:\n",
    "\n",
    "### **Parametrlər və Təsvirləri:**\n",
    "\n",
    "1. **`max_depth`**\n",
    "   - **Təsvir:** Hər bir decision tree-nin maksimum dərinliyi.\n",
    "   - **Qeyd:** Dərinlik artdıqca model daha kompleks olur, amma overfitting riski də artır.\n",
    "\n",
    "2. **`learning_rate` (və ya `eta`)**\n",
    "   - **Təsvir:** Hər bir tree-nin təsirini azaldır. Kiçik dəyərlər daha yaxşı nəticələr verə bilər, amma daha çox tree tələb edə bilər.\n",
    "   - **Qeyd:** Düşük learning rate modelin öyrənməsini yavaşlatır və bu, daha çox `n_estimators` tələb edir.\n",
    "\n",
    "3. **`objective`**\n",
    "   - **Təsvir:** Məqsəd funksiyasını təyin edir. `multi:softmax` çox sinifli təsnifat üçün istifadə olunur.\n",
    "   - **Qeyd:** Məqsəd funksiyası, modelin öyrənmə məqsədini müəyyən edir. `multi:softmax` birbaşa siniflərin proqnozlaşdırılmasını təmin edir.\n",
    "\n",
    "4. **`num_class`**\n",
    "   - **Təsvir:** Siniflərin sayı. `objective=\"multi:softmax\"` istifadə edilərkən təyin edilir.\n",
    "   - **Qeyd:** Bu parametr çox sinifli təsnifat problemini həll edərkən mütləq olaraq təyin edilməlidir.\n",
    "\n",
    "5. **`booster`**\n",
    "   - **Təsvir:** Hansı növ boosterlə (tree-based, linear) modelləşdiriləcəyini müəyyən edir. `gbtree`, `gblinear` və `dart` seçimləri var.\n",
    "   - **Qeyd:** `gbtree` ağac əsaslı boosterlə daha kompleks problemləri həll edə bilər, `gblinear` isə xətləri modelləşdirmək üçün istifadə olunur.\n",
    "\n",
    "6. **`n_estimators`**\n",
    "   - **Təsvir:** Modeldə istifadə olunan tree-lərin sayı.\n",
    "   - **Qeyd:** Daha çox tree əlavə edildikdə model daha yaxşı öyrənə bilər, amma daha uzun təlim vaxtı və overfitting riski ilə birlikdə gəlir.\n",
    "\n",
    "7. **`random_state`**\n",
    "   - **Təsvir:** Modelin təsadüfi işlərini təyin edən ədədi dəyər.\n",
    "   - **Qeyd:** Təyin edilmiş dəyər modelin hər dəfə eyni nəticələr verməsini təmin edir, bu da nəticələrin təkrarolunmasını asanlaşdırır.\n",
    "\n",
    "### Parametrlərin Tətbiqi\n",
    "\n",
    "Yuxarıdakı parametr dəyərləri ilə XGBoost modeli qurarkən:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbt = XGBClassifier(\n",
    "    max_depth=2,                 # Tree dərinliyi\n",
    "    learning_rate=0.2,           # Təsir dərəcəsi\n",
    "    objective=\"multi:softmax\",   # Çox sinifli təsnifat\n",
    "    num_class=2,                 # İki sinif\n",
    "    booster=\"gbtree\",            # Tree-based boosting\n",
    "    n_estimators=10,             # 10 tree\n",
    "    random_state=123             # Təkrar nəticələr üçün təsadüfi dəyər\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri öz modelinizdə tənzimləməklə, performansını artırmaq və modelin tələblərinə uyğunlaşdırmaq mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e05a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "173a3fd2",
   "metadata": {},
   "source": [
    "## Regression ucun en yaxsi model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff926d",
   "metadata": {},
   "source": [
    "Regressiya problemləri üçün ən yaxşı model müxtəlif amillərə, o cümlədən verilənlərin təbiətinə, modelin interpretasiya qabiliyyətinə və performansına bağlı olaraq dəyişir. Aşağıda ən çox istifadə olunan və yüksək performans verən regressiya modellərinin qısa izahı verilmişdir:\n",
    "\n",
    "### 1. **Linear Regression**\n",
    "   - **İstifadə Sahəsi:** Sadə, xətti əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Asan tətbiq edilir və interpretasiya olunur.\n",
    "   - **Məhdudiyyətləri:** Non-linear (qeyri-xətti) əlaqələri yaxşı tuta bilmir; outliers-ə həssasdır.\n",
    "\n",
    "### 2. **Ridge və Lasso Regression**\n",
    "   - **İstifadə Sahəsi:** Yüksək ölçülü verilənlər və ya multicollinearity olan problemlər.\n",
    "   - **Üstünlükləri:** Regularization (cərimələndirmə) istifadə edərək modelin sadələşdirilməsi və overfitting-in azaldılması.\n",
    "   - **Məhdudiyyətləri:** Non-linear əlaqələr üçün optimal deyil.\n",
    "\n",
    "### 3. **Polynomial Regression**\n",
    "   - **İstifadə Sahəsi:** Non-linear əlaqələr.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Model çox kompleks ola bilər və overfitting riski yüksəkdir.\n",
    "\n",
    "### 4. **Decision Tree Regressor**\n",
    "   - **İstifadə Sahəsi:** Həm xətti, həm də xətti olmayan əlaqələr üçün istifadə olunur.\n",
    "   - **Üstünlükləri:** Asan interpretasiya olunur və outliers-lərə daha az həssasdır.\n",
    "   - **Məhdudiyyətləri:** Overfitting riski yüksəkdir, xüsusilə ağac çox dərin olduqda.\n",
    "\n",
    "### 5. **Random Forest Regressor**\n",
    "   - **İstifadə Sahəsi:** Decision Tree-dən daha stabil və dəqiq proqnozlar tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Overfitting riski azdır; yaxşı ümumi performans verir.\n",
    "   - **Məhdudiyyətləri:** Ağacların sayı çox olduqda daha çox hesablama resursu tələb edir.\n",
    "\n",
    "### 6. **Gradient Boosting Regressor**\n",
    "   - **İstifadə Sahəsi:** Yüksək dəqiqlik tələb edən non-linear əlaqələri olan verilənlər.\n",
    "   - **Üstünlükləri:** Yüksək performans; yaxşı ümumi dəqiqlik.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və parametr tuning mürəkkəbdir.\n",
    "\n",
    "### 7. **XGBoost, LightGBM, və CatBoost Regressor**\n",
    "   - **İstifadə Sahəsi:** Böyük verilənlər, kompleks əlaqələr və yüksək dəqiqlik tələb olunan hallar.\n",
    "   - **Üstünlükləri:** Yüksək sürət və performans; müxtəlif optimallaşdırmalar və avtomatik kateqorik xüsusiyyətlərin emalı.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning mürəkkəb ola bilər.\n",
    "\n",
    "### 8. **Support Vector Regression (SVR)**\n",
    "   - **İstifadə Sahəsi:** Xətti olmayan və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan əlaqələri yaxşı tuta bilir və margin-based regressiya təmin edir.\n",
    "   - **Məhdudiyyətləri:** Parametr tuning çətin ola bilər və böyük verilənlər dəstləri üçün yavaş ola bilər.\n",
    "\n",
    "### 9. **Neural Networks (MLP Regressor)**\n",
    "   - **İstifadə Sahəsi:** Çox böyük və kompleks verilənlər.\n",
    "   - **Üstünlükləri:** Xətti olmayan kompleks əlaqələri tuta bilir.\n",
    "   - **Məhdudiyyətləri:** Hesablama baxımından bahalı və interpretasiya etmək çətindir.\n",
    "\n",
    "### Hansı Modeli Seçmək?\n",
    "- **Sadə, xətti əlaqələr:** Linear Regression, Ridge və ya Lasso Regression.\n",
    "- **Xətti olmayan əlaqələr:** Polynomial Regression, Decision Tree Regressor, SVR.\n",
    "- **Yüksək performans və kompleks modellər:** Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "- **Böyük və çox ölçülü verilənlər:** LightGBM, CatBoost, Neural Networks.\n",
    "\n",
    "Verilənlərinizin xüsusiyyətlərinə uyğun olaraq modelləri sınamaq və onların performansını müqayisə etmək daha yaxşı nəticələr əldə etməyə kömək edəcək."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e6dea1",
   "metadata": {},
   "source": [
    "## Polynomial Regression model qur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26e2bc",
   "metadata": {},
   "source": [
    "Aşağıda Polynomial Regression modelini qurmaq üçün tam kod nümunəsi verilmişdir:\n",
    "\n",
    "### Adım 1: Lazımi kitabxanaların yüklənməsi\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "\n",
    "### Adım 2: Verilənlərin yüklənməsi və hazırlanması\n",
    "```python\n",
    "# Verilənləri yükləyin\n",
    "credit = pd.read_csv(r\"C:\\Users\\dell\\Desktop\\Python\\week6\\day1\\CreditCardDefault.csv\")\n",
    "credit.drop([\"ID\"], axis=1, inplace=True)\n",
    "\n",
    "# X və y dəyişənlərini təyin edin\n",
    "X = credit.iloc[:, 0:23]\n",
    "y = credit.iloc[:, -1]\n",
    "\n",
    "# Verilənləri təlim və test dəstlərinə bölün\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "\n",
    "### Adım 3: Polynomial Features yaratmaq\n",
    "```python\n",
    "# Polynomial features yaratmaq üçün PolynomialFeatures sinifini çağırın\n",
    "degree = 2  # Polynomial dərəcəsi\n",
    "poly = PolynomialFeatures(degree)\n",
    "\n",
    "# Təlim və test dəstləri üçün polynomial xüsusiyyətlər yaradın\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "```\n",
    "\n",
    "### Adım 4: Linear Regression modeli ilə polynomial regression tətbiqi\n",
    "```python\n",
    "# Linear Regression modelini qurun\n",
    "model = LinearRegression()\n",
    "\n",
    "# Polynomial verilənlər üzərində modeli təlim edin\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Test dəsti üzərində proqnozlar edin\n",
    "y_pred = model.predict(X_test_poly)\n",
    "```\n",
    "\n",
    "### Adım 5: Model performansının qiymətləndirilməsi\n",
    "```python\n",
    "# MSE və R2 dəyərlərini hesablayın\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "```\n",
    "\n",
    "### İcra\n",
    "Bu kod parçalarını addım-addım icra etdikdən sonra Polynomial Regression modelini qurmuş olacaqsınız. Kodun sonunda modelin performansını `Mean Squared Error (MSE)` və `R-squared (R2)` dəyərləri ilə ölçə biləcəksiniz.\n",
    "\n",
    "**Qeyd:** `degree` parametrini dəyişdirərək polynomial modelin dərəcəsini tənzimləyə bilərsiniz. Çox yüksək dərəcələr overfitting-ə səbəb ola bilər, buna görə dərəcəni diqqətlə seçmək vacibdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a4509e",
   "metadata": {},
   "source": [
    "## Modellerde threshold-u nece deyisim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2571ed",
   "metadata": {},
   "source": [
    "Threshold (məhdud) dəyərini dəyişdirmək, xüsusilə təsnifat modelləri üçün vacibdir, çünki default olaraq modellər çox vaxt 0.5 threshold ilə işləyir. Threshold dəyərini dəyişdirməklə, modelin proqnoz nəticələrinin \"positive\" və \"negative\" olaraq təsnif edilmə meyarını dəyişdirə bilərsiniz.\n",
    "\n",
    "### Adımlar\n",
    "\n",
    "1. **Model proqnozlarını ehtimallar kimi əldə edin.**\n",
    "   - Təsnifat modellərində `.predict_proba()` metodu istifadə edərək ehtimal dəyərlərini ala bilərsiniz.\n",
    "\n",
    "2. **Threshold-a əsasən proqnozları təsnif edin.**\n",
    "   - İstədiyiniz threshold dəyərinə əsasən ehtimalları `positive` və ya `negative` olaraq təsnif edəcəksiniz.\n",
    "\n",
    "### Kod nümunəsi\n",
    "\n",
    "Məsələn, bir təsnifat modeli üçün threshold-u necə dəyişdirəcəyinizi göstərən bir nümunə:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Model proqnozları ehtimal kimi alındı\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Positive class üçün ehtimal\n",
    "\n",
    "# Threshold dəyərini təyin edin\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "# Threshold-a əsasən proqnozları təsnif edin\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print()\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### İzahat:\n",
    "- **`predict_proba`:** Bu metod modelin hər bir sinif üçün ehtimallarını qaytarır. `[:, 1]` pozitiv sinifin ehtimallarını seçir.\n",
    "- **`threshold = 0.3`:** İstədiyiniz threshold dəyərini təyin edirsiniz. Burada 0.3 olaraq seçilib.\n",
    "- **`np.where(y_probs > threshold, 1, 0)`:** Ehtimalları seçilən threshold dəyərinə əsasən 1 (positive) və ya 0 (negative) olaraq təsnif edir.\n",
    "\n",
    "Bu yanaşma, threshold dəyərini dəyişdirərək modelin performansını müxtəlif threshold dəyərləri üçün optimallaşdırmağa və balanslaşdırmağa imkan verir. Bu, xüsusilə sinif disbalansı olan verilənlər dəstlərində faydalı ola bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e4b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb3e2f3",
   "metadata": {},
   "source": [
    "## Butun modellerde threshold-u eyni cur deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6350af",
   "metadata": {},
   "source": [
    "Bəli, threshold dəyərini dəyişdirmə üsulu demək olar ki, bütün təsnifat modellərində tətbiq edilə bilər. Lakin bu proses yalnız ehtimal əsaslı təsnifat modelləri üçün keçərlidir. Aşağıda bəzi ümumi modellər üçün bu yanaşmanı necə tətbiq etmək barədə izahatlar verilmişdir:\n",
    "\n",
    "### 1. **Logistic Regression, Random Forest, Gradient Boosting və s.**  \n",
    "Bu modellər ehtimal proqnozları qaytarır və threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 2. **Support Vector Machine (SVM)**\n",
    "SVM modelləri default olaraq ehtimal vermir, lakin `probability=True` olaraq modeli qurmaqla ehtimal əsaslı proqnozlar əldə edə bilərsiniz. Sonra threshold-u dəyişdirmək mümkündür.\n",
    "\n",
    "### 3. **Neural Networks (MLPClassifier)**\n",
    "Sinif ehtimalları `.predict_proba()` metodu ilə əldə edilir və threshold-a əsasən dəyişdirilə bilər.\n",
    "\n",
    "### 4. **Naive Bayes**\n",
    "Naive Bayes modelləri də ehtimallar qaytarır, buna görə threshold-u dəyişdirmək üçün `.predict_proba()` metodundan istifadə edilə bilər.\n",
    "\n",
    "### 5. **K-Nearest Neighbors (KNN)**\n",
    "KNN də `.predict_proba()` metodu ilə ehtimallar qaytarır və bu ehtimallara əsasən threshold dəyişdirilə bilər.\n",
    "\n",
    "### Tətbiq etmək üçün Ümumi Yanaşma:\n",
    "```python\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Pozitiv sinif ehtimalları\n",
    "\n",
    "threshold = 0.3  # İstədiyiniz threshold dəyəri\n",
    "\n",
    "y_pred_threshold = np.where(y_probs > threshold, 1, 0)\n",
    "\n",
    "# Performansı qiymətləndirin\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred_threshold))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_threshold))\n",
    "print('ROC AUC Score: ', roc_auc_score(y_test, y_pred_threshold))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred_threshold))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred_threshold))\n",
    "print('Recall Score: ', recall_score(y_test, y_pred_threshold))\n",
    "```\n",
    "\n",
    "### Qeyd:\n",
    "- **Decision Tree** kimi bəzi modellərdə ehtimal dəyərləri, leaf düyünlərinin təxminləri ilə hesablanır və `.predict_proba()` metodu vasitəsilə əldə edilə bilər.\n",
    "- **Threshold dəyərini dəyişdirmək**, modelin təsnifat balansını dəyişdirməyə imkan verir, bu da sinif disbalansı olan verilənlərdə xüsusilə faydalıdır.\n",
    "\n",
    "Beləliklə, ehtimal əsaslı təsnifat modellərində threshold dəyərini dəyişdirmək mümkün və çox vaxt faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a600c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6ead7e",
   "metadata": {},
   "source": [
    "## Boosting modellerinde, meselen XGBoost goturek. Bu modeli tetbiq ederken ozu arxa plnada hansi modeller qurur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ab38e",
   "metadata": {},
   "source": [
    "Boosting modellərində, məsələn, XGBoost istifadə edərkən, modelin arxa planda qurduğu zəif modelləri (base learners) başa düşmək vacibdir. Boosting metodları ardıcıl olaraq zəif modelləri birləşdirərək daha güclü bir model qurur. XGBoost, Gradient Boosting metodunun optimallaşdırılmış bir versiyasıdır və bu arxa planda aşağıdakı addımları həyata keçirir:\n",
    "\n",
    "### XGBoost-un Arxa Planında Qurulan Modellər\n",
    "\n",
    "1. **Zəif Modellərin (Base Learners) Qurulması:**\n",
    "   - **Decision Trees:** XGBoost, ən çox Decision Tree-lərdən istifadə edir. Bu ağaclar, çox vaxt \"stumps\" olaraq da adlandırılan çox sadə qərar ağaclarıdır. XGBoost-da bu ağaclar çox vaxt qısa və çox dərin olmur; bu, hər bir ağacın çox sadə olduğunu bildirir.\n",
    "\n",
    "2. **Ardıcıl Tətbiq:**\n",
    "   - **Gradient Descent:** XGBoost, hər yeni ağacı əvvəlki modellərin səhvlərini düzəltmək üçün qurur. Bu, gradient descent optimallaşdırma metodu vasitəsilə həyata keçirilir. Hər bir ağacın yaradılması, əvvəlki ağacların səhvlərinə (residuals) əsaslanır.\n",
    "\n",
    "3. **Öyrənmə Sürəti (Learning Rate):**\n",
    "   - **Learning Rate:** Hər yeni ağacın təsirini tənzimləyən bir öyrənmə sürəti (learning rate) istifadə edilir. Bu, hər bir ağacın proqnozlarına əlavə ediləcək ağırlığı müəyyən edir. Daha kiçik öyrənmə sürəti ilə daha çox ağac tələb olunur.\n",
    "\n",
    "4. **Regularization:**\n",
    "   - **Regularization:** XGBoost-da modelin overfitting riskini azaltmaq üçün cərimələndirmə (regularization) tətbiq edilir. Bu, ağacların böyüməsini tənzimləmək və modelin kompleksliyini azaltmaq məqsədini güdür. XGBoost L1 (Lasso) və L2 (Ridge) cərimələndirmə üsullarını dəstəkləyir.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **Early Stopping:** XGBoost, modelin performansını qiymətləndirmək üçün validasiya dəstini istifadə edir və performansın yaxşılaşmadığı yerə qədər ağacları qurmağa davam edir. Bu, overfitting-i azaltmağa kömək edir.\n",
    "\n",
    "### XGBoost-un İş Prinsipi:\n",
    "\n",
    "1. **Başlanğıc Model:** Əvvəlcə bütün verilənlər üçün proqnozları təsadüfi bir başlanğıc dəyəri ilə başlatır.\n",
    "2. **Residual Hesablanması:** Təxmin edilmiş dəyərlərlə real dəyərlər arasındakı fərqi hesablamaqla qalıqları (residuals) tapır.\n",
    "3. **Yeni Ağacın Qurulması:** Qalıqları (residuals) düzəltmək üçün yeni bir Decision Tree qurur. Bu ağac qalıqların düzəldilməsinə kömək edir.\n",
    "4. **Proqnozlara Əlavə:** Yeni ağacın təxminləri əvvəlki proqnozlara əlavə edilir.\n",
    "5. **Yenidən Hesablamalar:** Bu proses təkrar olunur, hər dəfə yeni ağac əvvəlki ağacların səhvlərini düzəldir.\n",
    "\n",
    "Bu üsulla, XGBoost bir neçə zəif modelin (Decision Trees) ardıcıl olaraq birləşdirilməsi ilə güclü və dəqiq bir model qurur. Bu yanaşma, modelin öyrənmə qabiliyyətini artırır və daha yaxşı nəticələr əldə etməyə kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdcfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a3e09c",
   "metadata": {},
   "source": [
    "## Boosting modelleri arxa planda hansi modelleri qurur. Bir bir hamisi haqqinda yaz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c83a6",
   "metadata": {},
   "source": [
    "Boosting modelləri arxa planda zəif modellər (base learners) qurur. Ən çox istifadə olunan boosting modelləri bunlardır: AdaBoost, Gradient Boosting, XGBoost, LightGBM, və CatBoost. Aşağıda hər birinin arxa planda qurduğu modellərin xüsusiyyətlərini izah edirəm:\n",
    "\n",
    "### 1. **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** Ən çox **Decision Trees** (adətən \"stumps\" adlanan çox sadə ağaclar) istifadə edilir. Hər bir ağac, əvvəlki ağacların səhvlərinə daha çox diqqət yetirir.\n",
    "- **İş Prinsipi:** AdaBoost, hər yeni ağacın əvvəlki ağacların səhvlərini düzəltməyə çalışır. Səhv təsnif edilən nümunələrə daha çox ağırlıq verilir, beləliklə, yeni ağaclar bu nümunələrə daha çox diqqət yetirir.\n",
    "\n",
    "### 2. **Gradient Boosting**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** istifadə edilir. Bu ağaclar adətən daha dərin və daha kompleks ola bilər, lakin hələ də çox sadə modellərdir.\n",
    "- **İş Prinsipi:** Hər yeni ağac, əvvəlki ağacların qalıqları üzərində işləyərək səhvləri düzəltməyə çalışır. Hər ağac gradient descent metodunu istifadə edərək əvvəlki ağacların səhvlərini azaldır.\n",
    "\n",
    "### 3. **XGBoost (Extreme Gradient Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha kiçik və daha sadə).\n",
    "- **İş Prinsipi:** XGBoost, Gradient Boosting metodunun optimallaşdırılmış versiyasıdır. Bu metod eyni zamanda L1 (Lasso) və L2 (Ridge) cərimələndirmə tətbiq edir, və ağacların böyüməsini tənzimləyərək overfitting-i azaldır. Hər ağac əvvəlki ağacların səhvlərinə əsaslanaraq yaradılır və öyrənmə sürəti (learning rate) ilə tənzimlənir.\n",
    "\n",
    "### 4. **LightGBM (Light Gradient Boosting Machine)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (adətən daha yüngül və daha sürətli öyrənən ağaclar).\n",
    "- **İş Prinsipi:** LightGBM, Gradient Boosting-in daha sürətli və daha az resurs tələb edən variantıdır. Ağacları \"leaf-wise\" bölmələrdən istifadə edərək qurur, bu da daha dərin və daha təkmil ağaclara gətirib çıxarır. Bu yanaşma, daha sürətli və daha yaxşı performans verir, xüsusilə böyük verilənlər dəstələrində.\n",
    "\n",
    "### 5. **CatBoost (Categorical Boosting)**\n",
    "\n",
    "- **Zəif Modellər:** **Decision Trees** (daha optimallaşdırılmış və kateqorik dəyişənləri avtomatik emal edən ağaclar).\n",
    "- **İş Prinsipi:** CatBoost, Gradient Boosting-in xüsusilə kateqorik verilənlərlə işləməyə optimallaşdırılmış versiyasıdır. Bu model kateqorik dəyişənlərin avtomatik kodlaşdırılmasını təmin edir və şumun qarşısını almaq üçün müxtəlif texnikalardan istifadə edir. Ağaclar adətən daha qısa və effektiv olur, modelin öyrənmə sürəti yüksəkdir.\n",
    "\n",
    "### Ümumi Yanaşma:\n",
    "Bütün bu boosting modelləri, zəif modelləri (adətən Decision Trees) bir-birinin ardınca quraraq, əvvəlki modellərin səhvlərini düzəltməyə çalışır. Hər yeni model əvvəlki modellərin qalıqları üzərində işləyir və modelin ümumi performansını artırır.\n",
    "\n",
    "Bu yanaşmaların hər biri, müxtəlif optimallaşdırma və cərimələndirmə üsulları ilə modelin performansını artırmağa və overfitting-i azaltmağa çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5b5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24406bd5",
   "metadata": {},
   "source": [
    "## Bagging-de eyni model qurulur? Boosting-de eyni model qurulur yoxsa bir nece model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a180d",
   "metadata": {},
   "source": [
    "Bagging və Boosting metodları arasında əsas fərq ondan ibarətdir ki, hər biri modelləri necə qurur və birləşdirir.\n",
    "\n",
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** metodunda bir neçə eyni model (həmin növ model) qurulur, lakin bu modellər fərqli verilənlər dəstləri üzərində öyrədilir. \n",
    "\n",
    "- **Eyni Model:** Bəli, **bagging** metodunda eyni növ modeldən bir neçə dəfə istifadə edilir. Məsələn, əgər bagging metodunda Decision Tree istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Verilənlər Dəstləri:** Bagging, orijinal verilənlər dəstinin təkrar nümunələri ilə yaradılan fərqli verilənlər dəstlərini (bootstrap samples) istifadə edərək hər bir modeli öyrədir.\n",
    "- **Final Nəticə:** Hər bir modelin proqnozları birləşdirilir. Təsnifat üçün bu, adətən səsvermə (majority voting) üsulu ilə, regresiyada isə ortalama (averaging) ilə həyata keçirilir.\n",
    "\n",
    "**Nəticə:** Bagging metodunda bir neçə eyni model yaradılır, lakin bu modellər fərqli verilənlər üzərində öyrədilir və final nəticə bütün modellərin nəticələrinin birləşdirilməsi ilə alınır.\n",
    "\n",
    "### Boosting\n",
    "\n",
    "**Boosting** metodunda isə modellər ardıcıl olaraq qurulur, və hər bir yeni model əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu prosesdə modellərin hər biri eyni növ olsa da, onların hər biri müxtəlif verilənlər üzərində və fərqli səhvlər üzərində öyrədilir.\n",
    "\n",
    "- **Eyni Model:** Bəli, **boosting** metodunda da eyni növ model istifadə edilir. Məsələn, Gradient Boosting-də Decision Tree-lər istifadə edilirsə, bütün modellər Decision Tree olacaq.\n",
    "- **Modellərin Qurulması:** Lakin burada fərq ondan ibarətdir ki, hər bir yeni model əvvəlki modellərin səhvlərini düzəltmək üçün qurulur, yəni hər model fərqli şəkildə optimallaşdırılmış bir variant kimi fəaliyyət göstərir.\n",
    "- **Final Nəticə:** Hər bir modelin nəticələri birləşdirilir, lakin bu birləşmə hər bir modelin səhvlərini düzəltmək üçün fərqli çəkilərdən istifadə edir.\n",
    "\n",
    "**Nəticə:** Boosting metodunda eyni növ model bir neçə dəfə qurulur, lakin bu modellər bir-birindən fərqlidir, çünki hər biri əvvəlki modellərin səhvlərini düzəltməyə çalışır. Bu modellər ardıcıl olaraq qurulur və nəticələr birləşdirilərək daha güclü bir model yaradılır.\n",
    "\n",
    "### Ümumi Qısa Xülasə:\n",
    "\n",
    "- **Bagging:** Eyni növ model, fərqli verilənlər dəstləri üzərində paralel olaraq öyrədilir və nəticələr birləşdirilir.\n",
    "- **Boosting:** Eyni növ model, ardıcıl olaraq qurulur və hər bir yeni model əvvəlki modellərin səhvlərini düzəldir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365bb8e2",
   "metadata": {},
   "source": [
    "## Modelin overfitting edib etmediyini nece bilim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0322b2",
   "metadata": {},
   "source": [
    "Modelin overfitting edib-etmədiyini müəyyən etmək üçün bir neçə üsul və göstəricidən istifadə etmək mümkündür. Overfitting, modelin təlim məlumatlarına həddindən artıq uyğunlaşdığı, lakin test və ya yeni məlumatlarla yaxşı performans göstərmədiyi bir vəziyyətdir. Overfitting olub-olmadığını müəyyən etmək üçün aşağıdakı yanaşmaları istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **Train-Test Split və Performans Müqayisəsi**\n",
    "   - **Train Score:** Modelin təlim (train) məlumatlarındakı performansını ölçün. Yüksək dəqiqlik (accuracy) və ya digər performans metrikləri (məsələn, R² score) modelin təlim məlumatlarına çox yaxşı uyğunlaşdığını göstərə bilər.\n",
    "   - **Test Score:** Modelin test (validation) məlumatlarındakı performansını ölçün. Əgər test dəstindəki performans əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   train_score = model.score(X_train, y_train)\n",
    "   test_score = model.score(X_test, y_test)\n",
    "   \n",
    "   print(\"Train Score:\", train_score)\n",
    "   print(\"Test Score:\", test_score)\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train score çox yüksək, test score isə əhəmiyyətli dərəcədə aşağıdırsa, bu, overfitting işarəsidir.\n",
    "\n",
    "### 2. **Learning Curve**\n",
    "   - **Learning Curve:** Train və test məlumatları üzərində modelin performansını müxtəlif təlim məlumatları ilə vizuallaşdıran bir əyri çəkin. Əgər train əyrisi yuxarıda qalır və test əyrisi daha aşağıda qalırsa, bu da overfitting göstəricisidir.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import learning_curve\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "   plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "   plt.xlabel(\"Training examples\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Əgər train əyrisi yüksək performans göstərir, lakin test əyrisi daha aşağı qalırsa və aradakı fərq böyükdürsə, bu da overfitting-i göstərir.\n",
    "\n",
    "### 3. **Cross-Validation**\n",
    "   - **Cross-Validation:** Verilənlər dəstini bir neçə hissəyə bölərək modelin performansını müxtəlif hissələr üzərində yoxlayın. Əgər modelin performansı müxtəlif hissələr arasında çox dəyişirsə, bu da overfitting işarəsi ola bilər.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "   print(\"Cross-Validation Scores:\", scores)\n",
    "   print(\"Mean Score:\", np.mean(scores))\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Cross-validation nəticələri arasında böyük fərqlər varsa, bu, modelin müəyyən verilənlər dəstlərinə həddən artıq uyğunlaşdığını göstərə bilər.\n",
    "\n",
    "### 4. **Validation Curve**\n",
    "   - **Validation Curve:** Modelin müəyyən parametrləri üzrə performansını yoxlamaq üçün validation curve çəkə bilərsiniz. Əgər train performansı yüksəkdir, lakin validation performansı aşağıdırsa, overfitting ehtimalı var.\n",
    "\n",
    "   **Misal:**\n",
    "   ```python\n",
    "   from sklearn.model_selection import validation_curve\n",
    "\n",
    "   param_range = np.logspace(-6, -1, 5)\n",
    "   train_scores, test_scores = validation_curve(model, X, y, param_name=\"alpha\", param_range=param_range, cv=5)\n",
    "\n",
    "   train_scores_mean = np.mean(train_scores, axis=1)\n",
    "   test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "   plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "   plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "   plt.xlabel(\"Parameter\")\n",
    "   plt.ylabel(\"Score\")\n",
    "   plt.legend(loc=\"best\")\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   - **Overfitting əlaməti:** Train əyrisi yüksək, validation əyrisi isə daha aşağı qalırsa, overfitting ehtimalı var.\n",
    "\n",
    "### 5. **Regularization Yoxlamaq**\n",
    "   - **Regularization:** Regularization (L1, L2 cərimələndirmə) metodlarından istifadə etməklə modelin overfitting edilib-etmədiyini yoxlaya bilərsiniz. Regularization overfitting-in qarşısını almağa kömək edir.\n",
    "\n",
    "   - **Overfitting əlaməti:** Regularization tətbiq etdikdə modelin performansı daha sabit və balanslı olursa, bu, overfitting-in qarşısını almağa kömək edə bilər.\n",
    "\n",
    "### 6. **Modelin Kompleksliyi**\n",
    "   - **Modelin Kompleksliyi:** Çox kompleks modellər (çox dərin decision tree-lər, çox sayda hidden layer-ləri olan neyron şəbəkələr və s.) asanlıqla overfitting edə bilər. Sadə modellər istifadə edərək performansı müqayisə etmək də overfitting-i müəyyən etməyə kömək edə bilər.\n",
    "\n",
    "   - **Overfitting əlaməti:** Çox kompleks model çox yüksək train performansı və aşağı test performansı göstərə bilər.\n",
    "\n",
    "Bu metodlar və yanaşmalarla modelin overfitting edib-etmədiyini müəyyən edə bilərsiniz və ehtiyac olduqda daha yaxşı performans əldə etmək üçün modelin quruluşunu və parametrlərini tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f3902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b126fc89",
   "metadata": {},
   "source": [
    "### train_score = model.score(X_train, y_train)\n",
    "### test_score = model.score(X_test, y_test)\n",
    "## Bunlar accuracy dir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66317e3",
   "metadata": {},
   "source": [
    "Bəli, `model.score(X_train, y_train)` və `model.score(X_test, y_test)` metodları adətən **accuracy** dəyərini qaytarır, amma bu, yalnız təsnifat (classification) modelləri üçün keçərlidir. \n",
    "\n",
    "### Təfsilatlı izah:\n",
    "- **Classification Modelləri üçün:** \n",
    "  - `model.score(X_train, y_train)` və `model.score(X_test, y_test)` təlim və test dəstlərində modelin təsnifat dəqiqliyini (accuracy) qaytarır. Bu, düzgün proqnozların ümumi nümunələrə nisbətidir.\n",
    "\n",
    "  ```python\n",
    "  accuracy = correct_predictions / total_predictions\n",
    "  ```\n",
    "\n",
    "- **Regression Modelləri üçün:** \n",
    "  - Əgər model regresiya modelidirsə (məsələn, Linear Regression, Ridge, Lasso), `model.score()` metodunun qaytardığı dəyər **R² score** olacaq. R² score, modelin verilənlər dəstinə nə qədər yaxşı uyğun gəldiyini ölçür və proqnozların faktiki dəyərlərə nə qədər yaxın olduğunu göstərir.\n",
    "\n",
    "  ```python\n",
    "  R² = 1 - (SS_res / SS_tot)\n",
    "  ```\n",
    "  Burada:\n",
    "  - `SS_res`: Proqnozlaşdırılmış dəyərlərin və faktiki dəyərlərin kvadrat fərqlərinin cəmi.\n",
    "  - `SS_tot`: Faktiki dəyərlərin və onların ortalama dəyərinin kvadrat fərqlərinin cəmi.\n",
    "\n",
    "### Nəticə:\n",
    "Əgər təsnifat modeli istifadə edirsinizsə, `model.score()` metodu **accuracy** dəyərini, əgər regresiya modeli istifadə edirsinizsə, **R² score** qaytaracaq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8df425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b7828c",
   "metadata": {},
   "source": [
    "## Bank datalari ucun ve elektronika satan sirket datalari ucun en cox hansi modeller istifade olunur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ba52d",
   "metadata": {},
   "source": [
    "Bank dataları və elektronika satan şirkətlərin məlumatları üçün ən uyğun modellər seçilərkən, bu dataların xarakteristikası və məqsəd nəzərə alınır. Hər bir sektorun özünəməxsus xüsusiyyətləri var, buna görə də istifadə olunan modellər fərqlənə bilər.\n",
    "\n",
    "### Bank Dataları Üçün\n",
    "Bank sektoru əsasən müştərilərin maliyyə məlumatları, kredit riski, fırıldaqçılıq aşkar edilməsi, müştəri seqmentasiyası kimi mövzulara yönəlir. Bu səbəbdən bank məlumatları üçün geniş istifadə olunan modellər:\n",
    "\n",
    "1. **Logistic Regression:**\n",
    "   - Əsasən kredit riski, müştəri davranışı və fırıldaqçılıq aşkar etmək üçün istifadə olunur.\n",
    "   - Məqsəd: İkili təsnifat (binary classification) məsələləri.\n",
    "\n",
    "2. **Random Forest:**\n",
    "   - Fərqli atributlar üzərində güclü performans göstərir və feature importance üçün istifadə edilir.\n",
    "   - Məqsəd: Təsnifat və regresiya məsələləri.\n",
    "\n",
    "3. **Gradient Boosting (XGBoost, LightGBM, CatBoost):**\n",
    "   - Bank dataları üçün xüsusilə effektivdir, çünki bu metodlar mürəkkəb əlaqələri və nümunələri öyrənə bilir.\n",
    "   - Məqsəd: Kredit skoring, risk analizi, müştəri churn təxminatı.\n",
    "\n",
    "4. **Support Vector Machines (SVM):**\n",
    "   - Daha az məlumat üçün güclü performans göstərir, lakin böyük datalar üçün hesablama baxımından bahalı ola bilər.\n",
    "   - Məqsəd: Kredit riski və fırıldaqçılıq aşkar edilməsi.\n",
    "\n",
    "5. **K-Means Clustering:**\n",
    "   - Müştəri seqmentasiyası üçün istifadə olunur.\n",
    "   - Məqsəd: Müştəri qruplarını tapmaq və onlara fərqli maliyyə məhsulları təklif etmək.\n",
    "\n",
    "### Elektronika Satan Şirkətlərin Dataları Üçün\n",
    "Elektronika satan şirkətlər əsasən satış təxminatı, müştəri davranışı, stok idarəsi və qiymət optimallaşdırması üzərində işləyirlər. Bu sahə üçün istifadə olunan əsas modellər:\n",
    "\n",
    "1. **Linear Regression:**\n",
    "   - Satış proqnozları və qiymət optimallaşdırılması üçün istifadə olunur.\n",
    "   - Məqsəd: Kəmiyyət (quantitative) proqnozlar.\n",
    "\n",
    "2. **Time Series Analysis (ARIMA, SARIMA):**\n",
    "   - Satış və tələbat proqnozları üçün zaman seriyası analizləri geniş istifadə olunur.\n",
    "   - Məqsəd: Gələcək satışların və tələbin proqnozlaşdırılması.\n",
    "\n",
    "3. **Random Forest və Gradient Boosting:**\n",
    "   - Elektronika məhsullarının satışlarına təsir edən müxtəlif faktorları təhlil etmək üçün istifadə olunur.\n",
    "   - Məqsəd: Təsnifat və proqnozlaşdırma.\n",
    "\n",
    "4. **Collaborative Filtering (Recommendation Systems):**\n",
    "   - Məhsul təklifləri və müştəriyə fərdi təkliflərin hazırlanması üçün geniş istifadə olunur.\n",
    "   - Məqsəd: Müştəriyə uyğun məhsul təklifləri vermək.\n",
    "\n",
    "5. **Clustering (K-Means, DBSCAN):**\n",
    "   - Müştəri seqmentasiyası və marketinq strategiyalarının tərtibi üçün.\n",
    "   - Məqsəd: Müştəri qruplarının aşkar edilməsi və hədəfli reklam strategiyalarının inkişafı.\n",
    "\n",
    "### Nəticə\n",
    "Bank məlumatları üçün daha çox təsnifat modelləri (logistic regression, random forest, gradient boosting) istifadə olunur. Elektronika satan şirkətlər isə satış proqnozları və tələbat analizi üçün zaman seriyaları modelləri və regresiya modellərinə üstünlük verirlər. Hər iki sahədə də clustering müştəri seqmentasiyası üçün geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46597909",
   "metadata": {},
   "source": [
    "## CatBoost-da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba7ccd",
   "metadata": {},
   "source": [
    "CatBoost modeli daxilində işləyən spesifik modelləri dəyişdirmək mümkün deyil, çünki CatBoost özü qərar ağacları (decision trees) əsasında işləyən bir **Gradient Boosting** metodudur və daxili arxitekturası istifadəçilər üçün dəyişdirilə bilən deyil. CatBoost, xüsusilə kateqorik verilənlərlə effektiv işləmək üçün optimallaşdırılmışdır və daxili olaraq xüsusi bir **symmetrical tree** strukturu istifadə edir.\n",
    "\n",
    "Bununla belə, siz CatBoost modelinin performansını və davranışını təsir edən müxtəlif hiperparametrləri tənzimləməklə modelin necə işlədiyini dəyişə bilərsiniz. Məsələn:\n",
    "\n",
    "### 1. **`depth` (ağacın dərinliyi):**\n",
    "   - Modelin hər bir ağacının maksimum dərinliyini təyin edir. Daha böyük dərinlik daha kompleks modellər və overfitting riskini artırır.\n",
    "\n",
    "### 2. **`learning_rate`:**\n",
    "   - Modelin hər bir iterasiyada addım böyüklüyünü təyin edir. Daha aşağı qiymətlər daha sabit, amma daha yavaş öyrənməyə səbəb olur.\n",
    "\n",
    "### 3. **`iterations`:**\n",
    "   - Ümumi ağac sayını müəyyən edir. Daha çox iterasiya adətən daha yüksək dəqiqlik, amma overfitting riski ilə nəticələnə bilər.\n",
    "\n",
    "### 4. **`l2_leaf_reg`:**\n",
    "   - Ağacların budaqlanmasının qarşısını almaq üçün cərimələndirmə əmsalıdır (regularization term). Daha böyük dəyər daha az kompleks modellərə və overfitting riskinin azalmasına səbəb ola bilər.\n",
    "\n",
    "### 5. **`one_hot_max_size`:**\n",
    "   - Kateqorik dəyişənlər üçün one-hot encoding tətbiq ediləcək maksimum unikal dəyər sayını təyin edir. Bu, xüsusilə çox unikal kateqorik dəyərə malik atributlar üçün faydalıdır.\n",
    "\n",
    "### 6. **`boosting_type`:**\n",
    "   - CatBoost modeli əsasən **Ordered Boosting** və **Plain Boosting** kimi iki növ boosting metodu təklif edir:\n",
    "     - **Ordered Boosting**: Overfitting-in qarşısını almaq üçün daha yaxşıdır.\n",
    "     - **Plain Boosting**: Daha sürətli öyrənməyə səbəb ola bilər, amma overfitting riskini artıra bilər.\n",
    "\n",
    "Bu hiperparametrləri dəyişərək CatBoost modelinin performansını və davranışını təyin edə bilərsiniz, amma daxildəki modeli fərqli bir model ilə əvəz etmək mümkün deyil, çünki CatBoost özü bir boosting alqoritmi kimi çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69438d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d07da7",
   "metadata": {},
   "source": [
    "## XGBoost da daxilde hansi model islesin, onu deyise bilirem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e74fbc",
   "metadata": {},
   "source": [
    "XGBoost (eXtreme Gradient Boosting) modelində də spesifik olaraq işləyən \"daxili model\" anlayışı yoxdur, çünki XGBoost özü qərar ağacları (decision trees) üzərində qurulmuş bir **Gradient Boosting** metodudur. Bununla belə, siz XGBoost modelində bəzi hiperparametrləri dəyişdirərək modelin necə işlədiyini təsir edə bilərsiniz. Amma bu, istifadə olunan modelin növünü deyil, qərar ağaclarının quruluşunu və iş rejimini dəyişdirir.\n",
    "\n",
    "### XGBoost Modelində Dəyişdirilə Bilən Əsas Hiperparametrlər:\n",
    "\n",
    "1. **`booster`:**\n",
    "   - **`gbtree`**: Standart qərar ağaclarına əsaslanan boosting (ən geniş yayılmış).\n",
    "   - **`gblinear`**: Qərar ağacları əvəzinə xətti modellər (linear boosters) istifadə edilir.\n",
    "   - **`dart`**: Dropouts təklif edir ki, bu da gbtree-də olan bəzi ağacları tərk edərək overfitting-i azalda bilər.\n",
    "\n",
    "   Yəni, XGBoost-da \"model\" əvəzinə, fərqli boosting metodlarını seçə bilərsiniz.\n",
    "\n",
    "2. **`max_depth`:**\n",
    "   - Hər bir ağacın maksimum dərinliyini təyin edir. Dərinlik nə qədər böyükdürsə, model o qədər kompleks və potensial olaraq overfitting riski böyükdür.\n",
    "\n",
    "3. **`learning_rate`:**\n",
    "   - Yeni ağacın verdiyi töhfənin ölçüsünü təyin edir. Aşağı dəyər daha stabillik gətirir, amma modelin öyrənmə sürətini azaldır.\n",
    "\n",
    "4. **`n_estimators`:**\n",
    "   - Ağacların ümumi sayını təyin edir. Daha çox ağac daha yaxşı performansa gətirib çıxara bilər, amma overfitting riski də artar.\n",
    "\n",
    "5. **`subsample`:**\n",
    "   - Hər bir ağac üçün istifadə edilən təlim məlumatlarının faizini təyin edir. Bu, modelin daha az məlumat üzərində öyrənməsinə və overfitting riskinin azalmasına kömək edir.\n",
    "\n",
    "6. **`colsample_bytree`, `colsample_bylevel`, `colsample_bynode`:**\n",
    "   - Hər bir ağac üçün (və ya səviyyə və ya düyün üçün) istifadə edilən atributların faizini təyin edir. Bu da overfitting riskini azaldır.\n",
    "\n",
    "7. **`gamma`:**\n",
    "   - Ağacların bölünməsinə nəzarət edən cərimələndirmə əmsalıdır. Yüksək dəyər daha az bölünmə və daha sadə modellərlə nəticələnir.\n",
    "\n",
    "### Yekun\n",
    "XGBoost-da da daxildə işləyən model dəyişdirilə bilməz, çünki o, Gradient Boosting-ə əsaslanan bir metoddur. Lakin siz modelin necə öyrəndiyini və performansını dəyişdirmək üçün müxtəlif hiperparametrləri tənzimləyə bilərsiniz. Məsələn, **`booster`** parametri ilə \"gbtree\", \"gblinear\", və \"dart\" arasında seçim edə bilərsiniz ki, bu da modelin davranışını xeyli dərəcədə dəyişə bilər."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c889867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7eee2b",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansinda daxili modeli deyismek olar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13699f",
   "metadata": {},
   "source": [
    "Boosting modellərində ümumiyyətlə \"daxili model\" anlayışı yoxdur, çünki bu modellərin strukturu müəyyən bir əsas modelə, adətən qərar ağaclarına (decision trees), əsaslanır. Bu səbəbdən, boosting metodlarında daxili modeli dəyişdirmək mümkün olmur. \n",
    "\n",
    "### Əsas Boosting Modelləri və Daxili Model Seçimi:\n",
    "\n",
    "1. **XGBoost, LightGBM, və CatBoost:**\n",
    "   - Bu modellər qərar ağacları əsasında işləyir və istifadəçilər üçün daxili modelin (məsələn, qərar ağacları əvəzinə xətti modellər və ya başqa bir model) dəyişdirilməsi imkanı təqdim etmir.\n",
    "   - Yeganə istisna XGBoost-da **`booster`** parametridir, burada \"gbtree\" (qərar ağacları) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz. Lakin bu, daha çox modelin strukturu ilə əlaqədardır və tam mənada \"daxili model\" dəyişimi kimi qəbul edilmir.\n",
    "\n",
    "2. **AdaBoost:**\n",
    "   - AdaBoost-da istifadə olunan zəif modellər adətən qərar ağaclarıdır, lakin bu modelin təbiətinə görə zəif model olaraq fərqli algoritmlər (məsələn, xətti regresiya, SVM və ya başqa modellər) istifadə oluna bilər. Bu, AdaBoost-un digər boosting metodlarından fərqli olmasının əsas səbəbidir.\n",
    "   - Yəni, **AdaBoost** modelində istifadə olunan zəif modelləri dəyişdirmək mümkündür.\n",
    "\n",
    "### Nəticə\n",
    "Əgər daxili modeli dəyişmək imkanını istəyirsinizsə, **AdaBoost** istifadə etməlisiniz. AdaBoost-da zəif model olaraq fərqli algoritmlərdən istifadə edə bilərsiniz. Lakin digər məşhur boosting metodları olan XGBoost, LightGBM, və CatBoost-da daxili model dəyişdirilməsi mümkün deyil, çünki bunlar xüsusi olaraq qərar ağacları üzərində işləmək üçün dizayn olunub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eccd75",
   "metadata": {},
   "source": [
    "## Gradient Boosting-de daxili modeli deyismek olmur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b54636",
   "metadata": {},
   "source": [
    "Bəli, Gradient Boosting metodunda da daxili model dəyişdirilmir. Gradient Boosting, adətən qərar ağacları (decision trees) istifadə edərək işləyən bir boosting metodudur və bu, metodun əsas strukturunu təşkil edir. \n",
    "\n",
    "### Əsas Nöqtələr:\n",
    "- **Gradient Boosting** metodunun əsas ideyası, zəif modellərin (adətən qərar ağacları) ardıcıl olaraq qurulması və hər bir yeni modelin əvvəlki modelin qalıqlarını (residuals) düzəltməyə çalışmasıdır.\n",
    "- **Qərar ağacları** bu metodun əsasında durur və buna görə də daxili model olaraq dəyişdirilə bilməz.\n",
    "\n",
    "### İstisnalar və Seçimlər:\n",
    "- **XGBoost** və **LightGBM** kimi Gradient Boosting-in optimallaşdırılmış versiyaları da əsasən qərar ağacları üzərində işləyir. Bu modellərdə də daxili modeli dəyişdirmək mümkün deyil.\n",
    "- Yalnız **XGBoost** modelində `booster` parametri ilə \"gbtree\" (qərar ağacı) və \"gblinear\" (xətti model) arasında seçim edə bilərsiniz, amma bu tam olaraq daxili modelin dəyişdirilməsi deyil. Bu parametr sadəcə olaraq modelin xətti və ya ağac əsaslı olacağını təyin edir.\n",
    "\n",
    "### Nəticə:\n",
    "Gradient Boosting metodunda daxili model adətən dəyişdirilmir və əsas olaraq qərar ağacları istifadə olunur. Bu, metodun fundamental xüsusiyyətlərindən biridir və onun effektivliyini təmin edən əsas səbəblərdən biridir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938f93d4",
   "metadata": {},
   "source": [
    "## Robust Scale haqqinda genis melumat?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3a003",
   "metadata": {},
   "source": [
    "**Robust Scaler** məlumat dəstələrinin (datasets) miqyasını dəyişdirmək üçün istifadə olunan bir normalizasiya texnikasıdır. Bu metod ənənəvi miqyaslandırma üsulları (məsələn, Standart Scaler, Min-Max Scaler) ilə müqayisədə məlumat dəstələrindəki outlier-lərə (məlumatların ümumi tendensiyasından uzaq olan dəyərlərə) qarşı daha davamlıdır. \n",
    "\n",
    "### Robust Scaler-in Əsas Xüsusiyyətləri:\n",
    "\n",
    "1. **Median Və İQR Əsaslı Miqyaslandırma:**\n",
    "   - **Median**: Məlumat dəstəsinin ortasında olan dəyərdir və bu dəyər məlumat dəstəsindəki dəyərlərin yarısının bu dəyərdən az, yarısının isə çox olduğunu göstərir.\n",
    "   - **IQR (Interquartile Range)**: Məlumat dəstəsinin 25-ci və 75-ci percentilləri arasındakı fərqdir. Bu aralıq mərkəzləşmiş dəyərləri əhatə edir və outlier-lərdən daha az təsirlənir.\n",
    "   - Robust Scaler məlumat dəstəsini median ətrafında mərkəzləşdirir və dəyərləri IQR ilə miqyaslandırır. Yəni, hər bir dəyərdən median çıxılır və sonra IQR-ə bölünür:\n",
    "     $$ \n",
    "     X_{\\text{scaled}} = \\frac{X - \\text{Median}(X)}{\\text{IQR}(X)}\n",
    "      $$\n",
    "   - Bu yanaşma məlumat dəstəsində outlier-lər olduğu halda belə, məlumatların düzgün miqyaslandırılmasını təmin edir.\n",
    "\n",
    "2. **Outlier-lərə Qarşı Dayanıqlıq:**\n",
    "   - Outlier-lər, məlumatların əksəriyyətinin uzaqında yerləşən ekstremal dəyərlərdir. Ənənəvi miqyaslandırma üsulları bu dəyərlərə həssas ola bilər və nəticədə miqyaslandırılmış dəyərlər qeyri-adekvat ola bilər.\n",
    "   - Robust Scaler, median və IQR istifadə etdiyi üçün outlier-lərin təsirini azaldır və məlumatların miqyaslandırılmasını daha sabit və dəqiq edir.\n",
    "\n",
    "3. **Tətbiq Sahələri:**\n",
    "   - **Maşın Öyrənməsi (Machine Learning):** Məlumat dəstələrinin outlier-lərlə zəngin olduğu vəziyyətlərdə xüsusən istifadə olunur. Bu, modellərin (məsələn, linear regression, SVM) performansını artırmağa kömək edə bilər.\n",
    "   - **Peyk Məlumatları:** Məsələn, peyk görüntüləri və ya uzaqdan zondlama (remote sensing) məlumatları çox sayda outlier-lərlə işləyə bilər.\n",
    "   - **Finans Məlumatları:** Finansal məlumatlarda outlier-lər tez-tez rast gəlinir. Robust Scaler bu məlumatları miqyaslandırmaq üçün əlverişlidir.\n",
    "\n",
    "### Nümunə: Python ilə Robust Scaler\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Random data with outliers\n",
    "data = np.array([[1, 2, 2], \n",
    "                 [3, 6, 4], \n",
    "                 [7, 18, 8], \n",
    "                 [9, 10, 12], \n",
    "                 [20, 50, 90]])\n",
    "\n",
    "# RobustScaler-in tətbiqi\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nScaled Data with RobustScaler:\\n\", scaled_data)\n",
    "```\n",
    "\n",
    "### Üstünlüklər:\n",
    "- **Outlier-lərə Dayanıqlıq:** Median və IQR istifadə edildiyi üçün məlumat dəstələrindəki outlier-lər bu miqyaslandırmaya az təsir edir.\n",
    "- **Daha Sabit Nəticələr:** Ənənəvi miqyaslandırma metodları ilə müqayisədə, outlier-lərin səbəb olduğu qeyri-stabilliklər azaldılır.\n",
    "\n",
    "### Çatışmazlıqlar:\n",
    "- **Küçük Məlumat Dəstələrində Problemlər:** Əgər məlumat dəstəsi çox kiçikdirsə və ya çox az sayda nümunədən ibarətdirsə, median və IQR düzgün hesablanmaya bilər.\n",
    "- **Yalnız Düzgün Paylanmış Məlumatlarda Effektivdir:** Məlumatların paylanışı çox qeyri-bərabərdirsə, Robust Scaler bəzən qeyri-adekvat miqyaslandırma apara bilər.\n",
    "\n",
    "### Nəticə:\n",
    "Robust Scaler outlier-lərlə zəngin olan məlumat dəstələrinin miqyaslandırılması üçün ideal bir vasitədir. Median və IQR kimi statistik ölçülərdən istifadə edərək məlumatları miqyaslandırmaqla, bu metod outlier-lərə qarşı daha sabit və dəqiq nəticələr verir. Bu səbəbdən, Robust Scaler maşın öyrənməsi tətbiqlərində və digər məlumat təhlili proseslərində geniş istifadə olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a586bc43",
   "metadata": {},
   "source": [
    "## Feature Importance Riyaziyyatda formulu? Hesablanma qaydasi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebb069",
   "metadata": {},
   "source": [
    "Decision Tree modellərində **feature importance** hər bir xüsusiyyətin (feature) modeldə nə qədər vacib olduğunu göstərir. Bu, xüsusiyyətin ağacın qurulmasında (tree-building process) nə qədər təsirli olduğunu qiymətləndirir. Decision Tree-də **feature importance** ümumiyyətlə impurity-based importance metodu ilə hesablanır.\n",
    "\n",
    "### Feature Importance Hesablanması\n",
    "\n",
    "1. **Impurity Measurement (Gini impurity, Entropy):** \n",
    "   - Hər bir düyündə (node) impurity (məsələn, Gini impurity, Entropy) hesablanır. Impurity, həmin düyündəki qarışıqlığın ölçüsüdür.\n",
    "\n",
    "2. **Impurity Reduction (Information Gain):**\n",
    "   - Hər bir xüsusiyyət üçün, həmin xüsusiyyətə əsaslanaraq ağacın düyünü bölündükdən sonra impurity azalması hesablanır. Bu azalma həmin xüsusiyyətin nə qədər məlumat verdiyini (information gain) göstərir.\n",
    "   \n",
    "   - Impurity reduction = impurity_before_split - impurity_after_split\n",
    "\n",
    "3. **Cumulative Importance:**\n",
    "   - Hər bir xüsusiyyət üçün, ağacın müxtəlif düyünlərindəki impurity reduction-lar toplanır. Bu, həmin xüsusiyyətin ümumi əhəmiyyətini (total importance) göstərir.\n",
    "\n",
    "4. **Normalization:**\n",
    "   - Hər bir xüsusiyyətin əhəmiyyəti normallaşdırılır (0 ilə 1 arasında) və nəticədə hər bir xüsusiyyətin final feature importance dəyəri əldə edilir.\n",
    "\n",
    "### Riyazi Formula\n",
    "\n",
    "Feature importance $  FI_j  $ hər bir xüsusiyyət $ j $ üçün aşağıdakı kimi hesablanır:\n",
    "\n",
    "$$ \n",
    "FI_j = \\sum_{t=1}^{T} \\Delta i_t \\cdot 1 \\{v(t) = j\\}\n",
    " $$\n",
    "\n",
    "burada:\n",
    "- $  \\Delta i_t  $ - düyün $ t $-də impurity reduction-dur (impurity_before_split - impurity_after_split).\n",
    "- $ 1 \\{v(t) = j\\}  $ - düyün $ t $-nin xüsusiyyət $ j $-yə aid olub-olmamasını göstərən göstərici funksiyasıdır.\n",
    "- $ T $ - ümumi düyünlərin sayıdır.\n",
    "\n",
    "Bu formula ağacın bütün düyünlərində xüsusiyyətin nə qədər məlumat verdiyini toplamaqla xüsusiyyətin ümumi əhəmiyyətini hesablayır. Nəticədə, daha çox informasiya verən xüsusiyyətlərin əhəmiyyəti yüksək olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e04d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a46511d",
   "metadata": {},
   "source": [
    "* scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "* X_train = scaler.transform(X_train)\n",
    "* X_test = scaler.transform(X_test)\n",
    "\n",
    "### Muellif scale -i bu formada edib. Niye X_test-i de X_traine gore scale edib? Umimiyyetle  train test splitden evvel ya sonra yaxsidir scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cc696",
   "metadata": {},
   "source": [
    "`StandardScaler` və digər skalarlardan istifadə edərkən `train` və `test` setlərinin uyğunlaşdırılması (scaling) çox önəmlidir. Niyə `X_test` də `X_train`-ə görə standartlaşdırılır və bu prosesin ən doğru vaxtı haqqında izahat verəcəyəm.\n",
    "\n",
    "### Niyə `X_test` `X_train`-ə görə standartlaşdırılır?\n",
    "\n",
    "1. **Məlumat Sızması (Data Leakage) Əngəllənməsi:**\n",
    "   - **Qarşısını Almaq:** Test məlumatlarına birbaşa müdaxilə etmək modelin qiymətləndirilməsini səhv edə bilər. Test məlumatlarına əvvəlcədən baxmaq, modelin test dəstinə uyğunlaşmasına və nəticədə daha yaxşı nəticələr əldə etməsinə səbəb ola bilər. Bu, məlumat sızması (data leakage) olaraq adlandırılır.\n",
    "   - **Nə Üçün:** Test setini `X_train`-ə görə standartlaşdırmaq, test məlumatlarının yalnız təlim məlumatları əsasında miqyaslandırılması və beləliklə, test dəstinin düzgün qiymətləndirilməsi təmin edilir.\n",
    "\n",
    "2. **Hesablama Uyğunluğu:**\n",
    "   - **Standartlaşdırma:** Təlim məlumatları (`X_train`) üzərində fit edilmiş `StandardScaler`, müvafiq olaraq bu məlumatların ortalaması və standart sapması ilə `X_train` və `X_test`-i miqyaslandırır.\n",
    "   - **Nə Üçün:** Bu, modelin test məlumatlarının dəqiq və obyektiv qiymətləndirilməsi üçün lazımdır. `X_test`-i `X_train`-ə görə miqyaslandırmaq, test dəstinin eyni skala ilə işlənməsini təmin edir.\n",
    "\n",
    "### Train Test Splitdən Əvvəl Yoxsa Sonra Scale Etmək?\n",
    "\n",
    "- **Testdən Sonra:** Test və təlim məlumatlarını ayırdıqdan sonra, `StandardScaler` fit edildikdə, yalnız təlim məlumatlarına əsaslanaraq fit edilir və sonra bu transformasiyanı test məlumatlarına tətbiq etmək məsləhətdir. Bu, test məlumatlarının təlim məlumatlarının xüsusiyyətlərindən istifadə etmədən miqyaslandırılmasını təmin edir.\n",
    "\n",
    "### Məsələn\n",
    "\n",
    "**Train Test Splitdən Sonra Skalarlama:**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# DataFrame və target dəyişənini ayırmaq\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# StandardScaler obyektini yaratmaq və fit etmək\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "### Əsas Məqamlar\n",
    "\n",
    "1. **Train Test Splitdən Sonra Skalarlama:**\n",
    "   - `StandardScaler`-in `fit()` metodu yalnız `X_train` üzərində çalışır.\n",
    "   - `transform()` metodu həm `X_train`, həm də `X_test` üzərində tətbiq olunur.\n",
    "\n",
    "2. **Test Seti Üzərində Fit Etmək:**\n",
    "   - Test setinə fit edilməməlidir, çünki bu, test məlumatlarının məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "Bu yanaşma test dəstinin düzgün qiymətləndirilməsini təmin edir və modelin yalnız təlim məlumatlarına əsaslanaraq necə performans göstərdiyini daha doğru qiymətləndirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a1f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f552a7",
   "metadata": {},
   "source": [
    "## RobustScaler haqqinda genis melumat ver?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632bc9aa",
   "metadata": {},
   "source": [
    "`RobustScaler`, xüsusilə outlier-lərlə zəngin məlumat dəstləri üçün nəzərdə tutulmuş bir miqyaslandırma metodudur. Bu metod, xüsusiyyətləri `median` və `interquartile range` (IQR) əsasında standartlaşdıraraq daha dayanıqlı və etibarlı nəticələr əldə etməyə imkan verir.\n",
    "\n",
    "### `RobustScaler` Nədir?\n",
    "\n",
    "`RobustScaler`, `sklearn.preprocessing` kitabxanasında olan bir vasitədir. Bu metod, xüsusiyyətləri aşağıdakı addımlar ilə miqyaslandırır:\n",
    "\n",
    "1. **Median ilə Mərkəzləşdirmə:**\n",
    "   - **Median**: Hər bir xüsusiyyət üçün orta dəyəri çıxarmaq məqsədilə istifadə olunur. Median outlier-lərdən təsirlənməz, çünki o, 50%-lik kvantildir və outlier-lərin təsirini azaltmağa kömək edir.\n",
    "\n",
    "2. **Interquartile Range (IQR) ilə Miqyaslandırma:**\n",
    "   - **IQR**: Bu metod, 25%-lik (Q1) və 75%-lik (Q3) kvantil arasındakı məsafəni ölçür. Bu məsafə xüsusiyyətlərin yayılmasını ölçmək üçün istifadə edilir və outlier-lərin təsirini azaltmaq məqsədilə miqyaslandırma prosesində tətbiq olunur.\n",
    "\n",
    "### Formula\n",
    "\n",
    "`RobustScaler`-in tətbiq etdiyi standartlaşdırma aşağıdakı kimi təyin olunur:\n",
    "\n",
    "- **Mərkəzləşdirmə (Mean subtraction):**\n",
    "  \n",
    "  \\[\n",
    "  X_{\\text{centered}} = X - \\text{Median}\n",
    "  \\]\n",
    "\n",
    "- **Miqyaslandırma (Scaling):**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{scaled}} = \\frac{X_{\\text{centered}}}{\\text{IQR}}\n",
    "  \\]\n",
    "\n",
    "  Burada:\n",
    "  - \\( X \\) - Xüsusiyyətlərin dəyərləri\n",
    "  - **Median** - Xüsusiyyətlərin medianı\n",
    "  - **IQR** - Interquartile range (Q3 - Q1)\n",
    "\n",
    "### İstifadə Misalı\n",
    "\n",
    "`RobustScaler`-dən istifadə edərkən adətən aşağıdakı addımlar izlənir:\n",
    "\n",
    "1. **DataFrame və target dəyişənini ayırmaq:**\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "   # Məsələn, DataFrame\n",
    "   data = {\n",
    "       'feature1': [1, 2, 3, 1000],  # Outlier daxil olan xüsusiyyətlər\n",
    "       'feature2': [4, 5, 6, 7]\n",
    "   }\n",
    "   df = pd.DataFrame(data)\n",
    "   X = df[['feature1', 'feature2']]\n",
    "   y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "   # Train və Test dəstini ayırmaq\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **`RobustScaler`-i tətbiq etmək:**\n",
    "\n",
    "   ```python\n",
    "   # RobustScaler obyektini yaratmaq və fit etmək\n",
    "   scaler = RobustScaler()\n",
    "   scaler.fit(X_train)\n",
    "\n",
    "   # Təlim və test məlumatlarını transformasiya etmək\n",
    "   X_train_scaled = scaler.transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "   # Dəyişdirilmiş məlumatları çap etmək\n",
    "   print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "   print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "   ```\n",
    "\n",
    "### Nə Zaman İstifadə Edilməlidir?\n",
    "\n",
    "- **Outlier-lərlə Mübarizə:** `RobustScaler` xüsusilə outlier-lərin olduğu məlumat dəstləri üçün uyğundur, çünki median və IQR outlier-lərdən təsirlənmir.\n",
    "- **Model Performansı:** Outlier-lərdən təsirlənməyən daha düzgün nəticələr və daha stabil model performansı təmin edir.\n",
    "\n",
    "### Müqayisə\n",
    "\n",
    "- **`StandardScaler` vs. `RobustScaler`:**\n",
    "  - `StandardScaler`-in istifadə etdiyi ortalama və standart sapma outlier-lərlə təsirlənə bilər, amma `RobustScaler`-in istifadə etdiyi median və IQR outlier-lərdən daha dayanıqlıdır.\n",
    "  - `StandardScaler` daha çox \"normal\" paylanma olan məlumat dəstləri üçün uyğundur, `RobustScaler` isə outlier-lərin olduğu hallarda daha uyğundur.\n",
    "\n",
    "Bu şəkildə, `RobustScaler` xüsusiyyətlərin daha dayanıqlı şəkildə standartlaşdırılmasını təmin edir və xüsusiyyətlərdəki outlier-lərin təsirini azaltmağa kömək edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc98f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9397e4e4",
   "metadata": {},
   "source": [
    "## RobustScaler de train test split den sonra olmalidir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7fc8f",
   "metadata": {},
   "source": [
    "Bəli, `RobustScaler` və digər miqyaslandırma metodları train-test splitdən sonra tətbiq olunmalıdır. Bu, məlumat sızmasının (data leakage) qarşısını almaq və modelin təlim və test mərhələlərində düzgün qiymətləndirilməsini təmin etmək üçün vacibdir.\n",
    "\n",
    "### Niyə Train-Test Splitdən Sonra Scalarlama?\n",
    "\n",
    "1. **Məlumat Sızmasının Qarşısını Almaq:**\n",
    "   - **Niyə:** Test məlumatlarına birbaşa təsir etmədən yalnız təlim məlumatlarından miqyaslandırma parametrləri (median və IQR) əldə etmək lazımdır. Bu, test dəstinin yalnız təlim məlumatları əsasında miqyaslandırıldığını təmin edir.\n",
    "   - **Nəticə:** Test məlumatlarına əvvəlcədən baxmaq və ya onları miqyaslandırmaq modelin nəticələrini təhrif edə bilər və məlumat sızması ilə nəticələnə bilər.\n",
    "\n",
    "2. **Miqyaslandırma Parametrlərinin Düzgün Tətbiqi:**\n",
    "   - **Təlim Məlumatları:** Miqyaslandırma parametrləri yalnız təlim məlumatlarına əsaslanaraq fit edilir.\n",
    "   - **Test Məlumatları:** Eyni parametrlər (mediana və IQR) test məlumatlarına tətbiq edilir.\n",
    "\n",
    "### Misal Kodu\n",
    "\n",
    "**1.** **Train-Test Splitdən Sonra Miqyaslandırma:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Nümunə DataFrame\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 1000],  # Outlier olan xüsusiyyət\n",
    "    'feature2': [4, 5, 6, 7]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = [0, 1, 0, 1]  # Target dəyişəni\n",
    "\n",
    "# Train və Test dəstini ayırmaq\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RobustScaler obyektini yaratmaq və fit etmək\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)  # Yalnız X_train üzərində fit edilir\n",
    "\n",
    "# Təlim və test məlumatlarını transformasiya etmək\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dəyişdirilmiş məlumatları çap etmək\n",
    "print(\"Scaled X_train:\\n\", X_train_scaled)\n",
    "print(\"Scaled X_test:\\n\", X_test_scaled)\n",
    "```\n",
    "\n",
    "**2.** **Nümunənin İslahatı:**\n",
    "\n",
    "- **DataFrame Yaratma:** Nümunə məlumatlar yaradılır.\n",
    "- **Train-Test Split:** `X` və `y` məlumatları train və test dəstlərinə bölünür.\n",
    "- **`RobustScaler` İstifadəsi:** `RobustScaler` yalnız təlim dəstində fit edilir, sonra həm təlim, həm də test dəstinə tətbiq edilir.\n",
    "\n",
    "### Nəticə\n",
    "\n",
    "`RobustScaler` və digər miqyaslandırma üsulları üçün düzgün təcrübə train-test splitdən sonra miqyaslandırma tətbiq etməkdir. Bu yanaşma, məlumat sızmasının qarşısını alır və modelin daha düzgün qiymətləndirilməsini təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15586d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f61de99c",
   "metadata": {},
   "source": [
    "## Bagging parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd5a4a",
   "metadata": {},
   "source": [
    "* bag = BaggingClassifier(estimator=svm,\n",
    "*                        n_estimators=10,\n",
    "*                        max_samples=0.4,\n",
    "*                        max_features=35,\n",
    "*                        bootstrap=True,\n",
    "*                        bootstrap_features=True,\n",
    "*                        random_state=123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61254f",
   "metadata": {},
   "source": [
    "`BaggingClassifier` parametrleri aşağıda izah olunur:\n",
    "\n",
    "1. **`estimator=svm`**: Burada `svm`, bagging ilə istifadə olunan əsas modelin növünü göstərir. SVM (Support Vector Machine) burada əsas model kimi istifadə olunur. `BaggingClassifier`, bu SVM modellərindən bir neçə nüsxə yaradaraq onları birləşdirir.\n",
    "\n",
    "2. **`n_estimators=10`**: Bu parametr, neçə dənə baza modelinin (bu halda SVM) yaradılacağını göstərir. `n_estimators=10` təyin edildikdə, `BaggingClassifier` 10 fərqli SVM modeli qurur və onları birləşdirir.\n",
    "\n",
    "3. **`max_samples=0.4`**: Bu parametr, hər bir baza modelinin təlimi üçün orijinal məlumat dəstindən seçilən nümunələrin faizi və ya sayı ilə bağlıdır. `max_samples=0.4` olduqda, hər baza modeli üçün orijinal məlumat dəstinin 40%-i təlim üçün istifadə olunur. Bu seçim prosesi \"bootstrapping\" metodu ilə həyata keçirilir.\n",
    "\n",
    "4. **`max_features=35`**: Bu parametr, hər bir baza modelinin qurulmasında istifadə olunan xüsusiyyətlərin maksimum sayını göstərir. `max_features=35` təyin edildikdə, hər baza modelinin təlimində 35 xüsusiyyət seçilir.\n",
    "\n",
    "5. **`bootstrap=True`**: Bu parametr, nümunələrin \"bootstrapping\" (yəni, geri qoyaraq nümunə götürmə) metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap=True` olduqda, hər bir baza modeli üçün təlim nümunələri geri qoyularaq seçilir, yəni eyni nümunə bir neçə dəfə seçilə bilər.\n",
    "\n",
    "6. **`bootstrap_features=True`**: Bu parametr, xüsusiyyətlərin \"bootstrapping\" metodu ilə seçilib-seçilməyəcəyini göstərir. `bootstrap_features=True` olduqda, hər bir baza modeli üçün xüsusiyyətlər geri qoyularaq seçilir.\n",
    "\n",
    "7. **`random_state=123`**: Bu parametr, nəticələrin təkrarlanabilirliyini təmin etmək üçün istifadə olunur. `random_state=123` təyin edildikdə, modelin təsadüfi seçimləri eyni şəkildə həyata keçirilir və nəticələr hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `BaggingClassifier` modelinin müxtəlif aspektlərini nəzarət edir və modelin qurulması, nümunə götürülməsi və xüsusiyyətlərin seçilməsi prosesi üzərində tam nəzarət təmin edir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafb507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bfaed9",
   "metadata": {},
   "source": [
    "## XGBoost parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31aee",
   "metadata": {},
   "source": [
    "* xgbt = XGBClassifier(max_depth = 2,\n",
    "*              learning_rate = 0.2,\n",
    "*              objective  = \"multi:softmax\",\n",
    "*              num_class = 2,\n",
    "*              booster = \"gbtree\",\n",
    "*              n_estimators = 10,\n",
    "*              random_state = 123)\n",
    "### Bu parametrler neyi izah edir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16db875",
   "metadata": {},
   "source": [
    "`XGBClassifier` modelinin parametrləri aşağıda izah olunur:\n",
    "\n",
    "1. **`max_depth = 2`**:\n",
    "   - Bu parametr hər bir fərdi ağacın maksimal dərinliyini təyin edir. `max_depth = 2` təyin edildikdə, hər ağac ən çox 2 dərinlik səviyyəsinə malik olacaq. Daha kiçik dərinlik overfitting-i azaldır, lakin çox kiçik olarsa, model kifayət qədər mürəkkəbliyi tuta bilməz.\n",
    "\n",
    "2. **`learning_rate = 0.2`**:\n",
    "   - Bu parametr, hər ağacın öyrənmə sürətini təyin edir. `learning_rate = 0.2` olduqda, hər bir ağacın modelə töhfəsi 0.2 faktorla azalır. Kiçik learning rate daha dəqiq və stabil model təlimi verir, amma daha çox iterasiya tələb edir.\n",
    "\n",
    "3. **`objective = \"multi:softmax\"`**:\n",
    "   - Bu parametr modelin məqsədini təyin edir. `multi:softmax` təyin edildikdə, model çoxlu sinifli təsnifat (multi-class classification) üçün istifadə edilir və proqnozlaşdırılan dəyər ən yüksək ehtimala malik sinif kimi verilir.\n",
    "\n",
    "4. **`num_class = 2`**:\n",
    "   - Bu parametr, modelin neçə sinif olduğunu təyin edir. `num_class = 2` təyin edildikdə, model iki sinifli (binary) təsnifat məsələsini həll edir. (Qeyd: `multi:softmax` ilə birlikdə istifadə olunduqda, bu parametr faktiki olaraq daha çox sinif sayı üçün istifadə olunur.)\n",
    "\n",
    "5. **`booster = \"gbtree\"`**:\n",
    "   - Bu parametr, modelin hansı boosting algoritmini istifadə edəcəyini təyin edir. `gbtree` təyin edildikdə, model qərar ağacları ilə qurulur. Alternativ olaraq `gblinear` (linear model) və ya `dart` (Dropouts meet Multiple Additive Regression Trees) kimi digər algoritmlər də istifadə edilə bilər.\n",
    "\n",
    "6. **`n_estimators = 10`**:\n",
    "   - Bu parametr, modeldə neçə dənə ağacın qurulacağını göstərir. `n_estimators = 10` təyin edildikdə, model 10 ağac qurur. Daha çox ağac modelin dəqiqliyini artıra bilər, lakin eyni zamanda təlim müddətini uzada bilər.\n",
    "\n",
    "7. **`random_state = 123`**:\n",
    "   - Bu parametr, təsadüfi proseslər üçün toxum (seed) təyin edir ki, nəticələr təkrarlanabilsin. `random_state = 123` təyin edildikdə, modelin nəticələri hər dəfə eyni olur.\n",
    "\n",
    "Bu parametrlər `XGBClassifier` modelinin qurulması və işləməsi zamanı onun davranışını təyin edir və modelin düzgün performans göstərməsi üçün müxtəlif aspektləri optimallaşdırmağa imkan verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e68d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5475dc",
   "metadata": {},
   "source": [
    "### XGBoots-da \n",
    "* scale_pos_weight=[default=1]\n",
    "### ???????????\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c25fe",
   "metadata": {},
   "source": [
    "`scale_pos_weight` parametri `XGBoost` modelində balanssız verilənlər dəstlərində (yəni, bir sinifin digərindən çox üstün olduğu hallarda) istifadə olunur. Bu parametr siniflər arasındakı balanssızlığı tənzimləmək və modelin daha yaxşı performans göstərməsini təmin etmək üçün istifadə edilir.\n",
    "\n",
    "### `scale_pos_weight` Parametrinin İzahı\n",
    "\n",
    "- **Məqsəd:** Əgər verilənlər dəstində siniflərdən biri (məsələn, pozitiv sinif) çox az sayda nümunə ilə təmsil olunursa, `scale_pos_weight` parametri bu sinifə daha çox əhəmiyyət vermək üçün istifadə olunur.\n",
    "- **Dəyər:** `scale_pos_weight = nöqativ_sinif_nümunələri / pozitiv_sinif_nümunələri` şəklində təyin edilir.\n",
    "\n",
    "### Necə İşləyir?\n",
    "\n",
    "- **Balanssızlıq Problemi:** Əgər siniflər arasında balanssızlıq varsa, model çoxluqda olan sinifi daha çox öyrənir və bu da nadir sinifin düzgün proqnozlaşdırılmasına mane ola bilər.\n",
    "- **`scale_pos_weight` Parametrinin Təyini:** Bu parametr, azlıqda olan sinifin təsirini artırmaq üçün istifadə olunur. Məsələn, pozitiv sinifdə 100 nümunə, neqativ sinifdə isə 1000 nümunə varsa, `scale_pos_weight = 1000/100 = 10` təyin etmək olar. Bu, modelin azlıqda olan sinifə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Misal:\n",
    "\n",
    "```python\n",
    "xgbt = XGBClassifier(max_depth=2,\n",
    "                     learning_rate=0.2,\n",
    "                     objective=\"binary:logistic\",\n",
    "                     n_estimators=10,\n",
    "                     scale_pos_weight=10,  # Siniflər arasındakı balanssızlıq nəzərə alınır\n",
    "                     random_state=123)\n",
    "```\n",
    "\n",
    "Burada `scale_pos_weight=10` təyin edilib. Bu, modelin pozitiv sinifə neqativ sinifdən 10 dəfə daha çox diqqət yetirməsini təmin edir.\n",
    "\n",
    "### Nəticə\n",
    "`scale_pos_weight` parametri balanssız verilənlər dəstlərində siniflər arasındakı balansı tənzimləyərək, modelin azlıqda olan sinifi düzgün proqnozlaşdırmasını təmin etmək üçün istifadə edilir. Bu parametr xüsusilə tibbi diaqnostika, fırıldaqçılığın aşkarlanması kimi sahələrdə faydalıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e67694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0befd682",
   "metadata": {},
   "source": [
    "## Datani Uderstanding etmek ucun nelere baxim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b969",
   "metadata": {},
   "source": [
    "Bu məlumat dəstini yaxşı anlamaq üçün aşağıdakı addımları ata bilərsiniz:\n",
    "\n",
    "### 1. **Veri Tipini Yoxlayın**\n",
    "   - **Hər bir sütunun** (dəyişənin) tipini yoxlayın (məsələn, integer, float, object). Bu, hər bir dəyişənin necə işlənməli olduğunu anlamağa kömək edəcək. Məsələn, `LoanAmount` və `ApplicantIncome` sayısal dəyərlərdir, amma `Gender`, `Married` və `Loan_Status` kimi dəyişənlər kateqoriyalardır.\n",
    "\n",
    "   ```python\n",
    "   df.info()\n",
    "   ```\n",
    "\n",
    "### 2. **Məlumatların Yoxluğu (Missing Data)**\n",
    "   - Məlumat dəstində boş dəyərlərin olub-olmadığını yoxlayın. Bu, model qurmaq üçün məlumatların keyfiyyətini artırmağa kömək edəcək. Boş dəyərlər varsa, onları doldurma (imputation) və ya silmə metodları tətbiq edə bilərsiniz.\n",
    "\n",
    "   ```python\n",
    "   df.isnull().sum()\n",
    "   ```\n",
    "\n",
    "### 3. **Məlumatların Dağılımı**\n",
    "   - Sayısal dəyişənlərin (məsələn, `ApplicantIncome`, `LoanAmount`) və kateqoriyal dəyişənlərin (`Gender`, `Education`) dağılımını yoxlayın. Bu, məlumatların normal paylanıb-paylanmadığını və ya bəzi dəyişənlərdə outlier-ların olub-olmadığını anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.describe()\n",
    "   df['ApplicantIncome'].hist()\n",
    "   ```\n",
    "\n",
    "### 4. **Kateqoriyal Dəyərlərin Paylanması**\n",
    "   - Kateqoriyal dəyişənlərin (məsələn, `Gender`, `Married`, `Education`) müxtəlif siniflər üzrə paylanmasını yoxlayın. Bu, məlumat dəstində hansı siniflərin dominant olduğunu və modelin necə performans göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df['Gender'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 5. **Hədəf Dəyişən (Target Variable)**\n",
    "   - `Loan_Status` dəyişəninin paylanmasını yoxlayın. Hədəf dəyişənin balanslı və ya balanssız olub-olmadığını bilmək vacibdir. Balanssızlıq varsa, xüsusi metodlar tətbiq etmək lazım ola bilər (məsələn, undersampling, oversampling).\n",
    "\n",
    "   ```python\n",
    "   df['Loan_Status'].value_counts()\n",
    "   ```\n",
    "\n",
    "### 6. **Məlumatların Əlaqəsi (Correlation)**\n",
    "   - Sayısal dəyişənlər arasındakı əlaqəni yoxlayın. Bu, hansı dəyişənlərin bir-biri ilə daha çox əlaqəli olduğunu və hansı dəyişənlərin modelə daha çox təsir göstərə biləcəyini anlamağa kömək edəcək.\n",
    "\n",
    "   ```python\n",
    "   df.corr()\n",
    "   ```\n",
    "\n",
    "### 7. **Pivotal Dəyişənlərin Təsiri**\n",
    "   - Hədəf dəyişən (`Loan_Status`) ilə digər dəyişənlər arasındakı əlaqəni yoxlayın. Məsələn, `ApplicantIncome`, `Credit_History` və `Education` dəyişənlərinin kredit təsdiqinə necə təsir etdiyini araşdırın.\n",
    "\n",
    "   ```python\n",
    "   pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "   ```\n",
    "\n",
    "### 8. **Outlier-ları Yoxlamaq**\n",
    "   - Sayısal dəyişənlərdə outlier-ları yoxlayın və onları necə idarə edəcəyinizi qərar verin. Outlier-lar modeli yanıltmaq riski daşıya bilər.\n",
    "\n",
    "   ```python\n",
    "   df.boxplot(column='LoanAmount')\n",
    "   ```\n",
    "\n",
    "Bu addımları yerinə yetirdikdən sonra məlumat dəstini daha yaxşı anlayacaq və model qurmağa başlamaq üçün yaxşı bir zəmin yaradacaqsınız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b084870e",
   "metadata": {},
   "source": [
    "## Datani Preparation usullari?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564113cf",
   "metadata": {},
   "source": [
    "Loan eligibility prediction üçün data preparation mərhələsi çox vacibdir və müvəffəqiyyətli modellər qurmaq üçün düzgün hazırlıq tələb olunur. Aşağıdakı addımlar bu prosesdə sizə kömək edə bilər:\n",
    "\n",
    "### 1. **Veri Yükləmə və İnspeksiya**\n",
    "   - Veriləri yükləyin və ilk baxışda statistik göstəricilərə baxın.\n",
    "   - Datalarda hər hansı bir boş (missing) dəyərin olub olmadığını yoxlayın.\n",
    "\n",
    "### 2. **Boş Dəyərlərin İdarə Edilməsi**\n",
    "   - Boş dəyərləri necə idarə edəcəyinizə qərar verin. Ən çox istifadə olunan metodlar:\n",
    "     - **İlkin Təsnifat**: Boş dəyərləri təyin olunmuş bir dəyər ilə əvəzləmək (məsələn, median, mod, və s.)\n",
    "     - **Sıxlıqla Təyin Edilməsi**: Boş dəyərləri sıxlıqla təyin edilmiş dəyərlərlə əvəzləmək.\n",
    "\n",
    "### 3. **Kateqoriyalı Dəyərlərin Kodlaşdırılması**\n",
    "   - Kateqoriyalı dəyişənləri (Gender, Married, Education, Self_Employed, Property_Area) rəqəmlə ifadə etməlisiniz.\n",
    "     - **Label Encoding**: Hər bir kateqoriyanı təmsil edən bir rəqəm verilir.\n",
    "     - **One-Hot Encoding**: Hər bir kateqoriya üçün ayrılıqda sütun yaradılır və müvafiq sütunda 1, digərlərində isə 0 olur.\n",
    "\n",
    "### 4. **Miqdar Dəyərlərin Miqyaslandırılması**\n",
    "   - **Standardization**: Orta və standart sapma əsasında dəyərlərin normallaşdırılması.\n",
    "   - **Normalization**: Dəyərlərin [0, 1] intervalına gətirilməsi.\n",
    "\n",
    "### 5. **Boş Dəyərlərin Aşağı Səviyyədə Dəyərləndirilməsi**\n",
    "   - Verilərin doğruluğunu və əlaqəliliyini artırmaq üçün boş dəyərləri analiz edin.\n",
    "\n",
    "### 6. **Feature Engineering**\n",
    "   - **Yeni Dəyişənlərin Yaradılması**: Yeni dəyərləri yaratmaq üçün mövcud dəyişənləri birləşdirə bilərsiniz (məsələn, **TotalIncome** = **ApplicantIncome** + **CoapplicantIncome**).\n",
    "   - **Dəyişənlərin Seçilməsi**: Modeliniz üçün ən əhəmiyyətli olan dəyişənləri seçin.\n",
    "\n",
    "### 7. **Outlierların İdarə Edilməsi**\n",
    "   - **Outlier Analizi**: Outlierları aşkar edin və onların təsirini azaltmaq üçün uyğun üsullar tətbiq edin.\n",
    "\n",
    "### 8. **Target Variableın İdarə Edilməsi**\n",
    "   - **Label Encoding**: Kreditin təsdiqi dəyişənini (Loan_Status) 0 və 1 ilə təmsil edin.\n",
    "\n",
    "### 9. **Data Split**\n",
    "   - **Train-Test Split**: Veriləri təlim (training) və test (testing) qruplarına ayırın.\n",
    "\n",
    "### 10. **Cross-validation**\n",
    "   - Modelin gücünü qiymətləndirmək üçün cross-validation tətbiq edin.\n",
    "\n",
    "### 11. **Feature Importance Analizi**\n",
    "   - **Feature Importance**: Hansı xüsusiyyətlərin model üçün daha əhəmiyyətli olduğunu müəyyən edin və nəticələri nəzərə alın.\n",
    "\n",
    "Bu addımları izləyərək data preparation mərhələsini uğurla tamamlayaraq modelinizi hazırlamağa başlaya bilərsiniz. Ehtiyacınıza uyğun addımları tətbiq edərək nəticələrinizi inkişaf etdirə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc7e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab60505b",
   "metadata": {},
   "source": [
    "## Men burda bezi data preparation isleri gormisem.  ne yaddan cixib elemimisem sen onlari izah ele ve kodu yaz. Yani sonra men model quracam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969d7cd",
   "metadata": {},
   "source": [
    "Mən notebook-dan aşağıdakı data preparation addımlarını aşkar etdim. Gördüyünüz işlər bunlardır:\n",
    "\n",
    "### 1. **Verilənlərin Yüklənməsi**\n",
    "   İlk addım olaraq, verilənlər `loan_prediction.csv` faylından yüklənib və `df` DataFrame-i yaradılıb.\n",
    "\n",
    "### 2. **Sütun Adlarının və Məlumatın Araşdırılması**\n",
    "   `df.columns` və `df.info()` metodları istifadə edilərək verilənlərin sütun adları və strukturu araşdırılıb.\n",
    "\n",
    "### 3. **Verilənlərin İzlənməsi**\n",
    "   - **Dəyərlərin sayılması**: `Dependents` və `Loan_Status` sütunlarında dəyərlərin sayı yoxlanılıb (`value_counts`).\n",
    "   - **Balansın yoxlanılması**: `Loan_Status` sütununda balansın olub-olmadığı yoxlanılıb. Görünür, 192/612 və 422/612 hesabatları göstərir ki, balans problemi yoxdur.\n",
    "\n",
    "### 4. **Boş Dəyərlərin Yoxlanılması və Doldurulması**\n",
    "   - **Boş Dəyərlərin Araşdırılması**: `df.isnull().sum()` metodu ilə dataset-də boş dəyərlər yoxlanılıb.\n",
    "   - **Boş Dəyərlərin Doldurulması**: Aşağıdakı sütunlarda boş dəyərlər doldurulub:\n",
    "     - `Gender`: Mod (ən çox təkrarlanan) dəyər ilə doldurulub.\n",
    "     - `Married`, `Dependents`, `Self_Employed`, `Credit_History`: Bu sütunlar da mod dəyəri ilə doldurulub.\n",
    "\n",
    "### 5. **Hələ Yadınızdan Çıxmış Ola Bilən Data Preparation Addımları**\n",
    "   - **Outlier Detection**: Məsələn, `ApplicantIncome` və `LoanAmount` üçün outlier-lərin yoxlanılması və ya uyğunlaşdırılması (log transformasiya və s.).\n",
    "   - **Sütunların Normalizasiyası/Standartizasiyası**: `ApplicantIncome`, `LoanAmount` kimi numeric dəyişənlər üçün normalizasiya və ya standartizasiya aparıla bilər.\n",
    "   - **Categorical Encoding**: Kategorik dəyişənlər (`Gender`, `Married`, `Education`, `Property_Area`, və s.) üçün one-hot encoding və ya label encoding tətbiq oluna bilər.\n",
    "   - **Feature Engineering**: Yeni dəyişənlərin yaradılması, məsələn, `TotalIncome` (müraciət edən və birgə müraciət edənin gəliri toplamı).\n",
    "\n",
    "Bu addımları tətbiq etmək üçün sizə uyğun kodları təqdim edə bilərəm. Hansı xüsusi addımlar haqqında məlumat əldə etmək istəyirsiniz?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133173aa",
   "metadata": {},
   "source": [
    "Aşağıda, qeyd etdiyiniz dataya əlavə olaraq həyata keçirə biləcəyiniz data preparation addımlarını təqdim edirəm:\n",
    "\n",
    "### 1. **Outlier-ların Yoxlanılması və Müxtəlifləşdirilməsi**\n",
    "Numeric dəyişənlər (`ApplicantIncome`, `LoanAmount`) üçün outlier-ların olub-olmadığını yoxlayıb, ehtiyac varsa log transformasiya tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ApplicantIncome üçün outlier-ların yoxlanılması\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['ApplicantIncome'])\n",
    "plt.title('ApplicantIncome Outliers')\n",
    "\n",
    "# Log transformasiya ilə outlier-ların uyğunlaşdırılması\n",
    "df['Log_ApplicantIncome'] = np.log1p(df['ApplicantIncome'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_ApplicantIncome'])\n",
    "plt.title('Log Transformed ApplicantIncome')\n",
    "plt.show()\n",
    "\n",
    "# LoanAmount üçün eyni proseduru həyata keçirə bilərsiniz\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)  # Boş dəyərləri median ilə doldurmaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['LoanAmount'])\n",
    "plt.title('LoanAmount Outliers')\n",
    "\n",
    "df['Log_LoanAmount'] = np.log1p(df['LoanAmount'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_LoanAmount'])\n",
    "plt.title('Log Transformed LoanAmount')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 2. **Categorical Encoding (One-Hot Encoding)**\n",
    "Kategorik dəyişənlər üçün one-hot encoding tətbiq edərək, onları model üçün istifadə edə biləcəyiniz numerik formaya çevirmək.\n",
    "\n",
    "```python\n",
    "# One-hot encoding tətbiq etmək\n",
    "df = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents'], drop_first=True)\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 3. **Yeni Dəyişənlər Yaratmaq (Feature Engineering)**\n",
    "Müraciət edən şəxsin və birgə müraciət edən şəxsin gəlirini toplam olaraq yeni bir dəyişən yaratmaq.\n",
    "\n",
    "```python\n",
    "# TotalIncome adlı yeni bir dəyişən yaratmaq\n",
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "\n",
    "# TotalIncome üçün log transformasiya tətbiq etmək\n",
    "df['Log_TotalIncome'] = np.log1p(df['TotalIncome'])\n",
    "\n",
    "# Vizual olaraq log transformasiyanın təsirini yoxlamaq\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df['TotalIncome'])\n",
    "plt.title('TotalIncome Outliers')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['Log_TotalIncome'])\n",
    "plt.title('Log Transformed TotalIncome')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 4. **Numeric Dəyərlərin Normalizasiyası və ya Standartizasiyası**\n",
    "Numeric dəyişənlər üçün normalizasiya (Min-Max scaling) və ya standartizasiya (Z-score normalization) tətbiq edə bilərsiniz.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Numeric dəyişənləri müəyyənləşdirin\n",
    "numeric_features = ['Log_ApplicantIncome', 'Log_LoanAmount', 'Log_TotalIncome']\n",
    "\n",
    "# Standartizasiya tətbiq etmək\n",
    "scaler = StandardScaler()\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "# Dəyişdirilmiş dataset-ə baxmaq\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Bu addımların hamısı datanızı model üçün hazır etmək məqsədi daşıyır. Bu addımlardan sonra model qurma mərhələsinə keçə bilərsiniz. Hansısa spesifik addım haqqında daha çox məlumat və ya əlavə izah istəsəniz, mənə bildirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa7140f",
   "metadata": {},
   "source": [
    "## Mene paired sample t test et bu iki sutunu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eab99b",
   "metadata": {},
   "source": [
    "Paired sample t-test, eyni subyektlərin iki müstəqil dəyişən üzərində ölçülən nəticələrinin statistik olaraq əhəmiyyətli fərqini yoxlamaq üçün istifadə olunur. Lakin sizin nümunədə `Loan_Status` (binary) və `ApplicantIncome` (numeric continuous) dəyişənləri arasında birbaşa `paired t-test` tətbiq etmək məntiqli olmayacaq, çünki `Loan_Status` müstəqil dəyişən, `ApplicantIncome` isə davamlı dəyişəndir. \n",
    "\n",
    "Bununla belə, eyni subyektlərdə iki ölçüm arasındakı fərqi təhlil etmək üçün `paired t-test` istifadə oluna bilər. Məsələn, iki müxtəlif zaman nöqtəsində eyni subyektlərdəki `ApplicantIncome` dəyişəni ölçülə bilər. Bu halda, `paired t-test` aşağıdakı şəkildə tətbiq oluna bilər:\n",
    "\n",
    "### Python Kod Nümunəsi\n",
    "\n",
    "1. **Bir Dataset Yaradın**: İki ölçüm arasındakı fərqi təsvir etmək üçün iki fərqli gəlir sütunu yaradın.\n",
    "2. **Paired Sample t-Test Tətbiq Edin**: `scipy.stats` kitabxanasını istifadə edərək.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataset yaratmaq üçün funksiyalar\n",
    "def generate_data(num_samples):\n",
    "    np.random.seed(42)\n",
    "    # Təsadüfi numeric continuous dəyərlər\n",
    "    applicant_income_1 = np.random.rand(num_samples) * 10000  # 0-10000 arasında gəlir\n",
    "    applicant_income_2 = applicant_income_1 + np.random.randn(num_samples) * 2000  # ikinci ölçüm, biraz fərqli\n",
    "    \n",
    "    # DataFrame yaratmaq\n",
    "    df = pd.DataFrame({\n",
    "        'ApplicantIncome_1': applicant_income_1,\n",
    "        'ApplicantIncome_2': applicant_income_2\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Data yaratmaq\n",
    "df = generate_data(1000)\n",
    "\n",
    "# Paired sample t-test tətbiq etmək\n",
    "t_statistic, p_value = ttest_rel(df['ApplicantIncome_1'], df['ApplicantIncome_2'])\n",
    "\n",
    "print(f'T-statistic: {t_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Korrelyasiya matrisini hesablamaq\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Korrelyasiya matrisini vizualizasiya etmək\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Açıklama\n",
    "\n",
    "1. **Data Yaratmaq**: `generate_data` funksiyası iki fərqli `ApplicantIncome` sütunu ilə bir dataset yaradır.\n",
    "2. **Paired Sample t-Test**: `ttest_rel` funksiyasını istifadə edərək iki müstəqil gəlir sütunu arasındakı fərqi test edir.\n",
    "3. **Korrelyasiya Matrisini Hesablamaq**: Yaradılan data üzərində korrelyasiya matrisini və heatmap vizualizasiyasını təqdim edir.\n",
    "\n",
    "Bu kod nümunəsi `ApplicantIncome` dəyişəninin iki fərqli ölçüm arasında əhəmiyyətli bir fərqin olub olmadığını yoxlamaq üçün `paired sample t-test` tətbiq edir. Əgər siz `Loan_Status` ilə `ApplicantIncome` arasında fərq yoxlamaq istəyirsinizsə, fərqli statistik testlər (məsələn, t-test, ANOVA) və ya metodlar istifadə etməlisiniz, çünki bu iki dəyişən fərqli növlərdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9586093",
   "metadata": {},
   "source": [
    "## plt.hist(np.log(df['CoapplicantIncome'] + 1))\n",
    "* niye +1 edirik?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d96f9",
   "metadata": {},
   "source": [
    "`np.log(df['CoapplicantIncome'] + 1)` əməliyyatında `+1` etməyin səbəbi, logaritma funksiyasının tərifinə görə sıfır və mənfi dəyərlərin logaritmı tərif olunmur. Əgər `CoapplicantIncome` sütununda sıfır dəyəri varsa, `np.log(0)` undefined (müəyyən edilməmiş) olacaq və bu da səhvlərə səbəb ola bilər.\n",
    "\n",
    "`+1` əlavə etməklə, bütün dəyərlər ən azı 1 olacaq və logaritmik transformasiyanın tətbiqi mümkün olacaq. Bu həmçinin dəyərləri bir qədər dəyişir, lakin ümumi tendensiyanı və ya datanın strukturu əhəmiyyətli dərəcədə dəyişmir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494f72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3901c039",
   "metadata": {},
   "source": [
    "## Boosting modellerin hansina random_state vermek lazimdir?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bcab5",
   "metadata": {},
   "source": [
    "Boosting modellərində `random_state` parametri modelin təkrarlanabilməsini təmin etmək üçün istifadə olunur. Bu, təlim zamanı təsadüfi elementlərin (məsələn, təlim məlumatlarının təsadüfi seçilməsi, təsadüfi başlanğıc ağırlıqları və s.) nəzarət edilməsi üçün bir toxum dəyəri (seed value) təmin edir.\n",
    "\n",
    "### Hər hansı bir `random_state` dəyəri təyin edərkən:\n",
    "\n",
    "- **Sabit bir dəyər təyin etmək** (məsələn, `random_state=42`): Təlim və test proseslərinin hər dəfə eyni nəticəni verməsini təmin edər. Bu, modeli təkrarlamaq və nəticələri yoxlamaq üçün faydalıdır.\n",
    "  \n",
    "- **`random_state`-i `None` olaraq buraxmaq**: Modelin hər dəfə fərqli bir təsadüfi dəyər seçməsinə səbəb olar. Bu da modelin hər dəfə fərqli nəticələr verməsi ilə nəticələnir.\n",
    "\n",
    "### Fərqli Boosting Modellərində `random_state`:\n",
    "\n",
    "1. **XGBoost (`xgboost.XGBClassifier` və ya `xgboost.XGBRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Sabit bir dəyər təyin edərək (məsələn, `random_state=42`), modeli təkrarlamaq mümkündür.\n",
    "\n",
    "2. **LightGBM (`lightgbm.LGBMClassifier` və ya `lightgbm.LGBMRegressor`)**\n",
    "   - Parametr: `random_state` və ya `seed`\n",
    "   - Burada da sabit bir dəyər təyin etməklə modeli təkrarlamaq mümkündür.\n",
    "\n",
    "3. **CatBoost (`catboost.CatBoostClassifier` və ya `catboost.CatBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Eyni təkrarlanabilirlik üçün sabit bir dəyər istifadə edilə bilər.\n",
    "\n",
    "4. **AdaBoost (`sklearn.ensemble.AdaBoostClassifier` və ya `sklearn.ensemble.AdaBoostRegressor`)**\n",
    "   - Parametr: `random_state`\n",
    "   - Təkrarlanabilirlik üçün sabit bir dəyər verilməsi məsləhətlidir.\n",
    "\n",
    "### Hansı `random_state` təyin edilməlidir?\n",
    "\n",
    "- **Sabit dəyər olaraq `42`**: Bu, elm sahəsində çox yaygın olaraq istifadə olunur və təkrarlanabilirlik təmin etmək üçün yaxşı bir seçimdir.\n",
    "- **Başqa bir sabit dəyər**: Əgər xüsusi olaraq fərqli bir toxum dəyəri istəyirsinizsə, istənilən ədəd seçilə bilər.\n",
    "- **`None` olaraq buraxmaq**: Hər dəfə təsadüfi dəyərlər istəyirsinizsə, `random_state=None` olaraq buraxıla bilər.\n",
    "\n",
    "Nəticədə, hansı `random_state` istifadə edəcəyiniz sizin proyektinizin tələblərindən asılıdır. Əgər nəticələrin hər dəfə təkrarlanmasını istəyirsinizsə, sabit bir `random_state` dəyəri təyin etmək yaxşı bir təcrübədir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f53ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad97c34",
   "metadata": {},
   "source": [
    "## [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
    "\n",
    "## Nece edim bu warning olmasin?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee933e3",
   "metadata": {},
   "source": [
    "Bu xəbərdarlıqlar, `LightGBM` modelini təlim etdirərkən yaranır və bu xəbərdarlıqların əsas səbəbi budur ki, model təlim zamanı bəzi node-lar üçün (qovşaqlar üçün) mənfi və ya sıfır dəyərində \"gain\" əldə edir. Bu halda, əlavə bir split (qovşaqda yeni bir bölmə) yaratmaq mənasız olur, çünki bu heç bir yaxşılaşma gətirməyəcək.\n",
    "\n",
    "### Bu xəbərdarlıqların səbəbləri:\n",
    "\n",
    "1. **Məlumatların Paylanması**: Məlumatların dəyər spektri çox dar ola bilər və ya bəzi dəyərlər çox nadir ola bilər. Bu halda, hər hansı bir split üçün müsbət \"gain\" əldə etmək çətin ola bilər.\n",
    "\n",
    "2. **Parametr Dəyərləri**: Modelin parametr dəyərləri (məsələn, `min_child_samples`, `min_split_gain`, `max_depth`) çox sərt olaraq təyin edilmiş ola bilər, bu da modelin çox sayda split yaratmasına maneə ola bilər.\n",
    "\n",
    "3. **Overfitting**: Model təlim zamanı çox sayda split etməyə çalışarkən overfitting-i önləmək üçün bu xəbərdarlıqları verir.\n",
    "\n",
    "### Çözüm Yolları:\n",
    "\n",
    "1. **Parametrləri Yenidən Dəyərləndirin**:\n",
    "   - `min_child_samples` dəyərini azaldaraq daha çox split imkanı verə bilərsiniz.\n",
    "   - `min_split_gain` dəyərini azaldaraq daha kiçik split-ləri icazə verə bilərsiniz.\n",
    "   - `max_depth` dəyərini artırmaq da bəzi hallarda kömək edə bilər.\n",
    "\n",
    "2. **Məlumatları Təkrardan Analiz Edin**:\n",
    "   - Məlumatlarınızı yoxlayın və fərqli qruplar arasında balansı təmin etməyə çalışın.\n",
    "   - Ehtiyac varsa, məlumatları transformasiya edərək (məsələn, log transformasiyası) modelin daha yaxşı öyrənməsi üçün şərait yaradın.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Məlumat dəstinizə əlavə xüsusiyyətlər əlavə etməyi və ya bəzi xüsusiyyətləri yenidən yaratmağı nəzərdən keçirə bilərsiniz. Bu, modelin daha yaxşı öyrənməsinə kömək edə bilər.\n",
    "\n",
    "Bu xəbərdarlıqlar modelin nəticələrinə birbaşa mənfi təsir göstərməyə bilər, ancaq məlumatların və parametrlərin diqqətlə incələnməsi modelin performansını artırmağa kömək edə bilər."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082dd5a4",
   "metadata": {},
   "source": [
    "LightGBM modelini qurarkən bu xəbərdarlıqların qarşısını almaq üçün bəzi parametrləri aşağıdakı kimi tənzimləyə bilərsiniz:\n",
    "\n",
    "1. **`min_split_gain`** dəyərini azaldaraq daha kiçik split-ləri icazə vermək.\n",
    "2. **`min_child_samples`** dəyərini artırmaq, beləliklə model yalnız müəyyən sayda nümunələr üçün split edər.\n",
    "3. **`max_depth`** dəyərini artırmaq və ya başqa dəyərlərini tənzimləməklə modelin daha yaxşı performans göstərməsini təmin etmək.\n",
    "\n",
    "Aşağıda bir nümunə LightGBM kodu verilmişdir:\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',  # Klassifikasiya üçün\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,  # Split üçün minimum \"gain\"\n",
    "    min_child_samples=20,  # Split üçün minimum nümunə sayı\n",
    "    max_depth=7,  # Maksimum dərinlik\n",
    "    random_state=42  # Reproducibility üçün\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### İpuçları:\n",
    "- **`min_split_gain`**: Bu dəyəri artırmaq splitlərin daha da mənalı olmasını təmin edəcək, beləliklə bu xəbərdarlıqların sayı azalacaq.\n",
    "- **`min_child_samples`**: Bu dəyəri artırmaq, çox kiçik nümunələr üçün split yaratmamaq üçün istifadə edilə bilər.\n",
    "- **`max_depth`**: Dərinliyi artırmaq, modelin daha çox split etməsinə icazə verəcək, ancaq overfitting riskini də nəzərə almaq lazımdır.\n",
    "\n",
    "Bu parametrlər xəbərdarlıqların qarşısını ala bilər və ya sayını azalda bilər. Ayrıca, bu parametrləri layihənizin spesifik tələblərinə uyğun tənzimləyə bilərsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52f56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13485373",
   "metadata": {},
   "source": [
    "## yene warning gelir...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492efc3",
   "metadata": {},
   "source": [
    "LightGBM-də xəbərdarlıqları azaltmaq üçün `min_split_gain`, `min_child_samples` və `max_depth` kimi parametrləri dəyişdirmək kifayət etmədikdə, digər variantları da yoxlamaq olar. Xəbərdarlıqları tamamilə aradan qaldırmaq üçün əlavə parametrlərdən istifadə edə bilərsiniz:\n",
    "\n",
    "### 1. **`verbosity` və ya `verbose` Parametrindən İstifadə Edin:**\n",
    "LightGBM modellərində `verbosity` (və ya `verbose`) parametrini istifadə edərək xəbərdarlıqların səviyyəsini tənzimləyə bilərsiniz. Xəbərdarlıqların qarşısını almaq üçün `verbosity` dəyərini `-1` təyin edə bilərsiniz:\n",
    "\n",
    "```python\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Modelin yaradılması\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1  # Warning-ları tamamilə söndürmək\n",
    ")\n",
    "\n",
    "# Modelin təlimi\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin testi\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### 2. **`early_stopping_rounds` İstifadə Edərək Overfitting-i Azaltmaq:**\n",
    "`early_stopping_rounds` parametrini istifadə edərək, təlim zamanı modelin lazımsız splitlər etməsinin qarşısını ala bilərsiniz. Bu, xəbərdarlıqların azalmasına kömək edə bilər.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "```\n",
    "\n",
    "### 3. **`min_child_weight` Parametrini Dəyişdirin:**\n",
    "`min_child_weight` parametrini artırmaqla xəbərdarlıqların qarşısını almaq mümkündür. Bu, splitlərin daha da məntiqli olmasını təmin edəcək.\n",
    "\n",
    "```python\n",
    "model = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    min_split_gain=0.01,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=1.0,  # Dəyəri artırmaqla xəbərdarlıqların qarşısını almaq\n",
    "    max_depth=7,\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "```\n",
    "\n",
    "Bu parametrləri birləşdirərək və tənzimləyərək xəbərdarlıqların sayını azalda bilərsiniz. Ancaq `verbosity=-1` parametri bu xəbərdarlıqları tamamilə gizlədəcəkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
