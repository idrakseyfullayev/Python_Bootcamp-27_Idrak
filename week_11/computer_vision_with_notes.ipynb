{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9cc3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #for using tensors\n",
    "import torch.nn as nn # creating neural network\n",
    "import torch.nn.functional as F # recallable functions like relu,sigmoid and etc.\n",
    "import torchvision # subpackage for vision models\n",
    "import torchvision.transforms as transforms # image augmentation\n",
    "import torch.optim as optim # optimisation functions like sgd, adam\n",
    "from torch.utils.data import DataLoader # creating a data loader\n",
    "from torchvision.transforms import ToTensor # converting image to tensor\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ebbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc9bc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape\n",
    "# test data is already formed as a tensor dataset which means it has 2 dimensions \n",
    "# first dimension is X variable ( images pixel values)\n",
    "# second one is it's label in which it is going to learn \n",
    "# when we write test_data[0] we are calling the first images X and y values\n",
    "# when we write test_data[0][0] we are calling first images X only and test_data[0][1] we are calling it's y\n",
    "# X variable has got 3 dimension \n",
    "# 1'st one is collor chanel\n",
    "# 2'nd and 3rd ones are height and width pixel counts\n",
    "# in this case we have gray scale image with 28x28 pixel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17234f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypers\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a373ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data,shuffle=True,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data,shuffle=True)\n",
    "# data loader is a tool which defines how our data will get into the model\n",
    "# in training fase we should define batch_size for increasing the model's speed and reducing underfiting \n",
    "# but in testing fase it is not necessary as we will not do backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17345afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     0: \"T-Shirt\",\n",
    "#     1: \"Trouser\",\n",
    "#     2: \"Pullover\",\n",
    "#     3: \"Dress\",\n",
    "#     4: \"Coat\",\n",
    "#     5: \"Sandal\",\n",
    "#     6: \"Shirt\",\n",
    "#     7: \"Sneaker\",\n",
    "#     8: \"Bag\",\n",
    "#     9: \"Ankle Boot\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a865b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([0, 9, 8, 3, 9, 5, 4, 6, 3, 3, 4, 6, 9, 0, 1, 7, 6, 4, 2, 5, 8, 5, 0, 3,\n",
      "        9, 7, 3, 9, 3, 2, 0, 2, 6, 5, 4, 3, 9, 1, 5, 9, 2, 3, 5, 7, 6, 4, 3, 0,\n",
      "        2, 2, 0, 5, 9, 8, 4, 2, 9, 4, 7, 8, 0, 9, 1, 7])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d694d",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62df7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()# first 3 lines is mandatory for inheriting properties of model\n",
    "# conv layer are special layer's which uses filter for collectiong various information from images\n",
    "# in_channels on first layer should be number of colur channels in our case it is 1\n",
    "# kernel size is a size of filter matrix 5x5 in this case\n",
    "# padding adds a pixel in every direction in oorder learning corner's better\n",
    "# stride is defining movement of kernel. default is 1\n",
    "# image size after each conv layer is detected by s = (Si-filter_size+2*padding)/stride + 1\n",
    "# and after pooling if we use 2,2 it will shrink from every side 2 times\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=5,padding=2)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=5,stride=2,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(128*7*7,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.out = nn.Linear(512,10)\n",
    "        self.drop_out = nn.Dropout()\n",
    "    def forward(self,X):\n",
    "        # input image size is [1,28,28]\n",
    "        X = self.conv1(X)\n",
    "        # after first layer of conv z absis will get number from out_channels (64)\n",
    "        # (28-5+2*2)/1+1 -> 28\n",
    "        # our result will be [64,28,28]\n",
    "        X = self.conv2(X)\n",
    "        # out_channel is 128\n",
    "        # (28-5+2*2)/2 + 1 ->14.5 aka 14\n",
    "        # [128,14,14]\n",
    "        X = self.pool(X)\n",
    "        # 14/2 -> 7\n",
    "        # [128,7,7]\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        # [ 128*7*7] -> one dimension\n",
    "        X = self.drop_out(X)\n",
    "        # Regularization and preventing the co-adaptation of neurons as described in the paper\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        out = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3febad06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(28-5+2*2)/2+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a3ba003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "# defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1350d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6  # how many times model will go back and forth\n",
    "num_classes = 10 # number of labels we should predict\n",
    "learning_rate = 0.001 #step of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57ddb27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #loss function for defining if we  predicted right or wronge\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # optimisation function for creating gradient decent and learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb190d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [100/938], Loss: 0.9275, Accuracy: 73.44%\n",
      "Epoch [1/6], Step [200/938], Loss: 0.3183, Accuracy: 89.06%\n",
      "Epoch [1/6], Step [300/938], Loss: 0.2421, Accuracy: 92.19%\n",
      "Epoch [1/6], Step [400/938], Loss: 0.3175, Accuracy: 85.94%\n",
      "Epoch [1/6], Step [500/938], Loss: 0.3206, Accuracy: 89.06%\n",
      "Epoch [1/6], Step [600/938], Loss: 0.4381, Accuracy: 85.94%\n",
      "Epoch [1/6], Step [700/938], Loss: 0.3380, Accuracy: 84.38%\n",
      "Epoch [1/6], Step [800/938], Loss: 0.4396, Accuracy: 82.81%\n",
      "Epoch [1/6], Step [900/938], Loss: 0.4532, Accuracy: 90.62%\n",
      "Epoch [2/6], Step [100/938], Loss: 0.5183, Accuracy: 78.12%\n",
      "Epoch [2/6], Step [200/938], Loss: 0.2295, Accuracy: 90.62%\n",
      "Epoch [2/6], Step [300/938], Loss: 0.3286, Accuracy: 89.06%\n",
      "Epoch [2/6], Step [400/938], Loss: 0.3901, Accuracy: 87.50%\n",
      "Epoch [2/6], Step [500/938], Loss: 0.2334, Accuracy: 92.19%\n",
      "Epoch [2/6], Step [600/938], Loss: 0.2473, Accuracy: 90.62%\n",
      "Epoch [2/6], Step [700/938], Loss: 0.3737, Accuracy: 84.38%\n",
      "Epoch [2/6], Step [800/938], Loss: 0.3774, Accuracy: 89.06%\n",
      "Epoch [2/6], Step [900/938], Loss: 0.1717, Accuracy: 96.88%\n",
      "Epoch [3/6], Step [100/938], Loss: 0.2721, Accuracy: 87.50%\n",
      "Epoch [3/6], Step [200/938], Loss: 0.2045, Accuracy: 90.62%\n",
      "Epoch [3/6], Step [300/938], Loss: 0.3169, Accuracy: 87.50%\n",
      "Epoch [3/6], Step [400/938], Loss: 0.2400, Accuracy: 92.19%\n",
      "Epoch [3/6], Step [500/938], Loss: 0.4251, Accuracy: 84.38%\n",
      "Epoch [3/6], Step [600/938], Loss: 0.1712, Accuracy: 93.75%\n",
      "Epoch [3/6], Step [700/938], Loss: 0.2371, Accuracy: 89.06%\n",
      "Epoch [3/6], Step [800/938], Loss: 0.2404, Accuracy: 90.62%\n",
      "Epoch [3/6], Step [900/938], Loss: 0.1976, Accuracy: 93.75%\n",
      "Epoch [4/6], Step [100/938], Loss: 0.1603, Accuracy: 92.19%\n",
      "Epoch [4/6], Step [200/938], Loss: 0.2147, Accuracy: 93.75%\n",
      "Epoch [4/6], Step [300/938], Loss: 0.3611, Accuracy: 85.94%\n",
      "Epoch [4/6], Step [400/938], Loss: 0.2243, Accuracy: 87.50%\n",
      "Epoch [4/6], Step [500/938], Loss: 0.1180, Accuracy: 95.31%\n",
      "Epoch [4/6], Step [600/938], Loss: 0.2130, Accuracy: 90.62%\n",
      "Epoch [4/6], Step [700/938], Loss: 0.1127, Accuracy: 98.44%\n",
      "Epoch [4/6], Step [800/938], Loss: 0.2355, Accuracy: 87.50%\n",
      "Epoch [4/6], Step [900/938], Loss: 0.1381, Accuracy: 93.75%\n",
      "Epoch [5/6], Step [100/938], Loss: 0.1726, Accuracy: 92.19%\n",
      "Epoch [5/6], Step [200/938], Loss: 0.0794, Accuracy: 95.31%\n",
      "Epoch [5/6], Step [300/938], Loss: 0.1778, Accuracy: 90.62%\n",
      "Epoch [5/6], Step [400/938], Loss: 0.2650, Accuracy: 89.06%\n",
      "Epoch [5/6], Step [500/938], Loss: 0.3250, Accuracy: 89.06%\n",
      "Epoch [5/6], Step [600/938], Loss: 0.2399, Accuracy: 90.62%\n",
      "Epoch [5/6], Step [700/938], Loss: 0.2011, Accuracy: 93.75%\n",
      "Epoch [5/6], Step [800/938], Loss: 0.2921, Accuracy: 90.62%\n",
      "Epoch [5/6], Step [900/938], Loss: 0.2079, Accuracy: 90.62%\n",
      "Epoch [6/6], Step [100/938], Loss: 0.1141, Accuracy: 98.44%\n",
      "Epoch [6/6], Step [200/938], Loss: 0.1967, Accuracy: 90.62%\n",
      "Epoch [6/6], Step [300/938], Loss: 0.1325, Accuracy: 93.75%\n",
      "Epoch [6/6], Step [400/938], Loss: 0.1617, Accuracy: 92.19%\n",
      "Epoch [6/6], Step [500/938], Loss: 0.2864, Accuracy: 89.06%\n",
      "Epoch [6/6], Step [600/938], Loss: 0.3108, Accuracy: 90.62%\n",
      "Epoch [6/6], Step [700/938], Loss: 0.4399, Accuracy: 92.19%\n",
      "Epoch [6/6], Step [800/938], Loss: 0.0646, Accuracy: 96.88%\n",
      "Epoch [6/6], Step [900/938], Loss: 0.1570, Accuracy: 93.75%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e65de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 90.86999999999999 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
