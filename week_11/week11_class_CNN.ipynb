{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8002fa6",
   "metadata": {},
   "source": [
    "**https://poloclub.github.io/cnn-explainer/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfabb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #for using tensors\n",
    "import torch.nn as nn # creating neural network\n",
    "import torch.nn.functional as F # recallable functions like relu,sigmoid and etc.\n",
    "import torchvision # subpackage for vision models\n",
    "import torchvision.transforms as transforms # image augmentation\n",
    "import torch.optim as optim # optimisation functions like sgd, adam\n",
    "from torch.utils.data import DataLoader # creating a data loader\n",
    "from torchvision.transforms import ToTensor # converting image to tensor\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58f2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch kitabxanasını yükləyir, tensor əməliyyatlarını və dərin öyrənmə üçün əsas modulları təmin edir\n",
    "import torch\n",
    "# Neyron şəbəkələr qurmaq üçün PyTorch-un `nn` modulunu yükləyir\n",
    "import torch.nn as nn \n",
    "# Neyron şəbəkələrdə müxtəlif funksiyalardan istifadə etmək üçün `nn.functional` modulunu yükləyir\n",
    "import torch.nn.functional as F \n",
    "# PyTorch-da kompüter görmə layihələri üçün faydalı modullar təmin edən `torchvision` kitabxanasını yükləyir\n",
    "import torchvision \n",
    "# Şəkillərə əvvəlcədən emal transformasiyaları tətbiq etmək üçün `transforms` modulunu yükləyir\n",
    "import torchvision.transforms as transforms \n",
    "# PyTorch-da optimizasiya alqoritmləri (məsələn, SGD, Adam) üçün `optim` modulunu yükləyir\n",
    "import torch.optim as optim \n",
    "# DataLoader PyTorch-da məlumat dəstlərini daha asan idarə etmək üçün istifadə olunur (məlumat dəstlərini \n",
    "#                                                                                     qruplaşdırır, sıraya qoyur)\n",
    "from torch.utils.data import DataLoader \n",
    "# Şəkilləri tensor formatına çevirmək üçün `ToTensor` funksiyasını `transforms` modulundan yükləyir\n",
    "from torchvision.transforms import ToTensor \n",
    "# Nəticələri görselləşdirmək üçün Matplotlib kitabxanasını yükləyir\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062229e",
   "metadata": {},
   "source": [
    "### FashionMNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb132db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a756d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].shape\n",
    "# test data is already formed as a tensor dataset which means it has 2 dimensions \n",
    "# first dimension is X variable ( images pixel values)\n",
    "# second one is it's label in which it is going to learn \n",
    "# when we write test_data[0] we are calling the first images X and y values\n",
    "# when we write test_data[0][0] we are calling first images X only and test_data[0][1] we are calling it's y\n",
    "# X variable has got 3 dimension \n",
    "# 1'st one is collor chanel\n",
    "# 2'nd and 3rd ones are height and width pixel counts\n",
    "# in this case we have gray scale image with 28x28 pixel dimension\n",
    "\n",
    "# test_data artıq tensor məlumat dəsti kimi formalaşdırılıb, yəni 2 ölçüyə malikdir\n",
    "# birinci ölçü X dəyişəni (şəkillərin piksel dəyərləri)\n",
    "# ikincisi isə onun etiketidir, yəni hansı sinifdə öyrəniləcəyidir\n",
    "# test_data[0] yazanda, birinci şəkilin X və y dəyərlərini çağırırıq\n",
    "# test_data[0][0] yazanda isə yalnız birinci şəkilin X dəyərini, test_data[0][1] yazanda isə onun y etiketini çağırırıq\n",
    "# X dəyişəni 3 ölçüyə malikdir\n",
    "# birinci ölçü rəng kanalını təmsil edir\n",
    "# ikinci və üçüncü ölçülər isə hündürlük və en piksel sayıdır\n",
    "# bu halda, bizdə 28x28 piksel ölçüdə boz tonlu (gray scale) bir şəkil var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc3fbe",
   "metadata": {},
   "source": [
    "### Set Batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6155d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hypers\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6fa8",
   "metadata": {},
   "source": [
    "### Train vs Test Data with batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f8a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Təlim məlumat dəstini yükləmək üçün DataLoader obyektini təyin edir\n",
    "# training_data - təlim məlumatlarıdır\n",
    "# shuffle=True - təlim məlumatlarını hər dövrədə qarışdırır\n",
    "# batch_size=batch_size - təlim zamanı məlumatları `batch_size` ölçüsündə paketlərə ayırır\n",
    "train_loader = DataLoader(training_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test məlumat dəstini yükləmək üçün DataLoader obyektini təyin edir\n",
    "# test_data - test məlumatlarıdır\n",
    "# shuffle=True - test məlumatlarını qarışdırır, test zamanı müxtəlif məlumatları təsadüfi sırada yoxlayır\n",
    "test_loader = DataLoader(test_data, shuffle=True)\n",
    "\n",
    "# data loader is a tool which defines how our data will get into the model\n",
    "# in training fase we should define batch_size for increasing the model's speed and reducing underfiting \n",
    "# but in testing fase it is not necessary as we will not do backprop\n",
    "\n",
    "# DataLoader - verilənlərin modelə necə daxil olacağını müəyyən edən bir alətdir\n",
    "# Təlim mərhələsində modelin sürətini artırmaq və underfitting-i azaltmaq üçün batch_size təyin edilməlidir\n",
    "# Lakin test mərhələsində batch_size təyin etmək zəruri deyil, çünki bu mərhələdə backprop (geri yayılma) aparılmır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b7ae304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     0: \"T-Shirt\",\n",
    "#     1: \"Trouser\",\n",
    "#     2: \"Pullover\",\n",
    "#     3: \"Dress\",\n",
    "#     4: \"Coat\",\n",
    "#     5: \"Sandal\",\n",
    "#     6: \"Shirt\",\n",
    "#     7: \"Sneaker\",\n",
    "#     8: \"Bag\",\n",
    "#     9: \"Ankle Boot\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3fbb3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "tensor([6, 5, 4, 1, 5, 4, 6, 7, 4, 2, 9, 9, 3, 1, 9, 0, 0, 4, 0, 4, 1, 2, 4, 1,\n",
      "        1, 7, 3, 9, 4, 6, 6, 5, 2, 3, 3, 4, 6, 7, 0, 9, 5, 2, 8, 8, 8, 7, 0, 5,\n",
      "        4, 3, 3, 0, 5, 7, 0, 9, 6, 6, 6, 9, 8, 0, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# train_loader-dəki verilənləri dövrə ilə oxuyur\n",
    "for x, y in train_loader:\n",
    "    # İlk batch-dəki giriş verilənlərinin formasını çap edir\n",
    "    print(x.shape)\n",
    "    # İlk batch-dəki hədəf etiketlərini çap edir\n",
    "    print(y)\n",
    "    # Yalnız ilk batch-i göstərir və dövrəni dayandırır\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42a596",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a54fae",
   "metadata": {},
   "source": [
    "### Creat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c1166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()# first 3 lines is mandatory for inheriting properties of model\n",
    "# conv layer are special layer's which uses filter for collectiong various information from images\n",
    "# in_channels on first layer should be number of colur channels in our case it is 1\n",
    "# kernel size is a size of filter matrix 5x5 in this case\n",
    "# padding adds a pixel in every direction in oorder learning corner's better\n",
    "# stride is defining movement of kernel. default is 1\n",
    "# image size after each conv layer is detected by s = (Si-filter_size+2*padding)/stride + 1\n",
    "# and after pooling if we use 2,2 it will shrink from every side 2 times\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=64,kernel_size=5,padding=2)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=5,stride=2,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(128*7*7,512)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.out = nn.Linear(512,10)\n",
    "        self.drop_out = nn.Dropout()\n",
    "    def forward(self,X):\n",
    "        # input image size is [1,28,28]\n",
    "        X = self.conv1(X)\n",
    "        # after first layer of conv z absis will get number from out_channels (64)\n",
    "        # (28-5+2*2)/1+1 -> 28\n",
    "        # our result will be [64,28,28]\n",
    "        X = self.conv2(X)\n",
    "        # out_channel is 128\n",
    "        # (28-5+2*2)/2 + 1 ->14.5 aka 14\n",
    "        # [128,14,14]\n",
    "        X = self.pool(X)\n",
    "        # 14/2 -> 7\n",
    "        # [128,7,7]\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        # [ 128*7*7] -> one dimension\n",
    "        X = self.drop_out(X)\n",
    "        # Regularization and preventing the co-adaptation of neurons as described in the paper\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        out = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b4b2b",
   "metadata": {},
   "source": [
    "### Creat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57543cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    # nn.Module sinfinin xüsusiyyətlərini irsən almaq üçün super().__init__() çağırırıq\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()  \n",
    "        # İlk konvolusiya qatını təyin edirik: 1 giriş kanalı (şəkilin rəng kanalı), 64 çıxış kanalı (filtrlər)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        # İkinci konvolusiya qatını təyin edirik: 64 giriş kanalı (öncəki qatın çıxışı), 128 çıxış kanalı\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        # Max Pooling qatını təyin edirik: 2x2 ölçüsündə pəncərə\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Birinci tam bağlı (fully connected) qat: giriş 128*7*7 ölçüsündə, çıxış 512\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        # İkinci tam bağlı qat: giriş 512, çıxış 512\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        # Son tam bağlı qat: çıxış siniflərin sayı ilə uyğunlaşdırılır (10 sinif)\n",
    "        self.out = nn.Linear(512, 10)\n",
    "        # Dropout təbəqəsi əlavə olunur, overfitting-in qarşısını almaq üçün istifadə olunur\n",
    "        self.drop_out = nn.Dropout()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # İlk konvolusiya qatına məlumat verilir\n",
    "        X = self.conv1(X)\n",
    "        # İkinci konvolusiya qatına məlumat verilir\n",
    "        X = self.conv2(X)\n",
    "        # Pooling qatına məlumat verilir, ölçü azaldılır\n",
    "        X = self.pool(X)\n",
    "        # Tensoru bir ölçüyə çeviririk (flatten)\n",
    "        X = X.reshape(X.size(0), -1)\n",
    "        # Dropout tətbiq edirik\n",
    "        X = self.drop_out(X)\n",
    "        # ReLU aktivləşdirmə funksiyasını birinci tam bağlı qata tətbiq edirik\n",
    "        X = F.relu(self.fc1(X))\n",
    "        # ReLU aktivləşdirmə funksiyasını ikinci tam bağlı qata tətbiq edirik\n",
    "        X = F.relu(self.fc2(X))\n",
    "        # Çıxış qatından sinif prediksiyasını əldə edirik\n",
    "        out = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47f0db74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(28-5+2*2)/2+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c995d6",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e02f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "# defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a87b9f",
   "metadata": {},
   "source": [
    "### Epoch vs Num_classes vs Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f45a5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6  # how many times model will go back and forth\n",
    "num_classes = 10 # number of labels we should predict\n",
    "learning_rate = 0.001 #step of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a27f9",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss vs Adam Optimzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79ad4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "#loss function for defining if we  predicted right or wronge\n",
    "# Modelin səhvini ölçmək üçün istifadə olunur, çoxlu sinifli təsnifat üçün uyğun itki funksiyasıdır\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimisation function for creating gradient decent and learning\n",
    "# Modelin parametrlərini optimallaşdırmaq üçün Adam optimallaşdırıcısını istifadə edir, \n",
    "#                                                                         təlim sürətini learning_rate ilə tənzimləyir\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8f6fc",
   "metadata": {},
   "source": [
    "### Train stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "087737bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [100/938], Loss: 0.1299, Accuracy: 96.88%\n",
      "Epoch [1/6], Step [200/938], Loss: 0.1886, Accuracy: 92.19%\n",
      "Epoch [1/6], Step [300/938], Loss: 0.0913, Accuracy: 96.88%\n",
      "Epoch [1/6], Step [400/938], Loss: 0.1637, Accuracy: 92.19%\n",
      "Epoch [1/6], Step [500/938], Loss: 0.1299, Accuracy: 93.75%\n",
      "Epoch [1/6], Step [600/938], Loss: 0.1110, Accuracy: 96.88%\n",
      "Epoch [1/6], Step [700/938], Loss: 0.2057, Accuracy: 92.19%\n",
      "Epoch [1/6], Step [800/938], Loss: 0.3355, Accuracy: 90.62%\n",
      "Epoch [1/6], Step [900/938], Loss: 0.3006, Accuracy: 87.50%\n",
      "Epoch [2/6], Step [100/938], Loss: 0.1427, Accuracy: 93.75%\n",
      "Epoch [2/6], Step [200/938], Loss: 0.1421, Accuracy: 95.31%\n",
      "Epoch [2/6], Step [300/938], Loss: 0.3411, Accuracy: 89.06%\n",
      "Epoch [2/6], Step [400/938], Loss: 0.2258, Accuracy: 89.06%\n",
      "Epoch [2/6], Step [500/938], Loss: 0.1533, Accuracy: 95.31%\n",
      "Epoch [2/6], Step [600/938], Loss: 0.1131, Accuracy: 96.88%\n",
      "Epoch [2/6], Step [700/938], Loss: 0.1616, Accuracy: 95.31%\n",
      "Epoch [2/6], Step [800/938], Loss: 0.3387, Accuracy: 90.62%\n",
      "Epoch [2/6], Step [900/938], Loss: 0.1666, Accuracy: 93.75%\n",
      "Epoch [3/6], Step [100/938], Loss: 0.0881, Accuracy: 95.31%\n",
      "Epoch [3/6], Step [200/938], Loss: 0.3781, Accuracy: 90.62%\n",
      "Epoch [3/6], Step [300/938], Loss: 0.2292, Accuracy: 90.62%\n",
      "Epoch [3/6], Step [400/938], Loss: 0.0610, Accuracy: 98.44%\n",
      "Epoch [3/6], Step [500/938], Loss: 0.1372, Accuracy: 92.19%\n",
      "Epoch [3/6], Step [600/938], Loss: 0.1575, Accuracy: 95.31%\n",
      "Epoch [3/6], Step [700/938], Loss: 0.2073, Accuracy: 92.19%\n",
      "Epoch [3/6], Step [800/938], Loss: 0.3315, Accuracy: 89.06%\n",
      "Epoch [3/6], Step [900/938], Loss: 0.0702, Accuracy: 96.88%\n",
      "Epoch [4/6], Step [100/938], Loss: 0.1428, Accuracy: 95.31%\n",
      "Epoch [4/6], Step [200/938], Loss: 0.1248, Accuracy: 93.75%\n",
      "Epoch [4/6], Step [300/938], Loss: 0.1608, Accuracy: 92.19%\n",
      "Epoch [4/6], Step [400/938], Loss: 0.1380, Accuracy: 98.44%\n",
      "Epoch [4/6], Step [500/938], Loss: 0.1833, Accuracy: 96.88%\n",
      "Epoch [4/6], Step [600/938], Loss: 0.1948, Accuracy: 89.06%\n",
      "Epoch [4/6], Step [700/938], Loss: 0.2016, Accuracy: 95.31%\n",
      "Epoch [4/6], Step [800/938], Loss: 0.1411, Accuracy: 93.75%\n",
      "Epoch [4/6], Step [900/938], Loss: 0.0764, Accuracy: 96.88%\n",
      "Epoch [5/6], Step [100/938], Loss: 0.1828, Accuracy: 92.19%\n",
      "Epoch [5/6], Step [200/938], Loss: 0.2720, Accuracy: 87.50%\n",
      "Epoch [5/6], Step [300/938], Loss: 0.0603, Accuracy: 98.44%\n",
      "Epoch [5/6], Step [400/938], Loss: 0.1833, Accuracy: 90.62%\n",
      "Epoch [5/6], Step [500/938], Loss: 0.0770, Accuracy: 98.44%\n",
      "Epoch [5/6], Step [600/938], Loss: 0.1605, Accuracy: 93.75%\n",
      "Epoch [5/6], Step [700/938], Loss: 0.1082, Accuracy: 92.19%\n",
      "Epoch [5/6], Step [800/938], Loss: 0.0669, Accuracy: 98.44%\n",
      "Epoch [5/6], Step [900/938], Loss: 0.1535, Accuracy: 92.19%\n",
      "Epoch [6/6], Step [100/938], Loss: 0.1072, Accuracy: 93.75%\n",
      "Epoch [6/6], Step [200/938], Loss: 0.2273, Accuracy: 93.75%\n",
      "Epoch [6/6], Step [300/938], Loss: 0.1150, Accuracy: 95.31%\n",
      "Epoch [6/6], Step [400/938], Loss: 0.1430, Accuracy: 95.31%\n",
      "Epoch [6/6], Step [500/938], Loss: 0.1955, Accuracy: 95.31%\n",
      "Epoch [6/6], Step [600/938], Loss: 0.5455, Accuracy: 90.62%\n",
      "Epoch [6/6], Step [700/938], Loss: 0.1556, Accuracy: 95.31%\n",
      "Epoch [6/6], Step [800/938], Loss: 0.1883, Accuracy: 93.75%\n",
      "Epoch [6/6], Step [900/938], Loss: 0.1254, Accuracy: 96.88%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader) # Təlim mərhələsindəki ümumi addım sayını əldə edir\n",
    "loss_list = [] # Hər epoch-da itki dəyərlərini izləmək üçün siyahı\n",
    "acc_list = [] # Hər epoch-da dəqiqlik dəyərlərini izləmək üçün siyahı\n",
    "for epoch in range(num_epochs): # Təlimi müəyyən edilmiş epoch sayda təkrarlamaq üçün döngü\n",
    "    for i, (images, labels) in enumerate(train_loader): # Təlim məlumatlarını batch-lər üzrə əldə edir\n",
    "\n",
    "        # Run the forward pass\n",
    "        outputs = model(images) # Modeldən görüntüləri keçirərək çıxışları hesablayır\n",
    "        loss = criterion(outputs, labels) # Modelin itki dəyərini müəyyən edir\n",
    "        loss_list.append(loss.item()) # İtki dəyərini `loss_list`-ə əlavə edir\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad() # Gradienti sıfırlayır ki, hər yeni batch üçün əvvəlki gradienlər yığılmasın\n",
    "        loss.backward() # Itkiyə görə gradienləri hesablamaq üçün geri yayılma addımı\n",
    "        optimizer.step() # Adam optimizatoru vasitəsilə model parametrlərini yeniləyir\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0) # Mövcud batch-dəki ümumi nümunə sayını tapır\n",
    "        _, predicted = torch.max(outputs.data, 1) # Hər bir nümunə üçün ehtimalı ən yüksək olan sinifi tapır\n",
    "        correct = (predicted == labels).sum().item() # Doğru proqnozların sayını hesablayır\n",
    "        acc_list.append(correct / total) # Dəqiqliyi `acc_list`-ə əlavə edir\n",
    "\n",
    "        if (i + 1) % 100 == 0: # Hər 100 addımdan bir məlumat çap edir\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56913d64",
   "metadata": {},
   "source": [
    "### Evolution stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6da8792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 91.23 %\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Modeli qiymətləndirmə rejiminə keçirir; dropout və batch normalization kimi qatları deaktiv edir\n",
    "with torch.no_grad(): # Hesablamalar zamanı gradienləri saxlamamaq üçün istifadə olunur, bu da yaddaşa qənaət edir\n",
    "    correct = 0 # Düzgün proqnozların sayını saxlamaq üçün dəyişən\n",
    "    total = 0 # Ümumi nümunə sayını saxlamaq üçün dəyişən\n",
    "    for images, labels in test_loader: # Test məlumatlarını batch-lərlə iterasiya edir\n",
    "        outputs = model(images) # Modeldən test görüntülərini keçirərək çıxışları hesablayır\n",
    "        _, predicted = torch.max(outputs.data, 1) # Hər bir nümunə üçün ehtimalı ən yüksək olan sinifi tapır\n",
    "        total += labels.size(0) # Mövcud batch-dəki ümumi nümunə sayını artırır\n",
    "        correct += (predicted == labels).sum().item() # Doğru proqnozların sayını artırır\n",
    "        \n",
    "    # Test dəsti üçün modelin dəqiqliyini çap edir\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09e095",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fba3b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.23%\n",
      "F1 Score: 0.91\n",
      "Recall: 0.91\n",
      "Precision: 0.91\n",
      "Confusion Matrix:\n",
      " [[891   0  17  17   2   0  65   2   6   0]\n",
      " [  3 986   0   8   2   0   0   0   1   0]\n",
      " [ 18   0 841   7  41   0  92   0   1   0]\n",
      " [ 20   4   9 922  18   0  26   0   1   0]\n",
      " [  0   1  65  36 816   0  82   0   0   0]\n",
      " [  0   0   0   0   0 983   0   8   1   8]\n",
      " [126   1  37  20  41   0 770   0   5   0]\n",
      " [  0   0   0   0   0   8   0 977   0  15]\n",
      " [  3   1   3   3   1   1   3   2 982   1]\n",
      " [  0   0   0   0   0   4   1  40   0 955]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86      1000\n",
      "           1       0.99      0.99      0.99      1000\n",
      "           2       0.87      0.84      0.85      1000\n",
      "           3       0.91      0.92      0.92      1000\n",
      "           4       0.89      0.82      0.85      1000\n",
      "           5       0.99      0.98      0.98      1000\n",
      "           6       0.74      0.77      0.76      1000\n",
      "           7       0.95      0.98      0.96      1000\n",
      "           8       0.98      0.98      0.98      1000\n",
      "           9       0.98      0.95      0.97      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score,\\\n",
    "                                                                                                    confusion_matrix\n",
    "\n",
    "model.eval() # Modeli qiymətləndirmə rejiminə keçirir\n",
    "all_labels = [] # Test dəstindəki həqiqi etiketləri saxlamaq üçün boş siyahı\n",
    "all_predictions = [] # Modelin təxmin etdiyi etiketləri saxlamaq üçün boş siyahı\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images) # Test məlumatlarını modeldən keçirir\n",
    "        _, predicted = torch.max(outputs, 1) # Hər bir nümunə üçün ehtimalı ən yüksək olan sinifi seçir\n",
    "        all_labels.extend(labels.cpu().numpy()) # Həqiqi etiketləri siyahıya əlavə edir\n",
    "        all_predictions.extend(predicted.cpu().numpy()) # Təxminləri siyahıya əlavə edir\n",
    "\n",
    "# Metrikləri hesablamaq\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad9700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8b7f8b6",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd2adcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelin proqnozu: Sinif: 2 ; Adı: Pullover\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# FashionMNIST sinif adları\n",
    "class_names = torchvision.datasets.FashionMNIST.classes\n",
    "\n",
    "# Şəkli yükləyin və modelin gözlədiyi ölçüyə və formata çevirin\n",
    "image_path = r\"C:\\Users\\dell\\Desktop\\Python\\week11\\sample_image-300x300.png\"  # Yeni şəkilinizin fayl yolu\n",
    "image = Image.open(image_path).convert(\"L\")  # Gri tonlara çevir\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Modelin gözlədiyi ölçüyə çevirir\n",
    "    transforms.ToTensor(),        # Tensora çevirir\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Eyni normalizasiya tətbiq edir\n",
    "])\n",
    "\n",
    "# Şəkli tensora çevir və batch boyutunu (1, 1, 28, 28) kimi göstər\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "# Modelin proqnozunu alın\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "       \n",
    "# Proqnoz nəticəsini sinif adı ilə göstər\n",
    "predicted_class_name = class_names[predicted_class.item()]\n",
    "print(f\"Modelin proqnozu: Sinif: {predicted_class.item()} ; Adı: {predicted_class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f1cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16c2e1c9",
   "metadata": {},
   "source": [
    "### Check Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d95e67f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx3klEQVR4nO3dS48k2VkG4DduGZH36q7qbnp6PDOedtsS4zFjG1lCBgkWBpZIbNjxK/gP7Fiz4DewQF4gIWwhGRBYtpDBYC6aGdrT17p0Z1Xe4sqiOGeya6qrv+9kZVRWxPtILV86Tp3IiBPnOxHZ8ZZXVVUFIiIiAP5V7wAREW0PFgUiIrJYFIiIyGJRICIii0WBiIgsFgUiIrJYFIiIyGJRICIiK5Ru6HneJveDrhHfP11LaMZEr9dDFEW27ZsURYEsy3BycuK0j9vI8zyEYYhut6s6dmVZQvOOaVVVKMsSi8VC1Y6aazgc4jd+4zfw13/912/cVlwUiIDTia3T6SAIAvH2vu/j3Xffxc7ODuI4FrVbLpc4ODjAz3/+83V2d6uEYYjRaIQvf/nL4jZVVWG5XKIoCvEEX1UV0jTF48ePsVgsXHeXGsQsSCRYFEjFTPLSogAAQRCg1+thOBwiSRJRm/l8jtls5rqbW8lcmP1+X9ymqir4vo+iKFCWpbiNOU9EhvTulEWB1Mzgkg4yM0FpikkQBI18ZOl5nv3zJquTe1mW4km+LMtGHjtaj3RMcClBREQWiwIREVksCkREZLEoEBGRxaJAREQWiwIREVksCkREZLEoEBGRxZfXtpD25bAwDDEYDPDWW2+pXlpKkgSDwQCdTkfcJggCJEkifmUeOM1KeueddzAej7/Ql4luMPtt/neapnjx4gXu3bsn7sfs37a+uLV6nqRWYy40bzRnWYbnz59juVyK+8qyDIvFAnmei/tJ0xRHR0eqOI2qqmyboijE7ageLApbSPPWK3A6ue/t7eFb3/qWakLc2dnB3bt3MRwOxW1cioLnedjd3UW320UURa/83euKQlEUmM/n+OY3vynuB4DNZdrGwuB5HqIoQq/XU7VL01SdfVSWJabTqWrSnc1mODw8FE/wVVXh+PgY//3f/42joyNxP2VZIs9z5HnOorCFWBS2kIk2kE5snU4HN2/exAcffKAqJrdv38aDBw+wt7cn3jff95EkiXri7fV6CMNQHHNRlqWdPDRMwdrG3B8zWWsnwizL1G2qqlInpL58+RKPHj0SJ9OWZYnDw0MAwJMnT8T7VRQFHj9+jMlkoto/qsf2XTlERHRlWBRoK2nueIjo8rAoEBGRxaJAREQWiwIREVksCkREZLEoEBGRxaJAW0v77+yJaH0sCrSVWBCIrgaLAm0lvqNAdDUYc7FBnuchCAJ15EKn01HFXHS7XcRxrI53MNtrJmBtWJ/Z1qz8Nfk969jGOw3tMVi3nZaJV5GOITO+oyhShSrmeY4kSZAkiSrkryxLZFkm7ofcsChsiOd5GI/H+OCDD7C7uytu43ke3n77bQyHwy+Ex71OkiTY3d3Fr/7qr6om636/j52dHSRJIm5jJg4tE+i26TsA0492H132SzqhGWZi0+Y5lWW5VkFY/Wzn/Rzz991uF3fu3MGNGzfEP/vWrVsYj8eYTqfifSzLEl/96ldV4XvT6RS//OUv8YMf/EC8b+SGRWGDRqMRvv3tb+PBgwfiNmEY4qOPPsLt27fFaZqu6ZtmktJMOCbQTDshmiA4aV+mQEoLo2H2TTvJa++YzHHQHjuX1a7L3Rlwfoz4RT8jSRL0+331cfjKV76i2q+yLPH1r38dx8fHSNNU1ObZs2f48Y9/jB/+8IdbeRfYJCwKG+T7PqIoQhzH4gstCAJ0u130+31VUQiCQHULD5xOoHmeqy4yUwy0E5RL8an7ewVpn+tMSi5tXY+FtphoHx8ZZnvpsauqCt1uF3meiyPYu92uenyTG37RTEREFosCkQM+wqCmYlEgcsTCQE3EokDkgL/vgZqKRYGIiCwWBSIHfHRETcWiQOSIhYGaiEWByAG/U6CmYlEgIiKLbzQL+b5vg+qk25u3krvdrqiNeTM5CAIAuscTJkJBw7xlrAmpqyucbbXPTW5/tp32WGjPUZ3Mm9CbHkcAVP2YGJIgCMRtoihCt9vFzZs3VfuXZRkWi4U6b6rNWBSExuMx3nvvPYxGI9H2vu/j7bffxje+8Q1VNowJ0guCAEVRiNuYbCGNN00A5oI1j0lWJ0KXCc7lcYv2M7nkHgH6fav7OLjQTu5mHNXBJPtKj18cx0jTFH/4h3+o2seHDx/iX/7lX/DkyRPXXW0dFgWh4XCI+/fv4+7du6LtPc/DvXv38LWvfQ33798X91NVFbIsQ1mWqsHvmqkj2eZNKZsSJnDuvBXleZPkOpNuHVzumMzn1IbvuXA5dq7HW9vO8zwkSYIoisR33r1eD0EQ4Pd+7/dUff3kJz/Bw4cPWRQUWBSETFDdYDAQbe95HgaDAfr9Prrdripo7eTkRF0QNjmBXvbP3bYvaNd95KTZfts++1kun8nl7iIIAoRhaB+VvklZluj1euIYemM0GqmTdtuOXzQTEZHFokBERBaLAhERWSwKRERksSgQEZHFokBERBaLAhE11ja/67KtWBSIqLEYXKjHokBERBbfaBbyPA++76sC8VziIXir+zltmNt1ov1cLqvd8/p43ZhcZzWtPU/rXBfa/eRdgh6LglAYhuj3++JAPM/z0O/34fu+c5JmHRNiHTk8hulnNWDvom03led00b5tug3gVhAuayyc/Tlnk1Q1n2mdidoEOGqKgkkq1oiiSBWnYfpq83cRLApCt2/fxm//9m/j29/+trhNFEUYDodYLBaqvvI8t7HWUi7PTqMoQpIkCMPNDoOqqrBcLlEUhc3JkeyrS/Kr6U/DxDhrjp/v++o2VVWhKAr1eHA5t5rAvtVttIsE18KYpinSNFW1iaIIDx48ULV5/Pgx7ty5I85MqqoKeZ7j5OQEWZap+moKFgUhc6cwHA7FbczjJu2dgusKWfpoyzCTjbadltm318VaX/Q4wXUy1NJOvOa4aYuCazS1S5iea6x3HUXBZdHj+7463E57p9DmOwSDRUHIDErNbehqVLQmJfXsf9+kTf/rjLOf/U19SR4tbdImn1mvk8Za57PxbX5u77rw4XcLciwKW4QDl4iuGv9JKhERWSwKDcG7DCK6DCwKDdH2L8eI6HLwO4UtwomdiK4a7xSIiMhiUSAiIotFgYiILBYFIiKy+EWzkMnH0eYEuUQGrL4JLW3nEgGQZRmeP3+OPM/P3dfz+l8ulzg4OFDl9/i+j3feeQc7OzviQDMTB2H2TcLzPIRh6PQGq7ZNVVVI01T9jwNc4khcuGQzmeiJs1EcF6WtajKWXvczpNtqUoqNTqeDwWAgDrI0OV3T6ZTZR3Qx16JQR2qn2TdtguR8PsezZ88wmUxe299qEmZVVTg+PsZ//Md/4OjoSNxPp9PB7/7u72IwGKDX64namIlJe/zCMEQYhqrJwwTVafoqyxLL5VKZZeTB9z118XbJ4zGJoppYljzPxZ9ptRi4ZDqZ60g6zk1BcC0KOzs75/792aTXqqowm81wcHCg6qdJWBSEXIqCa8rn2Yhpyfa+7yOOY1U/eZ5jf38fz58/F21fFAUODw/xz//8z3j8+LG4n263i48++gjvvfee+PitJqpKVVWFMAzVk6GZ4DUxzqd3Cpn6TiaKIsRxrCr6LmNotSho+tIcBwDqCGwjCALVJO+SPQacFgVN5L0JbdT20yT8ToGI1sL3a5qFRYGoVpxAabuxKBARkcWiQERrYRhjs7AoEBGRxaJAREQWiwKJ8THBZWjeMeS/PmoWFgUiWgsXC83CokBiXBESNR+LAhGthYuFZmHMxQadDQ3bNibzRxr8VZYl8jxXR1Bo4yoA90cSLlk8RVEgTVNVnESWZVgul6qYC9/3axsTJrBPkxWUZRmKolAdh20d26s0YYer56itWBSEXFIaq6pSpZ26ckkFBYCnT5/iH/7hH/Dv//7vou2rqsJ0OsUnn3yC4+NjcT/9fh8nJyeqycY162YymaiyeEye09/8zd+o8pxMMdUUn+FwiPv37+MP/uAPxG2A0+yoKIrE57gsSxwcHOD73/8+nj59Kj4W4/EYX/rSl8Q5QcDpuX333XcxHo/Fbcw+1lFQer0efuVXfkU8XquqwuHhIT7++OMN79n2YlEQMiFZ2qC1dfrb9EVzdHSEn/3sZ/jHf/xH0fZVVSHPcxwfH6tihYfDIWazmQ0bk/J9XxVAaC7o+XwuXsHneY5PP/0Uf/mXf4l/+7d/Ux1z7fm5desWvvvd7+L3f//3Ve36/T6iKBIfizRNMZvN8P3vfx8/+9nPxOPwy1/+Mr773e/i3r17ou09z8OtW7fw7rvvYjAYiNoAp8fNnCPtMdQufpIkwd7eHqbTqWj7sixtmGBbsSgo1Xlruem+yrJEmqbi342w+rhJs+pfXblLP5PL9p7noSxL+0e6b1mW4eTkBJPJRNTGVa/Xw2KxUBdHQHc3aI7DbDbDZDIRT7zT6RTL5VJc8D3Pe+V3cUj2z5wn17Htctw0i4uiKNTJsk3DL5qJiMhiUSAiWnEdvjzfJBYFIqIVbX50BLAoEBG9gncKRERk8U6BiIjo/7EoEBGRxaJAREQWiwIREVl8o3mLrP6rB03MhesbolEUYTAYYGdnR7x/aZqqw+NcubzFO51O8fLlSyyXS1E7E9sRx7Eq88cE72n0ej0EQaB6c9rzPIRhiMViIX4r17yhXVUVwjBUxa2kaYr5fC7et+VyuVaci5b2TXeTnyWNpzFxNm3+srm1RUE7kWqC8M7SXjRmvzad7Li3t4fvfOc7uHHjhmh7E7T2ox/9SBUe58LEE0gzaEwEx09+8hP84he/wIsXL8Tt8jzH22+/jdu3b4v3ryxLVfAeAMRxjCRJ8Ld/+7fiNp7nYTweI45j8cRWFAUODg5QVRVu3Lgh3kff9/Ho0SNxeJzv+zZnyYVmzJptTUSI9DOFYYjRaITd3V3R9mVZYj6fI4oi8b41TWuLgnYidV2Nu/ybZ21frvu2t7eHX//1X8d7770n2r4oCnz66af4+c9/vvGiAJyeI83FmWUZfvrTn+Lv/u7v7P6ZrJ2L+rh58ya+973v4e7du+LjWBSFepWcZRmOj4/xgx/8QNzG8zwMh0NEUSQuClVVYblcoqoq8V2g8ejRI/ExCIIAvu87FQWXMW5orqkwDDEcDm2e0+p4OHtnDpye18lkogpibJrWfnLtarxudRQF8/hI+igoz3McHh7WcsFoz4+52KfTKV68eIGDgwNRO1N4Op0ORqORONStKAp0Oh1VUVgsFjakTqMoCnVRMAmkmnNlfq+E5s6i7sdHWuZxkOY4hGG4tfNCHfhFMxERWSwKRERksSgQEZHFokBERBaLAhERWSwKRERksSgQEZHFokBERBaLAhERWa18o9m8AazJM1on+0gbdVHn25Sat6FdM5ZcmLeG0zQVt8myzL7Jq+0rz3OkaSr+fGZ7bcxFnueqNp7nIc9zG/gnYY5dWZaqvsqyRFVVqjYuwYB1Mtf5No7xbdXKouD7PobD4StpiOcNhNXB3u/3neId1sk+0iSEugzkIAgQxzF6vd65OTBn5XmOJEnWKpBSy+USBwcH+Nd//VdxmzzP8ezZM3FCKvB5TtAnn3wiTgcFPo+E0KTF5nmO6XSq6gcAXr58aSdsKZcxYfKcpJ8pCAJMp9NaEnNdhWGIXq+HPM9F2+d5jl6vx+yjtgmCAOPxGEmSiNuMx+PaBop2dbNOUeh2u+LJJs9zdLvdWorCYrHAw4cP8Vd/9VfiNmVZ4uHDh6pJt6oqzOdz/OIXv8Avf/lLcTtTFFwScDXnqqoqPHnyBLPZTDyxmbjtvb09ccoscHp+5/O5DY97ExMDLt3+KoRhiMFgIB6zeZ5jNBqJc6aaqJVFYXUl7prUuGmuE71rX5e53WUxj3Wk22offay2NVHY0kA882jGpShIC74p1qv9Sfm+b4+JJlRQczey7Y+OVmnHeJsfIfGLZiIislgUiIjIYlEgIiKLRYGIiCwWBSIislgUiIjIYlEgIiKLRYGIiCwWBSIislgUiIjIal3MhYmP6HQ6SJJE/Dp7p9OpNXbCJYJDG7ng+z46nY4qAK3T6dggwU1GHBRFgdlshsePH4vblGWJ+XyuCmjzfR/9fh8ffvgh7ty5s/HYE200RFEU+OlPf4pnz56JM51838dgMMA3v/lN7OzsiPdzuVzi5OREnEzr+z7ef/99jEYj1TFfJx7DZYxHUSTeP3NNtDnmonVFATg98XEco9vtik9+HMe1BMEBbkUBcIvojqJIFYi3ehw2GdhnisJnn30mblNVFWazmWqC8jzPTqAPHjwQ72cYhkiSRBWcZjKMNHHgeZ5juVwiSRJMJhNRmyAIsLu7i9/5nd/BW2+9Jf5M8/kcL168FKfMep6HW7f2ai0Kmx7jQRAgiqLarvVt1Mqi4Hke4jhW3SnUOVBcft8D4H7BSPvJssxuv+kVtSkKT548EW1vPrtLUej1evjwww/xrW99S3ws4jjGaDRSpZCWZWmTSCWqqkKWZfjP//xPLJdLdLtdUbsgCHDv3j385m/+Ju7fvy/+TNPpFPv7B1gsFqLtgc+Pg/mdD1LmfG16RW7GuOb3KWi2b6L2lkOihmrzhEbrY1EgapjrFGlN24dFgYiILBYFogbiIyRyxaJA1DAsCLQOFgUiIrJYFIgahl800zpYFIgaxuVlQSKDRYGIiKzWvtG8+p+aNtuqjscF68Rv5HmONE3FEQpZlqnfTK6qCkEQIAxDcUZOGIYIggB5niPLMlV0R5qmquNuYi60bbTH3Wyn/UxZlqEsS1W+UF1vJp/tb1Ncx3iTtLIoAKevs2tya7TRDnUyF4rLBeP7vrid7/uvRCZI25VliaOjIzx+/FjcZn9/H0dHR+psodFopJp4wzBEv9/Hy5cv8ejRI1VA4suXLxFFkXj/TIRCv98XtzGFLooicV/mmB0cHKjyvbIsx2IxR57Li3EQ+LZw1cF1jEujPoIgaHXuEdDCouB5HsIwxO7uLnZ3d8Xtbty4oZoAVvvTJoq6rFRMro6G7/uqSSPPc/R6Pfi+r1pNzmYz/Nmf/Rn+/M//HGEoG3KmaN+8eVPcj+/7+K3f+i289dZb4om3LEscHBzgL/7iL/Do0SPxeTLZVJrztLu7i+985zv4kz/5E3GbPM8xGo3w1ltvYTAYiNpUVYXFYoE//dM/xXQ6Ffd17949/NqvfYTbt28DADwPOHs4zP/neafHYG9vDzdv7mJ3Nxb3Y74IL4pCdV1oE1KBz1NwNWN8OByKx2kTtfaTm0lHE5S1zmqojpWUyyrK8zzxatysWl0K1uHhIY6OjlQppMPhEF/60pfE/QRBgNFohN3dXQyHQ1GboiiwXC5xdHSEzz77zClUUGqxWGB/f181uZVlCd/3EYahOHyvLEvb19HRkbivTqeD6fQEy+VItL3neVgul6iqUj0mXK8H7fkxdwnS1X9VVVv9VKAOrS0Krs9pm6au56ea7wdW22j3zRR7zUrP/C6KLMu0u6hivieRPm5ZfV7vcp7yPFfdPeZ57vSdhyH9TFdxLTXp+8NNa/fDMyIiegWLAhERWSwKRERksSgQEZHFoiDELBkiagMWBSIislgUhNr+z9SIqB1YFIiIyGJRICIiq9VvNGteZ29qcqL2F7KYLJnxeLzBvTqNXBiNRtjZ2RG3MfvW7XaRJImoTZ7n6Pf7uHnzJm7duqXaR+142Nvbw3g8FkeFmPPS7XZVIXpFUaAoCptRpTm/mrfBTY7YtgfI8W1mnVYWBd/3EccxkiRRpWK6REZv82/B0oaSFUWBJEnwwQcf2GOxGsVwXoSBS/QzcHq8x+Mx7t+/L27j+z6+/vWv49atW+h2u6I2ZVliPB7je9/7Hj788ENxXyYzSvOZhsMhHjx4IM5lMvv3/vvvY2dnRxw7nuc5nj59ih/96Ef/n00kO79VVaHf7+PGjRui7T3Pw2g0cg6P0wZFAu75XtLCxejslhYFz/MQx7EqITSOY1WMs+FaFDQDuSxL1farzKpS2k+328VHH32EO3fuALi4KKz+nbkr0xThnZ0dfPWrXxV/Fs/zcO/ePQwGA3GibVVVmM/nGAwGODk5Efdl4qw1xzyKIoxGI4xGssA5s38PHjzAcrkUn6csy/DJJ58gDEMsFgtxAF9VVRgOh+JkWs/zMB6P10oP1gTVuS6uNP20PQwPaHFRMMmJTQjEq3Nl4/s+er2earULwK6qNUVhPB6rH1P1+30kSaIqCgCws7ODOJbHP/u+j06no1ooBEGAbrcrHndm35IkQRAE4sk9TVO74NE+PjK/80Gz/bZfG3W0aZLtfhhIRES1YlEgIiKLRYGIiCwWBSIislgUiIjIYlEgIiKLRYGIiCwWBSIislgUiIjIauUbzUC9GSeaN0rrfptSGx1gQtC00QbanKAoitRtVrN0NHk/pq2mL21sx9k+tPunYd7W7/V6GAwG4p/R6/Wc3tKmZmllUfB9H0mSoN/viy/qJEnWSoOUXJirk5rLhKPdv7Is7R+pTqeD999/H3fv3lX15TLpxnGMwWCg6gc4jXnIsky8fVEU6PV6qpgL4HRC1Bxzc47m87mqHxOpIVUUBe7evYs/+qM/wtOnT8Xt7t69i2984xu4ffu2uE2n07HHzTXjS6POcMlt3rdNal1RMBemtih0u92NB+KtJopqrHPXU5alOGgNOA0G/MpXvuKcjCllElw1kztwOiGmaaq+O9OMhdV2GlVVoSxLdVHodrvq8L1er4c//uM/VucehWG48eNg2mhSUrV3WOvS7l+TtK4oGOYC27bwK5c7hdU2rv1pBEGgfrSjZe5e8jxXt3VdsW5zZLLLnaP5nRKb/EzrTpraMV43zbFrSgHhF82ktq0TJxGtj0WB1JqyIqLm41jVY1EgFV5kdJ3wrlaPRYFUeJERNRuLAjUK72SI1sOiQI3COxmi9bAoEG05FjqqE4sC0ZbjIzGqE4sCqXHlStRcLAqkxpUrUXO1MubC8zxEUYQ4jsWrXm32zDpMRo6UyQiaTCaqfkzWjTbULcsyVV7S66wWl7PnwRwDbT+uBasoinOPw0URIpJ4kdVtzGfS3mmZY6BtZ5JcNcqyVB1Dk1F10d8bq/uiDRM0+1bHeAiCAP1+H6PRSNXPfD5HlmXXftHEoqAoCnU9NnEpCvP5HAcHB6oB2e12MRqNVDHYVVWpQ+oAnPt5LioK522zSa9LizUhha7Oa6/9eXmeOxVhbT5VVVXI81w19sqyvLDNeYXT8zwMh0OboSXdtzp4nocgCDAYDLCzsyNuZ8ZPURSXsmC6Sq0sCr7vI4oidDod8WqlzjuFsizh+774QsjzHIvFAoeHh6p+iqLAYDBQpb+aFFLNxGHaafm+r06mdQm20xbhdZiUXs3+ua4+TbGX9uVybouiwHK5VLUxqbTmTlVCewezDlMUxuOxaHszfk5OTuD7/rUvCvxOgWpRV4IrEa2HRWEL8V/3ENFVYVEgIiKLRYGIiCwWBSIislgUthC/YCWiq8KisIX4RfMpHgei+rEobKEm3im4fKYmHgeibceiQGLrTNJc9RNdD618o9nY9olKm0HjklvjYtuPW9OY3CTXtpumfSNc87b+VZIe96ZdD60tCr7vq+IGXE+8NnLBDMSiKMTt8jxHlmWYzWbq7CNNQNu6g1/bvs6LbZ2J18Wm+zLHzuWYS9uYYpCmKdI0FfdhoiC2uTCY+UHbpglaWRQ8z0OSJOh2u+ILII5jdQ6PyVjStKuqCovFAnmeiy+a4+Nj/O///i9++MMfqi60r33ta9jb28OdO3fEbYqiEO2bOa5mO+0Foy2m67TzPM+udC87/O4ymMWL9NyaYxCGoXpBApwfXnieqqownU7xX//1X9jf3xf3EwQBkiRBHMfodDridnUJwxDD4RA3b95U5Y89f/68EYWhdUVhNTBtG+8UgM9XYNKL0wSZae8UTJCZ6+R7mdu9rp3r3YU2EG+dPs3PWKe9xCZX/avba49dmqaYz+fiNlEUNe5OYVMLgqvQuqJwVh0nUnM7blaE2u8G3pRrf1E77f4B9R23TRasddtcZvtN/vxNPbYzY1Q7XutKpF2H9jFandfFpl3/e52aNOFkExG9CYsCERFZLApC2/z8k4josrAoEBGRxaJAREQWiwIREVksCkREZLEo0Nbil/tE9WNRoK3Fd0OI6tfKN5pNLkwUReKJR5sjY1a5QRCo3uA0ERd5novfUDaBZFmWqVbXmnyl1f0D9G+lrhM/oe1nm+8w6tq3847z6/pe3db3ffG5NdtOJhMcHByI9y2KIiwWC6c3m13Ga1mW4mtJmzS8qimLmNYWhSiK0Ol01EVBO2A0+UoA7ABO0xR5novamKKwXC5V+6ctIqv76Dphb5pLBs02FxFjnaiKN32+1ewnkwkmYcbqZDJRBeJ1Oh0sl0t1LIsLUxSkx89kjmnGhGscy7ZqZVEAYO8UpBeA9k4B0A8WM6EVRYEsy9RFQRNfDJzeKbiu1rQTaZMumquyblCftA/tIub4+BhHR0fiNqYoaMee62/v0xSFoijUibnXYVGhwe8UiMiJy+KAth+LAhE5adpjEzrFokBERBaLAhERWSwKRERksSgQEZHFokBERBaLAhERWSwKRERksSgQEZHV2pgL1xdvtJkoq7kym+7L9311G1fb/CbrNu/bOlzGnjava51AQUm7bX/ZbTW4UXosmvYSXyuLgu/7ToF4JkdFShMutqooCuR5Ls4+KssSYRhid3cXgHzyGI/H6HQ66v1jvMH1ock+8jwPQRCofrY2KHKdceM68WrbBUGAJEnQ7/fF+1sUhSp1eZu1siispqRqAvEAXWS0WUFoB4qJzs6yTLx9FEXY29tTXXTD4dB+LilTELQXd1mW6sRYcueaYhuGoSol1SUoss4FxerK/2y/5+23Wchpi0Ke507HYhu1sigYmgn7OpxsbQG6Dp+paa7DMV8npnvT1unLJS1W0sYlrn2b8YtmInLWpMmQTrEoCG3zM/R1nvHzoiaiVSwKRERksSi03DbfARFR/VgUiIjIYlFogHVenuF3CkS0ikVBiJMnEbUBi0LL8TsFIlrFotAAjJ0gosvS6jeat5HJVzL5RxImemM1t+aiImFe+a8zdqJpoWHXgUsgnksfZrxq2mzzIsZEXWjSDlwjbbZRa4uC60nc9GCuqgrz+RzHx8eYz+dv3N7zPBueNxqNzv1557UBgF6vp84+uujnvonmeG/zpHEVXCYbTU7Xah+aybCqKiwWC0ynU3E/ZsHjOoa022vjLXzfR6fTQbfbFbcLgoDZR02wrdlHi8UCJycnmM1mb9zWXJhFUWA4HIovNM/z0O12VamY69BOOJfR1zZap5ieXfWbz3n2/zMrcW1RMHcW2qKwXC5FC5jVfuoqCi7tTFFIkkS8j77vOyUib6PWFoVtnjjKsrR/JMwkoJ3ggyDY+O95cLXpxx7X1euSPd+0jYTLYx2XAlQUxbV5hCRR96PYTWtGaatBnQN42y8YouuiKc/568SiIMSBRURtwKJAREQWiwIREVksCkREZLEoEBGRxaJAREQWiwIREVksCkREZLX2jeZNOC+C4LJ+3mVsR3TZY9T8TM0YXN1+G8euS15Sk7S6KKyeTEmuzHn/+6zVZErXwVKWJfI8R5Zlou3NINbGXLi+mt+0i6DJzhuvl5meanKMpGMVOI1XKYrC/pH2s841paWJuTDbN+Xt6VYWBXPCpdk/JttEmke0erFpB3JVVXjx4gWePn2KyWQiatPpdDAYDPDuu++K+wGA4XCoTkldPXar3rQCreuCacJFeZbrsbsok+h1xUIrTVPs7+/js88+E7X3PA9JkmB/fx9HR0fodDqifsyYk26/2p9Lsmocx0iS5Av//+sWjr7vMyX1upOmdta5OjHMCkr6+xR833cK5VonxOt1F8h527mq6/dO1/kIo47j4fpoxuU4mIReTeppGIZ2jGtCH10neC3Tz3l3Cq9b8DQlIRVoYVGo83nhOpPNOrHCdd5iE63znYJWnWOurb/7oznljYiI1ta6OwWipuMdHK2DdwpERGSxKBARkcWiQEREFosCERFZLApEDcPf8U3rYFEgIiKLRYGIiKzWvadgbq3LskRRFOLb7LIs1bflrv9e3PSj6SsMQ/T7fVWfSZLA8zxx1IDZN7oarjEUmxQEAQaDAcbjsbhNkiTwfR9ZlmG5XIramEiWda6p1f9cdd7PPC/f6yJFUTTm/ZDWFQXg86AwM9FLmAKifZ3fZd/Mf0rbe56HMAwxGAzURcHkJkn37TrEE9Rl3Qlqk1wWFi7n14y7GzduiNvEcQzf95HnubgoBEEgDrBcdfYzSVKOq6pCEAQIw1B8PIIgWCtLbJu0sigAn4fOSYOsNAVkXdp+TFHo9/v2f0uYi9OlKDRh8F8Gl0mqrmNXx3gNggD9fl91pxBFkS0KaZqK2oRhiCiK1Plekjv81b8zd82aOwVTRJpyTbT2O4WmnEAiosvU2qLAZ+NERF/U2sdHRE3V5AXPtt7hb+t+uWjtnUKTTiIRXa0mFeLWFgUiIvqi1haFJlV2IrpaTXry0NqiQER0WZq0yGxtUWhSZSeiq9Wk+aS1RYGoqZo0QZ21rQmw27hPrlgUiBqmSRMU1a+17ylcFJB1nWlWiWbbph0DFy7HoM4VuXb/6jynvu+L42IAvBIfoYlYAerLm3K5jpqilUVhNSlVOpjrmjQ8z7MXmbR9EASIosimnkqZwK+iKETbm2MGQJWZRPVaPS+e56mCFbWTYbfbxTvvvIMsy8TtgiBAkiRYLBaYTCaiNp1OR1V4VrkUhE6ng263qw6lbEKBaGVRANxSJNeJ7tUyxUHChHfFcazaPzOx53kubnN2wqHNWyeZdtMBfHEc4+7du6r4dc/zEEURlsuluF2SJEiS5JWfIaW5zs3xCsMQnU5H1QdTUluqjpOuTYJct52G6218Ey6W62jTx91MoFEUObXX/j6TumgWgKaQNGWM84tmIiKyWBSoFvz+geh6YFGgWjTl1pqo6VgUqBa8UyC6HlgUqBa8UyC6HlgUiIjIYlGgWvDxEdH1wKJAteDjI6LrgUWBasE7BaLrgW80K9T1Fq/v+wjD8JXgsIsEQSDe9jx1TNi8U9h+62R1BUFwbgTKeTlMLvEqYRiKM7poPa0uCqsBbxKe56kmX9dX33u9Hkaj0Suhc6s/ZzXPpqoq9Ho99Hq912YlXZSGqvn8Z3+ei02mVdKr6ggt9H0fSZKg3++r2qVpijRNxdv3ej10u111lIu5BrXHQZP8uul8qbq1tiiYkCxtgJx2Re5yd9HtdjEcDsV9JUlyYVEwP/fshbFuKKDms7kUnzo16aI2Vs/VRZlV68RS+76PbrerKgpVVeHo6AiLxUJ8t5BlGUajkVNRcKEJpGxSGB7Q8qKgYS6wOk68KT5hKDs9QRC8MoBft48XTQhSTVsVtcVF52yd82kmT+lYXY2tz/Nc/EioKIqtX1gAzVhctPaL5iacPCKiy9baokBEV6dJUdNNw6IgxH9SSURtwKJARLXjImt7sSgQUe346Gh7sSgIcRATURuwKAjxdpfo8mjfj6H6tLYo8M1aIqIvam1RIKKrw3+Sur1a+0bzRXlA56n7VldzwbhGT6yrjmPickfnsl+ud4519LVuu033o8kJAk4jT7a5INQVfLmtWlsUyrJUDU6XZ6CuqZNRFCGKIsRxLGrT6XQQRZGNGpD26xodsK2Tk2nzuvDAy+5HMxGu05cL18+U57l6TAyHQ3VRmEwmWCwW4piLuifeMAxV0R1hGDamOLSyKFRVhaIoUBSFeDDXmbsShiHiOBZf2HEco9PpIAgC9cAsiqJxX/idF/y3CS4rXtd9qWPCKcsSWZapJnjP83Djxg0Mh0NxmzzP8eTJE5ycnKiSUuti0pClRQGAai7Zdq0sCsB66Yl1cb2NbcqKha4Hl0TR1bbbSvMUoUmaUdqUtnkgEhFdpVYWhaZVdiKiy9LKogCwMBARnae1RYGPkIiIvqiVRYEFgYjofK0sCkREdD4WBSIislgUiIjIYlEgIiKrlUWBWe5EROdrbcxFVVWqPCMToCcN8AI+f/1f86+dyrLEs2fP8PDhQ5ycnIja9Pt9pGmKBw8eqPpyCXUzAXPnFdWLEkpX/07a/qr+ldjr9s383Xn7dTaE76K/13INVnRJOw2CQDUmiqLAYrHAYrEQt8nzHPv7+3jy5Amm06mozXg8xmg0EgUcrjILQOm1vjpGpf2YbZsSB97KoqAdKIB7UdAOlLIs8fjxY/zP//wPjo6ORG3G4zGA0wtUe8GY8K/z/g44f7K7aOJ/HXMcLrqot/mCWt23NxWF8ybkdYtCHcF7vu8jDENVUUjTFLPZTLyAAU6LwrNnz/Do0SNxu/l8jt3dXXEfqzRPBsy2mjwnAOrF3zZrZVEAPj/50sFiCoKmkLgMkqqq8OzZM3z88cfY398Xtbl58yaGw6EtCpp+tYPZFFPXif1NoX1XcWFpJvCzf6f9HOsEMbqskDXMncJ5i4TX9QEAi8VCPLmbeO7nz5/jyZMnOD4+FrXLsgz37t0TbbuO1cWQNuSvKUWhld8pEBHR+VgUGoJfnBPRZWBRaIim3LoS0dViUSAiIotFgYiILBaFhuB3CkR0GVgUGoLfKRDRZWBRICIii0WhIfj4iIguQ2vfaG6ii3KJzrPOI6emFSGXz+OSL+RiNXdJw2XfXNto2q1u24Rx1LRHt60uCtrQK0A3iN8Ukvam/dLumyaCA4B9jd8lV0fb10X9SELOLtrmMtpvu7ona20/mvFQFMUrf6RtXMacob3+TOSHVBAEjSkOrSwKvu+j0+kgjmPxiQzD00OlGVxnA/TelABaVZXNLwrD0Pb5JkEQoKoqTKdTVYhXHMcYDAaI4/jC7VYn1CzLMJvNkGWZuJ9VTblw3uTs5zwvYFBqnYKgaZskCaIoUvVTFAWm0ykmk4ntF7j4c6ZpiqOjIxweHtp2bxJFkThRdZUpWJpAPN/3MR6PxdcfcBrYF8exOnV4G7WyKABuq3HXCe28AXnRIK3rTsFM9tIQO/Pf17lT2NZV+WUXq9d9Tm30M6A/r6YfbVEw/Uj3bzU1V5MefBV3ChomOVhTFEzkeBMWPa0tCkYdJ1E6EVxG3LLLpKOdBGg9LufItZ861PGI6iq09bq4/vc6RER0aVgUiIjIYlHYMk27FSWi64VFgYiILBYFIiKyWBSIiMhiUSAiIotFYcs04eUXIrq+WBSIiMhq5RvNVVUhz3NkWSZemcdx7JxBo4l3MHlJZv8klsslTk5O8OzZM9WdRhzHmE6nb8w+Msxxm0wmSNNU3M9FzoYGnj1Wr8uIuqj9Rdu86e827TKyj1735vvZz6+NhYjjGLPZDJ1OR7xfL1++xMHBAQ4PD8X95HmONE2dYivKsrRZX9J91FqdH6TyPK8thmPTWlcUzAl/8eIF9vf3Va+y37p1y24vmehdcoKKosDJyQlevHghvtBOTk4wm82wXC7F/QCnIWNxHKsyXoqiwGKxUGXd0KnLzM6SttO0jaIISZKo0kHn8zmePHmiCqurqgqPHz9Wj1dz7WrznEzqqbSPsixxfHyMly9fivuZz+dYLpeNKAytKwrAaVU/PDzEcDhUpaQWRfHK4FotEMD5F692kJRliclkgoODA+zv74vbfPzxx/j7v/97p4hgrSYM/LbYZGS7+flZlqnGhO/7uHHjBpIkES9IzALLdUUuHedlWdprUHr9VVWF5XLZmMVSK4sCcLrizbJMPFiKonhjkNlFjykkzMrOpEfmeS5ql+c5FouFeBATXSXP80SR7atW73q0gYLabc01KL3+XO5gthm/aCYiIotFgYiILBYFIiKyWBSIiMhiUSAiIotFgYiILBYFIiKyWBSIiMhiUSAiIquVbzSbfJOyLFVBdVoXxV9ctL2heUOyKW9TEp1n9drYdIiheatZe/015RpsXVEwr7AfHByI0yABYDAYoCgKVXic6UsTuuf7vnpQNmUwUnuY3C1p+F4QBAjD0F4fmsJg2kiYfLOyLFXRFWbbJlyLrSsKAJBlmTonaHd318b2Sq1mtUj5vm8Hl6YoNGEwUnuYgqApCr7v2z+afjRFwexXURSqePg0TRsRhge0tCgAnz9C0qzitUFcZqLW9OGCBYGuI+2Kf3V77SMk7fZtfnTLL5qJiMhiUSAiIotFgYiILBYFIiKyWBSIiMhiUSAiIotFgYiILBYFIiKyWBSIiMhiUSAiIqt1MRcm3mI+n2M2m4nbTadTnJycYDKZqPoqikL1Gvx8PsdiscByuRRnrxRFgTzPxX2ctckIAKLzzOdzAEAURaLtoyjC4eEhnj9//oW8pIviZ3zfRxiG4jFeliWOjo5wdHSEg4MDURvgNPtouVw2Iv+odUUBgC0K0+lU3Ob4+BiTycSpKGhit02xMoVBwiQ6utDmzxCdpV0kVFWF6XSKNE3F4XZBEGB/fx/Pnj1TFYUgCBBFkbifsiyxv79v/0ilaYrFYuEUsb9tWlsUlsslptOpeEKcTqc4Pj7G8fGxqh/NnUJVVVgsFpjP50jTFFmWqfpxoS0K69wlsPg0j+t4mM1mqvEQhiEODw9xcHCgGrNBEKDT6Ygj74uisHcJh4eHojZVVSHPcyyXy0YUBX6nQEREFosCERFZLApERGSxKBARkcWiQEREFosCERFZLApERGSxKBARkdXKl9cA95duXNoxFoLocpi3l13eot7Etk3UyqJg3kDMskz8ZuRyucTx8TFevnyp6kcbc2Eyj7RvQq9jkxeY4XIhU7NpxkNZlsiyDMfHx+o3oTudzheiMS7q5/j4GIvFQpw9BgBZlqlzzuom3bfWFQUzUU8mEyyXS/EA+/TTT/HjH/8YDx8+PPdnAp/HOKwefE38RFVVyLIMjx49wmw2E+cZmZA/Fxflxrxu+zr6oeuhrvGQpimeP3+Of/qnfwIgj0wxgXjaovDxxx/j6OhI1MZcf7PZbKsD8aRzhFdtc2kjIqJa8YtmIiKyWBSIiMhiUSAiIotFgYiILBYFIiKyWBSIiMhiUSAiIotFgYiILBYFIiKy/g8gpqHFy4UtiQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Şəkili açın və göstərmək üçün plt.imshow funksiyasına verin\n",
    "image_path = r\"C:\\Users\\dell\\Desktop\\Python\\week11\\sample_image-300x300.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")  # Oxları gizlətmək üçün\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e1b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
